{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages') \n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install spacy==2.3.0\n",
    "import spacy  \n",
    "# !python -m spacy download en\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "\n",
    "# !python -m spacy download en_core_web_sm   # pretrained models: https://spacy.io/usage/models \n",
    "import en_core_web_sm                        # a small English model trained on written web text (blogs, news, comments)\n",
    "nlp = en_core_web_sm.load()                  # load pretrained models, return an instance of Language with a pipeline set and access to the binary data and language data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/linguistic-features#native-tokenizers\" style=\"text-decoration:none\">Customize tokenization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/processing-pipelines#custom-components-attributes\" style=\"text-decoration:none\">Extensions</a>\n",
    "Extensions are always added globally to Doc, Span, or Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atrribute extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span, Token \n",
    "doc = nlp(\"Lily live in Spain.\")\n",
    "\n",
    "# register new attributes\n",
    "Token.set_extension(\"is_name\", default=False)\n",
    "Span.set_extension(\"has_name\", default=False)\n",
    "Doc.set_extension(\"catgory\", default=None)\n",
    "\n",
    "# access and overwrite\n",
    "doc[0]._.is_name = True\n",
    "doc[0:2]._.has_name = True\n",
    "doc._.catgory = \"blog\"\n",
    "\n",
    "# can also use the built-in set, get and has methods to modify and retrieve the attributes. \n",
    "doc[0]._.set(\"is_name\", True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lily', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Set the is_country attribute to True for the token \"Spain\"\n",
    "doc[3]._.is_country = True\n",
    " \n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property extension\n",
    "- Define a getter and an optional setter function.\n",
    "- Getter and setter functions are called when retrieve the property\n",
    "- Often use _attribute_ to refer to _property_, because they are very similar. <br> \n",
    "  Their main difference is one is commonly shared by tokens/spans/docs and another diverse according to the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue is color:  True\n"
     ]
    }
   ],
   "source": [
    "# Define a getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Alternatively: \n",
    "# colors = [\"red\", \"yellow\", \"blue\"]\n",
    "# get_is_color = lambda token: token.text in colors\n",
    "\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3].text , \"is color: \", doc[3]._.is_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky is blue  has color:  True\n",
      "The sky  has color:  False\n",
      "The sky is blue.  has color:  True\n"
     ]
    }
   ],
   "source": [
    "# Define a getter function\n",
    "def get_has_color(obj):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return any(token.text in colors for token in obj)\n",
    "\n",
    "# Alternatively: \n",
    "# get_has_color = lambda obj: any([t.text in colors for t in obj])\n",
    "\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "Doc.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4].text, \" has color: \", doc[1:4]._.has_color)\n",
    "print(doc[0:2].text, \" has color: \", doc[0:2]._.has_color)\n",
    "print(doc.text, \" has color: \", doc._.has_color )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: ehT\n",
      "reversed: yks\n",
      "reversed: si\n",
      "reversed: eulb\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp( \"David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method extension\n",
    "Can pass argument(s) to the extension function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doc has the token 'blue':  True\n",
      "The doc has the token 'cloud':  False\n"
     ]
    }
   ],
   "source": [
    "# Define a method\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(\"The doc has the token 'blue': \", doc._.has_token(\"blue\"))\n",
    "print(\"The doc has the token 'cloud': \", doc._.has_token(\"cloud\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://course.spacy.io/en/chapter3\" style=\"text-decoration:none\">Customize pipline</a>\n",
    "A component receives a Doc object and modify it, and then return the modified Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fdf7fcba390>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fdf7fcafd08>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fdf7fcafd68>)]\n"
     ]
    }
   ],
   "source": [
    "# Print current processing pipeline components \n",
    "print(nlp.pipeline)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <a href=\"https://spacy.io/usage/processing-pipelines#built-in\" style=\"text-decoration:none\">Built-in pipeline components</a> \n",
    "Only apply the pipeline components we need for efficient processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|COMPONENT STRING NAME | COMPONENT MODULE| DESCRIPTION | \n",
    "|:- |:------------|:------------|  \n",
    "|tagger\t|Tagger\t|Assign part-of-speech-tags.\n",
    "|parser\t|DependencyParser\t|Assign dependency labels.\n",
    "|ner\t|EntityRecognizer\t|Assign named entities.\n",
    "|entity_linker\t|EntityLinker\t|Assign knowledge base IDs to named entities. Should be added after the entity recognizer.\n",
    "|textcat\t|TextCategorizer\t|Assign text categories.\n",
    "|entity_ruler\t|EntityRuler\t|Assign named entities based on pattern rules.\n",
    "|sentencizer\t|Sentencizer\t|Add rule-based sentence segmentation without the dependency parse.\n",
    "|merge_noun_chunks\t|merge_noun_chunks\t|Merge all noun chunks into a single token. Should be added after the tagger and parser.\n",
    "|merge_entities\t|merge_entities\t|Merge all entities into a single token. Should be added after the entity recognizer.\n",
    "|merge_subtokens\t|merge_subtokens\t|Merge subtokens predicted by the parser into single tokens. Should be added after the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fdf7fcba390>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fdf7fcafd08>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fdf7fcafd68>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7fdf7fca3d68>), ('sentencizer', <spacy.pipeline.pipes.Sentencizer object at 0x7fdf7fca3080>)]\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Import and initialize\n",
    "from spacy.pipeline import EntityRuler\n",
    "ruler = EntityRuler(nlp)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "# Option 2: Using nlp.create_pipe\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "\n",
    "print(nlp.pipeline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <a href=\"https://spacy.io/usage/processing-pipelines#custom-components\" style=\"text-decoration:none\">Create components</a> \n",
    "- Add a component to the pipeline using the <a href=\"https://spacy.io/api/language#add_pipe\" style=\"text-decoration:none\">nlp.add_pipe</a> method.\n",
    "- Can add the new component *before* or *after* a specified component, or add it *first* or *last* (default) in the pipeline.\n",
    "- Can also replace the existing component with <a href=\"https://spacy.io/api/language#replace_pipe\" style=\"text-decoration:none\">nlp.replace_pipe</a> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'entity_ruler', 'sentencizer', 'my_component']\n"
     ]
    }
   ],
   "source": [
    "def my_component(doc):\n",
    "   # do something to the doc here\n",
    "   return doc\n",
    "\n",
    "nlp.add_pipe(my_component)                           # add at last (default) in the pipeline\n",
    "# nlp.add_pipe(my_component, first=True)\n",
    "# nlp.add_pipe(my_component, before=\"parser\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <a href=\"https://spacy.io/usage/processing-pipelines#component-example1\" style=\"text-decoration:none\">Customize Sentencizer component</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is. A sentence. |\n",
      "This is. Another sentence.\n"
     ]
    }
   ],
   "source": [
    "def custom_sentencizer(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # Define sentence start if pipe + titlecase token\n",
    "        if token.text == \"|\" and doc[i+1].is_title:\n",
    "            doc[i+1].is_sent_start = True\n",
    "        else: \n",
    "            doc[i+1].is_sent_start = False\n",
    "    return doc\n",
    " \n",
    "nlp.add_pipe(custom_sentencizer, before=\"parser\")  # Insert before the parser\n",
    "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a href=\"https://spacy.io/usage/processing-pipelines#component-example2\" style=\"text-decoration:none\">Customize NER component</a>\n",
    "\n",
    "- To create entity spans from token-based tags: <a href='https://spacy.io/api/goldparse#spans_from_biluo_tags' style='text-decoration:none'>gold.spans_from_biluo_tags</a>\n",
    "- Each token can only be part of one entity, so overlapping entity spans are not allowed.\n",
    "- When adding spans to the *doc.ents*, the *Token.ent_type* and *Token.ent_iob* attributes of their underlying tokens would be set automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example1: animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example2: tech companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ['tagger', 'parser', 'TechCompanyRecognizer']\n",
      "Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
      "Doc has_tech_org True\n",
      "Token 0 is_tech_org True\n",
      "Token 1 is_tech_org False\n",
      "Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: [W013] As of v2.1.0, Span.merge is deprecated. Please use the more efficient and less error-prone Doc.retokenize context manager instead.\n",
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: [W013] As of v2.1.0, Doc.merge is deprecated. Please use the more efficient and less error-prone Doc.retokenize context manager instead.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "class TechCompanyRecognizer(object):\n",
    "    \"\"\"\n",
    "    Example of a spaCy v2.0 pipeline component that sets entity annotations based on list of single or multiple-word company names. \n",
    "    - Companies are labelled as ORG\n",
    "    - company spans are merged into one token. \n",
    "    - ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token respectively.\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, nlp, companies=tuple(), label=\"ORG\"):\n",
    "        \"\"\"Initialise the pipeline component.\"\"\"\n",
    "        \n",
    "        self.label = label               \n",
    "\n",
    "        # Set up the PhraseMatcher \n",
    "        patterns = [nlp(org) for org in companies]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(\"TECH_ORGS\", None, *patterns)\n",
    "\n",
    "        # Register attribute \n",
    "        Token.set_extension(\"is_tech_org\", default=False)\n",
    "\n",
    "        # Register properties on Doc and Span \n",
    "        Doc.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
    "        Span.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object\"\"\"\n",
    "        \n",
    "        # Apply the matcher to the doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        # Create a Span for each match and assign the label  \n",
    "        spans = []        \n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set(\"is_tech_org\", True)\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        \n",
    "        # Merge tokens in each span to one token\n",
    "        for span in spans:\n",
    "            span.merge()      \n",
    "            \n",
    "        return doc   \n",
    "\n",
    "    def has_tech_org(self, tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span properties. \n",
    "        Returns True if one of the tokens is a tech org.  \n",
    "        \"\"\"\n",
    "        return any([t._.get(\"is_tech_org\") for t in tokens])\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "component = TechCompanyRecognizer(nlp, companies = [\"Alphabet Inc.\", \"Google\", \"Netflix\", \"Apple\"])  # initialize \n",
    "nlp.add_pipe(component, last=True)                         # add last to the pipeline\n",
    "\n",
    "with nlp.disable_pipes(\"ner\"):   # otherwise, set conflicting doc.ents. A token can only be part of one entity \n",
    "    doc = nlp(\"Alphabet Inc. is the company behind Google.\") \n",
    "    print(\"Pipeline\", nlp.pipe_names)                          # pipeline contains component name\n",
    "print(\"Tokens\", [t.text for t in doc])                     # company names from the list are merged\n",
    "print(\"Doc has_tech_org\", doc._.has_tech_org)              # Doc contains tech orgs\n",
    "print(\"Token 0 is_tech_org\", doc[0]._.is_tech_org)         # \"Alphabet Inc.\" is a tech org\n",
    "print(\"Token 1 is_tech_org\", doc[1]._.is_tech_org)         # \"is\" is not\n",
    "print(\"Entities\", [(e.text, e.label_) for e in doc.ents])  # all orgs are entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <a href=\"https://spacy.io/usage/processing-pipelines#disabling\" style=\"text-decoration:none\">Disable components</a> \n",
    "- Disabled components would **not be called** for efficient processing\n",
    "- print(nlp.pipeline) still see the disabled components, because they are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable:  \n"
     ]
    }
   ],
   "source": [
    "# option1 \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\"])\n",
    "doc = nlp(\"I won't be tagged\")\n",
    "print(\"disable: \", doc[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option2\n",
    "texts = [\"This is a text\", \"These are lots of texts\", \"...\"]  \n",
    "docs = list(nlp.pipe(texts, disable=[\"parser\"]))\n",
    "docs[0][1].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable:  ()\n",
      "not disable:  (Arizona,)\n"
     ]
    }
   ],
   "source": [
    "# option3: temporarily disable and automatically restore\n",
    "with nlp.disable_pipes(\"ner\"):\n",
    "    doc = nlp(\"I won't recognize the named entity Arizona\")\n",
    "    print(\"disable: \", doc.ents)\n",
    "    \n",
    "doc = nlp(\"I recognize the named entity Arizona\")\n",
    "print(\"not disable: \", doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable:  ()\n",
      "not disable:  (Arizona,)\n"
     ]
    }
   ],
   "source": [
    "# option4: disable and maually restore\n",
    "disabled = nlp.disable_pipes(\"ner\")\n",
    "doc = nlp(\"I won't recognize the named entity Arizona\")\n",
    "print(\"disable: \", doc.ents)\n",
    "\n",
    "disabled.restore()\n",
    "doc = nlp(\"I recognize the named entity Arizona\")\n",
    "print(\"not disable: \", doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/processing-pipelines#custom-components-user-hooks\" style=\"text-decoration:none\">Customize the built-in methods: User hooks</a>\n",
    "- The built-in method will check the user_hooks dict, and delegate to hook function if set one. \n",
    "- The hooks only live on the Doc object.\n",
    "\n",
    "|Hook Name\t|Built-in methods|\n",
    "|:---|:---|\n",
    "|user_hooks|Doc.vector, Doc.has_vector, Doc.vector_norm, Doc.sents|\n",
    "|user_token_hooks\t|Token.similarity, Token.vector, Token.has_vector, Token.vector_norm, Token.conjuncts|\n",
    "|user_span_hooks|Span.similarity, Span.vector, Span.has_vector, Span.vector_norm, Span.root|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityModel(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        doc.user_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_span_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_token_hooks[\"similarity\"] = self.similarity\n",
    "\n",
    "    def similarity(self, obj1, obj2):\n",
    "        y = self._model([obj1.vector, obj2.vector])\n",
    "        return float(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/vectors-similarity#custom\" style=\"text-decoration:none\">Customize word vectors</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/training\" style=\"text-decoration:none\">Train Models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/rule-based-matching#models-rules\" style=\"text-decoration:none\">Combine models and rules</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacy.io/usage/visualizers\" style=\"text-decoration:none\">Customize visulization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/api/top-level#options-dep\" style=\"text-decoration:none\">Dependency Visualizer options</a>\n",
    "\n",
    "|NAME\t|TYPE\t|DESCRIPTION|\tDEFAULT|\n",
    "|:---|:---|:---|:---|\n",
    "|fine_grained\t|bool\t|Use fine-grained part-of-speech tags (Token.tag_) instead of coarse-grained tags (Token.pos_).\t|False\n",
    "|add_lemma|bool\t|Print the lemma’s in a separate row below the token texts.\t|False\n",
    "|collapse_punct\t|bool\t|Attach punctuation to tokens. Can make the parse more readable, as it prevents long arcs to attach punctuation.\t|True\n",
    "|collapse_phrases\t|bool\t|Merge noun phrases into one token.\t|False\n",
    "|compact\t|bool\t|“Compact mode” with square arrows that takes up less space.\t|False\n",
    "|color\t|unicode\t|Text color (HEX, RGB or color names).\t|'#000000'\n",
    "|bg\t|unicode\t|Background color (HEX, RGB or color names).\t|'#ffffff'\n",
    "|font\t|unicode\t|Font name or font family for all text.\t|'Arial'\n",
    "|offset_x\t|int\t|Spacing on left side of the SVG in px.\t|50\n",
    "|arrow_stroke\t|int\t|Width of arrow path in px.\t|2\n",
    "|arrow_width\t|int\t|Width of arrow head in px.\t|10 / 8 (compact)\n",
    "|arrow_spacing\t|int\t|Spacing between arrows in px to avoid overlaps.\t|20 / 12 (compact)\n",
    "|word_spacing\t|int\t|Vertical spacing between words and arcs in px.\t|45\n",
    "|distance\t|int\t|Distance between words in px.\t|175 / 150 (compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\", \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/api/top-level#displacy_options-ent\" style=\"text-decoration:none\">Named Entity Visualizer options</a>\n",
    "\n",
    "|NAME\t|TYPE\t|DESCRIPTION|\tDEFAULT|\n",
    "|:---|:---|:---|:---|\n",
    "|ents\t|list\t|Entity types to highlight\t|None\n",
    "|colors\t|dict\t|Color overrides\t|{}\n",
    "|template |unicode\t|Optional template to overwrite the HTML |see <a href=\"https://github.com/explosion/spaCy/blob/master/spacy/displacy/templates.py\" style=\"text-decoration:none\">templates.py</a>| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"ents\": [\"PERSON\", \"ORG\", \"PRODUCT\"], \"colors\": {\"ORG\": \"yellow\"}}\n",
    "displacy.render(doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/usage/visualizers#manual-usage\" style=\"text-decoration:none\">Visulize user data</a>  \n",
    "- when using style=\"ent, make sure the data in the right order, i.e. starting with the lowest start position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [{\n",
    "       \"text\": \"But Google is starting from behind.\",\n",
    "       \"ents\": [{\"start\": 4, \"end\": 10, \"label\": \"ORG\"}],\n",
    "       \"title\": None\n",
    "     }]\n",
    "displacy.render(ex, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [{\n",
    "        \"words\": [\n",
    "            {\"text\": \"This\", \"tag\": \"DT\"},\n",
    "            {\"text\": \"is\", \"tag\": \"VBZ\"},\n",
    "            {\"text\": \"a\", \"tag\": \"DT\"},\n",
    "            {\"text\": \"sentence\", \"tag\": \"NN\"}\n",
    "        ],\n",
    "        \"arcs\": [\n",
    "            {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"left\"},\n",
    "            {\"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\"},\n",
    "            {\"start\": 1, \"end\": 3, \"label\": \"attr\", \"dir\": \"right\"}\n",
    "        ]\n",
    "     }]\n",
    "displacy.render(ex, style=\"dep\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/usage/visualizers#ent-titles\" style=\"text-decoration:none\">Add title</a> \n",
    "- Add a headline to each visualization\n",
    "- Use for a brief description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence about Google.\")\n",
    "doc.user_data[\"title\"] = \"This is a title\"\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/usage/visualizers#examples-export-svg\" style=\"text-decoration:none\">Export SVG image</a> \n",
    "- SVG (Scalable Vector Graphics) image format uses XML markup.\n",
    "- SVG can be embedded online in an \\<img\\> tag, or inlined in an HTML document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "svg = displacy.render(doc, style=\"dep\")\n",
    "output_path = Path(\"/images/sentence.svg\")   # or file_name = '-'.join([w.text for w in doc if not w.is_punct]) + \".svg\"\n",
    "output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"https://spacy.io/usage/visualizers#webapp\" style=\"text-decoration:none\">Embed into a webpage</a>  \n",
    "- <a href=\"https://github.com/explosion/displacy\" style=\"text-decoration:none\">displaCy.js</a>  \n",
    "- <a href=\"https://github.com/kengz/spacy-nlp\" style=\"text-decoration:none\">spacy-nlp</a>: Expose Spacy nlp text parsing to Nodejs via socketIO\n",
    "- example: <a href=\"https://explosion.ai/demos/displacy\" style=\"text-decoration:none\">online demo</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
