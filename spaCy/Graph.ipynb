{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='https://github.com/bdmarius/python-knowledge-graph' style='text-decoration:none'>python-knowledge-graph</a> \n",
    "- <a href='https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/' style='text-decoration:none'>Knowledge Graph â€“ A Powerful Data Science Technique to Mine Information from Text</a> \n",
    "- <a href='https://medium.com/analytics-vidhya/knowledge-graph-creation-part-ii-675fa480773a' style='text-decoration:none'>Knowledge Graph Creation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy  \n",
    "import en_core_web_sm                        # a small English model trained on written web text \n",
    "nlp = en_core_web_sm.load()                  # load pretrained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"The Empire of Japan aimed to dominate Asia and the Pacific and was \"\n",
    "    \"already at war with the Republic of China in 1937, but the world war is \"\n",
    "    \"generally said to have begun on 1 September 1939 with the invasion of \"\n",
    "    \"Poland by Germany and subsequent declarations of war on Germany by \"\n",
    "    \"France and the United Kingdom. From late 1939 to early 1941, in a \"\n",
    "    \"series of campaigns and treaties, Germany conquered or controlled much \"\n",
    "    \"of continental Europe, and formed the Axis alliance with Italy and \"\n",
    "    \"Japan. Under the Molotov-Ribbentrop Pact of August 1939, Germany and the \"\n",
    "    \"Soviet Union partitioned and annexed territories of their European \"\n",
    "    \"neighbours, Poland, Finland, Romania and the Baltic states. The war \"\n",
    "    \"continued primarily between the European Axis powers and the coalition \"\n",
    "    \"of the United Kingdom and the British Commonwealth, with campaigns \"\n",
    "    \"including the North Africa and East Africa campaigns, the aerial Battle \"\n",
    "    \"of Britain, the Blitz bombing campaign, the Balkan Campaign as well as \"\n",
    "    \"the long-running Battle of the Atlantic. In June 1941, the European Axis \"\n",
    "    \"powers launched an invasion of the Soviet Union, opening the largest \"\n",
    "    \"land theatre of war in history, which trapped the major part of the \"\n",
    "    \"Axis' military forces into a war of attrition. In December 1941, Japan \"\n",
    "    \"attacked the United States and European territories in the Pacific \"\n",
    "    \"Ocean, and quickly conquered much of the Western Pacific.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://github.com/krzysiekfonal/textpipeliner' style='text-decoration:none'>textpipeliner</a>\n",
    "- Extracting parts of sentences in the form of structured tuples from unstructured text.\n",
    "- This lib provides _Pipes_ and _PipelineEngine_. \n",
    "    - pipes: extract parts from every sentence.  \n",
    "        - AggregatePipe: This pipe gets a list of other pipes and collects results from them.\n",
    "        - SequencePipe: This pipe gets a list of other pipes and processes them in sequence, passing tokens as an argument to next one.\n",
    "        - AnyPipe: This pipe gets list of another pipes and processes them until one returns a non-empty result.\n",
    "        - GenericPipe: This pipe takes a function as a argument. This function will be called with 2 arguments: context and tokens list.\n",
    "        - FindTokensPipe: This pipe takes a regex-like pattern to extract using the grammaregex library.\n",
    "        - NamedEntityFilterPipe: This pipe filters passed tokens choosing the ones which are part of a named entity. During creation of this pipe it is possible to pass a specific named entity type we want to filter (like PERSON, LOC...).\n",
    "        - NamedEntityExtractorPipe: This pipe collects a whole chain from a single token which is part of an entity.\n",
    "    - PipelineEngine: use this pipes structure and return list of extracted tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([Germany], [conquered], [Europe]),\n",
       " ([Japan], [attacked], [the, United, States])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install textpipeliner\n",
    "import sys\n",
    "sys.path.insert(-1, '/home/u32/fanluo/.local/lib/python3.5/site-packages') \n",
    "from textpipeliner import PipelineEngine, Context\n",
    "from textpipeliner.pipes import *\n",
    "pipes_structure = [\n",
    "    SequencePipe([\n",
    "        FindTokensPipe(\"VERB/nsubj/*\"),\n",
    "        NamedEntityFilterPipe(),\n",
    "        NamedEntityExtractorPipe()\n",
    "    ]),\n",
    "    FindTokensPipe(\"VERB\"),\n",
    "    AnyPipe([\n",
    "        SequencePipe([\n",
    "            FindTokensPipe(\"VBD/dobj/NNP\"),\n",
    "            AggregatePipe([\n",
    "                NamedEntityFilterPipe(\"GPE\"),\n",
    "                NamedEntityFilterPipe(\"PERSON\")\n",
    "            ]),\n",
    "            NamedEntityExtractorPipe()\n",
    "        ]),\n",
    "        SequencePipe([\n",
    "            FindTokensPipe(\"VBD/**/*/pobj/NNP\"),\n",
    "            AggregatePipe([\n",
    "                NamedEntityFilterPipe(\"LOC\"),\n",
    "                NamedEntityFilterPipe(\"PERSON\")\n",
    "            ]),\n",
    "            NamedEntityExtractorPipe()\n",
    "        ])\n",
    "    ])\n",
    "]\n",
    "\n",
    "engine = PipelineEngine(pipes_structure, Context(doc), [0, 1, 2])\n",
    "engine.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\" style=\"text-decoration:none\">TextRank</a>\n",
    "<a href=\"https://github.com/DerwenAI/pytextrank\" style='text-decoration:none'>PyTextRank</a>: spaCy pipeline extension\n",
    "-  <a href=\"https://colab.research.google.com/github/DerwenAI/pytextrank/blob/master/explain_algo.ipynb\" style='text-decoration:none'>Explain PyTextRank: the algorithm</a>\n",
    "    1. Create a lemma_graph:\n",
    "        - node: lemmas of tokens whose pos is in the POS_KEPT list (merge node when there are multiple occurences with same pos.)\n",
    "        - link: between each kept lemma and 3 kept lemmas before it\n",
    "    2. Run page rank algorithm on lemma_graph to rank nodes.\n",
    "    3. For each phrase (noun_chunk or named entity), get its score from all the scores of nodes it contains.  \n",
    "        - merge phrases that contain same nodes\n",
    "        - if a pharse is both noun_chunk and named entity, it would be count twice.\n",
    "- <a href=\"https://colab.research.google.com/github/DerwenAI/pytextrank/blob/master/explain_summ.ipynb\" style='text-decoration:none'>Explain PyTextRank: extractive summarization</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install pytextrank\n",
    "# Fan: changed pytextrank.py so that p.text are the joint of lemma tokens with pos_ in kept_pos, and maintain the order when join \n",
    "import pytextrank\n",
    "tr = pytextrank.TextRank()     \n",
    "nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2105     1  First   [First]\n",
      "0.1579     1  Arthur Magazine   [Arthur's Magazine]\n",
      "0.1562     1  woman   [Women]\n",
      "0.0828     1  magazine   [Which magazine]\n",
      "0.0793     1  Arthur   [first Arthur's]\n"
     ]
    }
   ],
   "source": [
    "text = \"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases:\n",
    "    print('{:.4f} {:5d}  {}   {}'.format(p.rank, p.count, p.text, p.chunks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which DET\n",
      "magazine NOUN\n",
      "was AUX\n",
      "started VERB\n",
      "first ADV\n",
      "Arthur PROPN\n",
      "'s PART\n",
      "Magazine PROPN\n",
      "or CCONJ\n",
      "First PROPN\n",
      "for ADP\n",
      "Women NOUN\n",
      "? PUNCT\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t.text, t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pronoun'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PRON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
