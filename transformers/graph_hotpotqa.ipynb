{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   \n",
    "from simple_colors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t> , '  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = ', </t> . '   # indicating the end of the title of a paragraph, add , to avoid tilte to be recognized as part of the first entity in the sentence after\n",
    "SENT_MARKER_END = ', </sent> , '  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which: no neuralcoref in (/opt/ohpc/pub/apps/R/4.0.0/bin:/opt/ohpc/pub/apps/python/3.6.5/bin:/xdisk/msurdeanu/fanluo/miniconda3/bin:/xdisk/msurdeanu/fanluo/miniconda3/condabin:/opt/TurboVNC/bin:/opt/ohpc/pub/mpi/openmpi3-gnu8/3.1.4/bin:/opt/ohpc/pub/compiler/gcc/8.3.0/bin:/opt/ohpc/pub/utils/prun/1.3:/opt/ohpc/pub/utils/autotools/bin:/opt/ohpc/pub/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u32/fanluo/.local/bin:/home/u32/fanluo/bin)\r\n"
     ]
    }
   ],
   "source": [
    "!which neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8bd1ee99c826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnlp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</t>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'</t>'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnlp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<t>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'<t>'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedyness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# between 0 and 1. The default value is 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '/home/u32/fanluo/.local/lib/python3.7/site-packages')\n",
    "sys.path.insert(0, '/xdisk/msurdeanu/fanluo/miniconda3/envs/spacy3.0/lib/python3.9/site-packages')\n",
    " \n",
    "from prettytable import PrettyTable\n",
    "import ujson \n",
    "import numpy\n",
    "import spacy    \n",
    "print(spacy.__version__)\n",
    "import en_core_web_lg          \n",
    "nlp1 = en_core_web_lg.load() \n",
    "nlp2 = en_core_web_lg.load() \n",
    "\n",
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "nlp1.tokenizer.add_special_case('</sent>', [{ ORTH: '</sent>'}]) \n",
    "nlp1.tokenizer.add_special_case('</t>', [{ORTH: '</t>'}]) \n",
    "nlp1.tokenizer.add_special_case('<t>', [{ORTH: '<t>'}])  \n",
    "import neuralcoref \n",
    "neuralcoref.add_to_pipe(nlp1, greedyness=0.55) # between 0 and 1. The default value is 0.5.\n",
    "\n",
    "\n",
    "\n",
    "# Fan: make 3 changes in pytextrank.py ( '/xdisk/msurdeanu/fanluo/miniconda3/envs/spacy/lib/python3.8/site-packages/pytextrank')\n",
    "# 1. phrase_text = ' '.join(key[0] for key in phrase_key) \n",
    "#  p.text are the joint of lemma tokens with pos_ in kept_pos, and maintain the order when join    \n",
    "# 2. add argumrnt 'chunk_type' to only consider named entity ('ner') or noun_chunks ('noun'), besides the default ('both') \n",
    "# 3. replace token.lemma_ with token.lemma_.lower().strip()\n",
    "import pytextrank\n",
    "tr = pytextrank.TextRank(pos_kept=[\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\", \"NUM\", \"ADV\"], chunk_type='both')  \n",
    "nlp2.add_pipe(tr.PipelineComponent, name='textrank', last=True)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "#!conda install networkx --yes\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import itertools \n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create phrases graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_para_graph(paras_phrases):\n",
    "    G = nx.Graph()     \n",
    "    for pi, para_phrases in enumerate(paras_phrases):        # each para \n",
    "        for si, sent_phrases in enumerate(para_phrases):     # each sent\n",
    "            \n",
    "            # complete graph for each sent\n",
    "            G.add_nodes_from([(phrase[0], {\"score\": phrase[1]}) for phrase in sent_phrases])  \n",
    "            for node1, node2 in itertools.combinations([phrase[0] for phrase in sent_phrases], 2):\n",
    "                if(G.has_edge(node1, node2)):\n",
    "                    G[node1][node2]['src'].append((pi, si))\n",
    "                else:\n",
    "                    G.add_edge(node1, node2, src = [(pi, si)])\n",
    "                                               \n",
    "                                                \n",
    "            # add edge between title phrases and first phrase of the sentence\n",
    "            # si = 0, sent_phrases = para_phrases[0] are phrases from title \n",
    "            for phrase in para_phrases[0]:\n",
    "                if(len(sent_phrases) > 0 and sent_phrases[0] != phrase):\n",
    "                    if(G.has_edge(sent_phrases[0], phrase)):\n",
    "                        G[sent_phrases[0]][phrase]['src'].append((pi, 'title', si))\n",
    "                    else:\n",
    "                        G.add_edge(sent_phrases[0], phrase, src = [(pi, 'title', si)]) \n",
    "     \n",
    " \n",
    "  #  Draw\n",
    "    print(blue(\"Paragraph phrase graph\" , ['bold']))\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    nx.draw(G, pos, with_labels=True, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='orange', alpha=0.9                           \n",
    "            )     \n",
    "    \n",
    "    \n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_graph(H, nodes, color_nodes):\n",
    "    \n",
    "    G = H.subgraph(nodes)  # Returns a subgraph containing nodes in nbunch\n",
    "    print(black(\"nodes to draw\", ['bold']), G.nodes())\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    color_map = dict.fromkeys(color_nodes, \"grey\")   \n",
    "    values = [color_map.get(node, \"yellow\" ) for node in G.nodes()]  # beside color_nodes are \"grey\", other nodes are \"yellow\"\n",
    "    pos = nx.random_layout(G)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'source')\n",
    "    nx.draw(G, pos, node_color=values, with_labels=True, node_size=800, font_size=20, font_color='blue')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=15) \n",
    "    plt.show() \n",
    "     \n",
    "    for n, nbrs in G.adj.items():  # each node and its neighbors   \n",
    "        for nbr, eattr in nbrs.items(): # each neighbor and edge attr\n",
    "            print(f\"({n}, {nbr}, {eattr})\")\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the reduced context with phrase graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the reduced context with phrase graph\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from networkx.algorithms import approximation as approx\n",
    "def reduce_context_with_phares_graph(example, q_id):\n",
    "    \"\"\"function to compute reduced context with phrase graph.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "    Returns:\n",
    "        a new file save additional phrase-related info and the reduced context\n",
    "\n",
    "    \"\"\"\n",
    "#     noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "#     new_dict = {\"data\": []} \n",
    "#     data = []\n",
    "#     common_phrases_num_le2 = 0\n",
    "#     extended = 0\n",
    "#     answer_in_reduced_context = 0\n",
    "#     answer_in_context = 0\n",
    "#     reduced_context_ratios = []\n",
    "    \n",
    "#     question = \"Who was the writer of 'These Boots Are Made for Walkin' and who died in 2007?\"\n",
    "#     question = example[\"question\"]\n",
    "#     question_doc = nlp2(question)\n",
    "    \n",
    "#     question_phrases_text = set([_normalize_text(entity.text) for entity in question_doc.ents])\n",
    "#     for chunk in question_doc.noun_chunks:\n",
    "#         question_phrases_text.add(_normalize_text(chunk.text))\n",
    "#     question_phrases_text = list(question_phrases_text)\n",
    "    \n",
    "    question = _normalize_text(example[\"question\"])\n",
    "    print(\"question: \", question)\n",
    "    question_doc = nlp2(question)\n",
    "    question_phrases = [(remove_punc(p.text.lower()), p.rank) for p in question_doc._.phrases if(p.text != '')] \n",
    "    question_phrases_text = [p[0] for p in question_phrases] \n",
    "             \n",
    "    print(black(\"Original question: \", 'bold'), example[\"question\"])\n",
    "    print(black(\"question: \", 'bold'), question)\n",
    "    print(black(\"question_phrases_text: \", 'bold'), question_phrases_text)\n",
    "#     print(black(\"question_phrases: \", 'bold'), question_phrases)\n",
    "    answer = remove_punc(lower(_normalize_text(example[\"answer\"])))  \n",
    "    print(black('answer: ', 'bold'), answer)    \n",
    "\n",
    "    raw_contexts = example[\"context\"]\n",
    "#         if gold_paras_only: \n",
    "#        raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]    \n",
    "    paras_phrases = []                                                # phrases of all 10 paragraghs\n",
    "    titles = []\n",
    "    for i, para_context in enumerate(raw_contexts):                   # each para\n",
    "#         print(black('title: ', 'bold'), para_context[0]) \n",
    "#         print(black(\"original sents: \" , 'bold'), para_context[1])\n",
    "        title = _normalize_text(para_context[0])   \n",
    "        titles.append(title)\n",
    "        sents = [ _normalize_text(sent) for sent in para_context[1]]\n",
    "        num_sents_before_coref_resolved = len(sents)\n",
    "#         print(\"numbe of sents before coref: \", num_sents_before_coref_resolved)\n",
    "        sents_joint =  (' ' + SENT_MARKER_END +' ').join(sents)\n",
    "        \n",
    "#         print(black(\"normalized sents: \" , 'bold'), sents_joint)\n",
    "        sents_doc = nlp1(sents_joint)\n",
    "#         print(black(\"resolved_sents: \", 'bold'), sents_doc._.coref_resolved ) \n",
    "        sents_coref_resolved = sents_doc._.coref_resolved.split(SENT_MARKER_END)\n",
    "        num_sents_after_coref_resolved = len(sents_coref_resolved)\n",
    "        \n",
    "#         print(\"numbe of sents after coref: \", num_sents_after_coref_resolved)\n",
    "        \n",
    " \n",
    "        if(num_sents_before_coref_resolved == num_sents_after_coref_resolved):\n",
    "            sent_docs = list(nlp2.pipe([title] + sents_coref_resolved))       \n",
    "        else:\n",
    "            sent_docs = list(nlp2.pipe([title] + sents))\n",
    " \n",
    "        para_phrases = []                                        \n",
    "        for sent_doc in sent_docs:                                      # each sent in a para \n",
    "            sent_phrases = [(remove_punc(p.text.lower()), p.rank) for p in sent_doc._.phrases if(p.text != '')]  # phrases from each sentence \n",
    "            para_phrases.append(sent_phrases)                                # para_phrases[0] are phrases from  title\n",
    "        paras_phrases.append(para_phrases)     \n",
    "    print(black(\"paras_phrases\", 'bold'))\n",
    "    for paras_phrase in paras_phrases: \n",
    "        print([list(flatten(s))[::2] for s in paras_phrase])\n",
    "        \n",
    "#     contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_contexts]  \n",
    "#     context = \" \".join(contexts)                                                     \n",
    "#     if (answer != '' and len(list(re.finditer(answer, context, re.IGNORECASE))) > 0):\n",
    "#         answer_in_context += 1\n",
    "#     all_sent_phrases_text =  list(flatten(paras_phrases))[::2]        # every other element is text, others are rank \n",
    "    \n",
    "    RG = create_relevant_graph(paras_phrases, question_phrases_text)\n",
    "    RG, common_phrases, mapping = find_common_mapping(RG, question_phrases_text)# mapping matchs paras_phrases with question_phrases\n",
    "    RG, dup_sets = dedup_nodes_in_graph(RG, question_phrases_text)              # dedup paras_phrases in RG for finding meanningful path\n",
    "#     print(black(\"nodes in RG: \", 'bold'), RG.nodes())\n",
    "    \n",
    "    question_only_phrase = list(set(question_phrases_text).difference(common_phrases)) \n",
    "    if(len(common_phrases) > 1): \n",
    "        path_phrases = list(approx.steinertree.steiner_tree(RG, common_phrases).nodes)  # to find the shortest path cover all common_phrases  \n",
    "        extended_phrases = path_phrases + question_only_phrase  \n",
    "    else: #  0 or 1 common phrases\n",
    "        path_phrases = list(common_phrases)             \n",
    "        extended_phrases = question_phrases_text\n",
    "\n",
    "    \n",
    "    print(black(\"common_phrases: \", 'bold'), common_phrases)\n",
    "    print(black(\"path_phrases: \", 'bold'), path_phrases)     \n",
    "    print(black(\"question_only_phrase: \", 'bold'), question_only_phrase)\n",
    "    print(black(\"extended_phrases before expand\", 'bold'), extended_phrases)\n",
    "    \n",
    "    # expand to include merged nodes, that is, also include phrases that from the same dup_set \n",
    "    extended_phrases_merged = set()\n",
    "    for phrase in extended_phrases:\n",
    "        idx_phrase = [idx for idx, dup_set in enumerate(dup_sets) if(phrase in dup_set)]   # the set where phrase in\n",
    "        if(len(idx_phrase) > 0):\n",
    "            extended_phrases_merged = extended_phrases_merged | dup_sets[idx_phrase[0]]\n",
    "            extended_phrases_merged.remove(phrase)\n",
    "    extended_phrases.extend(list(extended_phrases_merged))\n",
    "    print(black(\"extended_phrases: \", 'bold'), extended_phrases)        \n",
    "    print(black(\"introduced_phrases: \", 'bold'), set(extended_phrases) - set(question_phrases_text))\n",
    "        \n",
    "    vis_graph(RG, common_phrases, question_phrases_text) \n",
    "#     print(blue(\"Min Subgraph covers all common entities\"))\n",
    "    vis_graph(RG, path_phrases, question_phrases_text) \n",
    "        \n",
    "#     print(black(\"phrases in relevant components\", 'bold'))    \n",
    "#     print(RG.nodes.data())\n",
    "        \n",
    "        \n",
    "    raw_reduced_contexts, kept_para_sent = construct_reduced_context(raw_contexts, paras_phrases, extended_phrases, mapping)     \n",
    "    reduced_contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_reduced_contexts]    \n",
    "    reduced_context_str = remove_punc(lower(_normalize_text(\" \".join(reduced_contexts)) )) \n",
    "    if (answer != '' and answer != 'yes'  and  answer != 'no' and len(list(re.finditer(answer, reduced_context_str, re.IGNORECASE))) > 0):\n",
    "        print(black(\"answer in reduced_context\", 'bold')) \n",
    "    elif(answer == 'yes' or answer == 'no'):\n",
    "        pass\n",
    "    else:\n",
    "        print(black(\"answer not in reduced_context\", 'bold')) \n",
    "        print(black(\"reduced_contexts\", 'bold'), raw_reduced_contexts)\n",
    "#     print(black('titles: ', 'bold'), titles) \n",
    "    \n",
    "    reduced_supporting_facts, reduced_supporting_facts_in_original_id = construct_reduced_supporting_facts(example[\"supporting_facts\"], raw_reduced_contexts, kept_para_sent) \n",
    "    for para_title, sent_id in example[\"supporting_facts\"]:\n",
    "        print(raw_contexts[titles.index(para_title)][1][sent_id])\n",
    "    print(black('reduced_supporting_facts_in_original_id: ', 'bold'), reduced_supporting_facts_in_original_id) \n",
    "#     print(black('reduced supportiing facts: ', 'bold'), reduced_supporting_facts) \n",
    "    for para_title, sent_id in reduced_supporting_facts_in_original_id:\n",
    "        print(raw_contexts[titles.index(para_title)][1][sent_id])\n",
    "    \n",
    "    example[\"question_phrases_text\"] = question_phrases_text\n",
    "    example[\"question_only_phrase\"] = question_only_phrase\n",
    "    P = RG.subgraph(path_phrases)  # to save the graph view that contains      \n",
    "    path_data = json_graph.node_link_data(P)\n",
    "    example[\"path\"] = path_data\n",
    "    example[\"question_phrases\"] = question_phrases\n",
    "    example[\"paras_phrases\"] = paras_phrases\n",
    "#     example[\"all_sent_phrases_text\"] = all_sent_phrases_text\n",
    "    example[\"common_phrases\"] = list(common_phrases)\n",
    "    example[\"path_phrases\"] = path_phrases\n",
    "    example[\"extended_phrases\"] = extended_phrases                     \n",
    "    example['context'] = raw_reduced_contexts\n",
    "    example['supporting_facts'] = reduced_supporting_facts\n",
    "    example['kept_para_sent'] = kept_para_sent\n",
    "\n",
    "    print(black(\"context: \", 'bold'), raw_contexts)   \n",
    "    print(black(\"reduced_context: \", 'bold'), raw_reduced_contexts)   \n",
    "    \n",
    "#     data.append(example)  \n",
    "#     with open(\"/home/u32/fanluo/Jupyter/experiments/transformers/outfile.json\", 'w') as out_file:\n",
    "#         json.dump(data, out_file)\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevant_graph(paras_phrases, question_phrases_text):\n",
    "    G = create_para_graph(paras_phrases)\n",
    "    Subgraphs = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    RG = nx.Graph()    # relevant components  \n",
    "    represnetive_nodes = []  # more likely to include the represnetive_nodes in the final path       \n",
    "    for S in Subgraphs:\n",
    "        for phrase in question_phrases_text:\n",
    "            if S.has_node(phrase):\n",
    "                RG = nx.compose(RG, S)  # joint the relevant components\n",
    "                represnetive_node = sorted(S.nodes.data('score'), key=lambda x: x[1], reverse=True)[0]  # node with highest score\n",
    "                represnetive_nodes.append(represnetive_node) \n",
    "                break\n",
    "    \n",
    "    for node1, node2 in itertools.combinations([phrase[0] for phrase in represnetive_nodes], 2):  \n",
    "        RG.add_edge(node1, node2, source = 'components')\n",
    "\n",
    " \n",
    "    #  Draw Relevant components graph RG\n",
    "#     print(blue('Relevant components graph'))\n",
    "    pos = nx.spring_layout(RG)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    nx.draw(RG, pos, with_labels=True, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='orange', alpha=0.9                           \n",
    "            )     \n",
    "#     print(black(\"nodes in joined components: \", 'bold'), RG.nodes())   \n",
    "    return RG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_mapping(G, question_phrases_text):\n",
    "    # fuzzy macth for common phrases: map pharse similar to question phrases to question phrase, then find common phrases\n",
    "    common_phrases = set()\n",
    "    mapping = {}\n",
    "    for phrase in G.nodes:\n",
    "        if(phrase in question_phrases_text):    # has a exact match\n",
    "            common_phrases.add(phrase)\n",
    "            continue\n",
    "            \n",
    "        # check partial match\n",
    "        inclusion_similar_phrase = inclusion_best_match(phrase, question_phrases_text)\n",
    "        if(inclusion_similar_phrase): \n",
    "            mapping[phrase] = inclusion_similar_phrase   \n",
    "            common_phrases.add(inclusion_similar_phrase)\n",
    "    \n",
    "    G = nx.relabel_nodes(G, mapping)      # match 'english government position' with 'government position'\n",
    "    print(black(\"node mapping for RG: \", 'bold'), mapping)\n",
    "    \n",
    "    return G, common_phrases, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inclusion_best_match(query, choices):\n",
    "    if (utils.full_process(query) and choices != []):  # only exectute when query is valid. To avoid WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0.\n",
    "        inclusion_phrases = [simi_phrase for (simi_phrase, similarity) in process.extractBests(query, choices, scorer=fuzz.token_set_ratio) if similarity ==100]  # match '1977 film' and '1977', but will not match substring 'woman' and 'businesswoman', avid nosiy such as 'music' and 'us'\n",
    "        if(inclusion_phrases!= []):\n",
    "            simi_phrase, similarity = process.extractOne(query, inclusion_phrases, scorer=fuzz.ratio) # most similar   \n",
    "            if(similarity >= 50):    \n",
    "                return simi_phrase\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def dedup_nodes_in_graph(G, grounded):\n",
    "    def find_inclusion_duplicates(contains_dupes): \n",
    "        dup_sets = []\n",
    "        for phrase in contains_dupes:\n",
    "            rest_phrases = [p for p in contains_dupes if p != phrase] \n",
    "            inclusion_similar_phrase = inclusion_best_match(phrase, rest_phrases)\n",
    "            if(inclusion_similar_phrase): \n",
    "                idx_phrase = [idx for idx, set in enumerate(dup_sets) if(phrase in set)]   # the set where phrase already in\n",
    "                idx_inclusion_similar_phrase = [idx for idx, set in enumerate(dup_sets) if(inclusion_similar_phrase in set)]\n",
    "                if(len(idx_phrase) > 0 and len(idx_inclusion_similar_phrase) == 0):\n",
    "                    dup_sets[idx_phrase[0]].add(inclusion_similar_phrase)\n",
    "                elif(len(idx_inclusion_similar_phrase) > 0 and len(idx_phrase) == 0):\n",
    "                    dup_sets[idx_inclusion_similar_phrase[0]].add(phrase)\n",
    "                elif(len(idx_inclusion_similar_phrase) > 0 and len(idx_phrase) > 0):\n",
    "                    dup_sets[idx_phrase[0]] = dup_sets[idx_phrase[0]] | dup_sets[idx_inclusion_similar_phrase[0]]\n",
    "                    dup_sets.pop(idx_inclusion_similar_phrase[0])\n",
    "                elif(len(idx_inclusion_similar_phrase) == 0 and len(idx_phrase) == 0):\n",
    "                    dup_sets.append(set([phrase, inclusion_similar_phrase]))\n",
    "                else:\n",
    "                    print(\"len(idx_inclusion_similar_phrase), len(idx_phrase)\")\n",
    "                    print(len(idx_inclusion_similar_phrase), len(idx_phrase) )\n",
    "                    \n",
    "        # dup_sets would looks like:  [{'1977 film', '1977', 'film'},  {'anime', 'japanese anime'}]\n",
    "\n",
    "        print(black(\"dup_sets: \", 'bold'), dup_sets)\n",
    "        \n",
    "        return dup_sets\n",
    "\n",
    "    def merge_dup_nodes(G, dup_sets, grounded):\n",
    "\n",
    "        for node_sets in dup_sets:     \n",
    "            assert len(node_sets) >= 2\n",
    "            # for each set, decide which one to be merged to\n",
    "            # longest phrase that is same as a question phrase \n",
    "            merged_node = sorted([p for p in node_sets if p in grounded], key=lambda x: len(x), reverse=True)\n",
    "            if(len(merged_node) == 0):   \n",
    "                merged_node = sorted(node_sets, key=lambda x: len(x), reverse=True) # longest node\n",
    "                        \n",
    "            # merged_node[0] is the node to be merged to for current node_set\n",
    "            for n in node_sets:\n",
    "                if(n != merged_node[0]):\n",
    "                    G = nx.contracted_nodes(G, merged_node[0], n)   # merge node\n",
    "                        \n",
    "        return G\n",
    "    \n",
    "    dup_sets = find_inclusion_duplicates(G.nodes)\n",
    "    return merge_dup_nodes(G, dup_sets, grounded), dup_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_reduced_context(raw_contexts, paras_phrases, extended_phrases, mapping):    \n",
    "    raw_reduced_contexts = []     # sentences contain one of the extended_phrases\n",
    "    number_sentences = 0\n",
    "    number_reduced_sentences = 0 \n",
    "    kept_para_sent = []\n",
    "    for para_id, (para_title, para_lines) in enumerate(raw_contexts):\n",
    "# #             print(\"para_id, para_title, para_lines\",para_id, para_title, para_lines)\n",
    "\n",
    "        number_sentences += len(para_lines)\n",
    "        reduced_para = []\n",
    "        kept_sent = []\n",
    "        for sent_id, sent in enumerate(para_lines):\n",
    "            sentence_phrases = list(flatten(paras_phrases[para_id][sent_id+1]))[::2]  # paras_phrases[para_id][0] are phrases from the title, every other element is text, others are rank  \n",
    "\n",
    "#                 print(\"sentence_phrases: \", sentence_phrases)\n",
    "#                 print('[sentence_phrase in mapping for sentence_phrase in sentence_phrases]: ', [sentence_phrase in mapping for sentence_phrase in sentence_phrases])\n",
    "            if(any([sentence_phrase in mapping for sentence_phrase in sentence_phrases])): # at least one of sentence_phrase mapped to question phrase\n",
    "                reduced_para.append(sent)\n",
    "                number_reduced_sentences += 1 \n",
    "                kept_sent.append(sent_id)\n",
    "                continue\n",
    "\n",
    "            for phrase in extended_phrases:                    \n",
    "                if(phrase in sentence_phrases):  # current sentence has a exact match to extended_phrases \n",
    "                    reduced_para.append(sent)\n",
    "                    number_reduced_sentences += 1 \n",
    "                    kept_sent.append(sent_id)\n",
    "                    break     # no need to continue checking whether current sentence contains other extended_phrases\n",
    "\n",
    " \n",
    "        if(len(reduced_para) > 0):\n",
    "            raw_reduced_contexts.append([para_title, reduced_para])\n",
    "            kept_para_sent.append(kept_sent)\n",
    "        else:\n",
    "            for phrase in extended_phrases:\n",
    "                if(phrase in list(flatten(paras_phrases[para_id][0]))[::2]):   # only tilte contains one of the extended_phrases\n",
    "                    raw_reduced_contexts.append([para_title, []])\n",
    "                    kept_para_sent.append(kept_sent)\n",
    "                    break\n",
    "                     \n",
    "      \n",
    "    assert number_reduced_sentences <= number_sentences   \n",
    "    print(black('reduced_context_ratio', 'bold'), number_reduced_sentences / number_sentences)    \n",
    "\n",
    "    return raw_reduced_contexts, kept_para_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_reduced_supporting_facts(supporting_facts, reduced_contexts, kept_para_sent):\n",
    "    \n",
    "    reduced_supporting_facts = []\n",
    "    reduced_supporting_facts_in_original_id = []\n",
    "    support_para = set(\n",
    "        para_title for para_title, _ in supporting_facts\n",
    "    )\n",
    "    sp_set = set(list(map(tuple, supporting_facts)))                              # a list of (title, sent_id in orignal context) \n",
    "    print(black('supportiing facts: ', 'bold'), sp_set) \n",
    "    for i, para_reduced_context in enumerate(reduced_contexts):                   # each para\n",
    "        if(para_reduced_context[0] in support_para):\n",
    "            for sent_id, orig_sent_id in enumerate(kept_para_sent[i]):\n",
    "                if( (para_reduced_context[0], orig_sent_id) in sp_set ):\n",
    "                    reduced_supporting_facts.append([para_reduced_context[0], sent_id])\n",
    "                    reduced_supporting_facts_in_original_id.append([para_reduced_context[0], orig_sent_id])\n",
    "                    \n",
    "    return reduced_supporting_facts, reduced_supporting_facts_in_original_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised for extractiing phrases, case matters for phrases extraction\n",
    "def _normalize_text(s):\n",
    "\n",
    "#     def remove_articles(text):\n",
    "#         return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def replace_sentence_end(text):\n",
    "        exclude = set(['.', '?'])\n",
    "        return ''.join(ch if ch not in exclude  else ',' for ch in text)\n",
    "\n",
    "#     def remove_stop_words(text):\n",
    "#         all_stopwords = set(nlp.Defaults.stop_words)\n",
    "#         return ' '.join(word for word in text.split() if word not in all_stopwords) \n",
    "    def remove_wh_words(text):\n",
    "        wh_words = set([\"what\", \"when\", 'where', \"which\", \"who\", \"whom\", \"whose\", \"why\", \"how\", \"whether\",\n",
    "                        \"What\", \"When\", 'Where', \"Which\", \"Who\", \"Whom\", \"Whose\", \"Why\", \"How\", \"Whether\"])\n",
    "        return ' '.join(word for word in text.split() if word not in wh_words) \n",
    "    \n",
    "    return white_space_fix(remove_wh_words(replace_sentence_end(s)))\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Compute the reduced context with phrase graph# debug: check args\n",
    "# import shlex\n",
    "# # argString ='--datafile /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_dev_distractor_v1.json --qid 5ade28cf5542997c77aded85'  # --outfile /xdisk/msurdeanu/fanluo/hotpotQA/small_out.json'\n",
    "# argString ='--datafile /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json  --qid 5a7a06935542990198eaf050'    \n",
    "\n",
    "# shlex.split(argString)\n",
    "\n",
    "# import json\n",
    "# import argparse \n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--datafile\", type=str, default='small.json')\n",
    "#     parser.add_argument(\"--qid\", type=str, default='5ae73acb5542991e8301cc07')\n",
    "# #    parser.add_argument(\"--outfile\", type=str, default='small_out.json')\n",
    "#     args = parser.parse_args(shlex.split(argString)) \n",
    "     \n",
    "#     print(args.datafile)\n",
    "#     print(args.qid)\n",
    "# #     question_json = !cat $args.datafile | /xdisk/msurdeanu/fanluo/helper/jq-linux64 -c --arg key $args.qid '.[] | select(._id | contains($key))'  # --arg key $args.qid is used to pass args.qid as a variable to jq   \n",
    " \n",
    "#     question_json = !/xdisk/msurdeanu/fanluo/helper/jq-linux64 -c '.[] | select(._id | contains(\"5a7a06935542990198eaf050\"))' /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json\n",
    "#     print(\"ok\")\n",
    "#     print(question_json)\n",
    "#     question = json.loads(question_json[0])    # Convert from JSON string to dict\n",
    "#     reduce_context_with_phares_graph(question, args.qid)\n",
    "#     print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_context_with_phares_graph(question, '5a7a06935542990198eaf050')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_json = !/xdisk/msurdeanu/fanluo/helper/jq-linux64 -c '.[] | select(._id | contains(\"5ade28cf5542997c77aded85\"))' /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_dev_distractor_v1.json\n",
    "example = ujson.loads(question_json[0]) \n",
    "example5ade28cf5542997c77aded85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_context_with_phares_graph() zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    question = _normalize_text(example[\"question\"])\n",
    "    print(\"question: \", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    question_doc = nlp2(question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    question_phrases = [(remove_punc(p.text.lower()), p.rank) for p in question_doc._.phrases if(p.text != '')] \n",
    "    question_phrases_text = [p[0] for p in question_phrases] \n",
    "             \n",
    "    print(black(\"Original question: \", 'bold'), example[\"question\"])\n",
    "    print(black(\"question: \", 'bold'), question)\n",
    "    print(black(\"question_phrases_text: \", 'bold'), question_phrases_text)\n",
    "#     print(black(\"question_phrases: \", 'bold'), question_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    answer = remove_punc(lower(_normalize_text(example[\"answer\"])))  \n",
    "    print(black('answer: ', 'bold'), answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    raw_contexts = example[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if gold_paras_only: \n",
    "#        raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]    \n",
    "paras_phrases = []                                                # phrases of all 10 paragraghs\n",
    "titles = []\n",
    "for i, para_context in enumerate(raw_contexts):                   # each para\n",
    "    print(black('title: ', 'bold'), para_context[0]) \n",
    "    print(black(\"original sents: \" , 'bold'), para_context[1])\n",
    "    title = _normalize_text(para_context[0])   \n",
    "    titles.append(title)\n",
    "    sents = [ _normalize_text(sent) for sent in para_context[1]]\n",
    "    num_sents_before_coref_resolved = len(sents)\n",
    "    print(\"numbe of sents before coref: \", num_sents_before_coref_resolved)\n",
    "    sents_joint =  (' ' + SENT_MARKER_END +' ').join(sents)\n",
    "\n",
    "    print(black(\"normalized sents: \" , 'bold'), sents_joint)\n",
    "    sents_doc = nlp1(sents_joint)\n",
    "    print(black(\"resolved_sents: \", 'bold'), sents_doc._.coref_resolved ) \n",
    "    sents_coref_resolved = sents_doc._.coref_resolved.split(SENT_MARKER_END)\n",
    "    num_sents_after_coref_resolved = len(sents_coref_resolved)\n",
    "\n",
    "    print(\"numbe of sents after coref: \", num_sents_after_coref_resolved)\n",
    "\n",
    "\n",
    "    if(num_sents_before_coref_resolved == num_sents_after_coref_resolved):\n",
    "        sent_docs = list(nlp2.pipe([title] + sents_coref_resolved))       \n",
    "    else:\n",
    "        sent_docs = list(nlp2.pipe([title] + sents))\n",
    "\n",
    "    para_phrases = []                                        \n",
    "    for sent_doc in sent_docs:                                      # each sent in a para \n",
    "        sent_phrases = [(remove_punc(p.text.lower()), p.rank) for p in sent_doc._.phrases if(p.text != '')]  # phrases from each sentence \n",
    "        para_phrases.append(sent_phrases)                                # para_phrases[0] are phrases from  title\n",
    "    paras_phrases.append(para_phrases)     \n",
    "print(black(\"paras_phrases\", 'bold'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for paras_phrase in paras_phrases: \n",
    "        print([list(flatten(s))[::2] for s in paras_phrase])\n",
    "        \n",
    "#     contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_contexts]  \n",
    "#     context = \" \".join(contexts)                                                     \n",
    "#     if (answer != '' and len(list(re.finditer(answer, context, re.IGNORECASE))) > 0):\n",
    "#         answer_in_context += 1\n",
    "#     all_sent_phrases_text =  list(flatten(paras_phrases))[::2]        # every other element is text, others are rank \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    RG = create_relevant_graph(paras_phrases, question_phrases_text)\n",
    "    RG, common_phrases, mapping = find_common_mapping(RG, question_phrases_text)# mapping matchs paras_phrases with question_phrases\n",
    "    RG, dup_sets = dedup_nodes_in_graph(RG, question_phrases_text)              # dedup paras_phrases in RG for finding meanningful path\n",
    "#     print(black(\"nodes in RG: \", 'bold'), RG.nodes())\n",
    "    \n",
    "    question_only_phrase = list(set(question_phrases_text).difference(common_phrases)) \n",
    "    if(len(common_phrases) > 1): \n",
    "        path_phrases = list(approx.steinertree.steiner_tree(RG, common_phrases).nodes)  # to find the shortest path cover all common_phrases  \n",
    "        extended_phrases = path_phrases + question_only_phrase  \n",
    "    else: #  0 or 1 common phrases\n",
    "        path_phrases = list(common_phrases)             \n",
    "        extended_phrases = question_phrases_text\n",
    "\n",
    "    \n",
    "    print(black(\"common_phrases: \", 'bold'), common_phrases)\n",
    "    print(black(\"path_phrases: \", 'bold'), path_phrases)     \n",
    "    print(black(\"question_only_phrase: \", 'bold'), question_only_phrase)\n",
    "    print(black(\"extended_phrases before expand\", 'bold'), extended_phrases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # expand to include merged nodes, that is, also include phrases that from the same dup_set \n",
    "    extended_phrases_merged = set()\n",
    "    for phrase in extended_phrases:\n",
    "        idx_phrase = [idx for idx, dup_set in enumerate(dup_sets) if(phrase in dup_set)]   # the set where phrase in\n",
    "        if(len(idx_phrase) > 0):\n",
    "            extended_phrases_merged = extended_phrases_merged | dup_sets[idx_phrase[0]]\n",
    "            extended_phrases_merged.remove(phrase)\n",
    "    extended_phrases.extend(list(extended_phrases_merged))\n",
    "    print(black(\"extended_phrases: \", 'bold'), extended_phrases)        \n",
    "    print(black(\"introduced_phrases: \", 'bold'), set(extended_phrases) - set(question_phrases_text))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    vis_graph(RG, common_phrases, question_phrases_text) \n",
    "#     print(blue(\"Min Subgraph covers all common entities\"))\n",
    "    vis_graph(RG, path_phrases, question_phrases_text) \n",
    "        \n",
    "#     print(black(\"phrases in relevant components\", 'bold'))    \n",
    "#     print(RG.nodes.data())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    raw_reduced_contexts, kept_para_sent = construct_reduced_context(raw_contexts, paras_phrases, extended_phrases, mapping)     \n",
    "    reduced_contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_reduced_contexts]    \n",
    "    reduced_context_str = remove_punc(lower(_normalize_text(\" \".join(reduced_contexts)) )) \n",
    "    if (answer != '' and answer != 'yes'  and  answer != 'no' and len(list(re.finditer(answer, reduced_context_str, re.IGNORECASE))) > 0):\n",
    "        print(black(\"answer in reduced_context\", 'bold')) \n",
    "    elif(answer == 'yes' or answer == 'no'):\n",
    "        pass\n",
    "    else:\n",
    "        print(black(\"answer not in reduced_context\", 'bold')) \n",
    "        print(black(\"reduced_contexts\", 'bold'), raw_reduced_contexts)\n",
    "#     print(black('titles: ', 'bold'), titles) \n",
    "    \n",
    "    reduced_supporting_facts, reduced_supporting_facts_in_original_id = construct_reduced_supporting_facts(example[\"supporting_facts\"], raw_reduced_contexts, kept_para_sent) \n",
    "    for para_title, sent_id in example[\"supporting_facts\"]:\n",
    "        print(raw_contexts[titles.index(para_title)][1][sent_id])\n",
    "    print(black('reduced_supporting_facts_in_original_id: ', 'bold'), reduced_supporting_facts_in_original_id) \n",
    "#     print(black('reduced supportiing facts: ', 'bold'), reduced_supporting_facts) \n",
    "    for para_title, sent_id in reduced_supporting_facts_in_original_id:\n",
    "        print(raw_contexts[titles.index(para_title)][1][sent_id])\n",
    "    \n",
    "    example[\"question_phrases_text\"] = question_phrases_text\n",
    "    example[\"question_only_phrase\"] = question_only_phrase\n",
    "    P = RG.subgraph(path_phrases)  # to save the graph view that contains      \n",
    "    path_data = json_graph.node_link_data(P)\n",
    "    example[\"path\"] = path_data\n",
    "    example[\"question_phrases\"] = question_phrases\n",
    "    example[\"paras_phrases\"] = paras_phrases\n",
    "#     example[\"all_sent_phrases_text\"] = all_sent_phrases_text\n",
    "    example[\"common_phrases\"] = list(common_phrases)\n",
    "    example[\"path_phrases\"] = path_phrases\n",
    "    example[\"extended_phrases\"] = extended_phrases                     \n",
    "    example['context'] = raw_reduced_contexts\n",
    "    example['supporting_facts'] = reduced_supporting_facts\n",
    "    example['kept_para_sent'] = kept_para_sent\n",
    "\n",
    "    print(black(\"context: \", 'bold'), raw_contexts)   \n",
    "    print(black(\"reduced_context: \", 'bold'), raw_reduced_contexts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"chris jones footballer born 1985\"\n",
    "sents =  ['chris is welsh semiprofessional footballer currently playing for cymru alliance side porthmadog', 'former professional with leeds united jones is currently in his fourth season with city', 'he made several appearances for leeds and was heavily involved with first team', 'chris was playing with likes of aaron lennon james milner rio ferdinand scott carson and alan smith during his spell with yorkshire outfit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = title  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_doc = nlp1(context)\n",
    "title, sents = context_doc._.coref_resolved.split(TITLE_END)  \n",
    "sents = sents.split(SENT_MARKER_END)\n",
    "sent_docs = list(nlp2.pipe([title] + sents))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"brown state fishing lake , </t> .  brown state fishing lake sometimes also known as brown state fishing lake and wildlife area is protected area in brown county kansas in united states , </sent> ,  lake is 62 acres 025 km² in area and up to 13 feet 4 m deep , </sent> ,  area was formerly known as brown county state park and is 8 miles 13 km east of hiawatha kansas\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_doc = nlp1(context)\n",
    "print(context_doc._.coref_resolved)\n",
    "print(context_doc._.coref_clusters)\n",
    "print(context_doc._.coref_clusters[1].mentions)\n",
    "print(context_doc._.coref_clusters[1].mentions[-1])\n",
    "print(context_doc._.coref_clusters[1].mentions[-1]._.coref_cluster.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"fishing lake , </t> .  fishing lake is lake in canadian province of saskatchewan , </sent> ,  lake is located between highway 5 and highway 16 22 km north of town of foam lake saskatchewan and 24 km east of town of wadena saskatchewan , </sent> ,  lake does not have effective outlet channel and so is prone to flooding , </sent> ,  record floods in 2007 resulted in plan by government of saskatchewan to lower level of lake by digging drainage channel , </sent> ,  fishing lake first nation opposed this plan and instead flood control berms were constructed\"\n",
    "context_resolved = nlp1(context)._.coref_resolved\n",
    "\n",
    "print(context_resolved)\n",
    "title_coref_resolved, sents_coref_resolved = context_resolved.split(TITLE_END)  \n",
    "sents_coref_resolved = sents_coref_resolved.split(SENT_MARKER_END) \n",
    "sent_docs = list(nlp2.pipe([title_coref_resolved] + sents_coref_resolved))     \n",
    "para_phrases = []                                        \n",
    "for sent_doc in sent_docs:                                    # each sent in a para\n",
    "    sent_phrases = [(p.text, p.rank) for p in sent_doc._.phrases if(p.text != '')]  # phrases from each sentence \n",
    "    para_phrases.append(sent_phrases)     \n",
    "print(para_phrases)\n",
    "print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "text = 'valhalla highlands historic district also known as lake valhalla is national historic district located near cold spring in putnam county new york'\n",
    "doc = nlp2(text)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('chunk: ', chunk.text) \n",
    "    print('root: ',  chunk.root.text)\n",
    "#     span = Span(doc, chunk.root.i, chunk.root.i+1)\n",
    "#     print(span)\n",
    "#     print(span.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"this list provides guide to opera composers as determined by their presence on majority of compiled lists of significant opera composers , </sent> ,  see lists consulted section for full details , </sent> ,  composers run from jacopo peri who wrote first ever opera in late 16th century italy to john adams one of leading figures in contemporary operatic world , </sent> ,  brief accompanying notes offer explanation as to why each composer has been considered major , </sent> ,  also included is section about major women opera composers compiled from same lists , </sent> ,  for introduction to operatic history see opera , </sent> ,  organisation of list is by birthdate\"\n",
    "context_resolved = nlp1(context)._.coref_resolved\n",
    "\n",
    "print(context_resolved)\n",
    "sents_coref_resolved = context_resolved.split(SENT_MARKER_END)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('../LIME/dreaddit-train.csv') \n",
    "test_df = pd.read_csv('../LIME/dreaddit-test.csv') \n",
    "\n",
    "train_df = train_df.loc[train_df.confidence>0.8]\n",
    "\n",
    "train_df[\"coref_resolved\"] = train_df[\"text\"]\n",
    "for i, train_text in enumerate(train_df.text.values.tolist()):\n",
    "    train_sents_doc = nlp1(_normalize_text(train_text))\n",
    "    train_df[\"coref_resolved\"].iloc[i] = train_sents_doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../LIME/dreaddit-train_resolved.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[test_df.confidence>0.8]\n",
    "\n",
    "test_df[\"coref_resolved\"] = test_df[\"text\"]\n",
    "for i, test_text in enumerate(test_df.text.values.tolist()):\n",
    "    test_sents_doc = nlp1(_normalize_text(test_text))\n",
    "    test_df[\"coref_resolved\"].iloc[i] = test_sents_doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.coref_resolved.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.text.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "test_df.to_csv(\"../LIME/dreaddit-test_resolved.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.find_module('pytextrank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy3.0",
   "language": "python",
   "name": "spacy3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
