{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7ff4c765d6d0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7ff4c77af520>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7ff4c77af5e0>), ('textrank', <bound method TextRank.PipelineComponent of <pytextrank.pytextrank.TextRank object at 0x7ff4cddc1700>>)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.8/site-packages')\n",
    " \n",
    "from prettytable import PrettyTable\n",
    "import spacy   \n",
    "import en_core_web_lg                         \n",
    "nlp = en_core_web_lg.load() \n",
    "#!python -m pip install pytextrank\n",
    "# Fan: make 3 changes in pytextrank.py \n",
    "# 1. phrase_text = ' '.join(key[0] for key in phrase_key) \n",
    "#  p.text are the joint of lemma tokens with pos_ in kept_pos, and maintain the order when join    \n",
    "# 2. add argumrnt 'chunk_type' to only consider named entity ('ner') or noun_chunks ('noun'), besides the default ('both') \n",
    "# 3. replace token.lemma_ with token.lemma_.lower().strip()\n",
    "import pytextrank\n",
    "tr = pytextrank.TextRank(pos_kept=[\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\", \"NUM\", \"ADV\"], chunk_type='both')  \n",
    "nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)\n",
    "print(nlp.pipeline)   \n",
    "# import neuralcoref\n",
    "# neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "#!conda install networkx --yes\n",
    "import networkx as nx\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create phrases graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_para_graph(paras_phrases):\n",
    "    G = nx.Graph()    \n",
    "    top_para_phrases = []                     # node of the first (top ranked) phrases from each para \n",
    "    for para_phrases in paras_phrases:        # each para\n",
    "        top_sent_phrases = []                 # node of the first (top ranked) phrases from each sent \n",
    "        for sent_phrases in para_phrases:     # each sent\n",
    "            \n",
    "            # complete graph for each sent\n",
    "            sent_G = nx.Graph()\n",
    "            sent_G.add_nodes_from([phrase[0] for phrase in sent_phrases])  \n",
    "            sent_G.add_edges_from(itertools.combinations([phrase[0] for phrase in sent_phrases], 2)) \n",
    "            G = nx.compose(G, sent_G)         # union of the node sets and edge sets\n",
    "            \n",
    "            \n",
    "            # add an edge between the top ranked phrases from each sent to bridge sents\n",
    "            if(sent_phrases):\n",
    "                for top_sent_phrase in top_sent_phrases:\n",
    "                    G.add_edge(top_sent_phrase[0], sent_phrases[0][0])  # sent_phrases[0] is the top ranked phrase of the sentence\n",
    "                top_sent_phrases.append(sent_phrases[0])     \n",
    "            \n",
    "        top_sent_phrases = sorted(top_sent_phrases, key=lambda x: x[1], reverse=True)      # x[0]: phrase text,  x[1]: phrase rank\n",
    "        \n",
    "        \n",
    "        # add an edge between the top ranked phrases from each para to bridge paras\n",
    "        if(top_sent_phrases):\n",
    "            for top_para_phrase in top_para_phrases: \n",
    "                G.add_edge(top_para_phrase[0], top_sent_phrases[0][0])  # top_sent_phrases[0] is the top ranked phrase of current para\n",
    "            top_para_phrases.append(top_sent_phrases[0])\n",
    "     \n",
    "    # Draw\n",
    "#     pos = nx.spring_layout(G)\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     nx.draw(G, pos, with_labels=True, edge_color='black', width=1, linewidths=1,\n",
    "#             node_size=500, node_color='orange', alpha=0.9                           \n",
    "#             )     \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the reduced context with phrase graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from networkx.algorithms import approximation as approx\n",
    "def reduce_context_with_phares_graph(example, q_id, gold_paras_only=False):\n",
    "    \"\"\"function to compute reduced context with phrase graph.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "    Returns:\n",
    "        a new file save additional phrase-related info and the reduced context\n",
    "\n",
    "    \"\"\"\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    new_dict = {\"data\": []} \n",
    "    common_phrases_num_le2 = 0\n",
    "    extended = 0\n",
    "    answer_in_reduced_context = 0\n",
    "    answer_in_context = 0\n",
    "    reduced_context_ratios = []\n",
    "\n",
    "    raw_contexts = example[\"context\"]\n",
    "#         if gold_paras_only: \n",
    "#        raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]    \n",
    "    paras_phrases = []                                                # phrases of all 10 paragraghs\n",
    "    for i, para_context in enumerate(raw_contexts):                   # each para\n",
    "\n",
    "        title = _normalize_text(para_context[0])          \n",
    "        sents = [_normalize_text(sent) for sent in para_context[1]]\n",
    "\n",
    "        sent_docs = list(nlp.pipe([title] + sents))   \n",
    "        para_phrases = []                                        \n",
    "        for sent_doc in sent_docs:                                      # each sent in a para\n",
    "            sent_phrases = [(p.text, p.rank) for p in sent_doc._.phrases if(p.text != '')]  # phrases from each sentence  \n",
    "            para_phrases.append(sent_phrases)                           # para_phrases[0] are phrases from  title\n",
    "        paras_phrases.append(para_phrases)    \n",
    "\n",
    "    contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_contexts]  \n",
    "    context = \" \".join(contexts)                                                     \n",
    "\n",
    "    answer = _normalize_text(example[\"answer\"])  \n",
    "    if (answer != '' and len(list(re.finditer(answer, context, re.IGNORECASE))) > 0):\n",
    "        answer_in_context += 1\n",
    "\n",
    "    paras_phrases_graph = create_para_graph(paras_phrases)\n",
    "\n",
    "    question = _normalize_text(example[\"question\"])\n",
    "    question_doc = nlp(question)\n",
    "    question_phrases = [(p.text, p.rank) for p in question_doc._.phrases if(p.text != '')] \n",
    "    question_phrases_text = [p[0] for p in question_phrases]\n",
    "\n",
    "    all_sent_phrases_text =  list(flatten(paras_phrases))[::2]        # every other element is text, others are rank. \n",
    "    common_phrases = list(set(all_sent_phrases_text).intersection(question_phrases_text)) \n",
    "    question_only_phrase = list(set(question_phrases_text).difference(common_phrases)) \n",
    "\n",
    "    example[\"question_phrases_text\"] = question_phrases_text\n",
    "    example[\"question_only_phrase\"] = question_only_phrase\n",
    "\n",
    "    if(len(common_phrases) > 1):\n",
    "        common_phrases_num_le2 += 1\n",
    "        path_phrases = list(approx.steinertree.steiner_tree(paras_phrases_graph, common_phrases).nodes)  # to find the shortest path cover all common_phrases  \n",
    "        extended_phrases = path_phrases + question_only_phrase  \n",
    "        if(len(extended_phrases) > len(question_phrases_text)):\n",
    "            extended += 1\n",
    "    else: #  0 or 1 common phrases\n",
    "        path_phrases = common_phrases             \n",
    "        extended_phrases = question_phrases_text\n",
    "\n",
    "#     print(\"common_phrases: \", common_phrases)\n",
    "#     print(\"extended_phrases: \", extended_phrases)\n",
    "\n",
    "\n",
    "    example[\"question_phrases\"] = question_phrases\n",
    "    example[\"paras_phrases\"] = paras_phrases\n",
    "#     example[\"all_sent_phrases_text\"] = all_sent_phrases_text\n",
    "    example[\"common_phrases\"] = common_phrases\n",
    "    example[\"path_phrases\"] = path_phrases\n",
    "    example[\"extended_phrases\"] = extended_phrases \n",
    "\n",
    "#             print(\"context: \", context)    \n",
    "#         print(\"\\n\\n\") \n",
    "#         print(\"question_phrases: \", question_phrases)    \n",
    "    # print(\"paras_phrases\")\n",
    "    # for paras_phrase in paras_phrases:\n",
    "    #     print(paras_phrase)\n",
    "    #     print(\"\\n\") \n",
    "    \n",
    "#         print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "    raw_reduced_contexts = []     # sentences contain one of the extended_phrases\n",
    "    number_sentences = 0\n",
    "    number_reduced_sentences = 0 \n",
    "    kept_para_sent = []\n",
    "    for para_id, (para_title, para_lines) in enumerate(raw_contexts):\n",
    "# #             print(\"para_id, para_title, para_lines\",para_id, para_title, para_lines)\n",
    "\n",
    "        number_sentences += len(para_lines)\n",
    "        reduced_para = []\n",
    "        kept_sent = []\n",
    "        for sent_id, sent in enumerate(para_lines):\n",
    "\n",
    "            for phrase in extended_phrases:\n",
    "                # every other element is text, others are rank \n",
    "                if(phrase in list(flatten(paras_phrases[para_id][sent_id+1]))[::2]):  # paras_phrases[para_id][0] are phrases from the title\n",
    "                    reduced_para.append(sent)\n",
    "                    number_reduced_sentences += 1 \n",
    "                    kept_sent.append(sent_id)\n",
    "                    break     # if current sentence contains one of the extended_phrases, this sentence is added to reduced sentence, and no need to continue checking whether it contains other phrases\n",
    "        if(len(reduced_para) > 0):\n",
    "            raw_reduced_contexts.append([para_title, reduced_para])\n",
    "            kept_para_sent.append(kept_sent)\n",
    "        else:\n",
    "            for phrase in extended_phrases:\n",
    "                if(phrase in list(flatten(paras_phrases[para_id][0]))[::2]):   # only tilte contains one of the extended_phrases\n",
    "                    raw_reduced_contexts.append([para_title, []])\n",
    "                    kept_para_sent.append(kept_sent)\n",
    "                    break\n",
    "                     \n",
    "      \n",
    "    assert number_reduced_sentences <= number_sentences                    \n",
    "    reduced_context_ratios.append(number_reduced_sentences / number_sentences)    \n",
    " \n",
    "    \n",
    "    reduced_contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_reduced_contexts]    \n",
    "    reduced_context_str = \" \".join(reduced_contexts)  \n",
    "\n",
    "    if (answer != '' and len(list(re.finditer(answer, reduced_context_str, re.IGNORECASE))) > 0):\n",
    "        answer_in_reduced_context += 1\n",
    "\n",
    "    supporting_facts = []\n",
    "    support_para = set(\n",
    "        para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "    )\n",
    "    sp_set = set(list(map(tuple, example['supporting_facts'])))                       # a list of (title, sent_id in orignal context) \n",
    "    for i, para_reduced_context in enumerate(raw_reduced_contexts):                   # each para\n",
    "        if(para_reduced_context[0] in support_para):\n",
    "            for sent_id, orig_sent_id in enumerate(kept_para_sent[i]):\n",
    "                if( (para_reduced_context[0], orig_sent_id) in sp_set ):\n",
    "                    supporting_facts.append([para_reduced_context[0], sent_id])\n",
    "\n",
    "    example['reduced_context'] = raw_reduced_contexts\n",
    "    example['supporting_facts'] = supporting_facts\n",
    "    example['kept_para_sent'] = kept_para_sent\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_text(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[2:16]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[380:400]' > small_dev.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[31:50]' > sample.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--datafile /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_dev_distractor_v1.json --qid 5a81ca0d5542990a1d231ebb'  # --outfile /xdisk/msurdeanu/fanluo/hotpotQA/small_out.json'\n",
    "shlex.split(argString)\n",
    "\n",
    "import json\n",
    "import argparse \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--datafile\", type=str, default='small.json')\n",
    "    parser.add_argument(\"--qid\", type=str, default='5a81ca0d5542990a1d231ebb')\n",
    "#    parser.add_argument(\"--outfile\", type=str, default='small_out.json')\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "     \n",
    "\n",
    "    question_json = !cat $args.datafile | ../helper/jq-linux64 -c --arg key $args.qid '.[] | select(._id | contains($key))'  # --arg key $args.qid is used to pass args.qid as a variable to jq\n",
    "    \n",
    "#     print(question_json)\n",
    "    question = json.loads(question_json[0])    # Convert from JSON string to dict\n",
    "    reduce_context_with_phares_graph(question, args.qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_phrases_text:  ['navy building', 'state war', 'office building', 'white house']\n",
      "common_phrases:  ['navy building', 'white house', 'office building']\n",
      "question_only_phrase:  ['state war']\n",
      "extended_phrases:  ['navy building', 'office building', 'white house office', 'congressional office building', 'white house', 'state war']\n",
      "all_sent_phrases_text:  ['congressional office building', 'congressional office', 'united states congress', 'united states', 'united states capitol', 'congressional office building', 'office building', 'congressional office building office building', 'limited space', 'united states capitol police', 'congressional office building', 'capitol complex', 'capitol', 'part', 'authority', 'architect', 'room staff room', 'committee hearing', 'office buildings house', 'support committee', 'office buildings', 'individual office', 'maintenance staff', 'multiple cafeteria', 'area', 'senator', 'us', 'us representative', 'capitol subway system', 'several underground train', 'capitol subway', 'congressional office building', 'capitol', 'mean', 'two chamber', 'message', 'congressional page', 'two', 'package', 'building', 'white house office', 'press secretary', 'white house', 'president white house staff', 'white house office', 'white house', 'three principal group', 'press secretary', 'press office', 'three', 'information', 'medium', 'white house press secretary', 'white house office', 'white house', 'executive office', 'office', 'subunit', 'part', 'president', 'art deco', 'united states', 'art deco style', 'united states', 'world war', 'important impact', '1920s', 'design', 'architecture', 'france', '1930', 'empire state building', 'new york', 'new york city', 'empire state building chrysler building rockefeller center', 'most famous example', 'rockefeller center', 'skyscraper', 'chrysler', 'modern aesthetic fine craftsmanship', 'expensive material', 'luxury', 'symbol', 'modernity', 'office building government building train station movie theater diner', 'department store', 'residence', 'automobile ocean liner', 'everyday object', 'toaster', 'radio set', 'furniture', 'design', 'public work administration', 'immense public work project', 'works progress administration', 'golden gate bridge', 'great depression', 'late 1930', 'hoover dam', 'architecture', 'world war ii', 'modernist architecture', 'abrupt end', 'period', 'beginning', '1939', 'style', 'original building', 'historical landmark', '1960', 'style', 'eisenhower executive office building', 'eisenhower', 'old executive office building oeob', 'eisenhower executive office building eeob', 'us government building', 'executive office', 'us capital', 'general services administration', 'vice president', 'state war navy building', 'washington dc', 'us', 'white house', 'united states', 'office', 'president', 'eisenhower', 'white house office', 'white house', 'white house office', 'executive office', 'white house', 'president', 'entity', 'united states', 'white house chief', 'white house office', 'white house', 'executive office', 'head', 'staff', 'president', 'white house eisenhower executive office building', 'new executive office building', 'various office', 'white house', 'east wing', 'west wing', 'eisenhower', 'staff', 'jeweler row district', 'historic district', 'jeweler row district', 'loop community area', 'chicago illinois', 'united states', 'illinois', 'chicago', 'graham anderson probst white john mills van osdel adler sullivan alfred alschuler d h burnham co', 'east monroe street building', 'east washington street', 'adler sullivan alfred alschuler', 'graham anderson probst', 'east monroe', 'italianate chicago school', 'many architect', 'wabash avenue', 'holabird roche', 'italianate chicago school art deco', 'art deco', 'variety', 'style', 'district', '1872 1941', 'small manufacturer mercantile building office building', 'loft building', 'building', 'early skyscraper', 'south loop print house district', 'south loop print house', 'downtown chicago loop area', 'south loop print house district', 'south loop print house', 'historic district', 'chicago illinois', 'chicago', 'illinois', 'congress polk state taylor', '28 contribute building', 'wells street', '28', 'district', 'chicagos print industry', 'print building', 'midwest', '1880', 'district', '1930s', 'large printing company', 'tall thin printing building land', 'dearborn station', 'thin property block', 'district', 'dearborn', 'proximity', 'many more small homogenous building', 'small number', 'less prominent printing firm', 'large detailed building', 'district', 'navy building', 'navy', '38', 'fagatogo american samoa', 'historic building', 'navy building', '38', '1', 'route', 'navy', 'roughly square singlestory building', 'fluted cast metal column', 'shallowslope pyramidal roof', 'concrete block wall', 'north side', 'roughly square', 'lanai', 'road', 'concrete block', 'construction', 'highpowered radio transmission equipment', 'naval station tutuila', 'highpowered radio transmission', 'united states navy', 'naval facility', 'first world war', 'hawaii', 'part', 'building', '1917', 'three building', 'time', 'three', 'one', 'material', 'combination', 'east wing', 'white house complex', 'white house', 'part', 'east wing', 'white house', 'twostory structure', 'president', 'united states', 'president executive office staff east wing', 'white house social secretary white house graphic calligraphy office', 'white house', 'office space', 'correspondence staff', 'east wing', 'west wing', 'first lady', 'first', 'staff', 'east colonnade corridor', 'east colonnade', 'visitor entrance', 'east wing', 'body', 'east', 'residence', 'white house theater', 'white house', 'family theater', 'corridor', 'white house', 'social visitor', 'east wing', 'longworth house office building', 'longworth house', 'longworth house office building lhob', 'united states house', 'three office building', 'longworth house', 'united states house representatives', 'three', 'one', 'lhob', 'representatives', 'independence avenue new jersey avenue c street se', 'south capitol street', 'south capitol', 'southeast washington', 'capitol', 'building', 'washington', 'five large committee room', 'seven small committee room', 'large assembly room', '251 congressional office', '599675 sqft', 'area', '599675', 'suite', 'way mean committee', 'seven', 'five', 'total', '251']\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
