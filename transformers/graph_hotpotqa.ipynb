{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null </s>\"\n",
    "\n",
    "def create_example_dict(context, answer, id, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answer\": answer,\n",
    "                \"id\": id,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f0076326cc0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f00766e13a8>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f00766e1528>), ('textrank', <bound method TextRank.PipelineComponent of <pytextrank.pytextrank.TextRank object at 0x7f0076326c88>>)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import spacy   \n",
    "import en_core_web_lg                         \n",
    "nlp = en_core_web_lg.load() \n",
    "#!python -m pip install pytextrank\n",
    "# Fan: make 3 changes in pytextrank.py \n",
    "# 1. phrase_text = ' '.join(key[0] for key in phrase_key) \n",
    "#  p.text are the joint of lemma tokens with pos_ in kept_pos, and maintain the order when join    \n",
    "# 2. add argumrnt 'chunk_type' to only consider named entity ('ner') or noun_chunks ('noun'), besides the default ('both') \n",
    "# 3. replace token.lemma_ with token.lemma_.lower().strip()\n",
    "import pytextrank\n",
    "tr = pytextrank.TextRank(pos_kept=[\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\", \"NUM\", \"ADV\"], chunk_type='both')  \n",
    "nlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)\n",
    "print(nlp.pipeline)   \n",
    "# import neuralcoref\n",
    "# neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "\n",
    "#!conda install networkx --yes\n",
    "import networkx as nx\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create phrases graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_para_graph(paras_phrases):\n",
    "    G = nx.Graph()    \n",
    "    top_para_phrases = []                     # node of the first (top ranked) phrases from each para \n",
    "    for para_phrases in paras_phrases:        # each para\n",
    "        top_sent_phrases = []                 # node of the first (top ranked) phrases from each sent \n",
    "        for sent_phrases in para_phrases:     # each sent\n",
    "            \n",
    "            # complete graph for each sent\n",
    "            sent_G = nx.Graph()\n",
    "            sent_G.add_nodes_from([phrase[0] for phrase in sent_phrases])  \n",
    "            sent_G.add_edges_from(itertools.combinations([phrase[0] for phrase in sent_phrases], 2)) \n",
    "            G = nx.compose(G, sent_G)         # union of the node sets and edge sets\n",
    "            \n",
    "            \n",
    "            # add an edge between the top ranked phrases from each sent to bridge sents\n",
    "            if(sent_phrases):\n",
    "                for top_sent_phrase in top_sent_phrases:\n",
    "                    G.add_edge(top_sent_phrase[0], sent_phrases[0][0])  # sent_phrases[0] is the top ranked phrase of the sentence\n",
    "                top_sent_phrases.append(sent_phrases[0])     \n",
    "            \n",
    "        top_sent_phrases = sorted(top_sent_phrases, key=lambda x: x[1], reverse=True)      # x[0]: phrase text,  x[1]: phrase rank\n",
    "        \n",
    "        \n",
    "        # add an edge between the top ranked phrases from each para to bridge paras\n",
    "        if(top_sent_phrases):\n",
    "            for top_para_phrase in top_para_phrases: \n",
    "                G.add_edge(top_para_phrase[0], top_sent_phrases[0][0])  # top_sent_phrases[0] is the top ranked phrase of current para\n",
    "            top_para_phrases.append(top_sent_phrases[0])\n",
    "     \n",
    "    # Draw\n",
    "#     pos = nx.spring_layout(G)\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     nx.draw(G, pos, with_labels=True, edge_color='black', width=1, linewidths=1,\n",
    "#             node_size=500, node_color='orange', alpha=0.9                           \n",
    "#             )     \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the reduced context with phrase graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from networkx.algorithms import approximation as approx\n",
    "def reduce_context_with_phares_graph(json_dict, outfile, gold_paras_only=False):\n",
    "    \"\"\"function to compute reduced context with phrase graph.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "    Returns:\n",
    "        a new file save additional phrase-related info and the reduced context\n",
    "\n",
    "    \"\"\"\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    new_dict = {\"data\": []} \n",
    "    common_phrases_num_le2 = 0\n",
    "    extended = 0\n",
    "    answer_in_reduced_context = 0\n",
    "    answer_in_context = 0\n",
    "    reduced_context_ratios = []\n",
    "    for e_id, example in enumerate(json_dict): \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "#         if gold_paras_only: \n",
    "#        raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]    \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        paras_phrases = []                                                # phrases of all 10 paragraghs\n",
    "        for i, para_context in enumerate(raw_contexts):                   # each para\n",
    "            is_supporting_para.append(para_context[0] in support_para)   \n",
    "            for sent_id, sent in enumerate(para_context[1]):\n",
    "                is_sup_fact.append( (para_context[0], sent_id) in sp_set )  \n",
    " \n",
    "            para_context[0] = normalize_answer(para_context[0])\n",
    "            para_context[1] = [normalize_answer(sent) for sent in para_context[1]]\n",
    "\n",
    "            sent_docs = list(nlp.pipe([para_context[0]] + para_context[1]))   \n",
    "            para_phrases = []                                        \n",
    "            for sent_doc in sent_docs:                                    # each sent in a para\n",
    "                sent_phrases = [(p.text, p.rank) for p in sent_doc._.phrases if(p.text != '')]  # phrases from each sentence \n",
    "                para_phrases.append(sent_phrases)       \n",
    "            paras_phrases.append(para_phrases)    \n",
    "\n",
    "        contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_contexts]  \n",
    "        context = \" \".join(contexts)                                                     answer = normalize_answer(example[\"answer\"])  \n",
    "        \n",
    "        if (len(list(re.finditer(answer, context, re.IGNORECASE))) > 0):\n",
    "            answer_in_context += 1\n",
    "        \n",
    "        paras_phrases_graph = create_para_graph(paras_phrases)\n",
    "        \n",
    "        question = normalize_answer(example[\"question\"])\n",
    "        question_doc = nlp(question)\n",
    "        question_phrases = [(p.text, p.rank) for p in question_doc._.phrases if(p.text != '')] \n",
    "        question_phrases_text = [p[0] for p in question_phrases]\n",
    "        \n",
    "        all_sent_phrases_text =  list(flatten(paras_phrases))[::2]        # every other element is text, others are rank. \n",
    "        common_phrases = list(set(all_sent_phrases_text).intersection(question_phrases_text)) \n",
    "        question_only_phrase = list(set(question_phrases_text).difference(common_phrases)) \n",
    "        \n",
    "        # print(\"question_phrases_text: \", question_phrases_text)\n",
    "        # print(\"common_phrases: \", common_phrases)\n",
    "#         print(\"question_only_phrase: \", question_only_phrase)\n",
    "        \n",
    "        if(len(common_phrases) > 1):\n",
    "            common_phrases_num_le2 += 1\n",
    "            path_phrases = list(approx.steinertree.steiner_tree(paras_phrases_graph, common_phrases).nodes)  # to find the shortest path cover all common_phrases  \n",
    "            extended_phrases = path_phrases + question_only_phrase  \n",
    "            if(len(extended_phrases) > len(question_phrases_text)):\n",
    "                extended += 1\n",
    "        else: #  0 or 1 common phrases\n",
    "            path_phrases = common_phrases             \n",
    "            extended_phrases = question_phrases_text\n",
    "            \n",
    "        # print(\"extended_phrases: \", extended_phrases)\n",
    "         \n",
    "        \n",
    "#         example[\"question_phrases\"] = question_phrases\n",
    "#         example[\"paras_phrases\"] = paras_phrases\n",
    "#         example[\"common_phrases\"] = common_phrases\n",
    "#         example[\"path_phrases\"] = path_phrases\n",
    "#         example[\"extended_phrases\"] = extended_phrases\n",
    "#         print(\"context: \", context)    \n",
    "#         print(\"\\n\\n\") \n",
    "#         print(\"question_phrases: \", question_phrases)    \n",
    "        # print(\"paras_phrases\")\n",
    "        # for paras_phrase in paras_phrases:\n",
    "        #     print(paras_phrase)\n",
    "        #     print(\"\\n\") \n",
    "#         print(\"all_sent_phrases_text: \", all_sent_phrases_text) \n",
    "#         print(\"\\n\\n\") \n",
    "        \n",
    " \n",
    "        raw_reduced_contexts = []     # sentences contain one of the phrases in the path \n",
    "        number_sentences = 0\n",
    "        number_reduced_sentences = 0 \n",
    "        for para_id, (para_title, para_lines) in enumerate(raw_contexts):\n",
    "# #             print(\"para_id, para_title, para_lines\",para_id, para_title, para_lines)\n",
    " \n",
    "            number_sentences += len(para_lines)\n",
    "            reduced_para = []\n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    " \n",
    "                for phrase in path_phrases:\n",
    "                    if(phrase in list(flatten(paras_phrases[para_id][sent_id]))[::2]):  # every other element is text, others are rank\n",
    "                        reduced_para.append(sent)\n",
    "                        number_reduced_sentences += 1 \n",
    "                        break     # if current sentence contains a phrase in path, this sentence is added to reduced sentence, and no need to continue checking whether it contains other phrases\n",
    "            if(len(reduced_para) > 0):\n",
    "                raw_reduced_contexts.append([para_title, reduced_para])\n",
    "        assert number_reduced_sentences <= number_sentences                    \n",
    "        reduced_context_ratios.append(number_reduced_sentences / number_sentences)    \n",
    "        \n",
    "        reduced_contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_reduced_contexts]    \n",
    "        reduced_context = \" \".join(reduced_contexts)  \n",
    "        \n",
    "        if (len(list(re.finditer(answer, reduced_context, re.IGNORECASE))) > 0):\n",
    "            answer_in_reduced_context += 1\n",
    "        \n",
    "\n",
    "        new_dict[\"data\"].append(\n",
    "            create_para_dict(\n",
    "                create_example_dict(\n",
    "                    context=reduced_context,\n",
    "                    answer=answer,\n",
    "                    id = example[\"_id\"],\n",
    "                    question=example[\"question\"],\n",
    "                    is_sup_fact = is_sup_fact,\n",
    "                    is_supporting_para = is_supporting_para \n",
    "                )\n",
    "            )\n",
    "        )         \n",
    "        \n",
    "        # print(\"number_sentences: \", number_sentences)\n",
    "        # print(\"number_reduced_sentences: \", number_reduced_sentences)\n",
    "\n",
    "#         now = datetime.now()\n",
    "#         current_time = now.strftime(\"%H:%M:%S\")\n",
    "#         print(\"Time =\", current_time)\n",
    "    print(\"number of questions with answer in context: \", answer_in_context)\n",
    "    print(\"common_phrases_num_le2: \", common_phrases_num_le2) \n",
    "    print(\"number of questions with extended phrases: \", extended)\n",
    "    print(\"number of questions with answer in reduced_context: \", answer_in_reduced_context)\n",
    "    print(\"reduced context ratios: \", reduced_context_ratios)\n",
    "    print(\"average ratio of reduced context: \", sum(reduced_context_ratios)/len(reduced_context_ratios))\n",
    "    \n",
    "    with open(outfile, 'w') as out_file:\n",
    "        json.dump(new_dict, out_file)\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[2:16]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[380:400]' > small_dev.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[31:50]' > sample.json\n",
    "\n",
    "import json\n",
    "with open(\"/xdisk/msurdeanu/fanluo/hotpotQA/small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    convert_hotpot_to_squad_format(json.load(f)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_dev.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 1)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', True)\n",
      "('model_path', '/Users/fan/Downloads/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "# argString ='--train_dataset small.json --dev_dataset small_dev.json  \\\n",
    "#     --gpus 0 --num_workers 1 \\\n",
    "#     --max_seq_len 4096 --doc_stride -1  \\\n",
    "#     --save_prefix hotpotqa-longformer  --model_path /Users/fan/Downloads/longformer-base-4096 --test '\n",
    " \n",
    "    \n",
    "import json\n",
    "import argparse \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--datafile\", type=str, default='small.json')\n",
    "    parser.add_argument(\"--outfile\", type=str, default='small_out.json')\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    with open(args.datafile, \"r\", encoding='utf-8') as f:  \n",
    "        reduce_context_with_phares_graph(json.load(f), args.outfile)  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
