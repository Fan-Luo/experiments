{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def create_example_dict(context, answers, id, is_impossible, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answers\": answers,\n",
    "                \"id\": id,\n",
    "                \"is_impossible\": is_impossible,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f1728ad3c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import spacy   \n",
    "import en_core_web_lg                         \n",
    "nlp = en_core_web_lg.load()\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "#         if gold_paras_only: \n",
    "#             raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]    \n",
    "\n",
    "\n",
    "        ##########  entity graphs and correference resolution  ########## \n",
    "        question_doc = nlp(example[\"question\"])\n",
    "        question = question_doc._.coref_resolved\n",
    "        question_entities = []\n",
    "        for chunk in question_doc.noun_chunks:\n",
    "            entity = ''\n",
    "            for token in chunk:\n",
    "                if(token.tag_ in noun_tags):\n",
    "                    entity += token.lemma_ + ' '                     # using the lemma, instead of the raw token text\n",
    "            if(entity.strip()!= ''):\n",
    "                question_entities.append(entity.strip())\n",
    "            \n",
    "        contexts = [\" <s> \".join(lst[1]) for lst in raw_contexts]    # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space \n",
    "        coref_resolved_paras = []                                    # a list of 10 paragragh text, with correferences resolved\n",
    "        paras_entities = []                                           # entities of all 10 paragraghs\n",
    "        para_docs = list(nlp.pipe(contexts))  \n",
    "        for para_doc in para_docs:\n",
    "            para_entities = []\n",
    "            coref_resolved_paras.append(para_doc._.coref_resolved)\n",
    "            for chunk in para_doc.noun_chunks:\n",
    "                entity = ''\n",
    "                for token in chunk:\n",
    "                    if(token.tag_ in noun_tags):\n",
    "                        entity += token.lemma_ + ' '\n",
    "                if(entity.strip()!= ''):\n",
    "                    para_entities.append(entity.strip())\n",
    "            paras_entities.append(para_entities)      \n",
    "\n",
    "        print(question)\n",
    "        print(question_entities)      \n",
    "        print(*(paras_entities))   \n",
    "#         print(coref_resolved_paras)   \n",
    "#         print(contexts)   \n",
    "        assert(1==2)\n",
    "            \n",
    "        context = \" <p> \" + \" <p> \".join(coref_resolved_paras)\n",
    "        \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )\n",
    "\n",
    "\n",
    "        answer = example[\"answer\"].strip() \n",
    "        if answer.lower() == 'yes':\n",
    "            answers = [{\"answer_start\": -1, \"answer_end\": -1, \"text\": answer}] \n",
    "        elif answer.lower() == 'no':\n",
    "            answers = [{\"answer_start\": -2, \"answer_end\": -2, \"text\": answer}] \n",
    "        else:\n",
    "            answers = []          # keep all the occurences of answer in the context\n",
    "            for m in re.finditer(re.escape(answer), context):    \n",
    "                answer_start, answer_end = m.span() \n",
    "                answers.append({\"answer_start\": answer_start, \"answer_end\": answer_end, \"text\": answer})\n",
    "             \n",
    "        if(len(answers) > 0): \n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=context,\n",
    "                        answers=answers,\n",
    "                        id = example[\"_id\"],\n",
    "                        is_impossible=(answers == []),\n",
    "                        question=question,\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#!conda install networkx --yes\n",
    "import networkx as nx\n",
    "\n",
    "# it is possible to obtain several, disconnected graphs\n",
    "def printGraph(triples):\n",
    "    G = nx.Graph()\n",
    "    for triple in triples:\n",
    "        G.add_node(triple[0])\n",
    "        G.add_node(triple[1])\n",
    "        G.add_node(triple[2])\n",
    "        G.add_edge(triple[0], triple[1])\n",
    "        G.add_edge(triple[1], triple[2])\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure()\n",
    "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='seagreen', alpha=0.9,\n",
    "            labels={node: node for node in G.nodes()})\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which magazine was started first Arthur's Magazine or First for Women?\n",
      "['magazine', 'Arthur Magazine', 'First', 'woman']\n",
      "['Radio City', 'India FM radio station', 'July', 'city', 'megahertz', 'Mumbai', 'Bengaluru', 'Hindi', 'English', 'song', 'Hyderabad', 'March', 'Chennai', 'July', 'Visakhapatnam October', 'Radio City', 'New medium', 'May', 'launch', 'music portal', 'planetradiocity.com', 'music news', 'video', 'song', 'music feature', 'Radio station', 'mix', 'Hindi', 'music', 'Abraham Thomas', 'ceo', 'company'] ['football', 'Albania', 'Albanian Football Federation', 'FSHF', 'team registration', 'Balkan Cup tournament', 'Albania', 'pressure', 'team', 'competition', 'competition', 'duel', 'Albanian National Team', 'June', 'Albania', 'match', 'Yugoslavia', 'Albania', 'FIFA', 'June convention', 'member', 'UEFA'] ['Echosmith', 'indie pop band', 'February', 'Chino', 'California', 'quartet', 'sibling', 'band', 'Sydney', 'Noah', 'Graham Sierota', 'departure', 'sibling Jamie', 'Echosmith', 'Warner Bros. s Records', 'May', 'hit song', 'Cool Kids', 'number', 'RIAA', 'sale', 'United States', 'ARIA', 'Australia', 'song', 'Warner Bros.', 's', 'Records selling song', 'download', 'band debut album', 'Dreams', 'October'] ['woman college', 'Southern United States', 'bachelor degree', 'institution', 'art college', 'student population', 'woman', 'Southern United States', 'girl seminary', 'academy', 'Salem College', 'institution', 'South Wesleyan College', 'college', 'woman', 'school', 'Mary Baldwin University', 'Salem College', 'course', 'graduate level'] ['First Arthur County Courthouse', 'Jail', 'court house', 'United States', 'museum'] ['Arthur Magazine', 'periodical', 'Philadelphia', 'century', 'T.S. Arthur', 'work', 'Edgar A. Poe', 'J.H. Ingraham', 'Sarah Josepha Hale', 'Thomas G. Spear', 'other', 'May', 'Godey Lady Book'] ['Ukrainian Hockey Championship', 'season', 'Ukrainian Hockey Championship', 'team', 'league', 'instability', 'Ukraine', 'club', 'issue', 'Generals Kiev', 'team', 'league', 'season', 'year end', 'season', 'round', 'team', 'semifinal', 'final', 'ATEK Kiev', 'season winner', 'HK Kremenchuk'] ['woman', 'woman magazine', 'Bauer Media Group', 'USA', 'magazine', 'Englewood Cliffs', 'New Jersey', 'circulation', 'magazine', 'copy'] ['Freeway Complex Fire', 'wildfire', 'Santa Ana Canyon area', 'Orange County', 'California', 'fire', 'fire', 'November', 'Freeway Fire', 'a.m.', 'Landfill Fire', 'fire', 'residence', 'Anaheim Hills', 'Yorba Linda'] ['William Rast', 'clothing line', 'Justin Timberlake', 'Trace Ayala', 'premium jean', 'October', 'Justin Timberlake Trace Ayala', 'fashion show', 'William Rast clothing line', 'label', 'clothing item', 'jacket', 'top', 'company', 'denim line', 'man woman clothing line']\n",
      "[\"Radio City is India's first private FM radio station and was started on 3 July 2001. <s>  Radio City broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where Radio City was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). <s>  It plays Hindi, English and regional songs. <s>  It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. <s>  Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. <s>  Radio City currently plays a mix of Hindi and Regional music. <s>  Abraham Thomas is the CEO of the company.\", \"Football in Albania existed before the Albanian Football Federation (FSHF) was created. <s>  This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) . <s>  Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play Albania first international match and then defeated Yugoslavia in 1946. <s>  In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.\", 'Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California. <s>  Originally formed as a quartet of siblings, an American, Corporate indie pop band formed in February 2009 in Chino, California currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016. <s>  Echosmith started first as \"Ready Set Go!\" <s>  until they signed to Warner Bros. <s>  Records in May 2012. <s>  they are best known for they hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia. <s>  The song was Warner Bros. <s>  Warner Bros. <s>  Records in May 2012 fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold. <s>  The band\\'s debut album, \"Talking Dreams\", was released on October 8, 2013.', \"Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States. <s>  Many started first as girls' seminaries or academies. <s>  Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women. <s>  Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.\", 'The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.', 'Arthur\\'s Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century. <s>  Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. <s>  In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\".', 'The 2014–15 Ukrainian Hockey Championship was the 23rd season of The 2014–15 Ukrainian Hockey Championship. <s>  Only four teams participated in The 2014–15 Ukrainian Hockey Championship the 23rd season of the Ukrainian Hockey Championship, because of the instability in Ukraine and that most of the clubs had economical issues. <s>  Generals Kiev was the only team that participated in The 2014–15 Ukrainian Hockey Championship the previous season, and the season started first after the year-end of 2014. <s>  The regular season included just 12 rounds, where all the teams went to the semifinals. <s>  In the final, Generals Kiev defeated the regular season winner HK Kremenchuk.', \"First for Women is a woman's magazine published by Bauer Media Group in the USA. <s>  a woman's magazine published by Bauer Media Group in the USA was started in 1989. <s>  a woman's magazine published by Bauer Media Group in the USA is based in Englewood Cliffs, New Jersey. <s>  In 2011 the circulation of a woman's magazine published by Bauer Media Group in the USA was 1,310,696 copies.\", 'The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California. <s>  The Freeway Complex Fire started as two separate fires on November 15, 2008. <s>  The Freeway Complex Fire started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later. <s>  two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.', 'William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. <s>  William Rast is most known for their premium jeans. <s>  On October 17, 2006, Justin Timberlake and Trace Ayala put on Justin Timberlake and Trace Ayala first fashion show to launch Justin Timberlake and Trace Ayala new William Rast clothing line. <s>  The label also produces other clothing items such as jackets and tops. <s>  The label started first as a denim line, later evolving into a men’s and women’s clothing line.']\n",
      "[\"Radio City is India's first private FM radio station and was started on 3 July 2001. <s>  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). <s>  It plays Hindi, English and regional songs. <s>  It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. <s>  Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. <s>  The Radio station currently plays a mix of Hindi and Regional music. <s>  Abraham Thomas is the CEO of the company.\", \"Football in Albania existed before the Albanian Football Federation (FSHF) was created. <s>  This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) . <s>  Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946. <s>  In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.\", 'Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California. <s>  Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016. <s>  Echosmith started first as \"Ready Set Go!\" <s>  until they signed to Warner Bros. <s>  Records in May 2012. <s>  They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia. <s>  The song was Warner Bros. <s>  Records\\' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold. <s>  The band\\'s debut album, \"Talking Dreams\", was released on October 8, 2013.', \"Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States. <s>  Many started first as girls' seminaries or academies. <s>  Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women. <s>  Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.\", 'The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.', 'Arthur\\'s Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century. <s>  Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. <s>  In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\".', 'The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship. <s>  Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues. <s>  Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014. <s>  The regular season included just 12 rounds, where all the teams went to the semifinals. <s>  In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.', \"First for Women is a woman's magazine published by Bauer Media Group in the USA. <s>  The magazine was started in 1989. <s>  It is based in Englewood Cliffs, New Jersey. <s>  In 2011 the circulation of the magazine was 1,310,696 copies.\", 'The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California. <s>  The fire started as two separate fires on November 15, 2008. <s>  The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later. <s>  These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.', 'William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. <s>  It is most known for their premium jeans. <s>  On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line. <s>  The label also produces other clothing items such as jackets and tops. <s>  The company started first as a denim line, later evolving into a men’s and women’s clothing line.']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fb81294b2f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"small.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mjson_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_hotpot_to_squad_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-bde3c503169e>\u001b[0m in \u001b[0;36mconvert_hotpot_to_squad_format\u001b[0;34m(json_dict, gold_paras_only)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoref_resolved_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" <p> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" <p> \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoref_resolved_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[0:16]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[17:30]' > small_dev.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[31:50]' > sample.json\n",
    "\n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "# !pip install jdc \n",
    "# !pip install test-tube \n",
    "# !conda install ipywidgets --yes\n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "\n",
    "from longformer.longformer import Longformer\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "            print(f'reading file: {self.file_path}')\n",
    "            self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "#             print(self.data_json[0])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "        print(tokenizer.all_special_tokens)\n",
    "        print(tokenizer.all_special_ids)\n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =  \\\n",
    "                {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     106,
     122,
     147,
     162
    ]
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "#                 print(\"question text: \", question_text)  \n",
    "                sp_sent = qa[\"is_sup_fact\"]\n",
    "                sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                \n",
    "                p_list = list(locate(doc_tokens , lambda x: x == \"<p>\")) \n",
    "                assert(len(p_list) == len(sp_para))\n",
    "                s_list = list(locate(doc_tokens , lambda x: x == \"<s>\"))\n",
    "#                 \n",
    "#                 if(len(s_list) + len(p_list) != len(sp_sent)):\n",
    "#                     print(\"len(s_list):\", len(s_list))\n",
    "#                     print(\"len(p_list):\", len(p_list))\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent)\n",
    "#                     print(\"paragraph_text\", paragraph_text)\n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    "                assert(len(s_list) + len(p_list) == len(sp_sent) )\n",
    "                \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                answer_spans = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "#                     print(\"orig_answer_text: \", orig_answer_text)\n",
    "                    answer_start = answer[\"answer_start\"]\n",
    "                    answer_end = answer[\"answer_end\"]  \n",
    "                    if(answer_start >= 0 and answer_end > 0):\n",
    "                        try:\n",
    "                            start_word_position = char_to_word_offset[answer_start]\n",
    "                            end_word_position = char_to_word_offset[answer_end-1]\n",
    "#                             print(\"answer by start_word_position and end_word_position: \", doc_tokens[start_word_position: end_word_position+1])\n",
    "                        except:\n",
    "                            print(f'error: Reading example {idx} failed')\n",
    "                            start_word_position = -3\n",
    "                            end_word_position = -3\n",
    "                            \n",
    "                    else:\n",
    "                        start_word_position = answer[\"answer_start\"]\n",
    "                        end_word_position = answer[\"answer_end\"]\n",
    "                    answer_spans.append({'start': start_word_position, 'end': end_word_position})\n",
    "\n",
    "                    \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                \n",
    "                # each original token in the context is tokenized to multiple sub_tokens\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "                \n",
    "                # all sub tokens, truncate up to limit\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len-3]\n",
    "\n",
    "                # The -3 accounts for [CLS], [q], [/q]  \n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 3\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [\"<cls>\"] + [\"<q>\"] + query_tokens + [\"</q>\"] + doc_slice_tokens   \n",
    "#                     print(\"tokens: \", tokens)\n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] *  len(doc_slice_tokens) \n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len\n",
    "\n",
    "                    # ===== answer positions tensors  ============\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    "                    q_type = None\n",
    "                    assert(len(answer_spans) > 0)\n",
    "                    for answer_span in answer_spans:\n",
    "                        start_position = answer_span['start']   # reletive to context\n",
    "                        end_position = answer_span['end']\n",
    "                        if(start_position >= 0):\n",
    "                            tok_start_position_in_doc = orig_to_tok_index[start_position]  # sub_tokens postion reletive to context\n",
    "                            not_end_of_doc = int(end_position + 1 < len(orig_to_tok_index))\n",
    "                            tok_end_position_in_doc = orig_to_tok_index[end_position + not_end_of_doc] - not_end_of_doc\n",
    "                            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                                assert(\"this answer is outside the current slice\")   # only has one slice with the large negative doc_stride\n",
    "                                continue                                \n",
    "                            start_positions.append(tok_start_position_in_doc + doc_offset)   # sub_tokens postion reletive to begining of all the tokens, including query sub tokens  \n",
    "                            end_positions.append(tok_end_position_in_doc + doc_offset)\n",
    "#                             print(\"answer by start_positions and end_positions: \", tokens[tok_start_position_in_doc + doc_offset: tok_end_position_in_doc + doc_offset+1])\n",
    "                            if(q_type != None and q_type != 0):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 0\n",
    "                \n",
    "                        elif(start_position == -1):\n",
    "                            if(q_type != None and q_type != 1):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 1\n",
    "                            start_positions.append(-1)  # -1 is the IGNORE_INDEX, will be ignored\n",
    "                            end_positions.append(-1)     \n",
    "                        elif(start_position == -2):\n",
    "                            if(q_type != None and q_type != 2):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 2\n",
    "                            start_positions.append(-1)\n",
    "                            end_positions.append(-1)     \n",
    "                        else:\n",
    "                            assert(\"unknown start_positions\")\n",
    "                            continue\n",
    "                    assert len(start_positions) == len(end_positions)\n",
    "                    \n",
    "                    \n",
    "                    if self.ignore_seq_with_no_answers and len(start_positions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "\n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                if (input_ids_list is None):\n",
    "                    print(\"input_ids_list is None\")\n",
    "                if (input_mask_list is None):\n",
    "                    print(\"input_mask_list is None\")\n",
    "                if (segment_ids_list is None):\n",
    "                    print(\"segment_ids_list is None\")\n",
    "                if (start_positions_list is None):\n",
    "                    print(\"start_positions_list is None\")\n",
    "                if (end_positions_list is None):\n",
    "                    print(\"end_positions_list is None\")\n",
    "                if (q_type_list is None):\n",
    "                    print(\"q_type_list is None\")\n",
    "                if (sp_sent_list is None):\n",
    "                    print(\"sp_sent_list is None\")\n",
    "                if (sp_para_list is None):\n",
    "                    print(\"sp_para_list is None\")\n",
    "                if (qa['id'] is None):\n",
    "                    print(\"qa['id'] is None\")\n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                      torch.tensor([sp_sent_list]),  torch.tensor([sp_para_list]),\n",
    "                                     qa['id']))    \n",
    "#                 tensors_list.append((doc_tokens))\n",
    "        return tensors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 1  # qids  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7f8d9c59ce10>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<cls>\", \"<p>\", \"<q>\", \"</q>\"]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        \n",
    "        self.dense_type = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 3)   #  question type (yes/no/span) classification \n",
    "        self.dense_sp_sent = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_sp_sent = torch.nn.Linear(self.model.config.hidden_size, 1)    \n",
    "        self.dense_sp_para = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_sp_para = torch.nn.Linear(self.model.config.hidden_size, 1) \n",
    "        self.train_dataloader_object = self.val_dataloader_object  = None  # = self.test_dataloader_object = None\n",
    "    \n",
    "    def load_model(self):\n",
    "#         model = Longformer.from_pretrained(self.args.model_path)\n",
    "        model = Longformer.from_pretrained('longformer-base-4096')\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "#%%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "###################################################### dataloaders ########################################################### \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    " \n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl\n",
    "        return self.train_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    # @pl.data_loader\n",
    "    # def test_dataloader(self):\n",
    "    #     if self.test_dataloader_object is not None:\n",
    "    #         return self.test_dataloader_object\n",
    "    #     dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "    #                               max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "    #                               doc_stride=self.args.doc_stride,\n",
    "    #                               max_num_answers=self.args.max_num_answers,\n",
    "    #                               max_question_len=self.args.max_question_len,\n",
    "    #                               ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "    #     dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "    #                     num_workers=self.args.num_workers, sampler=None,\n",
    "    #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "    #     self.test_dataloader_object = dl\n",
    "    #     return self.test_dataloader_object\n",
    "\n",
    "#%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para):\n",
    "#         print(\"input_ids: \" + str(input_ids)) \n",
    "#         print(\"attention_mask: \" + str(attention_mask)) \n",
    "#         print(\"segment_ids: \" + str(segment_ids)) \n",
    "#         print(\"start_positions: \" + str(start_positions)) \n",
    "#         print(\"end_positions: \" + str(end_positions)) \n",
    "        print(\"q_type: \" + str(q_type))\n",
    "#         print(\"sp_sent: \" + str(sp_sent)) \n",
    "#         print(\"sp_para: \" + str(sp_para)) \n",
    "        print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "        print(\"size of attention_mask: \" + str(attention_mask.size()))\n",
    "        print(\"size of segment_ids: \" + str(segment_ids.size()))\n",
    "        print(\"size of start_positions: \" + str(start_positions.size()))\n",
    "        print(\"size of end_positions:\" + str(end_positions.size()))\n",
    "        print(\"size of q_type:\" + str(q_type.size()))\n",
    "        print(\"size of sp_sent: \" + str(sp_sent.size()))\n",
    "        print(\"size of sp_para: \" + str(sp_para.size())) \n",
    "#         if(input_ids.size(0) > 1):\n",
    "#             assert(\"multi rows per document\")\n",
    "        # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "        # Make sure all rows have the same question length.\n",
    "        \n",
    "#         size of input_ids: torch.Size([1, 1495])\n",
    "#         size of attention_mask: torch.Size([1, 1495])\n",
    "#         size of segment_ids: torch.Size([1, 1495])\n",
    "#         size of start_positions: torch.Size([1, 64])   # multiple occurences of the same answer string, -1 padding up to self.max_num_answers\n",
    "#         size of end_positions: torch.Size([1, 64])\n",
    "#         size of q_type: torch.Size([1, 1])\n",
    "#         size of sp_sent: torch.Size([1, 40])           # number of sentences in context\n",
    "#         size of sp_para: torch.Size([1, 10])\n",
    "#         print(\"input tokens: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "#         print(\"sp_para: \" + str(sp_para)) \n",
    "#         print(\"sp_sent: \" + str(sp_sent)) \n",
    "#         print(\"sp_sent_index: \" + str(torch.where(sp_sent.squeeze())[0].tolist()))\n",
    "        # sp_para: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]], device='cuda:0')\n",
    "        # sp_sent: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
    "        #          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]], device='cuda:0')\n",
    "        # sp_sent_index: [14, 17, 18, 35, 36]\n",
    "\n",
    "        # local attention everywhere\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "        # global attention for the cls and all question tokens\n",
    "        question_end_index = self._get_special_index(input_ids, [\"</q>\"])\n",
    "#         if(question_end_index.size(0) == 1):\n",
    "#             attention_mask[:,:question_end_index.item()] = 2  \n",
    "#         else:\n",
    "        attention_mask[:,:question_end_index[0].item()] = 2  # from <cls> until </q>\n",
    "#             print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "        \n",
    "        # global attention for the sentence and paragraph special tokens  \n",
    "        sent_indexes = self._get_special_index(input_ids, [\"<p>\", \"<s>\"])\n",
    "        attention_mask[:, sent_indexes] = 2\n",
    "#         p_index = self._get_special_index(input_ids, [\"<p>\"])\n",
    "#         print(\"size of p_index: \" + str(p_index.size()))\n",
    "#         attention_mask[:, p_index] = 2 \n",
    "#         s_index = self._get_special_index(input_ids, [\"<s>\"])\n",
    "#         print(\"size of s_index: \" + str(s_index.size()))\n",
    "#         attention_mask[:, s_index] = 2\n",
    "        \n",
    "#         print(\"p_index:\", p_index) \n",
    "#         print(\"attention_mask: \", attention_mask)\n",
    "        \n",
    "\n",
    "        # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "        input_ids, attention_mask = pad_to_window_size(\n",
    "            input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "\n",
    "        sequence_output = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask)[0]\n",
    "        print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "\n",
    "        # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "        # before computing loss and decoding.\n",
    "        padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "        if padding_len > 0:\n",
    "            sequence_output = sequence_output[:, :-padding_len]\n",
    "        print(\"size of sequence_output after removing padding: \" + str(sequence_output.size()))\n",
    "              \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "#         print(\"size of logits: \" + str(logits.size())) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "#         print(\"size of start_logits: \" + str(start_logits.size())) \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "#         print(\"size of start_logits after squeeze: \" + str(start_logits.size())) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    " \n",
    "        ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "#         print(\"size of sequence_output[:,0]: \" + str(sequence_output[:,0].size()))\n",
    "        type_logits = self.linear_type(sequence_output[:,0])\n",
    "        print(\"size of type_logits: \" + str(type_logits.size()))\n",
    "        \n",
    "        ### 3. supporting paragraph classification ### \n",
    "        p_index = self._get_special_index(input_ids, [\"<p>\"])\n",
    "        print(\"size of p_index: \" + str(p_index.size()))\n",
    "        sp_para_output = sequence_output[:,p_index,:]\n",
    "        print(\"size of sp_para_output: \" + str(sp_para_output.size()))      \n",
    "        sp_para_output_t = self.linear_sp_para(sp_para_output)\n",
    "#         print(\"size of sp_para_output_t: \" + str(sp_para_output_t.size()))  \n",
    "\n",
    "         # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no. \t\n",
    "        # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "        sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "        predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous() \n",
    " \n",
    "        ### 4. supporting fact classification ###     \n",
    "        # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    "        \n",
    "#         sent_indexes = torch.sort(torch.cat((s_index, p_index)))[0] # torch.sort returns a 'torch.return_types.sort' object has 2 items: values, indices\n",
    "#         print(\"size of sent_indexes: \" + str(sent_indexes.size()))\n",
    "        print(\"sent_indexes: \", sent_indexes)\n",
    "        sp_sent_output = sequence_output[:,sent_indexes,:]\n",
    "#         print(\"size of sp_sent_output: \" + str(sp_sent_output.size()))      \n",
    "        sp_sent_output_t = self.linear_sp_sent(sp_sent_output)\n",
    "#         print(\"size of sp_sent_output_t: \" + str(sp_sent_output_t.size()))       \n",
    "        sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "        predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous() \n",
    "        \n",
    "        outputs = (start_logits, end_logits, type_logits, sp_para_output_t, sp_sent_output_t)  \n",
    "        #outputs = (torch.sigmoid(start_logits), torch.sigmoid(end_logits), torch.sigmoid(type_logits), torch.sigmoid(sp_para_output_t), torch.sigmoid(sp_sent_output_t))  \n",
    "        answer_loss, type_loss, sp_para_loss, sp_sent_loss  = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)\n",
    "#         print(\"answer_loss: \" + str(answer_loss))\n",
    "#         print(\"type_loss: \" + str(type_loss))\n",
    "#         print(\"sp_para_loss: \" + str(sp_para_loss))\n",
    "#         print(\"sp_sent_loss: \" + str(sp_sent_loss))\n",
    "        outputs = (answer_loss, type_loss, sp_para_loss, sp_sent_loss,) + outputs    \n",
    "        return outputs\n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    "#                 print(\"start_positions: \" + str(start_positions)) \n",
    "#                 print(\"start_loss: \" + str(start_loss)) \n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "#                 print(\"end_positions: \" + str(end_positions)) \n",
    "#                 print(\"end_loss: \" + str(end_loss)) \n",
    "\n",
    "#                 binary_loss = torch.nn.BCELoss()\n",
    "# #                 print(\"sp_para_output_t.squeeze().type(): \", sp_para_output_t.squeeze().type())\n",
    "# #                 print(\"sp_para.to(dtype=torch.half, device=sp_para.device).type(): \", sp_para.to(dtype=torch.half, device=sp_para.device).type())\n",
    "#                 sp_para_loss = binary_loss(sp_para_output_t.squeeze(), sp_para.squeeze().to(dtype=torch.half, device=sp_para.device))\n",
    "#                 sp_sent_loss = binary_loss(sp_sent_output_t.squeeze(), sp_sent.squeeze().to(dtype=torch.half, device=sp_sent.device))\n",
    "                \n",
    "#                 sp_para_loss = torch.tensor([0.0], device = predict_support_para.device )\n",
    "# #                 print(\"predict_support_para.squeeze(): \", predict_support_para.squeeze())\n",
    "# #                 print(\"sp_para.squeeze(): \", sp_para.squeeze())\n",
    "#                 for para_predict, para_gold in zip(predict_support_para.squeeze(), sp_para.squeeze()):\n",
    "# #                     print(\"para_predict.unsqueeze(0): \", para_predict.unsqueeze(0))\n",
    "# #                     print(\" para_gold.unsqueeze(0): \",  para_gold.unsqueeze(0))\n",
    "\n",
    "                # only one example per batch\n",
    "    \n",
    "#                 print(\"size of sp_para_output_t: \" + str(sp_para_output_t.size()))      \n",
    "#                 print(\"size of sp_sent_output_t: \" + str(sp_sent_output_t.size()))  \n",
    "\n",
    "            else: \n",
    "                start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = crossentropy(start_logits, start_positions[:, 0])\n",
    "                end_loss = crossentropy(end_logits, end_positions[:, 0])\n",
    "                \n",
    "\n",
    "            crossentropy = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            type_loss_or_softmax_cross_entropy = self.or_softmax_cross_entropy_loss_one_doc(type_logits, q_type.unsqueeze(0), ignore_index=-1)\n",
    "            type_loss = crossentropy(type_logits, q_type) \n",
    "            print(\"type_loss_or_softmax_cross_entropy: \", type_loss_or_softmax_cross_entropy)\n",
    "            print(\"type_loss: \", type_loss) \n",
    "            \n",
    "            crossentropy_average = torch.nn.CrossEntropyLoss(reduction = 'mean', ignore_index=-1)     \n",
    "# #         print(\"predict_support_para.view(-1, 2).size()\", predict_support_para.view(-1, 2).size())\n",
    "# #         print(\"sp_para.view(-1).size()\", sp_para.view(-1).size()) \n",
    "            sp_para_loss = crossentropy_average(predict_support_para.view(-1, 2), sp_para.view(-1))\n",
    "            sp_sent_loss = crossentropy_average(predict_support_sent.view(-1, 2), sp_sent.view(-1))      \n",
    "                \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss, type_loss, sp_para_loss, sp_sent_loss  \n",
    "\n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_tokens):\n",
    "        assert(input_ids.size(0)==1) \n",
    "        mask = input_ids != input_ids # initilaize \n",
    "        for special_token in special_tokens:\n",
    "            mask = torch.logical_or(mask, input_ids.eq(self.tokenizer.convert_tokens_to_ids(special_token))) \n",
    "#             print(\"mask: \", mask)\n",
    "        token_indices = torch.nonzero(mask)    \n",
    "         \n",
    "        ### FOR DEBUG ###\n",
    "        # input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "        # token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#         assert logits.ndim == 2\n",
    "#         assert target.ndim == 2\n",
    "#         assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    "#         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "#         print(\"logits: \" + str(logits)) \n",
    "#         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "#         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#         print(\"target: \" + str(target)) \n",
    "\n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#         print(\"masked_target: \" + str(masked_target))     \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "#         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#         print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#         print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "        \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    "#         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#         print(\"log_score: \" + str(log_score)) \n",
    "        # denominator = log(sum(exp(logits)))\n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#         print(\"log_norm: \" + str(log_norm)) \n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm)\n",
    "#         print(\"loss: \" + str(loss))\n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "#         loss = torch.tanh(loss)\n",
    "#         print(\"final loss: \" + str(loss)) \n",
    "        return loss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.nonzero(sp_sent, as_tuple=True)[1]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2\n",
    "\n",
    "\n",
    "# import torch\n",
    "# special_tokens = [1,2]\n",
    "# input_ids = torch.tensor([[ 1, 0, 2, 1, 0, 2]])\n",
    "\n",
    "# mask = input_ids != input_ids # initilaize \n",
    "# for special_token in special_tokens:\n",
    "#     mask = torch.logical_or(mask, input_ids.eq(special_token)) \n",
    "#     print(\"mask: \", mask)\n",
    "# torch.nonzero(mask)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # print(tokenizer.all_special_tokens)    \n",
    "# # # print(tokenizer.all_special_ids)     \n",
    "# # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # tokenizer.sep_token\n",
    "\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    self.scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)  # scheduler is not saved in the checkpoint, but global_step is, which is enough to restart\n",
    "    self.scheduler.step(self.global_step)\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimizer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook to do a lot of non-standard training tricks such as learning-rate warm-up\n",
    "def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.scheduler.step(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook\n",
    "def on_epoch_start(self):\n",
    "    print(\"Start epoch \", self.current_epoch)\n",
    "    \n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_idx):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch \n",
    "    print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    print(\"answer_loss: \", answer_loss)\n",
    "    print(\"type_loss: \", type_loss)\n",
    "    print(\"sp_para_loss: \", sp_para_loss)\n",
    "    print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "    \n",
    "#     loss  = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    loss = answer_loss + type_loss + sp_para_loss + sp_sent_loss\n",
    "#     print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it. \n",
    "    if(q_type == 1 or q_type == 2 ):\n",
    "        print(\"answer_loss of q_type == 1 or q_type == 2: \", answer_loss)\n",
    "    print(\"lr: \", lr)    # lr will increading over time\n",
    "    tensorboard_logs = {'train_answer_loss': answer_loss, 'train_type_loss': type_loss, 'train_sp_para_loss': sp_para_loss, 'train_sp_sent_loss': sp_sent_loss, \n",
    "                        'lr': lr,\n",
    "                        'input_size': torch.tensor(input_ids.numel()).type_as(loss) ,\n",
    "                        'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) }\n",
    "    \n",
    "    if(self.current_epoch == self.args.epochs-1):\n",
    "        print(\"training_step \", batch_idx)\n",
    "                            \n",
    "    return {'loss': loss, 'log': tensorboard_logs}  # It is necessary that the output dictionary contains the loss key. This is the minimum requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "# # the function is called after every epoch is completed\n",
    "# def training_end(self, outputs):\n",
    "#     print(\"self.current_epoch: \", self.current_epoch)\n",
    "\n",
    " \n",
    "\n",
    "#     # calculating average loss  \n",
    "#     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "#     if self.trainer.use_ddp:\n",
    "#         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_loss /= self.trainer.world_size \n",
    "        \n",
    "#     epoch_dictionary={\n",
    "#         'loss': avg_loss # required \n",
    "#     }\n",
    "\n",
    "#     return epoch_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch\n",
    "    print(\"validation_step\")\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    print(\"answer_loss: \" + str(answer_loss))\n",
    "\n",
    "    answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "    print(\"answers_pred: \" + str(answers_pred))\n",
    "    \n",
    "    # answers_pred only contains the top one predicted answer['text', 'score']\n",
    "#     answers_pred = sorted(answers_pred, key=lambda x: x['score'], reverse=True)[0:1] # each batch is one document\n",
    "#     print(\"answers_pred after sorted: \" + str(answers_pred))\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "    answers_pred = answers_pred[0]\n",
    "\n",
    "    answer_score = answers_pred['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    print(\"pred answer_score: \" + str(answer_score))\n",
    "    \n",
    "    print(\"pred answer_text: \" + str(answers_pred['text'])) \n",
    "\n",
    "    if(q_type == 1):\n",
    "        answer_gold = 'yes'\n",
    "    elif(q_type == 2):\n",
    "        answer_gold = 'no' \n",
    "    else:\n",
    "        # even though there can be multiple gold start_postion (subword_start) and end_position(subword_end), the corresponing answer string are same\n",
    "        answer_gold_token_ids = input_ids[0, subword_starts[0][0]: subword_ends[0][0] + 1]\n",
    "        print(\"answer_gold_token_ids: \" + str(answer_gold_token_ids))\n",
    "        answer_gold_tokens = self.tokenizer.convert_ids_to_tokens(answer_gold_token_ids.tolist())\n",
    "        print(\"answer_gold_tokens: \" + str(answer_gold_tokens))\n",
    "        answer_gold = self.tokenizer.convert_tokens_to_string(answer_gold_tokens)\n",
    "    print(\"answer_gold: \" + str(answer_gold))\n",
    " \n",
    "    f1, prec, recall = self.f1_score(answers_pred['text'], answer_gold)\n",
    "    em = self.exact_match_score(answers_pred['text'], answer_gold) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"prec: \" + str(prec))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"em: \" + str(em)) \n",
    "\n",
    "    if(len(sp_sent_pred) > 0):\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "        sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "        sp_sent_precision = torch.tensor(sp_sent_precision).type_as(loss)\n",
    "        sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "        sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "        \n",
    "        #         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "#         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "#         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "#         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "        \n",
    "        joint_prec = prec * sp_sent_precision\n",
    "        joint_recall = recall * sp_sent_recall\n",
    "        if joint_prec + joint_recall > 0:\n",
    "            joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "        else:\n",
    "            joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "        joint_em = em * sp_sent_em \n",
    "\n",
    "    else:\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "        joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "         \n",
    "    print(\"return\") \n",
    "#     return {'qids': [qids], 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "#             'answer_score': [answer_score], 'f1': [f1], 'prec':[prec], 'recall':[recall], 'em': [em],\n",
    "#             'sp_em': [sp_sent_em], 'sp_f1': [sp_sent_f1], 'sp_prec': [sp_sent_precision], 'sp_recall': [sp_sent_recall],\n",
    "#             'joint_em': [joint_em], 'joint_f1': [joint_f1], 'joint_prec': [joint_prec], 'joint_recall': [joint_recall]}\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "                'answer_score': answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em,\n",
    "                'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "                'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits):\n",
    "    print(\"decode\")\n",
    "    \n",
    "    question_end_index = self._get_special_index(input_ids, [\"</q>\"])\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "    \n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "#     print(\"start_logits_indices: \", start_logits_indices)\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#     print(\"p_type: \", p_type)\n",
    "#     print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#         print(\"sorted_answers: \" + str(sorted_answers))\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#             score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            score = (torch.sigmoid(answer['start_logit']) + torch.sigmoid(answer['end_logit']) + torch.sigmoid(p_type_score)) / 3\n",
    "            answers.append({'text': text, 'score': score})\n",
    "            print(\"answers: \" + str(answers))\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score})\n",
    "    else:\n",
    "        assert False \n",
    "\n",
    "    p_index = self._get_special_index(input_ids, [\"<p>\"])\n",
    "# #     print(\"p_index: \" + str(p_index))\n",
    "#     s_index = self._get_special_index(input_ids, [\"<s>\"])\n",
    "# #     print(\"s_index: \" + str(s_index))\n",
    "#     sent_indexes = torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "    sent_indexes = self._get_special_index(input_ids, [\"<p>\", \"<s>\"])\n",
    "    \n",
    "    s_to_p_map = []\n",
    "    for s in sent_indexes:\n",
    "        s_to_p = torch.where(torch.le(p_index, s))[0][-1]     # last p_index smaller or equal to s\n",
    "        s_to_p_map.append(s_to_p.item()) \n",
    "#     print(\"s_to_p_map: \" + str(s_to_p_map))\n",
    "    \n",
    "#     print(\"sp_para_logits\", sp_para_logits)\n",
    "#     print(\"sp_sent_logits\", sp_sent_logits)\n",
    "\n",
    "#     print(\"sp_para_logits.squeeze().size(0): \", sp_para_logits.squeeze().size(0))\n",
    "#     print(\"sp_sent_logits.squeeze().size(0): \", sp_sent_logits.squeeze().size(0))\n",
    "    sp_para_top2 = sp_para_logits.squeeze().topk(k=2).indices\n",
    "    if(sp_sent_logits.squeeze().size(0) > 12):\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=12).indices\n",
    "    else:\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=sp_sent_logits.squeeze().size(0)).indices\n",
    "#     print(\"sp_para_top2\", sp_para_top2)\n",
    "#     print(\"sp_sent_top12\", sp_sent_top12)\n",
    "    \n",
    "    sp_sent_pred = set()\n",
    "    sp_para_pred = set()\n",
    "    for sp_sent in sp_sent_top12:\n",
    "        sp_sent_to_para = s_to_p_map[sp_sent.item()]\n",
    "        if sp_sent_to_para in sp_para_top2:\n",
    "            sp_sent_pred.add(sp_sent.item())\n",
    "            sp_para_pred.add(sp_sent_to_para) \n",
    "#     print(\"sp_sent_pred: \" + str(sp_sent_pred))\n",
    "#     print(\"sp_para_pred: \" + str(sp_para_pred))\n",
    "    return (answers, sp_sent_pred, sp_para_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def normalize_answer(self, s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = self.normalize_answer(prediction)\n",
    "    normalized_ground_truth = self.normalize_answer(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "    \n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def sp_metrics(self, prediction, gold):\n",
    "    print(\"prediction: \", prediction)\n",
    "    print(\"gold: \", gold)\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in prediction:\n",
    "        if e in gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "#             print(\"e: \", e)\n",
    "#             print(\"gold: \", gold)\n",
    "#             print(\"e not in gold!!!\")\n",
    "    for e in gold:\n",
    "        if e not in prediction:\n",
    "            fn += 1\n",
    "#             print(\"e: \", e)\n",
    "#             print(\"prediction: \", prediction)\n",
    "#             print(\"e not in prediction!!!\")\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    print(\"sp prec: \", prec)\n",
    "    print(\"sp recall: \", recall)\n",
    "    print(\"sp f1: \", f1)\n",
    "    print(\"sp em: \", em)\n",
    "    return em, prec, recall, f1 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_end(self, outputs):\n",
    "    print(\"validation_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "        \n",
    "#     string_qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "#     int_qids = [self.val_dataloader_object.dataset.val_qid_string_to_int_map[qid] for qid in string_qids]\n",
    "    answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "    f1_scores = [x['f1'] for x in outputs] # [item for sublist in outputs for item in sublist['f1']] #torch.stack([x['f1'] for x in outputs]).mean()  #\n",
    "    em_scores = [x['em'] for x in outputs] # [item for sublist in outputs for item in sublist['em']] #torch.stack([x['em'] for x in outputs]).mean()  #\n",
    "    prec_scores =  [x['prec'] for x in outputs] #[item for sublist in outputs for item in sublist['prec']] #torch.stack([x['prec'] for x in outputs]).mean()  #\n",
    "    recall_scores = [x['recall'] for x in outputs] #[item for sublist in outputs for item in sublist['recall']]  #torch.stack([x['recall'] for x in outputs]).mean()  #\n",
    "    \n",
    "    sp_sent_f1_scores = [x['sp_f1'] for x in outputs] #[item for sublist in outputs for item in sublist['sp_f1']] #torch.stack([x['sp_f1'] for x in outputs]).mean() #\n",
    "    sp_sent_em_scores = [x['sp_em'] for x in outputs] # [item for sublist in outputs for item in sublist['sp_em']] #torch.stack([x['sp_em'] for x in outputs]).mean() #\n",
    "    sp_sent_prec_scores = [x['sp_prec'] for x in outputs]  #[item for sublist in outputs for item in sublist['sp_prec']] #torch.stack([x['sp_prec'] for x in outputs]).mean() #\n",
    "    sp_sent_recall_scores = [x['sp_recall'] for x in outputs] # [item for sublist in outputs for item in sublist['sp_recall']]  #torch.stack([x['sp_recall'] for x in outputs]).mean() #\n",
    "     \n",
    "    joint_f1_scores = [x['joint_f1'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_f1']] #torch.stack([x['joint_f1'] for x in outputs]).mean() #\n",
    "    joint_em_scores = [x['joint_em'] for x in outputs]   # [item for sublist in outputs for item in sublist['joint_em']] #torch.stack([x['joint_em'] for x in outputs]).mean() #\n",
    "    joint_prec_scores = [x['joint_prec'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_prec']] #torch.stack([x['joint_prec'] for x in outputs]).mean() #\n",
    "    joint_recall_scores = [x['joint_recall'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_recall']] #torch.stack([x['joint_recall'] for x in outputs]).mean() #     \n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_para_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "#         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "        joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "        joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "        joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        \n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    print(\"answer_scores: \", answer_scores)\n",
    "    print(\"f1_scores: \", f1_scores)\n",
    "    print(\"em_scores: \", em_scores)\n",
    "    \n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "    print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "        \n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    print(\"len(f1_scores): \", len(f1_scores))\n",
    "    print(\"avg_val_f1: \", avg_val_f1)\n",
    "    avg_val_em = sum(em_scores) / len(em_scores)\n",
    "#     print(\"len(em_scores): \", len(em_scores))\n",
    "    print(\"avg_val_em: \", avg_val_em)\n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)\n",
    "#     print(\"len(prec_scores): \", len(prec_scores))\n",
    "    print(\"avg_val_prec: \", avg_val_prec)\n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores) \n",
    "#     print(\"len(recall_scores): \", len(recall_scores))\n",
    "    print(\"avg_val_recall: \", avg_val_recall)\n",
    "    \n",
    "    avg_val_sp_sent_f1 = sum(sp_sent_f1_scores) / len(sp_sent_f1_scores)\n",
    "    print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1)\n",
    "    avg_val_sp_sent_em = sum(sp_sent_em_scores) / len(sp_sent_em_scores)\n",
    "    print(\"avg_val_sp_sent_em: \", avg_val_sp_sent_em)\n",
    "    avg_val_sp_sent_prec = sum(sp_sent_prec_scores) / len(sp_sent_prec_scores)\n",
    "    print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec)\n",
    "    avg_val_sp_sent_recall = sum(sp_sent_recall_scores) / len(sp_sent_recall_scores) \n",
    "    print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)\n",
    "        \n",
    "    avg_val_joint_f1 = sum(joint_f1_scores) / len(joint_f1_scores)\n",
    "    print(\"avg_val_joint_f1: \", avg_val_joint_f1)\n",
    "    avg_val_joint_em = sum(joint_em_scores) / len(joint_em_scores)\n",
    "    print(\"avg_val_joint_em: \", avg_val_joint_em)\n",
    "    avg_val_joint_prec = sum(joint_prec_scores) / len(joint_prec_scores)\n",
    "    print(\"avg_val_joint_prec: \", avg_val_joint_prec)\n",
    "    avg_val_joint_recall = sum(joint_recall_scores) / len(joint_recall_scores) \n",
    "    print(\"avg_val_joint_recall: \", avg_val_joint_recall)\n",
    "     \n",
    "    \n",
    "    \n",
    "#     print(\"avg_loss: \", avg_loss)\n",
    "    \n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss, \n",
    "            'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall,\n",
    "            'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,\n",
    "            'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall\n",
    "           }\n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "#     answer_scores =  torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "#     f1_scores =  torch.stack([x['f1'] for x in outputs]).mean()  #\n",
    "#     em_scores =  torch.stack([x['em'] for x in outputs]).mean()  #\n",
    "#     prec_scores =  torch.stack([x['prec'] for x in outputs]).mean()  #\n",
    "#     recall_scores =  torch.stack([x['recall'] for x in outputs]).mean()  #\n",
    "    \n",
    "#     sp_sent_f1_scores =  torch.stack([x['sp_f1'] for x in outputs]).mean() #\n",
    "#     sp_sent_em_scores =  torch.stack([x['sp_em'] for x in outputs]).mean() #\n",
    "#     sp_sent_prec_scores =  torch.stack([x['sp_prec'] for x in outputs]).mean() #\n",
    "#     sp_sent_recall_scores =  torch.stack([x['sp_recall'] for x in outputs]).mean() #\n",
    "     \n",
    "#     joint_f1_scores =  torch.stack([x['joint_f1'] for x in outputs]).mean() #\n",
    "#     joint_em_scores =  torch.stack([x['joint_em'] for x in outputs]).mean() #\n",
    "#     joint_prec_scores =  torch.stack([x['joint_prec'] for x in outputs]).mean() #\n",
    "#     joint_recall_scores = torch.stack([x['joint_recall'] for x in outputs]).mean() #     \n",
    "\n",
    "#     return {'avg_val_loss': avg_loss}\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    loss, start_logits, end_logits = output[:3]\n",
    "    answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "    # each batch is one document\n",
    "    answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "    qids = [qids]\n",
    "    assert len(answers) == len(qids)\n",
    "    return {'qids': qids, 'answers': answers}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_end(self, outputs):\n",
    "    qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "    answers = [item for sublist in outputs for item in sublist['answers']]\n",
    "\n",
    "    qa_with_duplicates = defaultdict(list)\n",
    "    for qid, answer in zip(qids, answers):\n",
    "        qa_with_duplicates[qid].append({'answer_score': answer['score'], 'answer_text': answer['text'], })\n",
    "\n",
    "    qid_to_answer_text = {}\n",
    "    for qid, answer_metrics in qa_with_duplicates.items():\n",
    "        top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "        qid_to_answer_text[qid] = top_answer['answer_text']\n",
    "\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(qid_to_answer_text, f)\n",
    "\n",
    "    return {'count': len(qid_to_answer_text)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_load_from_state_dict',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'normalize_answer',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'sp_metrics',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_step',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_step',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T_destination', ~T_destination),\n",
       " ('__abstractmethods__', frozenset({'configure_optimizers', 'training_step'})),\n",
       " ('__annotations__',\n",
       "  {'dump_patches': bool,\n",
       "   '_version': int,\n",
       "   'training': bool,\n",
       "   'forward': typing.Callable[..., typing.Any],\n",
       "   '__call__': typing.Callable[..., typing.Any]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset({'configure_optimizers',\n",
       "                           'training_step'}),\n",
       "                '_abc_registry': <_weakrefset.WeakSet at 0x7f8d9c4f9e48>,\n",
       "                '_abc_cache': <_weakrefset.WeakSet at 0x7f8d9c4f9eb8>,\n",
       "                '_abc_negative_cache': <_weakrefset.WeakSet at 0x7f8d9c4f9f28>,\n",
       "                '_abc_negative_cache_version': 230,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'optimizer_step': <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>,\n",
       "                'on_epoch_start': <function __main__.on_epoch_start(self)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_idx)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>,\n",
       "                'normalize_answer': <function __main__.normalize_answer(self, s)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'sp_metrics': <function __main__.sp_metrics(self, prediction, gold)>,\n",
       "                'validation_end': <function __main__.validation_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'test_step': <function __main__.test_step(self, batch, batch_nb)>,\n",
       "                'test_end': <function __main__.test_end(self, outputs)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7f8d9c4f79e8>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'ABC' objects>),\n",
       " ('_abc_cache', <_weakrefset.WeakSet at 0x7f8d9c4f9eb8>),\n",
       " ('_abc_negative_cache', <_weakrefset.WeakSet at 0x7f8d9c4f9f28>),\n",
       " ('_abc_negative_cache_version', 230),\n",
       " ('_abc_registry', <_weakrefset.WeakSet at 0x7f8d9c4f9e48>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self:~T) -> ~T>),\n",
       " ('cuda',\n",
       "  <function torch.nn.modules.module.Module.cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self:~T) -> ~T>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self:~T) -> ~T>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self:~T) -> ~T>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method LightningModule.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method LightningModule.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('normalize_answer', <function __main__.normalize_answer(self, s)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start', <function __main__.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_idx)>),\n",
       " ('type',\n",
       "  <function torch.nn.modules.module.Module.type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self:~T) -> ~T>),\n",
       " ('cuda',\n",
       "  <function torch.nn.modules.module.Module.cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self:~T) -> ~T>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self:~T) -> ~T>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self:~T) -> ~T>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('normalize_answer', <function __main__.normalize_answer(self, s)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start', <function __main__.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_idx)>),\n",
       " ('type',\n",
       "  <function torch.nn.modules.module.Module.type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/u32/fanluo/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/u32/fanluo/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.tokenization_utils_base:Assigning ['<cls>', '<p>', '<q>', '</q>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <cls> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <p> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <q> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding </q> to the vocabulary\n",
      "INFO:transformers.configuration_utils:loading configuration file longformer-base-4096/config.json\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file longformer-base-4096/pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing Longformer.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of Longformer were initialized from the model checkpoint at longformer-base-4096.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use Longformer for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with config:\n",
      "RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hotpotqa(\n",
       "  (model): Longformer(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50269, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dense_type): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_type): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dense_sp_sent): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_sp_sent): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dense_sp_para): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_sp_para): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "    model.to('cuda')    # this is necessary to use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/callbacks/pt_callbacks.py:224: UserWarning: Checkpoint directory jupyter-hotpotqa/hotpotqa-longformer/checkpoints exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  f\"Checkpoint directory {filepath} exists and is not empty with save_top_k != 0.\"\n"
     ]
    }
   ],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_size:  16.0\n",
      "num_devices:  1\n",
      ">>>>>>> #train_set_size: 90447.0, #steps: 271341.0,  #warmup steps: 1000, #epochs: 6, batch_size: 2 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "    train_set_size = 16 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size) \n",
    "    \n",
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus is not \"\" else None\n",
    "    num_devices = len(args.gpus) #1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "    \n",
    "    train_set_size = 90447 * args.train_percent    # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps},  #warmup steps: {args.warmup}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:gpu available: True, used: True\n",
      "INFO:root:VISIBLE GPUS: 0\n",
      "INFO:root:using 16bit precision\n"
     ]
    }
   ],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                         track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                         accumulate_grad_batches=args.batch_size,\n",
    "                         train_percent_check = args.train_percent,\n",
    "#                          val_check_interval=args.val_every,\n",
    "                         val_percent_check=args.val_percent_check,\n",
    "                         test_percent_check=args.val_percent_check,\n",
    "                         logger=logger if not args.disable_checkpointing else False,\n",
    "                         checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                         show_progress_bar=args.no_progress_bar,\n",
    "                         use_amp=not args.fp32, amp_level='O1',\n",
    "                         check_val_every_n_epoch=1\n",
    "                         )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "INFO:root:set slurm handle signals\n",
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: \n",
      "            You're using multiple gpus and multiple nodes without using a DistributedSampler\n",
      "            to assign a subset of your data to each process. To silence this warning, pass a\n",
      "            DistributedSampler to your DataLoader.\n",
      "\n",
      "            ie: this:\n",
      "            dataset = myDataset()\n",
      "            dataloader = Dataloader(dataset)\n",
      "\n",
      "            becomes:\n",
      "            dataset = myDataset()\n",
      "            dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
      "            dataloader = Dataloader(dataset, sampler=dist_sampler)\n",
      "\n",
      "            If you want each process to load the full dataset, ignore this warning.\n",
      "            \n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: small.json\n",
      "['<mask>', '</s>', '<s>', '<p>', '<pad>', '<unk>', '</q>', '</s>', '<cls>', '<q>', '<s>']\n",
      "[50264, 2, 0, 50266, 1, 3, 50268, 2, 50265, 50267, 0]\n",
      "reading file: small_dev.json\n",
      "['<mask>', '</s>', '<s>', '<p>', '<pad>', '<unk>', '</q>', '</s>', '<cls>', '<q>', '<s>']\n",
      "[50264, 2, 0, 50266, 1, 3, 50268, 2, 50265, 50267, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:145: UserWarning: \n",
      "                    Your val_dataloader(s) don't use DistributedSampler.\n",
      "\n",
      "                    You're using multiple gpus and multiple nodes without using a\n",
      "                    DistributedSampler to assign a subset of your data to each process.\n",
      "                    To silence this warning, pass a DistributedSampler to your DataLoader.\n",
      "\n",
      "                    ie: this:\n",
      "                    dataset = myDataset()\n",
      "                    dataloader = Dataloader(dataset)\n",
      "\n",
      "                    becomes:\n",
      "                    dataset = myDataset()\n",
      "                    dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
      "                    dataloader = Dataloader(dataset, sampler=dist_sampler)\n",
      "\n",
      "                    If you want each process to load the full dataset, ignore this warning.\n",
      "                    \n",
      "  warnings.warn(msg)\n",
      "INFO:root:\n",
      "                                       Name               Type Params\n",
      "0                                     model         Longformer  148 M\n",
      "1                          model.embeddings  RobertaEmbeddings   41 M\n",
      "2          model.embeddings.word_embeddings          Embedding   38 M\n",
      "3      model.embeddings.position_embeddings          Embedding    3 M\n",
      "4    model.embeddings.token_type_embeddings          Embedding  768  \n",
      "..                                      ...                ...    ...\n",
      "242                             linear_type             Linear    2 K\n",
      "243                           dense_sp_sent             Linear  590 K\n",
      "244                          linear_sp_sent             Linear  769  \n",
      "245                           dense_sp_para             Linear  590 K\n",
      "246                          linear_sp_para             Linear  769  \n",
      "\n",
      "[247 rows x 3 columns]\n",
      "INFO:root:model and trainer restored from checkpoint: jupyter-hotpotqa/hotpotqa-longformer/checkpoints/_ckpt_epoch_4.ckpt\n",
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/ipykernel_launcher.py:294: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1660])\n",
      "size of attention_mask: torch.Size([1, 1660])\n",
      "size of segment_ids: torch.Size([1, 1660])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 57])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1660, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  24,   50,   81,  118,  130,  161,  201,  237,  269,  297,  356,  389,\n",
      "         415,  462,  480,  500,  520,  540,  562,  591,  620,  662,  678,  706,\n",
      "         754,  773,  782,  813,  839,  854,  883,  923,  955,  991, 1027, 1069,\n",
      "        1083, 1109, 1126, 1154, 1194, 1253, 1287, 1316, 1325, 1360, 1374, 1406,\n",
      "        1436, 1462, 1476, 1497, 1524, 1569, 1586, 1615, 1632], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1543, device='cuda:0')\n",
      "type_loss:  tensor(1.1545, device='cuda:0')\n",
      "answer_loss: tensor(6.7012, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1526], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1526], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {8, 10, 11, 12, 25, 26, 28, 31}\n",
      "gold:  [19, 39]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1011])\n",
      "size of attention_mask: torch.Size([1, 1011])\n",
      "size of segment_ids: torch.Size([1, 1011])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1011, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 52,  78, 111, 127, 144, 167, 192, 213, 234, 261, 276, 293, 321, 345,\n",
      "        369, 394, 428, 453, 480, 505, 521, 558, 572, 604, 620, 670, 697, 718,\n",
      "        736, 747, 766, 779, 783, 808, 834, 861, 885, 900, 928, 963, 980],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2227, device='cuda:0')\n",
      "type_loss:  tensor(1.2220, device='cuda:0')\n",
      "answer_loss: tensor(4.1562, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2817], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2817], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {0}\n",
      "gold:  [10, 33]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1187])\n",
      "size of attention_mask: torch.Size([1, 1187])\n",
      "size of segment_ids: torch.Size([1, 1187])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1187, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  31,   53,   72,   85,  110,  144,  174,  201,  230,  261,  272,  289,\n",
      "         344,  374,  401,  428,  443,  466,  503,  537,  558,  581,  631,  664,\n",
      "         678,  709,  764,  791,  816,  839,  879,  924,  941,  967,  983, 1008,\n",
      "        1047, 1066, 1111, 1131, 1163], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2500, device='cuda:0')\n",
      "type_loss:  tensor(1.2498, device='cuda:0')\n",
      "answer_loss: tensor(5.4551, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2118], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2118], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {11, 12, 13}\n",
      "gold:  [10, 11, 13]\n",
      "sp prec:  0.6666666666666666\n",
      "sp recall:  0.6666666666666666\n",
      "sp f1:  0.6666666666666666\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 933])\n",
      "size of attention_mask: torch.Size([1, 933])\n",
      "size of segment_ids: torch.Size([1, 933])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 37])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 933, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 21,  54,  68,  97, 157, 176, 190, 216, 263, 282, 315, 362, 398, 417,\n",
      "        441, 456, 474, 488, 510, 523, 543, 554, 578, 599, 616, 645, 675, 696,\n",
      "        709, 745, 757, 772, 805, 829, 849, 871, 913], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2715, device='cuda:0')\n",
      "type_loss:  tensor(1.2714, device='cuda:0')\n",
      "answer_loss: tensor(6.7871, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2783], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2783], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {32, 33, 10, 11}\n",
      "gold:  [8, 25]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 851])\n",
      "size of attention_mask: torch.Size([1, 851])\n",
      "size of segment_ids: torch.Size([1, 851])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 35])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 851, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 13,  34,  61,  75,  97, 116, 151, 185, 214, 237, 258, 295, 316, 353,\n",
      "        374, 385, 414, 433, 455, 501, 522, 550, 572, 590, 618, 645, 657, 683,\n",
      "        706, 720, 747, 761, 775, 797, 826], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2734, device='cuda:0')\n",
      "type_loss:  tensor(1.2739, device='cuda:0')\n",
      "answer_loss: tensor(6.0332, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1793], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1793], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {11, 12, 13}\n",
      "gold:  [16, 23]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_end\n",
      "before sync --> sizes:  5, 5, 5\n",
      "after sync --> sizes: 5, 5, 5\n",
      "answer_scores:  [0.152587890625, 0.28173828125, 0.2117919921875, 0.2783203125, 0.1793212890625]\n",
      "f1_scores:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "em_scores:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "avg_loss:  tensor(8.3086, device='cuda:0')\tavg_answer_loss:  tensor(5.8266, device='cuda:0')\tavg_type_loss:  tensor(1.2343, device='cuda:0')\tavg_sp_para_loss:  tensor(0.6861, device='cuda:0')\tavg_sp_sent_loss:  tensor(0.5616, device='cuda:0')\tlen(f1_scores):  5\n",
      "avg_val_f1:  0.0\n",
      "avg_val_em:  0.0\n",
      "avg_val_prec:  0.0\n",
      "avg_val_recall:  0.0\n",
      "avg_val_sp_sent_f1:  0.13333333730697633\n",
      "avg_val_sp_sent_em:  0.0\n",
      "avg_val_sp_sent_prec:  0.13333333730697633\n",
      "avg_val_sp_sent_recall:  0.13333333730697633\n",
      "avg_val_joint_f1:  0.0\n",
      "avg_val_joint_em:  0.0\n",
      "avg_val_joint_prec:  0.0\n",
      "avg_val_joint_recall:  0.0\n",
      "Start epoch  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1118])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of attention_mask: torch.Size([1, 1118])\n",
      "size of segment_ids: torch.Size([1, 1118])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 46])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1118, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  16,   36,   84,   95,  123,  167,  182,  192,  212,  264,  298,  327,\n",
      "         351,  387,  401,  410,  417,  475,  483,  507,  526,  571,  585,  615,\n",
      "         638,  666,  693,  732,  750,  770,  798,  829,  848,  868,  887,  896,\n",
      "         911,  927,  950,  965,  993, 1015, 1035, 1046, 1077, 1092],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2607, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.1152, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([1.9500e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of attention_mask: torch.Size([1, 1079])\n",
      "size of segment_ids: torch.Size([1, 1079])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 39])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1079, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  22,   50,   76,   91,  111,  119,  144,  171,  190,  231,  289,  322,\n",
      "         336,  349,  381,  398,  430,  457,  521,  552,  576,  594,  621,  644,\n",
      "         664,  681,  715,  742,  759,  801,  855,  887,  921,  931,  964,  979,\n",
      "         998, 1011, 1043], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2197, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.3828, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([1.9500e-06], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of attention_mask: torch.Size([1, 2078])\n",
      "size of segment_ids: torch.Size([1, 2078])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 68])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2560, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 2078, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  34,   52,   67,  103,  123,  156,  172,  209,  248,  284,  313,  344,\n",
      "         357,  391,  424,  459,  483,  518,  549,  583,  606,  624,  642,  675,\n",
      "         689,  733,  743,  766,  813,  834,  855,  870,  893,  903, 1028, 1059,\n",
      "        1087, 1103, 1115, 1157, 1179, 1210, 1242, 1288, 1316, 1350, 1364, 1396,\n",
      "        1419, 1456, 1486, 1514, 1541, 1591, 1612, 1656, 1694, 1723, 1746, 1785,\n",
      "        1810, 1842, 1905, 1952, 1979, 2017, 2037, 2059], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1680, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.6426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([1.9500e-06], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 746])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of attention_mask: torch.Size([1, 746])\n",
      "size of segment_ids: torch.Size([1, 746])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 23])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 746, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 12,  54,  79, 107, 127, 153, 169, 212, 244, 283, 299, 326, 348, 382,\n",
      "        430, 452, 490, 546, 571, 604, 634, 663, 721], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2441, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5273, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([1.9500e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of attention_mask: torch.Size([1, 1314])\n",
      "size of segment_ids: torch.Size([1, 1314])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 51])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1314, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  23,   49,   71,   93,  125,  142,  189,  200,  212,  295,  308,  344,\n",
      "         364,  376,  402,  437,  452,  465,  481,  511,  528,  545,  563,  581,\n",
      "         625,  641,  664,  676,  709,  754,  775,  794,  833,  856,  890,  899,\n",
      "         918,  930,  947,  970, 1000, 1028, 1049, 1093, 1116, 1129, 1146, 1182,\n",
      "        1199, 1232, 1269], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.4619, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(4.9922, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.0000e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of attention_mask: torch.Size([1, 1872])\n",
      "size of segment_ids: torch.Size([1, 1872])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 61])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1872, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  20,   60,   92,  124,  163,  210,  273,  304,  377,  409,  461,  493,\n",
      "         533,  553,  579,  618,  628,  663,  684,  712,  732,  769,  785,  823,\n",
      "         834,  881,  923,  944,  982,  994, 1019, 1030, 1044, 1066, 1097, 1110,\n",
      "        1166, 1197, 1238, 1277, 1301, 1337, 1352, 1397, 1447, 1482, 1493, 1509,\n",
      "        1526, 1554, 1589, 1611, 1637, 1664, 1684, 1715, 1754, 1778, 1795, 1829,\n",
      "        1858], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2305, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.3320, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.0000e-06], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of attention_mask: torch.Size([1, 878])\n",
      "size of segment_ids: torch.Size([1, 878])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 32])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 878, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 20,  49,  85, 114, 149, 169, 187, 197, 232, 251, 311, 324, 353, 390,\n",
      "        414, 434, 454, 481, 517, 552, 569, 595, 604, 626, 651, 677, 697, 720,\n",
      "        739, 808, 821, 841], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2559, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(5.4473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.0500e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of attention_mask: torch.Size([1, 1494])\n",
      "size of segment_ids: torch.Size([1, 1494])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1494, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  47,   86,  110,  139,  182,  208,  229,  247,  302,  349,  366,  393,\n",
      "         424,  466,  498,  522,  550,  588,  614,  666,  708,  767,  804,  828,\n",
      "         886,  915,  948,  967,  982, 1027, 1090, 1120, 1165, 1187, 1222, 1246,\n",
      "        1280, 1325, 1388, 1420], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.0500e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of attention_mask: torch.Size([1, 1459])\n",
      "size of segment_ids: torch.Size([1, 1459])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1459, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  26,   59,  100,  132,  145,  169,  185,  212,  218,  237,  253,  269,\n",
      "         282,  303,  327,  345,  367,  397,  418,  437,  448,  470,  493,  516,\n",
      "         540,  567,  597,  618,  643,  673,  697,  772,  837,  883,  908,  951,\n",
      "         965,  991, 1029, 1057, 1074, 1099, 1122, 1142, 1157, 1188, 1234, 1275,\n",
      "        1295, 1320, 1360, 1374, 1411], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2119, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.4023, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.1000e-06], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 994])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of attention_mask: torch.Size([1, 994])\n",
      "size of segment_ids: torch.Size([1, 994])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 29])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 994, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 24,  46,  96, 143, 162, 208, 259, 287, 316, 331, 354, 374, 408, 451,\n",
      "        493, 509, 529, 564, 585, 633, 675, 691, 731, 774, 816, 866, 891, 910,\n",
      "        953], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2324, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.1000e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of attention_mask: torch.Size([1, 1376])\n",
      "size of segment_ids: torch.Size([1, 1376])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 35])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1376, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  22,   50,   85,  114,  158,  207,  244,  285,  302,  322,  353,  370,\n",
      "         430,  459,  479,  492,  553,  582,  645,  790,  796,  858,  882,  931,\n",
      "         968,  978,  999, 1034, 1114, 1129, 1161, 1185, 1217, 1243, 1260],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.4336, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.4337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.2324, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.4337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.1500e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of attention_mask: torch.Size([1, 1316])\n",
      "size of segment_ids: torch.Size([1, 1316])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 38])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1316, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  17,   45,   85,  113,  152,  240,  277,  310,  375,  402,  450,  469,\n",
      "         493,  513,  529,  562,  616,  652,  704,  755,  760,  788,  814,  850,\n",
      "         878,  912,  928,  967,  998, 1031, 1072, 1095, 1135, 1149, 1193, 1228,\n",
      "        1284, 1303], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1807, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.2012, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.1500e-06], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of attention_mask: torch.Size([1, 1548])\n",
      "size of segment_ids: torch.Size([1, 1548])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 43])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1548, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   62,  148,  164,  187,  250,  282,  322,  340,  366,  395,  457,\n",
      "         483,  534,  568,  600,  665,  718,  771,  786,  806,  852,  863,  885,\n",
      "         913,  940,  962,  991, 1038, 1049, 1073, 1090, 1150, 1172, 1207, 1246,\n",
      "        1293, 1322, 1350, 1383, 1439, 1474, 1498], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2227, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.0840, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.2000e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of attention_mask: torch.Size([1, 1597])\n",
      "size of segment_ids: torch.Size([1, 1597])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 50])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1597, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  17,   60,   78,  123,  138,  175,  196,  227,  236,  255,  277,  327,\n",
      "         370,  393,  421,  474,  497,  524,  540,  557,  577,  599,  629,  653,\n",
      "         694,  747,  783,  804,  865,  909,  937,  968,  998, 1036, 1072, 1126,\n",
      "        1141, 1169, 1197, 1223, 1280, 1309, 1334, 1353, 1384, 1443, 1463, 1503,\n",
      "        1532, 1562], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.7520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.2000e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of attention_mask: torch.Size([1, 1285])\n",
      "size of segment_ids: torch.Size([1, 1285])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 48])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1285, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   51,   77,  107,  135,  158,  225,  251,  306,  334,  343,  355,\n",
      "         372,  390,  406,  422,  431,  454,  474,  498,  520,  540,  569,  606,\n",
      "         616,  636,  698,  736,  772,  792,  821,  846,  881,  903,  933,  954,\n",
      "        1002, 1025, 1045, 1087, 1102, 1131, 1165, 1182, 1210, 1236, 1250, 1268],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1875, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.1113, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.2500e-06], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1345])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of attention_mask: torch.Size([1, 1345])\n",
      "size of segment_ids: torch.Size([1, 1345])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1345, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  37,   79,  117,  158,  189,  229,  331,  359,  388,  410,  435,  458,\n",
      "         474,  505,  523,  542,  574,  604,  651,  666,  703,  777,  803,  823,\n",
      "         860,  885,  895,  915,  984, 1030, 1055, 1093, 1114, 1137, 1167, 1189,\n",
      "        1215, 1240, 1271, 1319], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1670, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5820, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.2500e-06], device='cuda:0')\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1660])\n",
      "size of attention_mask: torch.Size([1, 1660])\n",
      "size of segment_ids: torch.Size([1, 1660])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 57])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1660, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  24,   50,   81,  118,  130,  161,  201,  237,  269,  297,  356,  389,\n",
      "         415,  462,  480,  500,  520,  540,  562,  591,  620,  662,  678,  706,\n",
      "         754,  773,  782,  813,  839,  854,  883,  923,  955,  991, 1027, 1069,\n",
      "        1083, 1109, 1126, 1154, 1194, 1253, 1287, 1316, 1325, 1360, 1374, 1406,\n",
      "        1436, 1462, 1476, 1497, 1524, 1569, 1586, 1615, 1632], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0859, device='cuda:0')\n",
      "type_loss:  tensor(1.0861, device='cuda:0')\n",
      "answer_loss: tensor(6.6914, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1025], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1025], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {8, 9, 11, 12, 25, 26, 28, 31}\n",
      "gold:  [19, 39]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1011])\n",
      "size of attention_mask: torch.Size([1, 1011])\n",
      "size of segment_ids: torch.Size([1, 1011])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1011, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 52,  78, 111, 127, 144, 167, 192, 213, 234, 261, 276, 293, 321, 345,\n",
      "        369, 394, 428, 453, 480, 505, 521, 558, 572, 604, 620, 670, 697, 718,\n",
      "        736, 747, 766, 779, 783, 808, 834, 861, 885, 900, 928, 963, 980],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1729, device='cuda:0')\n",
      "type_loss:  tensor(1.1720, device='cuda:0')\n",
      "answer_loss: tensor(4.1504, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2542], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2542], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {10}\n",
      "gold:  [10, 33]\n",
      "sp prec:  1.0\n",
      "sp recall:  0.5\n",
      "sp f1:  0.6666666666666666\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1187])\n",
      "size of attention_mask: torch.Size([1, 1187])\n",
      "size of segment_ids: torch.Size([1, 1187])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1187, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  31,   53,   72,   85,  110,  144,  174,  201,  230,  261,  272,  289,\n",
      "         344,  374,  401,  428,  443,  466,  503,  537,  558,  581,  631,  664,\n",
      "         678,  709,  764,  791,  816,  839,  879,  924,  941,  967,  983, 1008,\n",
      "        1047, 1066, 1111, 1131, 1163], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2070, device='cuda:0')\n",
      "type_loss:  tensor(1.2071, device='cuda:0')\n",
      "answer_loss: tensor(5.4492, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1841], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1841], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {11, 12, 13}\n",
      "gold:  [10, 11, 13]\n",
      "sp prec:  0.6666666666666666\n",
      "sp recall:  0.6666666666666666\n",
      "sp f1:  0.6666666666666666\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 933])\n",
      "size of attention_mask: torch.Size([1, 933])\n",
      "size of segment_ids: torch.Size([1, 933])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 37])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 933, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 21,  54,  68,  97, 157, 176, 190, 216, 263, 282, 315, 362, 398, 417,\n",
      "        441, 456, 474, 488, 510, 523, 543, 554, 578, 599, 616, 645, 675, 696,\n",
      "        709, 745, 757, 772, 805, 829, 849, 871, 913], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2324, device='cuda:0')\n",
      "type_loss:  tensor(1.2323, device='cuda:0')\n",
      "answer_loss: tensor(6.7754, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2729], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2729], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {32, 33, 10, 11}\n",
      "gold:  [8, 25]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 851])\n",
      "size of attention_mask: torch.Size([1, 851])\n",
      "size of segment_ids: torch.Size([1, 851])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 35])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 851, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 13,  34,  61,  75,  97, 116, 151, 185, 214, 237, 258, 295, 316, 353,\n",
      "        374, 385, 414, 433, 455, 501, 522, 550, 572, 590, 618, 645, 657, 683,\n",
      "        706, 720, 747, 761, 775, 797, 826], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2275, device='cuda:0')\n",
      "type_loss:  tensor(1.2280, device='cuda:0')\n",
      "answer_loss: tensor(6.0254, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1539], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1539], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {11, 12, 13}\n",
      "gold:  [16, 23]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1320])\n",
      "size of attention_mask: torch.Size([1, 1320])\n",
      "size of segment_ids: torch.Size([1, 1320])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 37])\n",
      "size of sp_para: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1320, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  29,   60,  100,  151,  168,  237,  329,  376,  438,  475,  516,  546,\n",
      "         578,  601,  639,  662,  692,  741,  765,  782,  817,  839,  895,  935,\n",
      "         953,  971,  992, 1032, 1068, 1098, 1127, 1160, 1192, 1208, 1238, 1264,\n",
      "        1287], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2344, device='cuda:0')\n",
      "type_loss:  tensor(1.2348, device='cuda:0')\n",
      "answer_loss: tensor(7.3691, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.3208], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.3208], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {36}\n",
      "gold:  [16, 33, 35]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1662])\n",
      "size of attention_mask: torch.Size([1, 1662])\n",
      "size of segment_ids: torch.Size([1, 1662])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1662, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  15,   45,   81,  126,  144,  171,  212,  231,  306,  334,  370,  416,\n",
      "         459,  504,  551,  586,  623,  649,  666,  704,  725,  767,  776,  805,\n",
      "         844,  886,  920,  940,  955,  978, 1003, 1046, 1074, 1114, 1159, 1204,\n",
      "        1226, 1274, 1295, 1342, 1365, 1378, 1398, 1426, 1460, 1463, 1503, 1529,\n",
      "        1545, 1582, 1602, 1617, 1635], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2715, device='cuda:0')\n",
      "type_loss:  tensor(1.2714, device='cuda:0')\n",
      "answer_loss: tensor(7.4766, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1200], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1200], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {10, 11, 22, 23, 25, 26}\n",
      "gold:  [0, 18]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1649])\n",
      "size of attention_mask: torch.Size([1, 1649])\n",
      "size of segment_ids: torch.Size([1, 1649])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1649, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,  148,  197,  240,  268,  323,  341,  352,  366,  401,  428,  458,\n",
      "         476,  495,  523,  548,  584,  642,  661,  670,  701,  755,  794,  828,\n",
      "         884,  931,  960,  973, 1003, 1026, 1042, 1082, 1104, 1127, 1163, 1182,\n",
      "        1205, 1232, 1259, 1278, 1297, 1325, 1347, 1365, 1418, 1489, 1509, 1528,\n",
      "        1548, 1570, 1596, 1610, 1624], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2188, device='cuda:0')\n",
      "type_loss:  tensor(1.2188, device='cuda:0')\n",
      "answer_loss: tensor(6.2227, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1564], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1564], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {25, 26, 27}\n",
      "gold:  [19, 50]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2018])\n",
      "size of attention_mask: torch.Size([1, 2018])\n",
      "size of segment_ids: torch.Size([1, 2018])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 58])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 2018, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   40,   74,  103,  184,  196,  262,  275,  295,  332,  354,  392,\n",
      "         433,  460,  470,  498,  525,  619,  654,  682,  705,  753,  801,  831,\n",
      "         856,  870,  884,  912,  936,  979,  993, 1032, 1078, 1131, 1176, 1179,\n",
      "        1225, 1279, 1333, 1360, 1375, 1398, 1416, 1446, 1482, 1513, 1547, 1572,\n",
      "        1608, 1632, 1646, 1673, 1717, 1743, 1781, 1879, 1912, 1994],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2803, device='cuda:0')\n",
      "type_loss:  tensor(1.2802, device='cuda:0')\n",
      "answer_loss: tensor(7.6582, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1946], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1946], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {22, 23, 26, 27, 28}\n",
      "gold:  [18, 55, 56]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1310])\n",
      "size of attention_mask: torch.Size([1, 1310])\n",
      "size of segment_ids: torch.Size([1, 1310])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1310, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  34,   60,   91,  127,  199,  211,  222,  240,  263,  316,  347,  381,\n",
      "         398,  453,  471,  479,  516,  579,  615,  651,  685,  711,  750,  785,\n",
      "         804,  837,  871,  932,  979,  997, 1027, 1048, 1069, 1092, 1115, 1155,\n",
      "        1207, 1251, 1277, 1301], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1855, device='cuda:0')\n",
      "type_loss:  tensor(1.1860, device='cuda:0')\n",
      "answer_loss: tensor(6.5723, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1715], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1715], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {6}\n",
      "gold:  [8, 9]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([2], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1400])\n",
      "size of attention_mask: torch.Size([1, 1400])\n",
      "size of segment_ids: torch.Size([1, 1400])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1400, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  21,  109,  128,  158,  182,  219,  243,  290,  322,  363,  389,  445,\n",
      "         480,  514,  553,  563,  572,  602,  612,  628,  661,  672,  691,  703,\n",
      "         745,  780,  833,  868,  967, 1072, 1116, 1132, 1152, 1198, 1219, 1251,\n",
      "        1297, 1325, 1349, 1368, 1380], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(0.8350, device='cuda:0')\n",
      "type_loss:  tensor(0.8351, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1089], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1089], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold: no\n",
      "f1: tensor(1., device='cuda:0')\n",
      "prec: tensor(1., device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(1., device='cuda:0')\n",
      "prediction:  {36, 38, 40, 24, 25, 26}\n",
      "gold:  [27, 28]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([2], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 943])\n",
      "size of attention_mask: torch.Size([1, 943])\n",
      "size of segment_ids: torch.Size([1, 943])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 27])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 943, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 17,  37,  70,  92, 115, 148, 168, 248, 261, 330, 367, 385, 430, 460,\n",
      "        495, 556, 594, 607, 634, 672, 687, 723, 746, 807, 832, 877, 898],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(0.8022, device='cuda:0')\n",
      "type_loss:  tensor(0.8025, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1713], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1713], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold: no\n",
      "f1: tensor(1., device='cuda:0')\n",
      "prec: tensor(1., device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(1., device='cuda:0')\n",
      "prediction:  {8, 9, 10, 19, 20, 21, 22, 23, 24}\n",
      "gold:  [5, 25]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1849])\n",
      "size of attention_mask: torch.Size([1, 1849])\n",
      "size of segment_ids: torch.Size([1, 1849])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 51])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1849, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  18,   60,   75,  117,  171,  192,  206,  225,  285,  319,  338,  393,\n",
      "         495,  521,  538,  568,  601,  622,  644,  652,  689,  722,  747,  805,\n",
      "         836,  870,  901,  941,  970, 1013, 1047, 1085, 1103, 1203, 1238, 1256,\n",
      "        1280, 1355, 1395, 1442, 1467, 1553, 1567, 1596, 1625, 1661, 1671, 1696,\n",
      "        1736, 1806, 1824], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.3203, device='cuda:0')\n",
      "type_loss:  tensor(1.3201, device='cuda:0')\n",
      "answer_loss: tensor(5.2676, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Epoch 00004: avg_val_f1 reached 0.15385 (best 0.15385), saving model to jupyter-hotpotqa/hotpotqa-longformer/checkpoints/_ckpt_epoch_4_v0.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_pred: [{'text': 'no', 'score': tensor([0.1996], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1996], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {49, 50}\n",
      "gold:  [4, 12]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_end\n",
      "before sync --> sizes:  13, 13, 13\n",
      "after sync --> sizes: 13, 13, 13\n",
      "answer_scores:  [0.1025390625, 0.254150390625, 0.18408203125, 0.27294921875, 0.1539306640625, 0.32080078125, 0.1199951171875, 0.1563720703125, 0.194580078125, 0.1715087890625, 0.10888671875, 0.1712646484375, 0.1995849609375]\n",
      "f1_scores:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "em_scores:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "avg_loss:  tensor(7.7167, device='cuda:0')\tavg_answer_loss:  tensor(5.3583, device='cuda:0')\tavg_type_loss:  tensor(1.1595, device='cuda:0')\tavg_sp_para_loss:  tensor(0.6557, device='cuda:0')\tavg_sp_sent_loss:  tensor(0.5431, device='cuda:0')\tlen(f1_scores):  13\n",
      "avg_val_f1:  0.15384615384615385\n",
      "avg_val_em:  0.15384615384615385\n",
      "avg_val_prec:  0.15384615384615385\n",
      "avg_val_recall:  0.15384615384615385\n",
      "avg_val_sp_sent_f1:  0.10256410562075101\n",
      "avg_val_sp_sent_em:  0.0\n",
      "avg_val_sp_sent_prec:  0.12820512973345244\n",
      "avg_val_sp_sent_recall:  0.08974359127191398\n",
      "avg_val_joint_f1:  0.0\n",
      "avg_val_joint_em:  0.0\n",
      "avg_val_joint_prec:  0.0\n",
      "avg_val_joint_recall:  0.0\n",
      "Start epoch  5\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of attention_mask: torch.Size([1, 1118])\n",
      "size of segment_ids: torch.Size([1, 1118])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 46])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1118, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  16,   36,   84,   95,  123,  167,  182,  192,  212,  264,  298,  327,\n",
      "         351,  387,  401,  410,  417,  475,  483,  507,  526,  571,  585,  615,\n",
      "         638,  666,  693,  732,  750,  770,  798,  829,  848,  868,  887,  896,\n",
      "         911,  927,  950,  965,  993, 1015, 1035, 1046, 1077, 1092],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1875, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.1133, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.3000e-06], device='cuda:0')\n",
      "training_step  0\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of attention_mask: torch.Size([1, 1079])\n",
      "size of segment_ids: torch.Size([1, 1079])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 39])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1079, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  22,   50,   76,   91,  111,  119,  144,  171,  190,  231,  289,  322,\n",
      "         336,  349,  381,  398,  430,  457,  521,  552,  576,  594,  621,  644,\n",
      "         664,  681,  715,  742,  759,  801,  855,  887,  921,  931,  964,  979,\n",
      "         998, 1011, 1043], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2012, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.3613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.3000e-06], device='cuda:0')\n",
      "training_step  1\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of attention_mask: torch.Size([1, 2078])\n",
      "size of segment_ids: torch.Size([1, 2078])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 68])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2560, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 2078, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  34,   52,   67,  103,  123,  156,  172,  209,  248,  284,  313,  344,\n",
      "         357,  391,  424,  459,  483,  518,  549,  583,  606,  624,  642,  675,\n",
      "         689,  733,  743,  766,  813,  834,  855,  870,  893,  903, 1028, 1059,\n",
      "        1087, 1103, 1115, 1157, 1179, 1210, 1242, 1288, 1316, 1350, 1364, 1396,\n",
      "        1419, 1456, 1486, 1514, 1541, 1591, 1612, 1656, 1694, 1723, 1746, 1785,\n",
      "        1810, 1842, 1905, 1952, 1979, 2017, 2037, 2059], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1807, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.6250, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.3500e-06], device='cuda:0')\n",
      "training_step  2\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of attention_mask: torch.Size([1, 746])\n",
      "size of segment_ids: torch.Size([1, 746])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 23])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 746, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 12,  54,  79, 107, 127, 153, 169, 212, 244, 283, 299, 326, 348, 382,\n",
      "        430, 452, 490, 546, 571, 604, 634, 663, 721], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2070, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.4941, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.3500e-06], device='cuda:0')\n",
      "training_step  3\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of attention_mask: torch.Size([1, 1314])\n",
      "size of segment_ids: torch.Size([1, 1314])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 51])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1314, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  23,   49,   71,   93,  125,  142,  189,  200,  212,  295,  308,  344,\n",
      "         364,  376,  402,  437,  452,  465,  481,  511,  528,  545,  563,  581,\n",
      "         625,  641,  664,  676,  709,  754,  775,  794,  833,  856,  890,  899,\n",
      "         918,  930,  947,  970, 1000, 1028, 1049, 1093, 1116, 1129, 1146, 1182,\n",
      "        1199, 1232, 1269], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1973, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(5.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.4000e-06], device='cuda:0')\n",
      "training_step  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1872])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of attention_mask: torch.Size([1, 1872])\n",
      "size of segment_ids: torch.Size([1, 1872])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 61])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1872, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  20,   60,   92,  124,  163,  210,  273,  304,  377,  409,  461,  493,\n",
      "         533,  553,  579,  618,  628,  663,  684,  712,  732,  769,  785,  823,\n",
      "         834,  881,  923,  944,  982,  994, 1019, 1030, 1044, 1066, 1097, 1110,\n",
      "        1166, 1197, 1238, 1277, 1301, 1337, 1352, 1397, 1447, 1482, 1493, 1509,\n",
      "        1526, 1554, 1589, 1611, 1637, 1664, 1684, 1715, 1754, 1778, 1795, 1829,\n",
      "        1858], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1885, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.5234, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.4000e-06], device='cuda:0')\n",
      "training_step  5\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of attention_mask: torch.Size([1, 878])\n",
      "size of segment_ids: torch.Size([1, 878])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 32])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 878, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 20,  49,  85, 114, 149, 169, 187, 197, 232, 251, 311, 324, 353, 390,\n",
      "        414, 434, 454, 481, 517, 552, 569, 595, 604, 626, 651, 677, 697, 720,\n",
      "        739, 808, 821, 841], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1680, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(5.3477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.4500e-06], device='cuda:0')\n",
      "training_step  6\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of attention_mask: torch.Size([1, 1494])\n",
      "size of segment_ids: torch.Size([1, 1494])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1494, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  47,   86,  110,  139,  182,  208,  229,  247,  302,  349,  366,  393,\n",
      "         424,  466,  498,  522,  550,  588,  614,  666,  708,  767,  804,  828,\n",
      "         886,  915,  948,  967,  982, 1027, 1090, 1120, 1165, 1187, 1222, 1246,\n",
      "        1280, 1325, 1388, 1420], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0898, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.4500e-06], device='cuda:0')\n",
      "training_step  7\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of attention_mask: torch.Size([1, 1459])\n",
      "size of segment_ids: torch.Size([1, 1459])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1459, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  26,   59,  100,  132,  145,  169,  185,  212,  218,  237,  253,  269,\n",
      "         282,  303,  327,  345,  367,  397,  418,  437,  448,  470,  493,  516,\n",
      "         540,  567,  597,  618,  643,  673,  697,  772,  837,  883,  908,  951,\n",
      "         965,  991, 1029, 1057, 1074, 1099, 1122, 1142, 1157, 1188, 1234, 1275,\n",
      "        1295, 1320, 1360, 1374, 1411], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1396, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.3477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.5000e-06], device='cuda:0')\n",
      "training_step  8\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of attention_mask: torch.Size([1, 994])\n",
      "size of segment_ids: torch.Size([1, 994])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 29])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 994, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 24,  46,  96, 143, 162, 208, 259, 287, 316, 331, 354, 374, 408, 451,\n",
      "        493, 509, 529, 564, 585, 633, 675, 691, 731, 774, 816, 866, 891, 910,\n",
      "        953], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1084, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5000, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.5000e-06], device='cuda:0')\n",
      "training_step  9\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of attention_mask: torch.Size([1, 1376])\n",
      "size of segment_ids: torch.Size([1, 1376])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 35])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1376, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  22,   50,   85,  114,  158,  207,  244,  285,  302,  322,  353,  370,\n",
      "         430,  459,  479,  492,  553,  582,  645,  790,  796,  858,  882,  931,\n",
      "         968,  978,  999, 1034, 1114, 1129, 1161, 1185, 1217, 1243, 1260],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1719, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.5500e-06], device='cuda:0')\n",
      "training_step  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1316])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of attention_mask: torch.Size([1, 1316])\n",
      "size of segment_ids: torch.Size([1, 1316])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 38])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1316, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  17,   45,   85,  113,  152,  240,  277,  310,  375,  402,  450,  469,\n",
      "         493,  513,  529,  562,  616,  652,  704,  755,  760,  788,  814,  850,\n",
      "         878,  912,  928,  967,  998, 1031, 1072, 1095, 1135, 1149, 1193, 1228,\n",
      "        1284, 1303], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2852, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.2051, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.5500e-06], device='cuda:0')\n",
      "training_step  11\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of attention_mask: torch.Size([1, 1548])\n",
      "size of segment_ids: torch.Size([1, 1548])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 43])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1548, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   62,  148,  164,  187,  250,  282,  322,  340,  366,  395,  457,\n",
      "         483,  534,  568,  600,  665,  718,  771,  786,  806,  852,  863,  885,\n",
      "         913,  940,  962,  991, 1038, 1049, 1073, 1090, 1150, 1172, 1207, 1246,\n",
      "        1293, 1322, 1350, 1383, 1439, 1474, 1498], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1436, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.0918, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.6000e-06], device='cuda:0')\n",
      "training_step  12\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of attention_mask: torch.Size([1, 1597])\n",
      "size of segment_ids: torch.Size([1, 1597])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 50])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1597, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  17,   60,   78,  123,  138,  175,  196,  227,  236,  255,  277,  327,\n",
      "         370,  393,  421,  474,  497,  524,  540,  557,  577,  599,  629,  653,\n",
      "         694,  747,  783,  804,  865,  909,  937,  968,  998, 1036, 1072, 1126,\n",
      "        1141, 1169, 1197, 1223, 1280, 1309, 1334, 1353, 1384, 1443, 1463, 1503,\n",
      "        1532, 1562], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0664, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.5801, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.6000e-06], device='cuda:0')\n",
      "training_step  13\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of attention_mask: torch.Size([1, 1285])\n",
      "size of segment_ids: torch.Size([1, 1285])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 48])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1285, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   51,   77,  107,  135,  158,  225,  251,  306,  334,  343,  355,\n",
      "         372,  390,  406,  422,  431,  454,  474,  498,  520,  540,  569,  606,\n",
      "         616,  636,  698,  736,  772,  792,  821,  846,  881,  903,  933,  954,\n",
      "        1002, 1025, 1045, 1087, 1102, 1131, 1165, 1182, 1210, 1236, 1250, 1268],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1270, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(7.0723, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.6500e-06], device='cuda:0')\n",
      "training_step  14\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of attention_mask: torch.Size([1, 1345])\n",
      "size of segment_ids: torch.Size([1, 1345])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1345, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  37,   79,  117,  158,  189,  229,  331,  359,  388,  410,  435,  458,\n",
      "         474,  505,  523,  542,  574,  604,  651,  666,  703,  777,  803,  823,\n",
      "         860,  885,  895,  915,  984, 1030, 1055, 1093, 1114, 1137, 1167, 1189,\n",
      "        1215, 1240, 1271, 1319], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1094, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "type_loss:  tensor(1.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "answer_loss:  tensor(6.6074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_para_loss:  tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "sp_sent_loss:  tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lr:  tensor([2.6500e-06], device='cuda:0')\n",
      "training_step  15\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1660])\n",
      "size of attention_mask: torch.Size([1, 1660])\n",
      "size of segment_ids: torch.Size([1, 1660])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 57])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1660, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  24,   50,   81,  118,  130,  161,  201,  237,  269,  297,  356,  389,\n",
      "         415,  462,  480,  500,  520,  540,  562,  591,  620,  662,  678,  706,\n",
      "         754,  773,  782,  813,  839,  854,  883,  923,  955,  991, 1027, 1069,\n",
      "        1083, 1109, 1126, 1154, 1194, 1253, 1287, 1316, 1325, 1360, 1374, 1406,\n",
      "        1436, 1462, 1476, 1497, 1524, 1569, 1586, 1615, 1632], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(0.9546, device='cuda:0')\n",
      "type_loss:  tensor(0.9548, device='cuda:0')\n",
      "answer_loss: tensor(6.6680, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' This is a list of episodes for the Canadian crime series \"Republic of Doyle\".', 'score': tensor([0.6260], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' This is a list of episodes for the Canadian crime series \"Republic of Doyle\".', 'score': tensor([0.6260], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.6260], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  This is a list of episodes for the Canadian crime series \"Republic of Doyle\".\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {26, 28, 30, 31}\n",
      "gold:  [19, 39]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1011])\n",
      "size of attention_mask: torch.Size([1, 1011])\n",
      "size of segment_ids: torch.Size([1, 1011])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1011, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 52,  78, 111, 127, 144, 167, 192, 213, 234, 261, 276, 293, 321, 345,\n",
      "        369, 394, 428, 453, 480, 505, 521, 558, 572, 604, 620, 670, 697, 718,\n",
      "        736, 747, 766, 779, 783, 808, 834, 861, 885, 900, 928, 963, 980],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0771, device='cuda:0')\n",
      "type_loss:  tensor(1.0767, device='cuda:0')\n",
      "answer_loss: tensor(4.1445, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1992], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1992], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {10, 11, 5}\n",
      "gold:  [10, 33]\n",
      "sp prec:  0.3333333333333333\n",
      "sp recall:  0.5\n",
      "sp f1:  0.4\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1187])\n",
      "size of attention_mask: torch.Size([1, 1187])\n",
      "size of segment_ids: torch.Size([1, 1187])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1187, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  31,   53,   72,   85,  110,  144,  174,  201,  230,  261,  272,  289,\n",
      "         344,  374,  401,  428,  443,  466,  503,  537,  558,  581,  631,  664,\n",
      "         678,  709,  764,  791,  816,  839,  879,  924,  941,  967,  983, 1008,\n",
      "        1047, 1066, 1111, 1131, 1163], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1094, device='cuda:0')\n",
      "type_loss:  tensor(1.1098, device='cuda:0')\n",
      "answer_loss: tensor(5.4375, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1149], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1149], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 933])\n",
      "size of attention_mask: torch.Size([1, 933])\n",
      "size of segment_ids: torch.Size([1, 933])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 37])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 933, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 21,  54,  68,  97, 157, 176, 190, 216, 263, 282, 315, 362, 398, 417,\n",
      "        441, 456, 474, 488, 510, 523, 543, 554, 578, 599, 616, 645, 675, 696,\n",
      "        709, 745, 757, 772, 805, 829, 849, 871, 913], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1338, device='cuda:0')\n",
      "type_loss:  tensor(1.1337, device='cuda:0')\n",
      "answer_loss: tensor(6.7266, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2246], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2246], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {32, 33, 34, 36}\n",
      "gold:  [8, 25]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 851])\n",
      "size of attention_mask: torch.Size([1, 851])\n",
      "size of segment_ids: torch.Size([1, 851])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 35])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 851, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 13,  34,  61,  75,  97, 116, 151, 185, 214, 237, 258, 295, 316, 353,\n",
      "        374, 385, 414, 433, 455, 501, 522, 550, 572, 590, 618, 645, 657, 683,\n",
      "        706, 720, 747, 761, 775, 797, 826], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1211, device='cuda:0')\n",
      "type_loss:  tensor(1.1211, device='cuda:0')\n",
      "answer_loss: tensor(6.0039, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.0858], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.0858], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {11, 12, 13}\n",
      "gold:  [16, 23]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1320])\n",
      "size of attention_mask: torch.Size([1, 1320])\n",
      "size of segment_ids: torch.Size([1, 1320])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 37])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1320, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  29,   60,  100,  151,  168,  237,  329,  376,  438,  475,  516,  546,\n",
      "         578,  601,  639,  662,  692,  741,  765,  782,  817,  839,  895,  935,\n",
      "         953,  971,  992, 1032, 1068, 1098, 1127, 1160, 1192, 1208, 1238, 1264,\n",
      "        1287], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0840, device='cuda:0')\n",
      "type_loss:  tensor(1.0846, device='cuda:0')\n",
      "answer_loss: tensor(7.3828, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_pred: [{'text': 'no', 'score': tensor([0.2269], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2269], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {4, 5, 7}\n",
      "gold:  [16, 33, 35]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1662])\n",
      "size of attention_mask: torch.Size([1, 1662])\n",
      "size of segment_ids: torch.Size([1, 1662])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1662, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  15,   45,   81,  126,  144,  171,  212,  231,  306,  334,  370,  416,\n",
      "         459,  504,  551,  586,  623,  649,  666,  704,  725,  767,  776,  805,\n",
      "         844,  886,  920,  940,  955,  978, 1003, 1046, 1074, 1114, 1159, 1204,\n",
      "        1226, 1274, 1295, 1342, 1365, 1378, 1398, 1426, 1460, 1463, 1503, 1529,\n",
      "        1545, 1582, 1602, 1617, 1635], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0947, device='cuda:0')\n",
      "type_loss:  tensor(1.0947, device='cuda:0')\n",
      "answer_loss: tensor(7.4629, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([-0.0041], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([-0.0041], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {19, 22, 23, 25, 26}\n",
      "gold:  [0, 18]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1649])\n",
      "size of attention_mask: torch.Size([1, 1649])\n",
      "size of segment_ids: torch.Size([1, 1649])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 53])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1649, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,  148,  197,  240,  268,  323,  341,  352,  366,  401,  428,  458,\n",
      "         476,  495,  523,  548,  584,  642,  661,  670,  701,  755,  794,  828,\n",
      "         884,  931,  960,  973, 1003, 1026, 1042, 1082, 1104, 1127, 1163, 1182,\n",
      "        1205, 1232, 1259, 1278, 1297, 1325, 1347, 1365, 1418, 1489, 1509, 1528,\n",
      "        1548, 1570, 1596, 1610, 1624], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0938, device='cuda:0')\n",
      "type_loss:  tensor(1.0937, device='cuda:0')\n",
      "answer_loss: tensor(6.2051, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1031], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1031], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {25, 26, 27}\n",
      "gold:  [19, 50]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2018])\n",
      "size of attention_mask: torch.Size([1, 2018])\n",
      "size of segment_ids: torch.Size([1, 2018])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 58])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 2018, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  25,   40,   74,  103,  184,  196,  262,  275,  295,  332,  354,  392,\n",
      "         433,  460,  470,  498,  525,  619,  654,  682,  705,  753,  801,  831,\n",
      "         856,  870,  884,  912,  936,  979,  993, 1032, 1078, 1131, 1176, 1179,\n",
      "        1225, 1279, 1333, 1360, 1375, 1398, 1416, 1446, 1482, 1513, 1547, 1572,\n",
      "        1608, 1632, 1646, 1673, 1717, 1743, 1781, 1879, 1912, 1994],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.1309, device='cuda:0')\n",
      "type_loss:  tensor(1.1313, device='cuda:0')\n",
      "answer_loss: tensor(7.6465, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.0963], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.0963], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {26, 27, 28, 22}\n",
      "gold:  [18, 55, 56]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1310])\n",
      "size of attention_mask: torch.Size([1, 1310])\n",
      "size of segment_ids: torch.Size([1, 1310])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 40])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1310, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  34,   60,   91,  127,  199,  211,  222,  240,  263,  316,  347,  381,\n",
      "         398,  453,  471,  479,  516,  579,  615,  651,  685,  711,  750,  785,\n",
      "         804,  837,  871,  932,  979,  997, 1027, 1048, 1069, 1092, 1115, 1155,\n",
      "        1207, 1251, 1277, 1301], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.0840, device='cuda:0')\n",
      "type_loss:  tensor(1.0836, device='cuda:0')\n",
      "answer_loss: tensor(6.5547, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.0969], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.0969], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {8, 6}\n",
      "gold:  [8, 9]\n",
      "sp prec:  0.5\n",
      "sp recall:  0.5\n",
      "sp f1:  0.5\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([2], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1400])\n",
      "size of attention_mask: torch.Size([1, 1400])\n",
      "size of segment_ids: torch.Size([1, 1400])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 41])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1536, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1400, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  21,  109,  128,  158,  182,  219,  243,  290,  322,  363,  389,  445,\n",
      "         480,  514,  553,  563,  572,  602,  612,  628,  661,  672,  691,  703,\n",
      "         745,  780,  833,  868,  967, 1072, 1116, 1132, 1152, 1198, 1219, 1251,\n",
      "        1297, 1325, 1349, 1368, 1380], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(0.8652, device='cuda:0')\n",
      "type_loss:  tensor(0.8654, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.0481], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.0481], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold: no\n",
      "f1: tensor(1., device='cuda:0')\n",
      "prec: tensor(1., device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(1., device='cuda:0')\n",
      "prediction:  {24, 25, 26}\n",
      "gold:  [27, 28]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([2], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 943])\n",
      "size of attention_mask: torch.Size([1, 943])\n",
      "size of segment_ids: torch.Size([1, 943])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 27])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 1024, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 943, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([ 17,  37,  70,  92, 115, 148, 168, 248, 261, 330, 367, 385, 430, 460,\n",
      "        495, 556, 594, 607, 634, 672, 687, 723, 746, 807, 832, 877, 898],\n",
      "       device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(0.8550, device='cuda:0')\n",
      "type_loss:  tensor(0.8550, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.0957], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.0957], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold: no\n",
      "f1: tensor(1., device='cuda:0')\n",
      "prec: tensor(1., device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(1., device='cuda:0')\n",
      "prediction:  {8, 9, 10, 20, 21, 22, 23, 24}\n",
      "gold:  [5, 25]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_step\n",
      "q_type: tensor([0], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1849])\n",
      "size of attention_mask: torch.Size([1, 1849])\n",
      "size of segment_ids: torch.Size([1, 1849])\n",
      "size of start_positions: torch.Size([1, 64])\n",
      "size of end_positions:torch.Size([1, 64])\n",
      "size of q_type:torch.Size([1])\n",
      "size of sp_sent: torch.Size([1, 51])\n",
      "size of sp_para: torch.Size([1, 10])\n",
      "size of sequence_output: torch.Size([1, 2048, 768])\n",
      "size of sequence_output after removing padding: torch.Size([1, 1849, 768])\n",
      "size of type_logits: torch.Size([1, 3])\n",
      "size of p_index: torch.Size([10])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "sent_indexes:  tensor([  18,   60,   75,  117,  171,  192,  206,  225,  285,  319,  338,  393,\n",
      "         495,  521,  538,  568,  601,  622,  644,  652,  689,  722,  747,  805,\n",
      "         836,  870,  901,  941,  970, 1013, 1047, 1085, 1103, 1203, 1238, 1256,\n",
      "        1280, 1355, 1395, 1442, 1467, 1553, 1567, 1596, 1625, 1661, 1671, 1696,\n",
      "        1736, 1806, 1824], device='cuda:0')\n",
      "type_loss_or_softmax_cross_entropy:  tensor(1.2041, device='cuda:0')\n",
      "type_loss:  tensor(1.2043, device='cuda:0')\n",
      "answer_loss: tensor(5.2539, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Epoch 00005: avg_val_f1 reached 0.15385 (best 0.15385), saving model to jupyter-hotpotqa/hotpotqa-longformer/checkpoints/_ckpt_epoch_5.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_pred: [{'text': 'no', 'score': tensor([0.1313], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1313], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "prediction:  {49, 50}\n",
      "gold:  [4, 12]\n",
      "sp prec:  0.0\n",
      "sp recall:  0.0\n",
      "sp f1:  0.0\n",
      "sp em:  0.0\n",
      "return\n",
      "validation_end\n",
      "before sync --> sizes:  13, 13, 13\n",
      "after sync --> sizes: 13, 13, 13\n",
      "answer_scores:  [0.6259765625, 0.19921875, 0.11492919921875, 0.224609375, 0.08575439453125, 0.2269287109375, -0.004085540771484375, 0.1031494140625, 0.0963134765625, 0.096923828125, 0.048126220703125, 0.095703125, 0.13134765625]\n",
      "f1_scores:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "em_scores:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "avg_loss:  tensor(7.4932, device='cuda:0')\tavg_answer_loss:  tensor(5.3451, device='cuda:0')\tavg_type_loss:  tensor(1.0622, device='cuda:0')\tavg_sp_para_loss:  tensor(0.5896, device='cuda:0')\tavg_sp_sent_loss:  tensor(0.4963, device='cuda:0')\tlen(f1_scores):  13\n",
      "avg_val_f1:  0.15384615384615385\n",
      "avg_val_em:  0.15384615384615385\n",
      "avg_val_prec:  0.15384615384615385\n",
      "avg_val_recall:  0.15384615384615385\n",
      "avg_val_sp_sent_f1:  0.0692307696892665\n",
      "avg_val_sp_sent_em:  0.0\n",
      "avg_val_sp_sent_prec:  0.06410256486672622\n",
      "avg_val_sp_sent_recall:  0.07692307692307693\n",
      "avg_val_joint_f1:  0.0\n",
      "avg_val_joint_em:  0.0\n",
      "avg_val_joint_prec:  0.0\n",
      "avg_val_joint_recall:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     if not args.test:\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_dev.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 1)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', True)\n",
      "('model_path', '/Users/fan/Downloads/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json --dev_dataset small_dev.json  \\\n",
    "    --gpus 0 --num_workers 1 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer  --model_path /Users/fan/Downloads/longformer-base-4096 --test '\n",
    "# hotpot_dev_distractor_v1.json\n",
    "\n",
    "import argparse \n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    for arg in vars(args):\n",
    "        print((arg, getattr(args, arg)))\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
