{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   \n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "# import sys\n",
    "# sys.path.insert(0, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.8/site-packages') \n",
    "# sys.path.insert(0, '/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.7/site-packages')  \n",
    "# sys.path.insert(0, '/home/u32/fanluo/.ipython') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "import transformers \n",
    "\n",
    "# QUESTION_START = '[question]'\n",
    "# QUESTION_END = '[/question]' \n",
    "# TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "# TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "# SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "# PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null </s> \"\n",
    "\n",
    " \n",
    "def create_example_dict(context, answer, id, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answer\": answer,\n",
    "                \"id\": id,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )    \n",
    "        \n",
    "        for lst in raw_contexts:\n",
    "            lst[0] = _normalize_text(lst[0])\n",
    "            lst[1] = [_normalize_text(sent) for sent in lst[1]]\n",
    "        \n",
    "        contexts = [lst[0]  + ' '  + ' '.join(lst[1]) for lst in raw_contexts]    \n",
    "        # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space; call the _normalize_text() which is same as the one used during evaluation\n",
    "        \n",
    "#         context = \" </s> \".join(contexts)\n",
    "#         print(context)\n",
    "        \n",
    "#         exit(0)\n",
    "\n",
    "        \n",
    "        answer = _normalize_text(example[\"answer\"]) \n",
    "#         print(\"answer: \", answer)\n",
    "        if(len(answer) > 0):   # answer can be '' after normalize\n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=contexts,\n",
    "                        answer=answer,\n",
    "                        id = example[\"_id\"],\n",
    "                        question=_normalize_text(example[\"question\"]),\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def _normalize_text(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": [\n",
      "        \"dr seuss how grinch stole christmas dr seuss how grinch stole christmas is video game based on dr seuss book with same name but mostly based on film game was released on november 8 2007\",\n",
      "        \"lorax film lorax also known as dr seuss lorax is 2012 american 3d computeranimated musical fantasy\\u2013comedy film produced by illumination entertainment and based on dr seusss childrens book of same name film was released by universal pictures on march 2 2012 on 108th birthday of dr seuss second film adaptation of book following 1972 animated television special film builds on book by expanding story of ted previously unnamed boy who visits onceler cast includes danny devito as lorax ed helms as onceler and zac efron as ted new characters introduced in film are audrey voiced by taylor swift aloysius ohare rob riggle mrs wiggins teds mother jenny slate and grammy norma betty white\",\n",
      "        \"horton hears who tv special horton hears who is 1970 television special based on dr seuss book of same name horton hears who  it was produced and directed by chuck jones \\u2013 who previously produced seuss special how grinch stole christmas \\u2013 for mgm television and first broadcast march 19 1970 on cbs special contains songs with lyrics by seuss and music by eugene poddany who previously wrote songs for seuss book cat in hat song book\",\n",
      "        \"dr seuss memorial dr seuss national memorial sculpture garden is sculpture garden in springfield massachusetts that honors theodor seuss geisel better known to world as dr seuss located at quadrangle dr seuss national memorial sculpture garden honors author and illustrator who was born in springfield in 1904 monument was designed by lark grey dimondcates authors stepdaughter and created by sculptor and artist ron henson\",\n",
      "        \"dr seuss bibliography theodor seuss geisel better known as dr seuss published over 60 childrens books over course of his long career though most were published under his wellknown pseudonym dr seuss he also authored over dozen books as theo lesieg and one as rosetta stone as one of most popular childrens authors of all time geisels books have topped many bestseller lists sold over 222 million copies and been translated into more than 15 languages in 2000 when publishers weekly compiled their list of bestselling childrens books of all time 16 of top 100 hardcover books were written by geisel including green eggs and ham at number 4 cat in hat at number 9 and one fish two fish red fish blue fish at number 13 and dr seusss abc in years following his death in 1991 several additional books based on his sketches and notes were published including hooray for diffendoofer day and daisyhead mayzie although they were all published under name dr seuss only my many colored days originally written in 1973 was entirely by geisel\",\n",
      "        \"how grinch stole christmas 2018 film dr seuss how grinch stole christmas promoted theatrically as dr seuss grinch is upcoming american 3d computeranimated christmas musical comedy film produced by illumination entertainment it is based on 1957 dr seuss story of same name film will be released on november 9 2018 by universal pictures\",\n",
      "        \"do you know what im going to do next saturday do you know what im going to do next saturday is 1963 childrens book published by beginner books and written by helen palmer geisel first wife of theodor seuss geisel dr seuss unlike most of beginner books do you know what im going to do next saturday did not follow format of text with inline drawings being illustrated with blackandwhite photographs by lynn fayman featuring boy named rawli davis it is sometimes misattributed to dr seuss himself books cover features photograph of young boy sitting at breakfast table with huge pile of pancakes\",\n",
      "        \"wubbulous world of dr seuss wubbulous world of dr seuss is liveactionpuppet television series based on characters created by dr seuss produced by jim henson company it aired from october 13 1996 to december 28 1998 on nickelodeon it is notable for its use of live puppets with digitally animated backgrounds and in its first season for refashioning characters and themes from original dr seuss books into new stories that often retained much of flavor of dr seuss own works\",\n",
      "        \"cat in hat film dr seuss cat in hat is 2003 american family comedy film directed by bo welch it is based on 1957 dr seuss book of same name film stars mike myers in title role of cat in hat and dakota fanning as sally sallys brother who is unnamed in book and 1971 tv special conrad is portrayed by spencer breslin film is second featurelength dr seuss adaptation after 2000 holiday film how grinch stole christmas\",\n",
      "        \"kyle balda kyle balda is american animator and film director best known for codirecting animated films lorax 2012 with chris renaud and minions 2015 with pierre coffin he has also worked as animator on several films including jumanji toy story 2 and despicable me he has worked for pixar for years and now he is working for illumination entertainment\"\n",
      "      ],\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answer\": \"lorax\",\n",
      "          \"id\": \"5ab990925542996be2020553\",\n",
      "          \"question\": \"what film did kyle balda work on that was based on dr seuss book\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/Data')\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/small.json | ../../helper/jq-linux64 -c '.[30:40]' > small_3.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[76200:76280]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[37:50]' > small_dev.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[31:50]' > sample.json\n",
    "\n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "\n",
    "# !python -m pip install --upgrade git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "#!python -m pip install jdc \n",
    "# !pip install test-tube \n",
    "#!python -m pip install ipywidgets \n",
    "#!conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        if(\"reduced_context\" not in self.file_path):\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "                \n",
    "        else:\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = json.load(f)['data']            \n",
    "                print(self.data_json[0])\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "\n",
    "#         print(tokenizer.all_special_tokens) \n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =                  {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        if(len(tensors_list) != 1):\n",
    "            print(\"tensors_list: \", tensors_list)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def map_answer_positions(answer_start, answer_end, context_ids, context_offset):\n",
    "            # form ids' character offset to tokens' positon\n",
    "            context_ids_str = \" \".join(map(str, context_ids)) \n",
    "            start_position = context_ids_str[:answer_start].count(\" \") + context_offset\n",
    "            end_position = context_ids_str[:answer_end-1].count(\" \") + context_offset\n",
    "  \n",
    "            return (start_position, end_position)\n",
    "        \n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            context = self.tokenizer.sep_token + ' ' + (' ' + self.tokenizer.sep_token + ' ').join(paragraph[\"context\"] )\n",
    "            context = EXTRA_ANSWERS + context\n",
    "            context = ' '.join(context.split()) # white space fix\n",
    "#             print(\"self.tokenizer.sep_token: \", self.tokenizer.sep_token)\n",
    "#             print(\"self.tokenizer.sep_token == '</s>': \", self.tokenizer.sep_token == '</s>')\n",
    "            \n",
    "#             context = '[SEP]' + ' ' + (' [SEP] ').join(paragraph[\"context\"] )\n",
    "#             context = EXTRA_ANSWERS + context\n",
    "#             context = ' '.join(context.split())\n",
    "            \n",
    "#             doc_tokens = []\n",
    "#             char_to_word_offset = []\n",
    "#             prev_is_whitespace = True\n",
    "#             for c in context:\n",
    "#                 if is_whitespace(c):\n",
    "#                     prev_is_whitespace = True\n",
    "#                 else:\n",
    "#                     if prev_is_whitespace:\n",
    "#                         doc_tokens.append(c) # add a new token\n",
    "#                     else:\n",
    "#                         doc_tokens[-1] += c  # append the character to the last token\n",
    "#                     prev_is_whitespace = False\n",
    "#                 char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            \n",
    "#             print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "#             print(\"char_to_word_offset: \", char_to_word_offset)\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "#                 print(\"question text: \", question_text)  \n",
    "                sp_sent = qa[\"is_sup_fact\"]\n",
    "                sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None \n",
    "\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent) \n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    " \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                \n",
    "                   \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                 \n",
    "#                 query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                question_text = \" \".join(question_text.split()[:self.max_question_len])\n",
    "#                 tok_to_orig_index = []\n",
    "#                 orig_to_tok_index = []\n",
    "#                 all_doc_tokens = []\n",
    "                \n",
    "#                 # each original token in the context is tokenized to multiple sub_tokens\n",
    "#                 for (i, token) in enumerate(doc_tokens):\n",
    "#                     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "#                     # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "#                     # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "#                     # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "#                     # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "#                     sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "#                     for sub_token in sub_tokens:\n",
    "#                         tok_to_orig_index.append(i)\n",
    "#                         all_doc_tokens.append(sub_token)\n",
    "                \n",
    "#                 # all sub tokens, truncate up to limit\n",
    "#                 all_doc_tokens = all_doc_tokens[:self.max_doc_len-8] \n",
    "\n",
    "#                 # The -8 accounts for CLS, QUESTION_START, QUESTION_END， [/par]， yes， no， null， </s>   \n",
    "#                 max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 8\n",
    "#                 if(max_tokens_per_doc_slice <= 0):\n",
    "#                     print(\"(max_tokens_per_doc_slice <= 0)\")\n",
    "#                 assert max_tokens_per_doc_slice > 0\n",
    "#                 if self.doc_stride < 0:                           # default\n",
    "#                     # negative doc_stride indicates no sliding window, but using first slice\n",
    "#                     self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "#                 segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "                inputs = self.tokenizer(question_text, context, return_tensors='pt', truncation=True, max_length=512) \n",
    "                \n",
    "                input_ids = inputs['input_ids'][0].tolist()\n",
    "                input_mask = [1] * len(input_ids)\n",
    "#                 print(\"size of input_ids in one_example\", str(len(input_ids)))\n",
    "#                 print(\"id: \", qa['id'], \"question_text: \", question_text, \"context: \", context)\n",
    "                    \n",
    "                sep_token_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.sep_token)\n",
    "                context_offset = input_ids.index(sep_token_id)+1\n",
    "                context_ids = input_ids[context_offset:]\n",
    "# #                 print(\"before for\")\n",
    "#                 for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "#                     slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "#                     doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "#                     tokens = [self.tokenizer.cls_token] + [QUESTION_START] + query_tokens + [QUESTION_END] + doc_slice_tokens + [PAR] + self.tokenizer.tokenize(\"yes\") + self.tokenizer.tokenize(\"no\") + self.tokenizer.tokenize(\"null\") +  [self.tokenizer.eos_token]   \n",
    "#                     segment_ids = [0] * (len(query_tokens) + 3) + [1] * (len(doc_slice_tokens) + 5) \n",
    "# #                     if(len(segment_ids) != len(tokens)):\n",
    "# #                         print(\"len(segment_ids): \", len(segment_ids))\n",
    "# #                         print(\"len(tokens): \", len(tokens))\n",
    "#                     assert len(segment_ids) == len(tokens)\n",
    "\n",
    "#                     input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "#                     input_mask = [1] * len(input_ids)\n",
    "\n",
    "#                     doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    \n",
    "                    # ===== answer positions tensors  ============\n",
    "\n",
    "                start_positions = []\n",
    "                end_positions = []\n",
    "\n",
    "                answer = qa[\"answer\"] \n",
    "#                 print(\"answer: \", answer)\n",
    "                if answer == 'yes':\n",
    "                    q_type = 1\n",
    "                    start_positions.append(context_offset)   \n",
    "                    end_positions.append(context_offset) \n",
    "                elif answer == 'no':\n",
    "                    q_type = 2\n",
    "                    start_positions.append(context_offset+1)   \n",
    "                    end_positions.append(context_offset+1)  \n",
    "                else:\n",
    "                    # keep all the occurences of answer in the context \n",
    "#                         for m in re.finditer(\"\\s?\".join(answer.split()), context):   # \"\\s?\".join(answer.split()) in order to match even with extra space in answer or context\n",
    "\n",
    "                    answer_ids = self.tokenizer(_normalize_text(answer), return_tensors='pt',max_length=512,truncation=True)['input_ids'][0].tolist()[1:-1]\n",
    "\n",
    "\n",
    "                    for m in re.finditer(\" \".join(map(str, answer_ids)), \" \".join(map(str, context_ids)), re.IGNORECASE):\n",
    "                        answer_start, answer_end = m.span() \n",
    "                        start_position, end_position = map_answer_positions(answer_start, answer_end, context_ids, context_offset)\n",
    "#                             if(start_position != -1):\n",
    "                        start_positions.append(start_position)   \n",
    "                        end_positions.append(end_position)\n",
    "\n",
    "                    if(len(start_positions) > 0): \n",
    "                        q_type = 0\n",
    "                    else: # answer not found in context\n",
    "                        q_type = 3 \n",
    "                        start_positions.append(context_offset+2)   \n",
    "                        end_positions.append(context_offset+2)  \n",
    "                            \n",
    "\n",
    "                # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                start_positions = start_positions[:self.max_num_answers]\n",
    "                end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                # -1 padding up to self.max_num_answers\n",
    "                padding_len = self.max_num_answers - len(start_positions)\n",
    "                start_positions.extend([-1] * padding_len)\n",
    "                end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                found_start_positions = set()\n",
    "                found_end_positions = set()\n",
    "                for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "\n",
    "                    if start_position in found_start_positions:\n",
    "                        start_positions[i] = -1\n",
    "                    if end_position in found_end_positions:\n",
    "                        end_positions[i] = -1\n",
    "                    found_start_positions.add(start_position)\n",
    "                    found_end_positions.add(end_position)\n",
    "\n",
    "#                         # for debug\n",
    "#                         if(start_position != -1):\n",
    "#                             answer_token_ids = input_ids[start_position: end_position+1]\n",
    "#                             answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids)\n",
    "#                             answer_text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#                             print(\"answer_text: \", answer_text)\n",
    "\n",
    "#                     if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                    # Zero-pad up to the sequence length.\n",
    "#                 padding_len = self.max_seq_len - len(input_ids)\n",
    "# #                 print(\"size of input_ids before padding\", str(len(input_ids)))\n",
    "#                 input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "# #                 print(\"size of input_ids after padding\", str(len(input_ids)))\n",
    "#                 input_mask = [1] * len(input_ids)\n",
    "#                 segment_ids = [0]  * len(input_ids)\n",
    "\n",
    "#                 assert len(input_ids) == self.max_seq_len\n",
    "#                 assert len(input_mask) == self.max_seq_len\n",
    "#                 assert len(segment_ids) == self.max_seq_len  \n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "#                 print(\"input_ids_list: \", input_ids_list)\n",
    "                input_mask_list.append(input_mask)\n",
    "#                 segment_ids_list.append(segment_ids)\n",
    "                start_positions_list.append(start_positions)\n",
    "                end_positions_list.append(end_positions)\n",
    "                q_type_list.append(q_type)\n",
    "                    \n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), \n",
    "#                                      torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                     qa['id'], answer))     \n",
    "#             print(\"tensors_list: \", tensors_list)\n",
    "        return tensors_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 2  # qid and answer  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7fb0003df6a0>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    " \n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#         num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [TITLE_START, TITLE_END, SENT_MARKER_END, QUESTION_START , QUESTION_END, PAR]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "#         self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.num_labels = 2  # start/end\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 4)   #  question type (yes/no/span/null) classification \n",
    "        \n",
    "#         self.fnn_sp_sent = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "        \n",
    "#         self.fnn_sp_para = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "         \n",
    "        \n",
    "        self.train_dataloader_object = self.val_dataloader_object  = None #= self.test_dataloader_object = None\n",
    "        \n",
    " \n",
    "    def load_model(self):\n",
    "        \n",
    "#         config = LongformerConfig.from_pretrained(self.args.model_path) \n",
    "#         config.attention_mode = self.args.attention_mode\n",
    "#         model = Longformer.from_pretrained(self.args.model_path, config=config)\n",
    "\n",
    "        model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#         tokenizer = tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') #AutoTokenizer.from_pretrained(\"/xdisk/msurdeanu/fanluo/hotpotQA/tiny-longformer-random\")\n",
    "\n",
    "#         model = TFAutoModel.from_pretrained(\"lysandre/tiny-longformer-random\")\n",
    "\n",
    "#         for layer in model.encoder.layer:\n",
    "#             layer.attention.self.attention_mode = self.args.attention_mode\n",
    "#             self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "# %%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "    ###################################################### dataloaders ########################################################### \n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl  \n",
    "        return self.train_dataloader_object\n",
    "    \n",
    " \n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "\n",
    "        \n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "#     @pl.data_loader\n",
    "#     def test_dataloader(self):\n",
    "#         if self.test_dataloader_object is not None:\n",
    "#             return self.test_dataloader_object\n",
    "#         dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "#                                   max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "#                                   doc_stride=self.args.doc_stride,\n",
    "#                                   max_num_answers=self.args.max_num_answers,\n",
    "#                                   max_question_len=self.args.max_question_len,\n",
    "#                                   ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "# #         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "#         dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "#                         num_workers=self.args.num_workers, \n",
    "#                         collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "#         self.test_dataloader_object = dl\n",
    "#         return self.test_dataloader_object\n",
    "\n",
    "    #%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, start_positions, end_positions, q_type):\n",
    " \n",
    "        if(input_ids.size(0) > 1):\n",
    "            assert(\"multi rows per document\")\n",
    "        # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "        # Make sure all rows have the same question length.\n",
    "        \n",
    "#         # local attention everywhere\n",
    "#         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "#         # global attention for the cls and all question tokens\n",
    "#         question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "# #         if(question_end_index.size(0) == 1):\n",
    "# #             attention_mask[:,:question_end_index.item()] = 2  \n",
    "# #         else:\n",
    "#         attention_mask[:,:question_end_index[0].item()+1] = 2  # from <cls> until </q>\n",
    "# #             print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "        \n",
    "#         # global attention for the sentence and paragraph special tokens  \n",
    "#         sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "#         attention_mask[:, sent_indexes] = 2\n",
    "        \n",
    "#         para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "#         attention_mask[:, para_indexes] = 2       \n",
    "         \n",
    "\n",
    "#         # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "#         input_ids, attention_mask = pad_to_window_size(\n",
    "#             input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id) \n",
    "        \n",
    "#         print(\"tokens: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n",
    "#         print(\"size of input_ids\", str(input_ids.size()))\n",
    "        if(input_ids.shape[0] == 0):\n",
    "            print(\"input_ids: \", input_ids)\n",
    "        sequence_output = self.model(input_ids)[0]\n",
    "#         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "        print(\"sequence_output \", sequence_output)\n",
    "#         (\n",
    "#                 input_ids,\n",
    "#                 attention_mask=attention_mask)[0]\n",
    "#         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "\n",
    "        # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "        # before computing loss and decoding.\n",
    "        padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "        if padding_len > 0:\n",
    "            sequence_output = sequence_output[:, :-padding_len]\n",
    "#         print(\"size of sequence_output after removing padding: \" + str(sequence_output.size()))\n",
    "              \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  \n",
    "    \n",
    "        start_logits = start_logits.squeeze(-1) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "#         print(\"start_logits in forward: \", start_logits)\n",
    "#         print(\"end_logits in forward: \", end_logits)\n",
    "#         ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "        type_logits = self.linear_type(sequence_output[:,0]) \n",
    "        \n",
    "#         ### 3. supporting paragraph classification ###  \n",
    "#         sp_para_output = sequence_output[:,para_indexes,:]  \n",
    "#         sp_para_output_t = self.fnn_sp_para(sp_para_output) \n",
    "\n",
    "#          # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no.   \n",
    "#         # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "#         sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "#         predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous() \n",
    " \n",
    "#         ### 4. supporting fact classification ###     \n",
    "#         # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    " \n",
    "#         sp_sent_output = sequence_output[:,sent_indexes,:]  \n",
    "#         sp_sent_output_t = self.fnn_sp_sent(sp_sent_output)     \n",
    "#         sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "#         predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous() \n",
    "        \n",
    " \n",
    "        answer_loss, type_loss = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits) \n",
    "#         explainer = shap.GradientExplainer( (logits, sequence_output), self.qa_outputs(sequence_output))\n",
    "#         print(self.explainer)\n",
    "\n",
    "        \n",
    "        return answer_loss, type_loss, start_logits, end_logits, type_logits \n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "#             if not self.args.regular_softmax_loss:\n",
    "#                 # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "#                 # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "#                 # but batch size is always 1, so this is not a problem\n",
    "            start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    "            end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "#             else: \n",
    "#             start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "#             end_positions = end_positions[:, 0:1]\n",
    "#             start_loss = crossentropy(start_logits, start_positions[:, 0])\n",
    "#             end_loss = crossentropy(end_logits, end_positions[:, 0])\n",
    "            crossentropy = torch.nn.CrossEntropyLoss()\n",
    "            type_loss = crossentropy(type_logits, q_type)  \n",
    " \n",
    " \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss , type_loss\n",
    "\n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_tokens):\n",
    "        assert(input_ids.size(0)==1) \n",
    "        mask = input_ids != input_ids # initilaize \n",
    "        for special_token in special_tokens:\n",
    "            mask = torch.logical_or(mask, input_ids.eq(self.tokenizer.convert_tokens_to_ids(special_token))) \n",
    " \n",
    "        token_indices = torch.nonzero(mask)    \n",
    "         \n",
    " \n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    " \n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "    \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    " \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    " \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    " \n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    " \n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm) \n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "#         loss = torch.tanh(loss)\n",
    "#         print(\"final loss: \" + str(loss)) \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.nonzero(sp_sent, as_tuple=True)[1]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2\n",
    "\n",
    "\n",
    "# import torch\n",
    "# special_tokens = [1,2]\n",
    "# input_ids = torch.tensor([[ 1, 0, 2, 1, 0, 2]])\n",
    "\n",
    "# mask = input_ids != input_ids # initilaize \n",
    "# for special_token in special_tokens:\n",
    "#     mask = torch.logical_or(mask, input_ids.eq(special_token)) \n",
    "#     print(\"mask: \", mask)\n",
    "# torch.nonzero(mask)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # # print(tokenizer.all_special_tokens)    \n",
    "# # # # print(tokenizer.all_special_ids)     \n",
    "# # # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # # tokenizer.sep_token\n",
    "# print(tokenizer.tokenize(\"yes\"))\n",
    "# print(tokenizer.tokenize(\"no\"))\n",
    "# print(tokenizer.tokenize(\"null\"))\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    self.scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)  # scheduler is not saved in the checkpoint, but global_step is, which is enough to restart\n",
    "    self.scheduler.step(self.global_step)\n",
    "    print(\"global step: \", self.global_step)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimizer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook to do a lot of non-standard training tricks such as learning-rate warm-up\n",
    "def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None,using_native_amp=None):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.scheduler.step(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, subword_starts, subword_ends, q_type, qid, answer = batch \n",
    "#     print(\"input_ids: \" + str(input_ids)) \n",
    "    # print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, subword_starts, subword_ends, q_type)\n",
    "    answer_loss, type_loss = output[:2]\n",
    "#     print(self.explainer(input_ids))\n",
    "#     answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    # print(\"answer_loss: \", answer_loss)\n",
    "    # print(\"type_loss: \", type_loss)\n",
    "    # print(\"sp_para_loss: \", sp_para_loss)\n",
    "    # print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "\n",
    "    loss = answer_loss + 5 * type_loss #+ sp_para_loss + sp_sent_loss\n",
    "#     print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it.  \n",
    "\n",
    "    tensorboard_logs = {'loss': loss, 'train_answer_loss': answer_loss, 'train_type_loss': type_loss,\n",
    "                        'lr': lr,\n",
    "                        'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) }\n",
    "    return tensorboard_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "    # # the function is called for each batch after every epoch is completed\n",
    "    # def training_end(self, output): \n",
    "    #     # print(\"training_end at epoch: \", self.current_epoch)\n",
    "    # #     print(\"len(outputs): \",len(outputs))\n",
    "    # #     print(\"output: \",output)\n",
    "    \n",
    "    #     # one batch only has one example\n",
    "    #     avg_loss = output['loss']    \n",
    "    #     avg_answer_loss = output['train_answer_loss']  \n",
    "    #     avg_type_loss = output['train_type_loss']    \n",
    "    #     avg_sp_para_loss = output['train_sp_para_loss']   \n",
    "    #     avg_sp_sent_loss = output['train_sp_sent_loss'] \n",
    "    #     avg_lr = output['lr']      \n",
    "         \n",
    "     \n",
    "    #     if self.trainer.use_ddp:\n",
    "    #         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_answer_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_type_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_para_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_sent_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_lr, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_lr /= self.trainer.world_size \n",
    "            \n",
    "     \n",
    "    #     tensorboard_logs = { #'avg_train_loss': avg_loss, \n",
    "    #             'avg_train_answer_loss': avg_answer_loss, 'avg_train_type_loss': avg_type_loss, 'avg_train_sp_para_loss': avg_sp_para_loss, 'avg_train_sp_sent_loss': avg_sp_sent_loss, 'lr': avg_lr\n",
    "    #           }\n",
    "    \n",
    "    #     return {'loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, subword_starts, subword_ends, q_type, qid, answer = batch \n",
    "    output = self.forward(input_ids, subword_starts, subword_ends, q_type)\n",
    "    answer_loss, type_loss, start_logits, end_logits, type_logits = output\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    answers_pred = self.decode(input_ids, start_logits, end_logits, type_logits)\n",
    "    loss = answer_loss + 5*type_loss \n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "    print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "    print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = _normalize_text(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "#         print(\"f1: \" + str(f1))\n",
    "#         print(\"prec: \" + str(prec))\n",
    "#         print(\"recall: \" + str(recall))\n",
    "#         print(\"em: \" + str(em))  \n",
    "\n",
    "#     if(len(sp_sent_pred) > 0):\n",
    "#         sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "#         sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "#         sp_sent_precision = torch.tensor(sp_sent_precision).type_as(loss)\n",
    "#         sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "#         sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "\n",
    "# #         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "# #         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "# #         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "# #         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "\n",
    "#         joint_prec = prec * sp_sent_precision\n",
    "#         joint_recall = recall * sp_sent_recall\n",
    "#         if joint_prec + joint_recall > 0:\n",
    "#             joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "#         else:\n",
    "#             joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "#         joint_em = em * sp_sent_em \n",
    "\n",
    "#     else:\n",
    "#         sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "#         joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, # 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "               'answer_score': pre_answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em #,\n",
    "            #   'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "           #    'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits):\n",
    "#         print(\"decode\")\n",
    "\n",
    "    question_end_index = self._get_special_index(input_ids, [self.tokenizer.sep_token])[0]\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "\n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "#     print(\"start_logits_indices: \", start_logits_indices)\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#         print(\"p_type: \", p_type)\n",
    "#         print(\"p_type_score: \", p_type_score)\n",
    "\n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#         print(\"sorted_answers: \" + str(sorted_answers))\n",
    "\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'null', 'score': 1})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "\n",
    "            # remove </s>\n",
    "            special_token = self.tokenizer.sep_token \n",
    "            try:\n",
    "                if(answer_tokens[0] == special_token):\n",
    "                    answer['start_logit'] = -2000000\n",
    "                elif(answer_tokens[-1] == special_token):\n",
    "                    answer['end_logit'] = -2000000\n",
    "                answer_tokens.remove(special_token)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#                 score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            score = (torch.sigmoid(answer['start_logit']) + torch.sigmoid(answer['end_logit']) + torch.sigmoid(p_type_score)) / 3\n",
    "            answers.append({'text': text, 'score': score})\n",
    "            print(\"pred answers: \" + str(answers))\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score})\n",
    "    elif p_type == 3:\n",
    "        answers.append({'text': 'null', 'score': p_type_score})\n",
    "    else:\n",
    "        assert False \n",
    "\n",
    "\n",
    "#     sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "#     para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "\n",
    "#     s_to_p_map = []\n",
    "#     for s in sent_indexes:\n",
    "#         s_to_p = torch.where(torch.le(para_indexes, s))[0][-1]     # last para_index smaller or equal to s\n",
    "#         s_to_p_map.append(s_to_p.item()) \n",
    "# #         print(\"s_to_p_map: \" + str(s_to_p_map))\n",
    "\n",
    "# #         print(\"sp_para_logits\", sp_para_logits)\n",
    "# #         print(\"sp_sent_logits\", sp_sent_logits)\n",
    "\n",
    "#     sp_para_top2 = sp_para_logits.squeeze().topk(k=2).indices\n",
    "#     if(sp_sent_logits.squeeze().size(0) > 12):\n",
    "#         sp_sent_top12 = sp_sent_logits.squeeze().topk(k=12).indices\n",
    "#     else:\n",
    "#         sp_sent_top12 = sp_sent_logits.squeeze().topk(k=sp_sent_logits.squeeze().size(0)).indices\n",
    "# #         print(\"sp_para_top2\", sp_para_top2)\n",
    "# #         print(\"sp_sent_top12\", sp_sent_top12)\n",
    "\n",
    "#     sp_sent_pred = set()\n",
    "#     sp_para_pred = set(sp_para_top2.tolist())\n",
    "#     for sp_sent in sp_sent_top12:\n",
    "#         sp_sent_to_para = s_to_p_map[sp_sent.item()]\n",
    "#         if sp_sent_to_para in sp_para_top2:\n",
    "#             sp_sent_pred.add(sp_sent.item())\n",
    "# #             sp_para_pred.add(sp_sent_to_para) \n",
    "# #         print(\"sp_sent_pred: \" + str(sp_sent_pred))\n",
    "# #         print(\"sp_para_pred: \" + str(sp_para_pred))\n",
    "#     return (answers, sp_sent_pred, sp_para_pred)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# def _normalize_text(self, s):\n",
    "\n",
    "#     def remove_articles(text):\n",
    "#         return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "#     def white_space_fix(text):\n",
    "#         return ' '.join(text.split())\n",
    "\n",
    "#     def remove_punc(text):\n",
    "#         exclude = set(string.punctuation)\n",
    "#         return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "#     def lower(text):\n",
    "#         return text.lower()\n",
    "\n",
    "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = _normalize_text(prediction)\n",
    "    normalized_ground_truth = _normalize_text(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(_normalize_text(prediction) == _normalize_text(ground_truth))\n",
    "\n",
    "\n",
    "def sp_metrics(self, prediction, gold): \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in prediction:\n",
    "        if e in gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "    for e in gold:\n",
    "        if e not in prediction:\n",
    "            fn += 1 \n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0 \n",
    "    return em, prec, recall, f1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_end(self, outputs):\n",
    "    print(\"validation_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "#     avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "#     avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs] \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "#     sp_sent_f1_scores = [x['sp_f1'] for x in outputs]   \n",
    "#     sp_sent_em_scores = [x['sp_em'] for x in outputs]   \n",
    "#     sp_sent_prec_scores = [x['sp_prec'] for x in outputs]   \n",
    "#     sp_sent_recall_scores = [x['sp_recall'] for x in outputs]   \n",
    "#     joint_f1_scores = [x['joint_f1'] for x in outputs]  \n",
    "#     joint_em_scores = [x['joint_em'] for x in outputs]  \n",
    "#     joint_prec_scores = [x['joint_prec'] for x in outputs]  \n",
    "#     joint_recall_scores = [x['joint_recall'] for x in outputs]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_sp_para_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "#         sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "#         sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "#         sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "#         sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "#         joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "#         joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "#         joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "#         joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)    \n",
    "    avg_val_em = sum(em_scores) / len(em_scores)    \n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)  \n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores)    \n",
    "#     avg_val_sp_sent_f1 = sum(sp_sent_f1_scores) / len(sp_sent_f1_scores)    \n",
    "#     avg_val_sp_sent_em = sum(sp_sent_em_scores) / len(sp_sent_em_scores)    \n",
    "#     avg_val_sp_sent_prec = sum(sp_sent_prec_scores) / len(sp_sent_prec_scores)  \n",
    "#     avg_val_sp_sent_recall = sum(sp_sent_recall_scores) / len(sp_sent_recall_scores)    \n",
    "#     avg_val_joint_f1 = sum(joint_f1_scores) / len(joint_f1_scores)  \n",
    "#     avg_val_joint_em = sum(joint_em_scores) / len(joint_em_scores)  \n",
    "#     avg_val_joint_prec = sum(joint_prec_scores) / len(joint_prec_scores)    \n",
    "#     avg_val_joint_recall = sum(joint_recall_scores) / len(joint_recall_scores)  \n",
    "\n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t')   \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "#     print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t')   \n",
    "#     print(\"avg_sp_sent_loss: \", avg_sp_sent_loss)   \n",
    "    print(\"avg_val_f1: \", avg_val_f1, end = '\\t')   \n",
    "    print(\"avg_val_em: \", avg_val_em, end = '\\t')   \n",
    "    print(\"avg_val_prec: \", avg_val_prec, end = '\\t')   \n",
    "    print(\"avg_val_recall: \", avg_val_recall)   \n",
    "#     print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1, end = '\\t')   \n",
    "#     print(\"avg_val_sp_sent_em: \" , avg_val_sp_sent_em, end = '\\t')  \n",
    "#     print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec, end = '\\t')   \n",
    "#     print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)   \n",
    "#     print(\"avg_val_joint_f1: \" , avg_val_joint_f1, end = '\\t')  \n",
    "#     print(\"avg_val_joint_em: \", avg_val_joint_em, end = '\\t')   \n",
    "#     print(\"avg_val_joint_prec: \", avg_val_joint_prec, end = '\\t')   \n",
    "#     print(\"avg_val_joint_recall: \", avg_val_joint_recall)   \n",
    "\n",
    "\n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, \n",
    "            #'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss,   \n",
    "    'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall#,    \n",
    "    #'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,    \n",
    "   # 'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall \n",
    "    }   \n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "# def test_step(self, batch, batch_nb):\n",
    "#     input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "\n",
    "#     print(\"test_step of qid: \", qid, end=\"\\t\") \n",
    "#     output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "#     answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "#     loss = answer_loss + 5*type_loss + 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "#     answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "\n",
    "#     if(len(answers_pred) != 1):\n",
    "#         print(\"len(answers_pred) != 1\")\n",
    "#         assert(len(answers_pred) == 1)\n",
    "\n",
    "#     pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "#     pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "#     # print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "#     # print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "#     gold_answer = _normalize_text(answer)\n",
    "\n",
    "#     print(\"pre_answer:\\t\", pre_answer, \"\\tgold_answer:\\t\", gold_answer)\n",
    "\n",
    "#     return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss, 'answer_score': pre_answer_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "# def test_end(self, outputs):\n",
    "#     print(\"test_end\")\n",
    "#     avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "#     avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "#     avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "#     avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "#     avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "#     answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "\n",
    "\n",
    "#     print(f'before sync --> sizes:  {len(answer_scores)}')\n",
    "#     if self.trainer.use_ddp:\n",
    "#         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_answer_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_type_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_sp_para_loss /= self.trainer.world_size \n",
    "#         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "#         avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "# #         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "#         answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "#     print(f'after sync --> sizes: {len(answer_scores)}')\n",
    "#     # print(\"answer_scores: \", answer_scores)\n",
    "\n",
    "#     # print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "#     # print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "#     # print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "#     # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "#     # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "\n",
    "#     logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss\n",
    "#            }\n",
    "\n",
    "#     return {'avg_val_loss': avg_loss, 'log': logs} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHECKPOINT_HYPER_PARAMS_KEY',\n",
       " 'CHECKPOINT_HYPER_PARAMS_NAME',\n",
       " 'CHECKPOINT_HYPER_PARAMS_TYPE',\n",
       " 'T_destination',\n",
       " '_LightningModule__get_hparams_assignment_variable',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_apply',\n",
       " '_auto_collect_arguments',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_init_slurm_connection',\n",
       " '_load_from_state_dict',\n",
       " '_load_model_state',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_hparams',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'amp_scale_loss',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'example_input_array',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'get_progress_bar_dict',\n",
       " 'get_tqdm_dict',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'hparams',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_fit_end',\n",
       " 'on_fit_start',\n",
       " 'on_gpu',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'optimizer_zero_grad',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'prepare_data',\n",
       " 'print',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'save_hyperparameters',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'sp_metrics',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'teardown',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_epoch_end',\n",
       " 'test_step',\n",
       " 'test_step_end',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_epoch_end',\n",
       " 'training_step',\n",
       " 'training_step_end',\n",
       " 'transfer_batch_to_device',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_epoch_end',\n",
       " 'validation_step',\n",
       " 'validation_step_end',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHECKPOINT_HYPER_PARAMS_KEY', 'hyper_parameters'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_NAME', 'hparams_name'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_TYPE', 'hparams_type'),\n",
       " ('T_destination', ~T_destination),\n",
       " ('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__abstractmethods__', frozenset()),\n",
       " ('__annotations__',\n",
       "  {'_device': Ellipsis, '_dtype': typing.Union[str, torch.dtype]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, start_positions, end_positions, q_type)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset(),\n",
       "                '_abc_registry': <_weakrefset.WeakSet at 0x7faf63329550>,\n",
       "                '_abc_cache': <_weakrefset.WeakSet at 0x7faf63329588>,\n",
       "                '_abc_negative_cache': <_weakrefset.WeakSet at 0x7faf633295f8>,\n",
       "                '_abc_negative_cache_version': 72,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'optimizer_step': <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None, using_native_amp=None)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'sp_metrics': <function __main__.sp_metrics(self, prediction, gold)>,\n",
       "                'validation_end': <function __main__.validation_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7faf633064e0>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'ABC' objects>),\n",
       " ('_abc_cache', <_weakrefset.WeakSet at 0x7faf63329588>),\n",
       " ('_abc_negative_cache', <_weakrefset.WeakSet at 0x7faf633295f8>),\n",
       " ('_abc_negative_cache_version', 72),\n",
       " ('_abc_registry', <_weakrefset.WeakSet at 0x7faf63329550>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_auto_collect_arguments',\n",
       "  <bound method LightningModule._auto_collect_arguments of <class '__main__.hotpotqa'>>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_load_model_state',\n",
       "  <bound method ModelIO._load_model_state of <class '__main__.hotpotqa'>>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('device', <property at 0x7faf754009f8>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('dtype', <property at 0x7faf75400958>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('example_input_array', <property at 0x7faf753f0908>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('hparams', <property at 0x7faf753a5b88>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method ModelIO.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method ModelIO.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_gpu', <property at 0x7fafe153b138>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None, using_native_amp=None)>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_dataloader(self) -> Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader]]>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('test_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None, using_native_amp=None)>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_dataloader(self) -> Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader]]>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('test_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if not args.test:     # if it needs to train, remove exsiting folder\n",
    "        import shutil\n",
    "        save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "        if os.path.exists(save_folder):\n",
    "            shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder \n",
    "        \n",
    "    import shutil\n",
    "    save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder\n",
    "  \n",
    "    print(\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/lysandre/tiny-longformer-random\n",
    "# import os \n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with config:\n",
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hotpotqa(\n",
       "  (model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (linear_type): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "    model.to('cuda')    # this is necessary to use gpu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_size:  9.0\n",
      "num_devices:  1\n",
      ">>>>>>> #train_set_size: 90447.0, #steps: 271341.0,  #warmup steps: 1000, #epochs: 6, batch_size: 2 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    " \n",
    "    train_set_size = 9 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size) \n",
    "\n",
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus!='' else None\n",
    "    num_devices = 1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "\n",
    "    train_set_size = 90447 * args.train_percent    # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps},  #warmup steps: {args.warmup}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Multi-processing is handled by Slurm.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                             track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                             accumulate_grad_batches=args.batch_size,\n",
    "                             train_percent_check = args.train_percent,\n",
    "        #                          val_check_interval=args.val_every,\n",
    "                             val_percent_check=args.val_percent_check,\n",
    "                             test_percent_check=args.val_percent_check,\n",
    "                             logger=logger if not args.disable_checkpointing else False,\n",
    "                             checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                             show_progress_bar=args.no_progress_bar,\n",
    "#                              use_amp=True, #not args.fp32, \n",
    "                             amp_level='O2',\n",
    "                             check_val_every_n_epoch=1\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=ddp\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name        | Type            | Params\n",
      "------------------------------------------------\n",
      "0 | model       | DistilBertModel | 66 M  \n",
      "1 | qa_outputs  | Linear          | 1 K   \n",
      "2 | linear_type | Linear          | 3 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: small_3.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2081,  0.1313,  0.0021,  ..., -0.1503,  0.3243,  0.4670],\n",
      "         [-0.6907, -0.1512,  0.5700,  ...,  0.0389,  0.3567,  0.3901],\n",
      "         [-0.4784,  0.0030,  0.7859,  ...,  0.0939,  0.0775,  0.5939],\n",
      "         ...,\n",
      "         [ 0.1300,  0.3617,  0.6407,  ..., -0.3245,  0.0267, -0.4984],\n",
      "         [-0.8131,  0.0707,  0.0075,  ...,  0.0313, -0.1685, -0.4562],\n",
      "         [ 0.5895,  0.0287,  0.0027,  ...,  0.1481, -0.1669, -0.3316]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '##ogene boundary k – t boundary diamond lane in united states and canada diamond lane', 'score': tensor([0.6127], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6127], device='cuda:0')\n",
      "pred answer_text: ogene boundary k – t boundary diamond lane in united states and canada diamond lane\n",
      "sequence_output  tensor([[[-0.3084, -0.0458,  0.0272,  ..., -0.0851,  0.4171,  0.1915],\n",
      "         [-0.6778, -0.0116, -0.1904,  ...,  0.2765,  0.9211, -0.4125],\n",
      "         [-0.9655,  0.1330,  0.3725,  ..., -0.2246,  0.6333,  0.0655],\n",
      "         ...,\n",
      "         [-0.2283,  0.2942,  0.1474,  ..., -0.1106, -0.2911, -0.6966],\n",
      "         [-0.0876, -0.2447,  0.2324,  ..., -0.3592,  0.0663, -1.0020],\n",
      "         [ 0.6905,  0.2877, -0.1340,  ...,  0.0823, -0.4717, -0.4543]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.3341], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/apps/python/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:245: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  2, 2, 2\n",
      "after sync --> sizes: 2, 2, 2\n",
      "avg_loss:  tensor(12.1461, device='cuda:0')\tavg_answer_loss:  tensor(6.2638, device='cuda:0')\tavg_type_loss:  tensor(1.1765, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "reading file: small.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46b1bd1d53942168f7de00a072f4cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.3904,  0.0459, -0.2371,  ..., -0.3261,  0.5910,  0.1487],\n",
      "         [-0.5020,  0.5125,  0.1685,  ..., -0.3094,  0.4663,  0.1151],\n",
      "         [-0.3808,  0.1151,  0.0027,  ..., -0.2887,  0.2294, -0.0303],\n",
      "         ...,\n",
      "         [ 0.0125,  0.4581, -0.0835,  ..., -0.0101,  0.1093, -0.8787],\n",
      "         [-0.8985,  0.3153, -0.2814,  ...,  0.1324,  0.1657, -0.5179],\n",
      "         [ 0.6341,  0.2662,  0.0062,  ...,  0.2137, -0.3857, -0.2166]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-4.6357e-04, -2.3459e-01, -1.0743e-01,  ...,  9.3042e-04,\n",
      "           5.0368e-01,  9.2797e-02],\n",
      "         [-4.4164e-01, -4.6205e-01,  3.0258e-01,  ...,  2.0298e-01,\n",
      "           1.0582e+00, -2.4062e-02],\n",
      "         [-3.1807e-01, -6.5877e-02,  8.2441e-02,  ...,  3.5384e-01,\n",
      "           2.6789e-02, -1.5009e+00],\n",
      "         ...,\n",
      "         [ 5.1507e-01, -3.1199e-01,  6.7351e-01,  ...,  2.0430e-02,\n",
      "           5.1879e-01, -1.0503e-01],\n",
      "         [ 5.1431e-01,  2.9910e-01,  1.4524e-01,  ...,  2.2721e-01,\n",
      "           9.6886e-02, -7.4572e-01],\n",
      "         [ 7.2895e-01,  1.4112e-01, -4.6202e-01,  ...,  2.0321e-01,\n",
      "          -4.7833e-01, -3.3038e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3870,  0.0222,  0.1126,  ..., -0.0999,  0.3763,  0.5514],\n",
      "         [-0.1564,  0.1651,  0.4247,  ...,  0.1211,  0.2990, -0.5199],\n",
      "         [-0.1306, -0.6498,  0.1905,  ...,  0.3442,  0.3978,  0.1776],\n",
      "         ...,\n",
      "         [-0.8486,  0.1788, -0.0120,  ...,  0.4235,  0.3454, -0.5461],\n",
      "         [-0.6788,  0.2186,  0.3051,  ...,  0.3183,  0.3765, -0.0713],\n",
      "         [ 0.6788,  0.2741, -0.2658,  ...,  0.2906, -0.4462, -0.4442]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3697,  0.0667,  0.2117,  ...,  0.1593,  0.2878,  0.0718],\n",
      "         [-0.0292, -0.0338,  0.1570,  ...,  0.4832,  0.2043, -0.3055],\n",
      "         [-0.0406,  0.3706, -0.1966,  ...,  0.1411,  0.3831, -0.3110],\n",
      "         ...,\n",
      "         [-0.7983, -0.3307,  0.3695,  ...,  0.3741,  0.3152,  0.3078],\n",
      "         [-0.4563,  0.1161,  0.4770,  ..., -0.0210, -0.1690, -0.4415],\n",
      "         [ 0.5206,  0.3836, -0.0949,  ...,  0.0298, -0.1822, -0.4882]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2869,  0.1821, -0.2543,  ..., -0.1111,  0.6019,  0.3458],\n",
      "         [-0.4279,  0.1928,  0.2067,  ...,  0.0646,  0.7926, -0.6749],\n",
      "         [-0.6408,  0.2244, -0.6158,  ..., -0.3683,  0.4180,  0.1266],\n",
      "         ...,\n",
      "         [ 1.0680,  0.6152,  0.3567,  ..., -0.4323,  0.4564, -0.0635],\n",
      "         [-0.6360, -0.1748, -0.0234,  ..., -0.2930,  0.2773, -0.0937],\n",
      "         [ 0.7198,  0.2172, -0.0682,  ...,  0.1670, -0.4030, -0.3150]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4137, -0.0385, -0.1106,  ..., -0.1086,  0.3341,  0.3403],\n",
      "         [ 0.3410,  0.1295,  0.4104,  ...,  0.5384,  0.6502, -0.3248],\n",
      "         [-0.8325, -0.5267,  0.1817,  ..., -0.0602,  0.1379, -0.0278],\n",
      "         ...,\n",
      "         [-0.6477,  0.2115, -0.0246,  ...,  0.2983,  0.4733, -0.3477],\n",
      "         [-0.2311, -0.3364, -0.1323,  ...,  0.2979, -0.0093, -0.4576],\n",
      "         [ 0.4965,  0.6371,  0.0756,  ...,  0.1102, -0.2420, -0.3609]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.0477e-01,  3.2633e-02, -3.1728e-01,  ...,  1.2294e-03,\n",
      "           3.5467e-01,  1.4987e-01],\n",
      "         [-5.5716e-01, -1.1592e-02,  9.2136e-02,  ...,  2.1327e-02,\n",
      "           3.4148e-01,  4.8016e-03],\n",
      "         [-6.2846e-01, -1.4136e-02,  3.1857e-01,  ..., -1.9286e-01,\n",
      "           1.6840e-01,  4.4128e-01],\n",
      "         ...,\n",
      "         [-4.1061e-01,  1.2061e-01, -1.0499e-01,  ...,  3.7049e-01,\n",
      "           3.2600e-01, -6.2592e-01],\n",
      "         [-1.3264e+00,  2.4022e-01,  1.5080e-01,  ...,  3.6018e-01,\n",
      "          -9.0085e-02, -8.6164e-01],\n",
      "         [ 7.7254e-01,  6.8490e-02, -2.0919e-01,  ...,  4.1098e-02,\n",
      "          -3.6276e-01, -4.6620e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5994, -0.1861, -0.2494,  ..., -0.0206,  0.7357,  0.0750],\n",
      "         [-0.5702, -0.0116, -0.1303,  ...,  0.2708,  0.6267, -0.1996],\n",
      "         [-0.3253,  0.5115, -0.2082,  ..., -0.0961,  0.3553,  0.1052],\n",
      "         ...,\n",
      "         [ 0.4649,  0.1773,  0.0875,  ...,  0.3442,  0.3325, -0.2650],\n",
      "         [-1.0597, -0.1968, -0.0550,  ...,  0.4956,  0.0163, -0.5287],\n",
      "         [ 0.3845,  0.0831, -0.2279,  ...,  0.1546, -0.0612, -0.4642]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3918,  0.0877,  0.1080,  ..., -0.0332,  0.5208,  0.1795],\n",
      "         [-0.3404,  0.2748,  0.3613,  ...,  0.3150,  0.6513,  0.4944],\n",
      "         [-0.3393, -0.0455,  0.0681,  ...,  0.1046,  0.0731, -0.1566],\n",
      "         ...,\n",
      "         [-0.0535,  0.5953,  0.0816,  ...,  0.0141,  0.0962, -1.0497],\n",
      "         [-0.6097, -0.0482, -0.0227,  ...,  0.3313,  0.4167, -0.5892],\n",
      "         [ 0.7678,  0.1836, -0.1633,  ...,  0.0713, -0.4468, -0.4812]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2226, -0.2310,  0.0965,  ...,  0.1793,  0.3274,  0.0568],\n",
      "         [-0.4948, -0.1132, -0.3210,  ..., -0.0450,  0.4213, -0.2713],\n",
      "         [-0.7240, -0.0126, -0.3579,  ...,  0.2638,  0.6216, -0.7539],\n",
      "         ...,\n",
      "         [ 0.2355, -0.0013,  0.3629,  ...,  0.2834,  0.0103, -0.6804],\n",
      "         [-0.3081, -0.0786,  0.1097,  ...,  0.2849, -0.1482, -0.9527],\n",
      "         [ 0.6602,  0.1109, -0.0196,  ...,  0.0403, -0.3276, -0.5827]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3135, -0.0312, -0.1152,  ...,  0.0554,  0.4132,  0.2590],\n",
      "         [-0.1965,  0.1690, -0.1282,  ...,  0.3847,  0.5301, -0.3160],\n",
      "         [-0.6028,  0.0948,  0.0594,  ...,  0.2271,  0.3997,  0.6648],\n",
      "         ...,\n",
      "         [-0.9395, -0.0116,  0.0511,  ...,  0.0396,  0.4956, -0.5108],\n",
      "         [ 0.0142, -0.3398, -0.0155,  ...,  0.1578,  0.0762, -0.5412],\n",
      "         [ 0.5604,  0.0503, -0.1949,  ..., -0.0119, -0.3318, -0.5876]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4064, -0.2687, -0.0505,  ...,  0.0702,  0.1826,  0.2776],\n",
      "         [-0.4110, -0.1165,  0.1333,  ...,  0.0870,  0.2889,  0.1047],\n",
      "         [-0.7550,  0.1443,  0.1232,  ..., -0.0023,  0.3267, -0.4666],\n",
      "         ...,\n",
      "         [ 0.3516, -0.0064,  0.7704,  ...,  0.4780, -0.1189, -1.0138],\n",
      "         [-0.5437, -0.2926, -0.0698,  ...,  0.3818,  0.4042, -0.4308],\n",
      "         [ 0.8098,  0.0412,  0.0255,  ...,  0.1459, -0.2989, -0.5201]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2537,  0.0323,  0.0991,  ...,  0.0402,  0.3042,  0.2092],\n",
      "         [-0.3466, -0.0015, -0.0550,  ...,  0.3797,  0.6894, -0.2084],\n",
      "         [-1.0383,  0.2295,  0.0659,  ...,  0.0516,  0.1723, -0.4592],\n",
      "         ...,\n",
      "         [-0.5692, -0.4321,  0.5966,  ..., -0.3128,  0.0368,  0.5100],\n",
      "         [-0.7202, -0.3016,  0.1868,  ...,  0.0456,  0.0487, -0.3863],\n",
      "         [ 0.6786, -0.1311,  0.0504,  ...,  0.0129, -0.4174, -0.4606]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-4.1259e-01, -3.3467e-02,  1.6395e-01,  ..., -1.2407e-01,\n",
      "           4.2692e-01,  1.2427e-01],\n",
      "         [-2.1611e-01, -5.2045e-02, -1.6751e-01,  ...,  2.6437e-01,\n",
      "           6.6341e-01, -3.3945e-01],\n",
      "         [-8.4250e-01, -5.7389e-04,  1.5216e-01,  ..., -8.6737e-03,\n",
      "           5.4976e-01,  1.8142e-01],\n",
      "         ...,\n",
      "         [ 7.2458e-01, -2.4413e-01,  1.0729e+00,  ...,  2.0981e-01,\n",
      "           2.6895e-01, -6.8063e-01],\n",
      "         [-1.8625e-01, -3.7418e-01,  5.3841e-01,  ..., -3.4635e-02,\n",
      "          -8.7702e-02, -6.2435e-03],\n",
      "         [ 7.5156e-01,  2.8763e-01,  4.4779e-02,  ...,  1.8969e-01,\n",
      "          -4.0421e-01, -4.9570e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2989, -0.0442, -0.0749,  ..., -0.1268,  0.3518,  0.1074],\n",
      "         [-0.1841,  0.2847, -0.1824,  ...,  0.4006,  0.9590, -0.1260],\n",
      "         [-1.0181,  0.6931,  0.2180,  ...,  0.0173,  0.6779,  0.9908],\n",
      "         ...,\n",
      "         [ 0.4094,  0.1942, -0.1673,  ...,  0.1424,  0.0279, -0.3470],\n",
      "         [ 0.1055,  0.0014,  0.2007,  ...,  0.3697,  0.0099, -0.3484],\n",
      "         [ 0.5180,  0.2169, -0.0107,  ..., -0.0335, -0.4352, -0.3535]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5142, -0.0879,  0.0229,  ..., -0.4140,  0.4435,  0.2116],\n",
      "         [-1.0106, -0.1898, -0.0224,  ...,  0.0429,  0.5266, -0.2882],\n",
      "         [-0.7883, -0.1038, -0.0043,  ..., -0.3033,  0.3038,  0.0010],\n",
      "         ...,\n",
      "         [-0.6516,  0.0832,  0.2987,  ..., -0.3011,  0.2552, -0.5871],\n",
      "         [ 0.1940, -0.2296,  0.4939,  ..., -0.1856,  0.2202,  0.0147],\n",
      "         [ 0.1090,  0.2565,  0.1056,  ..., -0.2879, -0.1975, -0.5458]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2033,  0.1306, -0.0358,  ..., -0.1263,  0.1861,  0.4801],\n",
      "         [-0.8557, -0.2174,  0.6216,  ...,  0.0014,  0.4200,  0.5031],\n",
      "         [-0.3822, -0.0371,  0.8095,  ...,  0.0956, -0.0407,  0.4688],\n",
      "         ...,\n",
      "         [ 0.1208,  0.3616,  0.6096,  ..., -0.3171,  0.0085, -0.5429],\n",
      "         [-0.8509,  0.0860, -0.0115,  ...,  0.0612, -0.1090, -0.5214],\n",
      "         [ 0.5817,  0.2646,  0.0635,  ...,  0.0961, -0.3221, -0.2930]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1455,  0.0615, -0.1608,  ..., -0.1001,  0.3265,  0.1327],\n",
      "         [-0.1869,  0.0140, -0.1366,  ...,  0.1964,  0.4264, -0.2942],\n",
      "         [-1.0685,  0.0141, -0.0424,  ..., -0.0672,  0.6264,  0.4623],\n",
      "         ...,\n",
      "         [-0.1176, -0.0398, -0.2111,  ..., -0.2693, -0.1418, -0.8128],\n",
      "         [-0.0450, -0.0683, -0.0558,  ..., -0.2868, -0.2066, -0.8483],\n",
      "         [ 0.5374, -0.0926, -0.0906,  ...,  0.2023, -0.3189, -0.4657]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5130,  0.0567, -0.0123,  ..., -0.0311,  0.3203,  0.2850],\n",
      "         [-0.0842, -0.0644,  0.1883,  ...,  0.3733,  0.2318,  0.0083],\n",
      "         [-0.0474, -0.2530,  0.1443,  ...,  0.0744,  0.3833,  0.7137],\n",
      "         ...,\n",
      "         [-0.3372,  0.5541,  0.0143,  ..., -0.2016,  0.0858, -0.3997],\n",
      "         [-0.6491, -0.4975, -0.0353,  ...,  0.2867,  0.5332, -0.3539],\n",
      "         [ 0.4005,  0.2485, -0.3602,  ..., -0.0412, -0.4812, -0.2955]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3234, -0.1369,  0.1933,  ..., -0.0173,  0.2508,  0.5523],\n",
      "         [-0.4080,  0.0474,  0.2648,  ...,  0.3064,  0.0829, -0.5121],\n",
      "         [-0.3423,  0.0251,  0.7865,  ..., -0.1384, -0.0274,  0.4826],\n",
      "         ...,\n",
      "         [ 0.0519, -0.6475,  0.5749,  ..., -0.1606, -0.0462, -0.9332],\n",
      "         [-0.2951,  0.0510,  0.2508,  ...,  0.1567, -0.0416, -0.2808],\n",
      "         [ 0.2095,  0.0175, -0.0373,  ...,  0.2044, -0.4303, -0.5171]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3122, -0.0427,  0.1212,  ..., -0.0301,  0.3262,  0.2840],\n",
      "         [ 0.0451,  0.4339,  0.1167,  ...,  0.0300,  0.4393, -0.5429],\n",
      "         [-0.5682,  0.0381,  0.3300,  ...,  0.4497,  0.1846,  0.5935],\n",
      "         ...,\n",
      "         [ 0.5931,  0.8051,  0.2614,  ...,  0.1871, -0.0460, -0.9290],\n",
      "         [-0.1695, -0.2160, -0.2645,  ...,  0.0241,  0.3762, -0.9001],\n",
      "         [ 0.6795,  0.5683, -0.0044,  ...,  0.0886, -0.4642, -0.2662]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2484, -0.0602, -0.1791,  ...,  0.0573,  0.4322,  0.1725],\n",
      "         [-0.8869, -0.2092,  0.5642,  ...,  0.2347,  1.3482, -0.3639],\n",
      "         [-0.5857,  0.1180,  0.3582,  ...,  0.1054, -0.0698, -0.3700],\n",
      "         ...,\n",
      "         [-0.0067, -0.3840,  0.4690,  ...,  0.0091, -0.2604,  0.1185],\n",
      "         [ 0.6702,  0.1863,  0.2464,  ..., -0.2844, -0.0166, -0.8776],\n",
      "         [ 0.5414,  0.1823,  0.1304,  ..., -0.0167, -0.2994, -0.5774]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3748, -0.0316,  0.0627,  ..., -0.0767,  0.4652,  0.2847],\n",
      "         [-0.6592,  0.2561,  0.1049,  ..., -0.1139,  0.8163, -0.2930],\n",
      "         [-0.5127,  0.0106,  0.6436,  ..., -0.1869,  0.1509,  0.0178],\n",
      "         ...,\n",
      "         [-0.7624, -0.0571,  0.2313,  ...,  0.2509,  0.4828, -0.0716],\n",
      "         [ 0.2784, -0.1479,  0.3907,  ...,  0.0434,  0.2194, -0.4338],\n",
      "         [ 0.6929,  0.1643, -0.0099,  ...,  0.0418, -0.1310, -0.3638]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.3196e-01, -1.1380e-01, -4.0605e-02,  ...,  2.3271e-02,\n",
      "           5.3200e-01,  1.6254e-02],\n",
      "         [-7.2466e-01,  8.4261e-02,  1.0456e-02,  ..., -2.6899e-02,\n",
      "           6.5916e-01, -4.8150e-01],\n",
      "         [-2.3188e-01,  1.9482e-01,  3.4762e-01,  ..., -3.9455e-01,\n",
      "           2.7324e-01, -2.9837e-01],\n",
      "         ...,\n",
      "         [-7.9051e-01, -1.3511e-01, -1.3245e-01,  ...,  6.5706e-02,\n",
      "          -1.4556e-02,  2.6915e-02],\n",
      "         [-9.4275e-01, -4.2585e-02,  4.1711e-04,  ...,  5.1237e-02,\n",
      "           2.9777e-01, -1.4593e+00],\n",
      "         [ 4.2716e-01,  1.6398e-01, -1.3406e-01,  ...,  1.0624e-01,\n",
      "          -3.8295e-01, -3.9555e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2987,  0.1240, -0.1963,  ..., -0.1030,  0.3717,  0.2305],\n",
      "         [-0.4611,  0.1732, -0.0729,  ..., -0.0947,  0.2753, -0.1232],\n",
      "         [ 0.1337,  0.9049,  0.1978,  ..., -0.3485,  0.2032, -0.2747],\n",
      "         ...,\n",
      "         [-0.3550, -0.4720,  0.2211,  ..., -0.5872,  0.0277, -0.3635],\n",
      "         [-0.1920, -0.1667,  0.2914,  ..., -0.0979,  0.1264, -0.3206],\n",
      "         [ 0.0897,  0.2204, -0.1190,  ..., -0.1029, -0.1951, -0.6116]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4904, -0.0920, -0.1479,  ..., -0.0735,  0.5179,  0.0036],\n",
      "         [-0.4954, -0.0397,  0.0091,  ...,  0.2623,  0.4408, -0.2387],\n",
      "         [-0.0515, -0.4155,  0.3565,  ...,  0.0700, -0.1349, -0.4525],\n",
      "         ...,\n",
      "         [-0.3491,  0.0728,  0.2361,  ..., -0.0146, -0.1445, -0.0318],\n",
      "         [-0.7432,  0.3678, -0.3872,  ..., -0.0906, -0.2184, -0.9424],\n",
      "         [ 0.5570,  0.1641, -0.0518,  ..., -0.0027, -0.3682, -0.3125]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2217, -0.0557, -0.1163,  ..., -0.1894,  0.5326,  0.4236],\n",
      "         [ 0.0950,  0.1435,  0.3792,  ...,  0.1298,  0.6971, -0.5938],\n",
      "         [-0.0928, -0.6584,  0.9041,  ...,  0.2775,  0.5455,  0.2943],\n",
      "         ...,\n",
      "         [ 0.2336,  0.4767,  0.4452,  ...,  0.0247,  0.0541, -1.0668],\n",
      "         [-0.1783,  0.1588,  0.0746,  ...,  0.1688,  0.1236, -0.7214],\n",
      "         [ 0.3747,  0.3827, -0.2599,  ...,  0.2043, -0.5586, -0.4828]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4131, -0.0590,  0.0114,  ..., -0.0173,  0.5538,  0.2943],\n",
      "         [-0.8742, -0.1170, -0.0824,  ...,  0.3446,  0.8798, -0.0875],\n",
      "         [-0.5732,  0.2019,  0.1385,  ...,  0.0765,  0.3876,  0.2320],\n",
      "         ...,\n",
      "         [-0.4637,  0.1932,  0.1609,  ..., -0.0681,  0.5904, -0.4048],\n",
      "         [ 0.1689,  0.0487,  0.1744,  ..., -0.1213,  0.1097, -0.2666],\n",
      "         [ 0.7891,  0.1665, -0.1114,  ..., -0.0535, -0.3717, -0.3189]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4033, -0.0446,  0.0252,  ...,  0.0215,  0.3642,  0.2333],\n",
      "         [-0.6568,  0.2168,  0.0660,  ...,  0.1174,  0.3100, -0.0082],\n",
      "         [-0.9145, -0.1855,  0.3860,  ..., -0.0492,  0.4368, -0.1269],\n",
      "         ...,\n",
      "         [ 0.0882, -0.4993,  0.5115,  ...,  0.3772,  0.2178, -0.7502],\n",
      "         [-0.7411,  0.4932,  0.1684,  ...,  0.1262, -0.2804, -0.0297],\n",
      "         [ 0.6739,  0.0203, -0.1316,  ...,  0.1637, -0.2837, -0.5164]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.6018e-01,  4.4274e-02, -1.8176e-02,  ..., -2.8596e-01,\n",
      "           5.3386e-01,  4.1697e-01],\n",
      "         [-1.4159e-01,  9.6153e-04, -2.7954e-01,  ...,  1.9792e-01,\n",
      "           2.9206e-01, -3.0085e-01],\n",
      "         [-6.7733e-01,  1.6751e-01,  1.2116e-01,  ..., -2.0343e-01,\n",
      "           1.2863e-01,  6.1135e-01],\n",
      "         ...,\n",
      "         [-1.0476e+00, -1.5524e-01, -2.5730e-01,  ..., -3.6322e-01,\n",
      "           2.8146e-01, -4.6030e-01],\n",
      "         [-5.1852e-01, -4.1959e-01,  3.1796e-01,  ..., -1.0406e-01,\n",
      "           4.9865e-01, -5.8906e-01],\n",
      "         [ 6.6100e-01,  2.2745e-01,  7.8298e-02,  ...,  3.2092e-02,\n",
      "          -3.5041e-01, -4.0923e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-2.6105e-01,  2.4691e-02,  4.6791e-02,  ...,  4.8209e-02,\n",
      "           4.5310e-01,  2.3395e-01],\n",
      "         [-5.1708e-01,  6.4815e-02,  1.0452e-02,  ...,  2.8290e-01,\n",
      "           9.8705e-01, -4.7198e-01],\n",
      "         [-4.5418e-01,  2.8172e-01, -5.0928e-04,  ..., -3.3615e-01,\n",
      "           4.9611e-01,  1.9540e-01],\n",
      "         ...,\n",
      "         [-8.6136e-01, -1.7780e-01,  4.4591e-01,  ..., -1.3690e-01,\n",
      "           1.4502e-01, -1.1581e-01],\n",
      "         [ 2.8050e-01, -1.2897e-01,  4.6603e-01,  ...,  2.1033e-01,\n",
      "           3.3714e-02,  1.0586e-01],\n",
      "         [ 6.1224e-01,  1.0043e-02, -1.3387e-02,  ...,  6.8648e-02,\n",
      "          -3.1890e-01, -4.5076e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3739,  0.0710, -0.1078,  ...,  0.0695,  0.4202,  0.1372],\n",
      "         [-0.7875,  0.0114, -0.2610,  ...,  0.4567,  0.7760, -0.3996],\n",
      "         [-0.9064,  0.2731, -0.1455,  ...,  0.2649,  0.4482,  0.2216],\n",
      "         ...,\n",
      "         [-1.1052,  0.2024, -0.3610,  ..., -0.1244,  0.3095, -0.5968],\n",
      "         [-0.3272, -0.0528, -0.1264,  ...,  0.2068,  0.2638, -0.8346],\n",
      "         [ 0.5824,  0.1126, -0.2536,  ...,  0.0672, -0.3644, -0.4352]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.3685,  0.0982, -0.0362,  ...,  0.0008,  0.5780,  0.1887],\n",
      "         [ 0.2125,  0.2672,  0.2238,  ...,  0.2417,  0.6659, -0.0519],\n",
      "         [-0.4902,  0.2510, -0.0287,  ..., -0.1733,  0.3015, -0.1347],\n",
      "         ...,\n",
      "         [ 0.6497, -0.2355,  0.5638,  ...,  0.1080,  0.1751, -0.3063],\n",
      "         [-0.3656,  0.1764,  0.2884,  ...,  0.1130, -0.0538, -0.6691],\n",
      "         [ 0.5780, -0.0800, -0.1034,  ...,  0.2769, -0.3481, -0.4480]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2664, -0.0312,  0.0609,  ..., -0.0455,  0.4980,  0.2478],\n",
      "         [-0.0633,  0.1826,  0.1515,  ...,  0.2131,  0.4940, -0.2158],\n",
      "         [-0.0515, -0.0559,  0.3018,  ...,  0.0988,  0.3411,  0.0968],\n",
      "         ...,\n",
      "         [-0.0167,  0.4915,  0.0196,  ...,  0.0526,  0.0146, -0.5638],\n",
      "         [-0.3565, -0.3488,  0.1569,  ...,  0.0346, -0.1392, -0.3732],\n",
      "         [ 0.8745,  0.0866, -0.1302,  ...,  0.0606, -0.3061, -0.4789]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3163, -0.1230, -0.0170,  ..., -0.0197,  0.3797,  0.0979],\n",
      "         [-0.4014, -0.0698,  0.1800,  ...,  0.2448,  0.5879, -0.1737],\n",
      "         [ 0.1790,  0.3781,  0.1650,  ..., -0.1187,  0.6805,  0.1343],\n",
      "         ...,\n",
      "         [-0.6750, -0.4145, -0.3969,  ..., -0.2762, -0.0387, -1.1239],\n",
      "         [ 0.6601, -0.3728,  0.0189,  ...,  0.1919, -0.1217,  0.1160],\n",
      "         [ 0.8667,  0.0639, -0.4434,  ...,  0.2385, -0.5371, -0.2926]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6906, -0.0917,  0.3886,  ..., -0.0601,  0.3252,  0.2507],\n",
      "         [-0.8836,  0.1290,  0.0802,  ...,  0.1064,  0.3628, -0.3520],\n",
      "         [-0.7465,  0.0965,  0.0834,  ...,  0.1299, -0.0222, -0.0817],\n",
      "         ...,\n",
      "         [-0.6701,  0.0432,  0.8912,  ...,  0.4208,  0.4352, -0.6370],\n",
      "         [-0.3594, -0.2223,  0.7707,  ...,  0.3643, -0.2477, -0.5216],\n",
      "         [ 0.4712,  0.3766, -0.1813,  ...,  0.2254, -0.6423, -0.5736]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2528,  0.0967, -0.1906,  ..., -0.1252,  0.3917,  0.3834],\n",
      "         [-0.3102,  0.5635,  0.1198,  ...,  0.0551,  0.5841,  0.3984],\n",
      "         [-0.3825,  0.0930,  0.3606,  ...,  0.3396,  0.5365,  1.0832],\n",
      "         ...,\n",
      "         [ 0.8231,  0.5914,  0.6536,  ..., -0.1502, -0.0460, -0.5390],\n",
      "         [-0.1402,  0.0382,  0.2328,  ...,  0.0650,  0.4841, -0.7845],\n",
      "         [ 0.6142,  0.6417, -0.0535,  ...,  0.0803, -0.4517, -0.1888]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1557, -0.0917,  0.0627,  ..., -0.0389,  0.7012, -0.0266],\n",
      "         [-0.5285,  0.1444,  0.0736,  ...,  0.1096,  0.4375,  0.2439],\n",
      "         [-0.4778, -0.3808,  0.6193,  ..., -0.0766,  0.3977, -0.1053],\n",
      "         ...,\n",
      "         [ 0.5347, -0.2692,  0.3898,  ..., -0.1090,  0.0593, -0.7364],\n",
      "         [-0.4327, -0.4037,  0.4863,  ..., -0.1413,  0.0550, -0.3127],\n",
      "         [ 0.5690,  0.0859,  0.0713,  ...,  0.1314, -0.3126, -0.7369]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6141,  0.1915, -0.2330,  ...,  0.1294,  0.4870,  0.2634],\n",
      "         [-0.3158,  0.3024, -0.0491,  ...,  0.3036,  0.0130,  0.3012],\n",
      "         [-0.2216, -0.0626,  0.1211,  ...,  0.1046,  0.0555,  0.2628],\n",
      "         ...,\n",
      "         [-0.8930,  0.3155,  0.5940,  ...,  0.6138, -0.0356, -0.8728],\n",
      "         [-0.1343,  0.5034, -0.0148,  ...,  0.4490, -0.2755, -0.1371],\n",
      "         [ 0.6469, -0.0056, -0.5400,  ...,  0.3915, -0.4135, -0.3662]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4251, -0.0499, -0.0922,  ..., -0.0462,  0.3061,  0.1018],\n",
      "         [-0.6776, -0.0064,  0.4252,  ...,  0.1388,  0.2069,  0.1505],\n",
      "         [-0.7609, -0.2178,  0.0736,  ...,  0.3437, -0.0565, -0.6872],\n",
      "         ...,\n",
      "         [-0.4696, -0.4030,  0.3356,  ...,  0.2052, -0.0211, -0.4517],\n",
      "         [-0.7388, -0.5054,  0.1804,  ...,  0.0767,  0.4712, -0.2118],\n",
      "         [ 0.6568, -0.0010, -0.0260,  ...,  0.0028, -0.0707, -0.5071]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2633,  0.0967,  0.0019,  ..., -0.2128,  0.5149,  0.1920],\n",
      "         [-0.5931, -0.0852,  0.3061,  ...,  0.1170,  0.6487, -0.0134],\n",
      "         [-0.0899, -0.0088,  0.3932,  ..., -0.1775,  0.6095,  0.2149],\n",
      "         ...,\n",
      "         [-0.6428, -0.5968,  0.1349,  ...,  0.0967,  0.5415, -0.3218],\n",
      "         [-0.4385,  0.0765,  0.3319,  ..., -0.2844,  0.4199, -0.0648],\n",
      "         [ 0.2740,  0.1341, -0.1312,  ...,  0.2177, -0.5161, -0.4045]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1813, -0.0171, -0.0705,  ...,  0.1523,  0.4687,  0.0873],\n",
      "         [-0.0517,  0.0097,  0.0920,  ...,  0.0369,  0.7526,  0.3103],\n",
      "         [-0.0481, -0.1138,  0.3602,  ..., -0.0527,  0.4246,  0.1662],\n",
      "         ...,\n",
      "         [ 0.1522,  0.1818,  0.2836,  ...,  0.1878,  0.2824,  0.0279],\n",
      "         [-0.0589, -0.2619,  0.3828,  ...,  0.4593, -0.2329, -0.2705],\n",
      "         [ 0.4034,  0.1472, -0.1827,  ...,  0.0880, -0.5364, -0.6030]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2798,  0.0223,  0.1509,  ..., -0.0743,  0.4351,  0.1244],\n",
      "         [-0.6298,  0.0195, -0.0493,  ...,  0.3128,  0.8503, -0.4255],\n",
      "         [-0.8899,  0.1570,  0.3043,  ..., -0.2408,  0.5761,  0.2773],\n",
      "         ...,\n",
      "         [-0.3744,  0.3632,  0.2336,  ..., -0.1362, -0.3222, -0.7185],\n",
      "         [-0.0542, -0.2219,  0.1845,  ..., -0.3915,  0.0524, -0.8506],\n",
      "         [ 0.5947,  0.3544, -0.2514,  ...,  0.0429, -0.5513, -0.4629]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-2.5098e-01, -3.1821e-02,  2.2074e-02,  ..., -8.7895e-03,\n",
      "           4.2519e-01,  3.0188e-01],\n",
      "         [ 1.0864e-01,  4.3674e-01,  4.4851e-01,  ..., -2.0303e-01,\n",
      "           3.8984e-01,  2.0373e-01],\n",
      "         [-4.5580e-01,  1.2186e-01,  4.8662e-01,  ..., -3.8777e-01,\n",
      "           1.2356e-01, -1.1942e-01],\n",
      "         ...,\n",
      "         [-4.6427e-01,  1.2438e-01,  2.2435e-01,  ...,  9.0472e-02,\n",
      "           6.0804e-01, -4.6884e-01],\n",
      "         [-5.9665e-01,  1.4911e-01, -4.2492e-02,  ...,  2.9730e-01,\n",
      "           2.2943e-01, -6.2704e-01],\n",
      "         [ 6.0588e-01,  2.9311e-01, -1.3380e-01,  ...,  2.7967e-04,\n",
      "          -2.7041e-01, -4.1557e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4010, -0.0286,  0.1439,  ..., -0.2720,  0.4022,  0.1607],\n",
      "         [-0.9269,  0.0201, -0.2555,  ..., -0.5459,  0.5881, -0.1306],\n",
      "         [-0.4162,  0.1426,  0.0495,  ..., -0.1921,  0.2070, -0.3059],\n",
      "         ...,\n",
      "         [ 0.8128,  0.7155,  0.2892,  ..., -0.2549, -0.1514, -0.4524],\n",
      "         [-0.5786,  0.0029,  0.0991,  ..., -0.0912,  0.0960, -0.6618],\n",
      "         [ 0.7781,  0.2217, -0.0836,  ...,  0.0742, -0.4125, -0.2713]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3982, -0.0857, -0.0670,  ..., -0.3298,  0.4064,  0.3183],\n",
      "         [-0.5500,  0.1034,  0.1455,  ..., -0.2711,  0.4979, -0.2221],\n",
      "         [-0.2182, -0.1337,  0.4054,  ..., -0.2824,  0.4710, -0.1914],\n",
      "         ...,\n",
      "         [ 0.0586,  0.3649,  0.4246,  ..., -0.1335, -0.1389, -0.0545],\n",
      "         [ 0.2143,  0.0360,  0.1947,  ..., -0.1563, -0.1448, -0.2754],\n",
      "         [ 0.3859,  0.1928, -0.1009,  ..., -0.1717, -0.3980, -0.5278]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0703, -0.3204, -0.1160,  ...,  0.0092,  0.4139,  0.2597],\n",
      "         [-0.3005, -0.0648,  0.0616,  ...,  0.2529,  0.3128, -0.2805],\n",
      "         [-0.6107,  0.1479,  0.1587,  ...,  0.0634,  0.2908, -0.0603],\n",
      "         ...,\n",
      "         [-0.2224,  0.3595,  0.4301,  ...,  0.2045,  0.1616, -0.0906],\n",
      "         [ 0.3675, -0.0956,  0.5165,  ...,  0.1485, -0.0637, -0.7170],\n",
      "         [ 0.6720,  0.0789, -0.0717,  ...,  0.2396, -0.4658, -0.4935]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2804, -0.1233, -0.0349,  ..., -0.0743,  0.2623,  0.1591],\n",
      "         [-0.3676,  0.0205, -0.3255,  ...,  0.1536,  0.6074, -0.4190],\n",
      "         [-0.9156, -0.4315,  0.1443,  ...,  0.0489, -0.1346, -0.1263],\n",
      "         ...,\n",
      "         [ 0.1555, -0.2260,  0.2816,  ..., -0.1663, -0.0038, -0.5250],\n",
      "         [-0.2946,  0.1066,  0.3947,  ...,  0.4237,  0.1421, -0.7460],\n",
      "         [ 0.6434,  0.3700, -0.3505,  ...,  0.1364, -0.4232, -0.5021]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5165,  0.0011,  0.0510,  ...,  0.0537,  0.2892,  0.3617],\n",
      "         [ 0.1829,  0.3013,  0.4067,  ...,  0.1967,  0.8035,  0.9651],\n",
      "         [-0.2189,  0.8352,  0.5533,  ...,  0.3078, -0.1096,  0.1008],\n",
      "         ...,\n",
      "         [-1.1102,  0.1660,  0.4634,  ...,  0.4520,  0.0364, -0.2838],\n",
      "         [-0.8175, -0.0874,  0.3084,  ..., -0.0481,  0.2350,  0.1661],\n",
      "         [ 0.7120,  0.2807, -0.0101,  ...,  0.3076, -0.5111, -0.3076]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2832, -0.0808,  0.1413,  ...,  0.1065,  0.2257,  0.1227],\n",
      "         [-0.5461, -0.0490, -0.1908,  ...,  0.2512,  0.8467, -0.3414],\n",
      "         [-0.3155, -0.2378,  0.2273,  ...,  0.1449,  0.2683, -0.3672],\n",
      "         ...,\n",
      "         [ 0.5433,  0.2416, -0.0225,  ...,  0.3242, -0.1701, -0.3590],\n",
      "         [-0.3713, -0.3391,  0.2059,  ...,  0.5071,  0.2989, -0.7536],\n",
      "         [ 0.6042,  0.2078,  0.0487,  ...,  0.0483, -0.3530, -0.5328]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2554, -0.2312,  0.1796,  ..., -0.0769,  0.2415, -0.1467],\n",
      "         [ 0.4848, -0.1972,  0.2894,  ..., -0.4435,  0.3912,  0.3302],\n",
      "         [-0.5912, -0.4250,  0.8819,  ...,  0.4024,  0.1697,  0.3784],\n",
      "         ...,\n",
      "         [ 0.1476, -0.2292,  0.3575,  ...,  0.2398, -0.0582,  0.0749],\n",
      "         [-0.5917, -0.0681,  0.1449,  ...,  0.4006,  0.4396, -0.7723],\n",
      "         [ 0.7613,  0.2155, -0.0691,  ...,  0.1746, -0.4095, -0.5573]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4407,  0.1257,  0.0130,  ...,  0.1330,  0.4616,  0.2978],\n",
      "         [-0.8148,  0.3381,  0.2299,  ...,  0.1247,  0.5130, -0.0930],\n",
      "         [-0.8280,  0.3227,  0.0422,  ...,  0.2455,  0.6455,  0.1889],\n",
      "         ...,\n",
      "         [-0.6151, -0.1162,  0.4106,  ...,  0.2512,  0.2864, -0.0580],\n",
      "         [ 0.3849,  0.1099,  0.4311,  ...,  0.4165,  0.0901,  0.0987],\n",
      "         [ 0.1251,  0.3679, -0.1762,  ...,  0.0379, -0.1632, -0.5308]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2339, -0.1397, -0.0669,  ...,  0.0911,  0.3682,  0.1845],\n",
      "         [-0.7243, -0.0161, -0.0699,  ...,  0.3830,  0.3556, -0.2602],\n",
      "         [ 0.3875, -0.1532,  0.1131,  ..., -0.0642,  0.1648, -0.0973],\n",
      "         ...,\n",
      "         [ 0.5399,  0.2602, -0.1886,  ...,  0.0550,  0.1187, -0.3348],\n",
      "         [-0.3311, -0.2042, -0.1769,  ...,  0.2047,  0.0328, -0.6128],\n",
      "         [ 0.7050,  0.2232, -0.1473,  ...,  0.1839, -0.4706, -0.3873]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3839, -0.1602,  0.0473,  ...,  0.0389,  0.3261,  0.4593],\n",
      "         [-0.3324, -0.1672,  0.2508,  ...,  0.1605,  0.1170, -0.2215],\n",
      "         [-0.6555,  0.1128,  0.1632,  ...,  0.0173,  0.0632, -0.4175],\n",
      "         ...,\n",
      "         [-0.5537,  0.2372,  0.3673,  ..., -0.3412,  0.2258, -0.8530],\n",
      "         [-0.7763,  0.0014, -0.0633,  ...,  0.0647, -0.1593, -0.9232],\n",
      "         [ 0.6828,  0.1359, -0.2452,  ...,  0.3569, -0.4122, -0.2420]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3660,  0.2080, -0.1829,  ..., -0.0747,  0.6322,  0.3095],\n",
      "         [-0.7958,  0.1516,  0.1414,  ...,  0.1000,  0.6901, -0.1156],\n",
      "         [-0.4615,  0.0474,  0.5051,  ..., -0.0982,  0.4172,  0.5853],\n",
      "         ...,\n",
      "         [ 1.0595,  0.2064,  0.6969,  ...,  0.1828,  0.0409, -0.1525],\n",
      "         [ 0.0411, -0.0683,  0.2501,  ...,  0.1314,  0.0831,  0.0098],\n",
      "         [ 0.5663,  0.3069, -0.0449,  ...,  0.0854, -0.4044, -0.3435]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2590, -0.0084, -0.0220,  ...,  0.0906,  0.3032,  0.2728],\n",
      "         [-0.0036,  0.0578,  0.1227,  ...,  0.3094,  0.0790, -0.2297],\n",
      "         [-0.4007, -0.2832,  0.2519,  ...,  0.0345,  0.3986, -0.1229],\n",
      "         ...,\n",
      "         [-0.7766, -0.5487,  0.3381,  ...,  0.0833, -0.2487, -0.3174],\n",
      "         [-0.1688, -0.2123,  0.4371,  ...,  0.3620,  0.0508, -0.5067],\n",
      "         [ 0.7131,  0.2616,  0.1274,  ...,  0.1037, -0.4771, -0.3027]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5565, -0.2756,  0.1883,  ..., -0.2382,  0.3226,  0.3471],\n",
      "         [-0.1970,  0.1990,  0.0151,  ...,  0.2817,  0.3977, -0.2653],\n",
      "         [-0.7470, -0.1783,  0.7752,  ..., -0.3225, -0.3782,  0.0916],\n",
      "         ...,\n",
      "         [ 0.3718,  0.3315,  0.4357,  ..., -0.2618, -0.0770, -0.3013],\n",
      "         [ 0.0108,  0.2817,  0.8026,  ..., -0.3057, -0.1895, -0.1042],\n",
      "         [ 0.7868, -0.2780,  0.1915,  ...,  0.1494, -0.4592, -0.3857]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4618, -0.0233,  0.0374,  ..., -0.1043,  0.3314,  0.2399],\n",
      "         [-0.5474,  0.3822,  0.0798,  ..., -0.0856,  0.2196, -0.0828],\n",
      "         [-0.3716,  0.0492,  0.4370,  ..., -0.5720,  0.0531, -0.1972],\n",
      "         ...,\n",
      "         [-0.8881,  0.0700,  0.1447,  ..., -0.1388,  0.1849,  0.0937],\n",
      "         [-0.2071,  0.0243, -0.0409,  ...,  0.0377,  0.0040, -0.6869],\n",
      "         [ 0.5819,  0.4038,  0.1117,  ...,  0.0416, -0.4788, -0.2856]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3314,  0.2551, -0.3390,  ..., -0.1986,  0.4578,  0.2742],\n",
      "         [-0.4796,  0.0209,  0.0786,  ...,  0.0816,  0.3303, -0.1627],\n",
      "         [-0.8423,  0.4865, -0.2096,  ..., -0.1321,  0.2735, -0.5030],\n",
      "         ...,\n",
      "         [ 0.1033,  0.2078, -0.2887,  ..., -0.0469,  0.1054, -0.1972],\n",
      "         [-0.2812,  0.0209, -0.2446,  ...,  0.1193,  0.1862, -0.4837],\n",
      "         [ 0.1152,  0.0799, -0.1690,  ...,  0.0676, -0.4347, -0.3555]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3531, -0.1095, -0.0264,  ..., -0.0954,  0.3217, -0.0160],\n",
      "         [-0.4356, -0.0328,  0.3793,  ...,  0.1255,  0.0760,  0.2072],\n",
      "         [-0.2070, -0.0775,  0.3518,  ...,  0.1307,  0.3464, -0.5930],\n",
      "         ...,\n",
      "         [-0.6570,  0.2020,  0.0732,  ...,  0.3731,  0.4813, -0.5230],\n",
      "         [-0.2299, -0.5071,  0.4974,  ...,  0.1371,  0.1405, -1.0802],\n",
      "         [ 0.5276,  0.1564, -0.0790,  ...,  0.0880, -0.5980, -0.6122]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5878, -0.1247, -0.2160,  ..., -0.1688,  0.6490,  0.3633],\n",
      "         [-0.9726, -0.6043,  0.2002,  ..., -0.4515,  0.6193,  0.1765],\n",
      "         [-0.6602,  0.0586,  0.3653,  ...,  0.0761,  0.2533,  0.2997],\n",
      "         ...,\n",
      "         [-0.5631,  0.9116,  0.1937,  ..., -0.0239,  0.0243, -0.4046],\n",
      "         [-0.8241, -0.0644, -0.0902,  ...,  0.1372,  0.3808, -0.4827],\n",
      "         [ 0.6469,  0.0837, -0.3002,  ..., -0.0855, -0.2751, -0.4315]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2066, -0.0293,  0.0217,  ..., -0.3642,  0.4870,  0.1789],\n",
      "         [-0.4683, -0.0352, -0.0014,  ..., -0.1481,  0.7875,  0.4779],\n",
      "         [-0.3237,  0.1424,  0.0715,  ..., -0.1459,  0.1077, -0.0248],\n",
      "         ...,\n",
      "         [ 0.8237,  0.6370,  0.2739,  ..., -0.3678, -0.0614, -0.2722],\n",
      "         [-0.7875,  0.1105, -0.0653,  ..., -0.2067,  0.1081, -0.7861],\n",
      "         [ 0.3873,  0.2403,  0.0967,  ..., -0.1288, -0.1463, -0.5853]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2410, -0.1158, -0.0326,  ..., -0.0022,  0.4107,  0.2330],\n",
      "         [-0.2647,  0.0361, -0.0953,  ...,  0.0922,  0.1871,  0.0674],\n",
      "         [-0.1968, -0.2335, -0.0831,  ..., -0.0300,  0.2909, -0.1823],\n",
      "         ...,\n",
      "         [ 0.1262, -0.0787,  0.1046,  ...,  0.3659,  0.1318, -0.6704],\n",
      "         [-0.0990, -0.2702,  0.0135,  ...,  0.1960, -0.0504, -0.5319],\n",
      "         [ 0.3176,  0.3076, -0.1899,  ..., -0.1911, -0.3506, -0.4248]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5559, -0.1870,  0.0967,  ...,  0.0669,  0.2531,  0.1660],\n",
      "         [-0.7106,  0.0501,  0.4239,  ...,  0.0747,  0.4821,  0.3581],\n",
      "         [ 0.0252,  0.5058,  0.2151,  ...,  0.2532,  0.0183,  0.3849],\n",
      "         ...,\n",
      "         [-0.1454, -0.1695,  0.7665,  ..., -0.0600, -0.1773, -0.6801],\n",
      "         [-0.1596, -0.3584,  0.1376,  ..., -0.0098, -0.1325, -1.2102],\n",
      "         [ 0.4596,  0.0643, -0.1936,  ..., -0.0329, -0.4893, -0.5280]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2386, -0.1178,  0.0646,  ...,  0.1543,  0.3360,  0.0195],\n",
      "         [-0.3321,  0.0173,  0.0518,  ...,  0.1435,  0.2268, -0.2811],\n",
      "         [-0.3053,  0.2658, -0.0786,  ...,  0.0146,  0.0500, -0.4483],\n",
      "         ...,\n",
      "         [ 0.6851,  0.3232,  0.3297,  ...,  0.7652,  0.1852, -0.9525],\n",
      "         [-0.2411, -0.3314,  0.4189,  ...,  0.2453,  0.0655, -0.3358],\n",
      "         [ 0.5942,  0.1629,  0.0030,  ...,  0.1242, -0.4225, -0.6745]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1204, -0.2016,  0.0809,  ...,  0.1826,  0.2733,  0.0867],\n",
      "         [-0.3616, -0.0031,  0.0775,  ...,  0.1886,  0.4076, -0.1736],\n",
      "         [-0.5856, -0.1389,  0.3184,  ...,  0.2085,  0.0960, -0.7091],\n",
      "         ...,\n",
      "         [ 0.7814,  0.4643,  0.4073,  ...,  0.4213,  0.3579, -0.5390],\n",
      "         [-0.6199,  0.2725,  0.2362,  ...,  0.6580,  0.1571, -0.7982],\n",
      "         [ 0.2419,  0.0626, -0.0125,  ...,  0.2187, -0.4165, -0.5470]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-3.9311e-01, -2.2248e-01,  2.4076e-01,  ..., -2.2931e-01,\n",
      "           1.9716e-01,  1.6749e-01],\n",
      "         [-3.6734e-01,  9.3842e-02,  3.2426e-01,  ...,  1.0422e-01,\n",
      "           7.1030e-01, -4.1403e-01],\n",
      "         [-4.1989e-01,  2.9075e-01,  1.1413e-01,  ..., -5.4332e-01,\n",
      "           5.7241e-01, -7.6662e-02],\n",
      "         ...,\n",
      "         [ 8.2916e-02, -7.7925e-01,  5.2205e-01,  ..., -1.4227e-01,\n",
      "          -1.4252e-01, -6.2098e-01],\n",
      "         [-1.7099e+00, -8.5676e-02,  3.3044e-01,  ...,  3.3016e-01,\n",
      "          -3.4030e-01, -2.6578e-01],\n",
      "         [ 8.3552e-01, -1.5142e-03, -5.9928e-02,  ..., -2.0441e-02,\n",
      "          -6.1354e-01, -1.6963e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2935, -0.1522,  0.2151,  ...,  0.0030,  0.4079,  0.5419],\n",
      "         [-0.5075, -0.1836, -0.2660,  ..., -0.0735,  0.1540, -0.7511],\n",
      "         [-0.0094, -0.5816,  0.6989,  ...,  0.2949,  0.2052,  0.3215],\n",
      "         ...,\n",
      "         [-0.2858, -0.4128,  0.6373,  ..., -0.0271, -0.0704, -0.1606],\n",
      "         [ 0.1475, -0.7537,  0.5322,  ...,  0.2929, -0.2702, -0.2059],\n",
      "         [ 0.8772,  0.3450, -0.0522,  ...,  0.3581, -0.3175, -0.2558]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3348, -0.2033,  0.2685,  ..., -0.1083,  0.2916,  0.1777],\n",
      "         [-0.1751, -0.1214, -0.1811,  ...,  0.2030,  0.1283, -0.2444],\n",
      "         [-0.7634, -0.2104, -0.0879,  ...,  0.0237,  0.3209,  0.4563],\n",
      "         ...,\n",
      "         [-0.5862,  0.1686,  0.9983,  ..., -0.0784, -0.0604,  0.2697],\n",
      "         [-1.4437,  0.2540,  0.1548,  ..., -0.0515,  0.2603, -0.4822],\n",
      "         [ 0.5997,  0.1617,  0.1340,  ...,  0.1439, -0.2289, -0.5816]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2594, -0.0266,  0.1229,  ...,  0.1764,  0.3727,  0.0863],\n",
      "         [-0.3128,  0.1891, -0.0411,  ...,  0.3652,  0.3002, -0.2342],\n",
      "         [-0.7046, -0.1452,  0.6367,  ...,  0.1767,  0.2541, -0.0233],\n",
      "         ...,\n",
      "         [ 0.5008,  0.2575,  0.0168,  ...,  0.5193, -0.0701, -0.5401],\n",
      "         [-0.3476,  0.0191,  0.3261,  ...,  0.4702, -0.2270, -0.2906],\n",
      "         [ 0.8346,  0.1662, -0.0143,  ...,  0.2408, -0.5422, -0.4116]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2524,  0.1488, -0.2207,  ..., -0.0364,  0.3575, -0.1316],\n",
      "         [-0.0887,  0.8167,  0.0236,  ...,  0.0271,  0.2183, -0.1418],\n",
      "         [-0.9909,  0.2052, -0.1287,  ..., -0.0364,  0.7199,  0.0518],\n",
      "         ...,\n",
      "         [ 0.0977,  0.3904, -0.0754,  ..., -0.2674,  0.0602, -0.3882],\n",
      "         [ 0.2873,  0.3430, -0.1536,  ..., -0.0885,  0.2038, -0.2340],\n",
      "         [ 0.5079,  0.0169, -0.2412,  ...,  0.1083, -0.3552, -0.3604]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5484, -0.0977, -0.0846,  ..., -0.1824,  0.6512,  0.0046],\n",
      "         [-0.2947, -0.0263,  0.2088,  ...,  0.1355,  0.4334,  0.0195],\n",
      "         [ 0.0936, -0.0256,  0.0560,  ..., -0.1639,  0.4574, -0.6295],\n",
      "         ...,\n",
      "         [ 0.4806,  0.1082,  0.4803,  ..., -0.1864,  0.1540, -1.0423],\n",
      "         [-0.2691,  0.3045,  0.0935,  ..., -0.3344,  0.0498, -1.3102],\n",
      "         [ 0.6809, -0.0507, -0.1821,  ...,  0.1244, -0.5162, -0.6274]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3991,  0.1176,  0.0043,  ..., -0.1894,  0.4613,  0.2193],\n",
      "         [-0.8648, -0.1908,  0.2272,  ..., -0.1757,  0.7640,  0.5169],\n",
      "         [-0.9068,  0.0184,  0.1374,  ..., -0.1228,  0.1645, -0.2822],\n",
      "         ...,\n",
      "         [-0.1673,  0.4610,  0.7818,  ..., -0.1429,  0.4584, -0.9045],\n",
      "         [-0.2126,  0.1426,  0.4276,  ..., -0.0463, -0.1763, -0.9049],\n",
      "         [ 0.7861,  0.1459, -0.1510,  ...,  0.1834, -0.4770, -0.2710]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4918,  0.1332, -0.2011,  ...,  0.0330,  0.4317,  0.2211],\n",
      "         [-0.7107, -0.2155,  0.4385,  ...,  0.0213,  0.5905,  0.1858],\n",
      "         [ 0.4203, -0.0443,  0.1665,  ...,  0.1040,  0.3325,  0.5510],\n",
      "         ...,\n",
      "         [ 0.1493,  0.0424,  0.3968,  ...,  0.0675,  0.0615,  0.1391],\n",
      "         [-0.0800,  0.0031,  0.2094,  ...,  0.2482,  0.2020, -0.6323],\n",
      "         [ 0.7263,  0.3000, -0.1338,  ...,  0.1985, -0.5660, -0.2841]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2363, -0.1442,  0.0526,  ..., -0.0675,  0.3571, -0.0588],\n",
      "         [-0.7177, -0.0237,  0.1556,  ...,  0.1221,  0.3025, -0.9781],\n",
      "         [-0.9232, -0.0482,  0.4210,  ...,  0.1334,  0.1748, -0.4272],\n",
      "         ...,\n",
      "         [-0.4229, -0.1684,  0.6606,  ...,  0.0222,  0.4005, -0.2545],\n",
      "         [-0.2733, -0.3266, -0.0385,  ...,  0.0486, -0.1167, -1.0337],\n",
      "         [ 0.3553,  0.2119,  0.0202,  ...,  0.1301, -0.4285, -0.3798]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3162, -0.0616, -0.3788,  ..., -0.2988,  0.6047,  0.1721],\n",
      "         [-0.7775, -0.1821, -0.1888,  ..., -0.3132,  0.5893,  0.4310],\n",
      "         [-0.2279,  0.1574, -0.6478,  ..., -0.1296,  0.1745, -0.1397],\n",
      "         ...,\n",
      "         [-0.2449,  0.2694, -0.7755,  ..., -0.4975,  0.2891, -0.2111],\n",
      "         [-0.4277, -0.6217, -0.1531,  ..., -0.2223,  0.0314, -0.2865],\n",
      "         [ 0.4760,  0.1544, -0.0684,  ..., -0.1539, -0.3881, -0.4506]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4751, -0.1596, -0.0768,  ..., -0.0188,  0.5165,  0.2720],\n",
      "         [-0.2037,  0.3046,  0.2400,  ..., -0.0925,  0.6795,  0.0714],\n",
      "         [-0.5618, -0.0966,  0.3401,  ...,  0.0350, -0.1186, -0.2702],\n",
      "         ...,\n",
      "         [-1.1461,  0.2082,  0.6072,  ..., -0.0543,  0.4825, -0.9898],\n",
      "         [-1.0307, -0.0323,  0.0660,  ..., -0.0655, -0.0597, -0.8849],\n",
      "         [ 0.5482, -0.0280, -0.0520,  ...,  0.1502, -0.2125, -0.3854]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1171, -0.1035, -0.0029,  ..., -0.0513,  0.3099,  0.3604],\n",
      "         [-0.4360, -0.2105,  0.4946,  ...,  0.2676,  0.3364,  0.0077],\n",
      "         [-0.0510,  0.2418,  0.3253,  ...,  0.1719,  0.1379, -0.3968],\n",
      "         ...,\n",
      "         [-0.8112,  0.0619,  0.4868,  ..., -0.1775,  0.5807, -0.7493],\n",
      "         [-0.3663, -0.2192,  0.2170,  ...,  0.0216,  0.2495, -0.7063],\n",
      "         [ 0.8822,  0.1576, -0.0874,  ...,  0.1544, -0.4645, -0.3451]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2476, -0.1284, -0.0562,  ..., -0.0379,  0.1605,  0.1563],\n",
      "         [-1.0355,  0.2029,  0.2454,  ...,  0.1369,  0.1121,  0.0438],\n",
      "         [-0.5952, -0.3007,  0.3701,  ..., -0.0082, -0.0457, -0.1684],\n",
      "         ...,\n",
      "         [-0.4861, -0.5840,  0.1640,  ...,  0.0513,  0.2238, -0.7024],\n",
      "         [-0.7595, -0.4930,  0.1701,  ...,  0.1250,  0.3492, -0.4645],\n",
      "         [ 0.5531,  0.1069,  0.0432,  ...,  0.0701, -0.4054, -0.6460]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1796,  0.0996, -0.0084,  ..., -0.1252,  0.2901,  0.4438],\n",
      "         [-0.6856, -0.1589,  0.5558,  ...,  0.0557,  0.3498,  0.3834],\n",
      "         [-0.4724, -0.0089,  0.7782,  ...,  0.1116,  0.0771,  0.5978],\n",
      "         ...,\n",
      "         [ 0.1417,  0.3473,  0.6198,  ..., -0.2958,  0.0353, -0.5032],\n",
      "         [-0.8071,  0.0443, -0.0185,  ...,  0.0640, -0.1767, -0.4626],\n",
      "         [ 0.6231,  0.0315,  0.0042,  ...,  0.1820, -0.2126, -0.3094]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '##ogene boundary k – t boundary diamond lane in united states and canada diamond lane', 'score': tensor([0.6228], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6228], device='cuda:0')\n",
      "pred answer_text: ogene boundary k – t boundary diamond lane in united states and canada diamond lane\n",
      "sequence_output  tensor([[[-0.2728, -0.0416,  0.0317,  ..., -0.0569,  0.3608,  0.1673],\n",
      "         [-0.6524, -0.0137, -0.2088,  ...,  0.3160,  0.9152, -0.4267],\n",
      "         [-0.9292,  0.1411,  0.3944,  ..., -0.1994,  0.6136,  0.0519],\n",
      "         ...,\n",
      "         [-0.2109,  0.2778,  0.1163,  ..., -0.0956, -0.2880, -0.7160],\n",
      "         [-0.0785, -0.2679,  0.2029,  ..., -0.3303,  0.0829, -1.0236],\n",
      "         [ 0.7120,  0.3028, -0.1246,  ...,  0.1029, -0.5083, -0.4332]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.3852], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2406, -0.0048, -0.0830,  ..., -0.0913,  0.2475,  0.1172],\n",
      "         [-0.1518,  0.3445, -0.2377,  ...,  0.3154,  0.8974, -0.0539],\n",
      "         [-1.0407,  0.5575,  0.2054,  ...,  0.0233,  0.6748,  0.9403],\n",
      "         ...,\n",
      "         [ 0.4906,  0.1678, -0.1514,  ...,  0.0877,  0.0653, -0.2710],\n",
      "         [ 0.2132, -0.0711,  0.0588,  ...,  0.3366,  0.2096, -0.5333],\n",
      "         [ 0.5055,  0.2430, -0.0426,  ...,  0.0409, -0.5063, -0.4573]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.4308], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2126, -0.0610, -0.0769,  ..., -0.0089,  0.4147,  0.2865],\n",
      "         [ 0.1040,  0.4038,  0.4453,  ..., -0.2345,  0.3492,  0.3875],\n",
      "         [-0.4202, -0.0954,  0.2853,  ..., -0.1587,  0.2070,  0.0715],\n",
      "         ...,\n",
      "         [-0.3546,  0.0637,  0.3799,  ...,  0.1211,  0.5808, -0.3910],\n",
      "         [-0.7704,  0.1763,  0.0687,  ...,  0.3023,  0.3111, -0.6072],\n",
      "         [ 0.6054,  0.2095,  0.0422,  ...,  0.1185, -0.2896, -0.4393]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.4086], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2899, -0.0243, -0.1144,  ...,  0.0521,  0.3974,  0.2189],\n",
      "         [-0.2401,  0.0607, -0.1132,  ...,  0.3511,  0.4530, -0.2043],\n",
      "         [-0.6159,  0.1023,  0.1459,  ...,  0.2272,  0.3408,  0.5299],\n",
      "         ...,\n",
      "         [-0.9585, -0.2035,  0.0540,  ...,  0.0529,  0.4914, -0.4774],\n",
      "         [ 0.0495, -0.4640,  0.0220,  ...,  0.2225,  0.1228, -0.6022],\n",
      "         [ 0.5613,  0.1052, -0.2196,  ..., -0.0179, -0.3832, -0.5182]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.2671], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4824, -0.1372, -0.0298,  ..., -0.3320,  0.3940,  0.2279],\n",
      "         [-0.9268, -0.1937, -0.0871,  ..., -0.0378,  0.4719, -0.3495],\n",
      "         [-0.7630, -0.2871, -0.0013,  ..., -0.2732,  0.3087, -0.0075],\n",
      "         ...,\n",
      "         [-0.4947,  0.0684,  0.2526,  ..., -0.2818,  0.2691, -0.5046],\n",
      "         [ 0.1763, -0.2899,  0.4381,  ..., -0.1451,  0.2154,  0.1817],\n",
      "         [ 0.4888,  0.2015, -0.0062,  ..., -0.1434, -0.2524, -0.4986]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.4394], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-1.8373e-01, -5.8522e-03, -1.2492e-01,  ...,  1.4676e-01,\n",
      "           4.1445e-01,  1.0275e-01],\n",
      "         [-4.5304e-02,  1.0058e-01,  1.0300e-01,  ...,  4.3586e-02,\n",
      "           6.6619e-01,  2.8016e-01],\n",
      "         [-2.0444e-01, -6.5850e-02,  2.3727e-01,  ..., -1.1268e-01,\n",
      "           4.9937e-01,  1.1340e-01],\n",
      "         ...,\n",
      "         [ 1.9708e-01,  2.7825e-01,  4.8396e-02,  ...,  1.5847e-01,\n",
      "          -2.6033e-04,  1.6389e-01],\n",
      "         [ 8.1123e-02, -3.9421e-01,  3.3592e-01,  ...,  2.7370e-01,\n",
      "          -1.4740e-01, -1.5341e-01],\n",
      "         [ 4.7976e-01,  1.2061e-01, -1.5923e-01,  ...,  2.3984e-01,\n",
      "          -4.8615e-01, -5.9876e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': '2007 their second record its great to be alive was released through side one dummy records on february 17 2009 their third', 'score': tensor([0.6130], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6130], device='cuda:0')\n",
      "pred answer_text: 2007 their second record its great to be alive was released through side one dummy records on february 17 2009 their third\n",
      "sequence_output  tensor([[[-0.4808, -0.1670,  0.2773,  ..., -0.1624,  0.2681,  0.3479],\n",
      "         [-0.2542,  0.3168,  0.0614,  ...,  0.2346,  0.3014, -0.1278],\n",
      "         [-0.7581, -0.0202,  0.7725,  ..., -0.2761, -0.3654, -0.0909],\n",
      "         ...,\n",
      "         [ 0.2243,  0.2149,  0.5482,  ..., -0.1059, -0.2657, -0.1162],\n",
      "         [-0.0178,  0.1510,  0.8663,  ..., -0.3321, -0.3116, -0.0621],\n",
      "         [ 0.7511, -0.2593,  0.1158,  ...,  0.1361, -0.5411, -0.2529]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.4513], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3435, -0.1133,  0.0127,  ...,  0.0065,  0.4228,  0.2012],\n",
      "         [-0.1376,  0.3321, -0.0464,  ..., -0.0444,  0.6223,  0.0947],\n",
      "         [-0.6187, -0.0538,  0.3127,  ..., -0.0070, -0.0642, -0.3827],\n",
      "         ...,\n",
      "         [-1.1777,  0.2294,  0.5957,  ..., -0.0814,  0.6655, -0.8973],\n",
      "         [-1.0683, -0.0026,  0.0071,  ..., -0.0223, -0.0040, -0.7682],\n",
      "         [ 0.6064,  0.0076, -0.0126,  ...,  0.0558, -0.2781, -0.4527]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.3806], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3601, -0.0290, -0.0742,  ..., -0.0967,  0.3079,  0.3248],\n",
      "         [ 0.3045,  0.0842,  0.5229,  ...,  0.6281,  0.6819, -0.3801],\n",
      "         [-0.9072, -0.4843,  0.1763,  ...,  0.0795,  0.1930,  0.0213],\n",
      "         ...,\n",
      "         [-0.6422,  0.1392, -0.0258,  ...,  0.3380,  0.4562, -0.3638],\n",
      "         [-0.3234, -0.2878, -0.1371,  ...,  0.2276,  0.0122, -0.4833],\n",
      "         [ 0.5544,  0.4329, -0.0356,  ...,  0.1667, -0.4237, -0.3425]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([0.3554], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.0 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00000: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer/_ckpt_epoch_0.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(12.0690, device='cuda:0')\tavg_answer_loss:  tensor(5.7435, device='cuda:0')\tavg_type_loss:  tensor(1.2651, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "sequence_output  tensor([[[-0.3358,  0.0457, -0.0524,  ..., -0.1165,  0.3267,  0.3038],\n",
      "         [-0.2762,  0.5107,  0.2064,  ...,  0.2236,  0.6973,  0.3374],\n",
      "         [-0.3196, -0.1117,  0.3742,  ...,  0.4420,  0.5132,  1.0292],\n",
      "         ...,\n",
      "         [ 0.6609,  0.5418,  0.6698,  ..., -0.0762,  0.0296, -0.5721],\n",
      "         [-0.1946, -0.0377,  0.2423,  ...,  0.0446,  0.4579, -0.5575],\n",
      "         [ 0.6112,  0.5053,  0.1254,  ...,  0.1624, -0.4212, -0.3446]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3318, -0.1165,  0.0703,  ..., -0.0446,  0.4465,  0.0602],\n",
      "         [-0.1397,  0.2357, -0.0989,  ..., -0.0089,  0.6835,  0.0681],\n",
      "         [-0.5525, -0.0438,  0.3402,  ...,  0.0777, -0.0166, -0.4302],\n",
      "         ...,\n",
      "         [-1.4333,  0.3318,  0.5706,  ..., -0.1310,  0.6042, -0.7707],\n",
      "         [-1.1738,  0.0962,  0.0207,  ...,  0.0188, -0.0735, -0.7218],\n",
      "         [ 0.2218,  0.2953,  0.0029,  ...,  0.0399, -0.2557, -0.4584]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5058, -0.1608, -0.1292,  ..., -0.0443,  0.4930,  0.1272],\n",
      "         [-0.5262, -0.0283, -0.0978,  ...,  0.3622,  0.6399, -0.2302],\n",
      "         [-0.2277,  0.4698, -0.2554,  ..., -0.0555,  0.3073,  0.0907],\n",
      "         ...,\n",
      "         [ 0.5685,  0.0459,  0.1984,  ...,  0.2714,  0.3223, -0.3199],\n",
      "         [-0.5459, -0.3717,  0.0709,  ...,  0.3955,  0.0812, -0.7391],\n",
      "         [ 0.4464,  0.0728, -0.1461,  ...,  0.2332, -0.0708, -0.5496]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2597, -0.0117,  0.0108,  ...,  0.0598,  0.4245,  0.1571],\n",
      "         [-0.4571, -0.1640, -0.0039,  ...,  0.3770,  0.8802, -0.4026],\n",
      "         [-0.5113,  0.1843, -0.1941,  ..., -0.2442,  0.3660,  0.2591],\n",
      "         ...,\n",
      "         [-0.9674, -0.1628,  0.5570,  ..., -0.1257,  0.2864,  0.0862],\n",
      "         [ 0.1665, -0.2086,  0.2514,  ...,  0.1790,  0.2070, -0.0258],\n",
      "         [ 0.6159, -0.0014, -0.0447,  ..., -0.0073, -0.3570, -0.3527]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1991,  0.1484, -0.0983,  ..., -0.0079,  0.5256,  0.3243],\n",
      "         [-0.5276,  0.1838,  0.3339,  ..., -0.0242,  0.6582, -0.5369],\n",
      "         [-0.6950,  0.0903, -0.5610,  ..., -0.4074,  0.4150,  0.3245],\n",
      "         ...,\n",
      "         [ 0.7894,  0.6646,  0.3311,  ..., -0.4743,  0.3263, -0.1322],\n",
      "         [-0.7769, -0.1154, -0.2017,  ..., -0.0582,  0.3206, -0.1637],\n",
      "         [ 0.7752,  0.2819, -0.0459,  ...,  0.2992, -0.3744, -0.2321]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4901, -0.1357, -0.1961,  ..., -0.0577,  0.6021,  0.1892],\n",
      "         [-0.8231, -0.4069,  0.1476,  ..., -0.4194,  0.4679, -0.1093],\n",
      "         [-0.5257,  0.0113,  0.2240,  ...,  0.0610,  0.4053,  0.2008],\n",
      "         ...,\n",
      "         [-0.4665,  0.8456,  0.1530,  ..., -0.0126, -0.0801, -0.2968],\n",
      "         [-0.8461, -0.0663, -0.1239,  ...,  0.2597,  0.4096, -0.5902],\n",
      "         [ 0.6203,  0.1222, -0.2368,  ..., -0.0997, -0.2466, -0.4363]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5380, -0.0110, -0.1994,  ...,  0.0109,  0.5070,  0.1708],\n",
      "         [-0.2048,  0.3213, -0.1227,  ...,  0.2654,  0.0561,  0.4738],\n",
      "         [-0.1482, -0.0708,  0.1826,  ...,  0.0856,  0.0690,  0.3001],\n",
      "         ...,\n",
      "         [-0.7093,  0.3130,  0.4985,  ...,  0.6336, -0.1747, -0.8560],\n",
      "         [-0.2268,  0.3826,  0.0055,  ...,  0.3149, -0.3414, -0.2136],\n",
      "         [ 0.6531,  0.0848, -0.3142,  ..., -0.0339, -0.3757, -0.4355]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4656, -0.1011,  0.0574,  ..., -0.0779,  0.3586,  0.2381],\n",
      "         [-0.9020,  0.2759, -0.0230,  ...,  0.3923,  0.4774, -0.1277],\n",
      "         [-0.8537, -0.1294,  0.0219,  ..., -0.0202,  0.4544, -0.0902],\n",
      "         ...,\n",
      "         [-0.0869, -0.6300,  0.4508,  ...,  0.3231,  0.3270, -0.4886],\n",
      "         [-0.8747,  0.3783,  0.1746,  ...,  0.2267, -0.1517,  0.3051],\n",
      "         [ 0.6563, -0.0104, -0.1361,  ...,  0.1809, -0.3450, -0.7341]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2398, -0.0044, -0.0721,  ..., -0.0578,  0.2479,  0.1591],\n",
      "         [-0.0532,  0.3875, -0.1360,  ...,  0.4522,  0.6225, -0.3171],\n",
      "         [-0.9901,  0.3831,  0.1095,  ...,  0.2252,  0.7439,  0.5856],\n",
      "         ...,\n",
      "         [ 0.2998,  0.2035, -0.1548,  ...,  0.1001,  0.2533, -0.2879],\n",
      "         [ 0.0758, -0.0482,  0.0410,  ...,  0.2601,  0.2057, -0.5646],\n",
      "         [ 0.5452,  0.2784, -0.0049,  ...,  0.0196, -0.5009, -0.4739]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5127, -0.2358,  0.0141,  ..., -0.0115,  0.1791,  0.1093],\n",
      "         [-0.7563,  0.0166,  0.2447,  ...,  0.0371,  0.5654,  0.0297],\n",
      "         [-0.0216,  0.4598,  0.1937,  ...,  0.3907, -0.1272,  0.3347],\n",
      "         ...,\n",
      "         [-0.3041, -0.1486,  0.6456,  ..., -0.0139, -0.0708, -0.7490],\n",
      "         [-0.0666, -0.4876,  0.1829,  ..., -0.0030, -0.0428, -1.1564],\n",
      "         [ 0.3970,  0.0880, -0.0904,  ..., -0.1119, -0.4191, -0.5682]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6155, -0.0848,  0.2818,  ...,  0.0094,  0.3011,  0.2253],\n",
      "         [-0.7678, -0.0757,  0.2025,  ...,  0.1554,  0.6533, -0.2685],\n",
      "         [-0.5815,  0.0033,  0.2525,  ...,  0.1845, -0.1039, -0.0045],\n",
      "         ...,\n",
      "         [-0.5309,  0.0664,  0.9863,  ...,  0.2378,  0.4111, -0.7377],\n",
      "         [-0.1743,  0.1352,  0.6868,  ...,  0.4108, -0.0888, -0.3562],\n",
      "         [ 0.5823,  0.1017, -0.2203,  ...,  0.1905, -0.6085, -0.5728]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2751, -0.0328, -0.0254,  ..., -0.1095,  0.2245,  0.2383],\n",
      "         [-0.5088, -0.0344,  0.3809,  ...,  0.1245,  0.4465, -0.0310],\n",
      "         [-0.2937,  0.2020,  0.0965,  ...,  0.0786,  0.1740, -0.4512],\n",
      "         ...,\n",
      "         [-0.8037,  0.1643,  0.3592,  ..., -0.1293,  0.5043, -0.7815],\n",
      "         [-0.0459, -0.2875,  0.1506,  ...,  0.2610,  0.2513, -0.6531],\n",
      "         [ 0.3114,  0.1791, -0.0885,  ...,  0.0885, -0.5414, -0.3211]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4086,  0.0390,  0.0339,  ..., -0.0073,  0.5351,  0.1870],\n",
      "         [ 0.1882,  0.2018,  0.4341,  ...,  0.3824,  0.5505, -0.0638],\n",
      "         [-0.3951, -0.2860, -0.0069,  ..., -0.2383,  0.2255, -0.3248],\n",
      "         ...,\n",
      "         [ 0.5368, -0.2270,  0.4756,  ...,  0.3090, -0.0160, -0.5200],\n",
      "         [-0.3904,  0.1545,  0.2847,  ...,  0.1681,  0.0162, -0.7404],\n",
      "         [ 0.6735, -0.0676, -0.1558,  ...,  0.2465, -0.4266, -0.3585]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2261, -0.0959,  0.0173,  ..., -0.0913,  0.3097,  0.2249],\n",
      "         [-0.2080,  0.1448,  0.0521,  ...,  0.1929,  0.7438, -0.0964],\n",
      "         [-0.0335, -0.2287,  0.2691,  ...,  0.0290,  0.4152,  0.1175],\n",
      "         ...,\n",
      "         [ 0.0913,  0.3271,  0.0421,  ...,  0.1154,  0.0281, -0.6250],\n",
      "         [-0.5583, -0.5611,  0.2318,  ...,  0.2344, -0.2009, -0.3644],\n",
      "         [ 0.7910,  0.0923, -0.0274,  ..., -0.0148, -0.3260, -0.4964]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3916, -0.1279,  0.2075,  ..., -0.0072,  0.1905,  0.5384],\n",
      "         [-0.3510,  0.0268,  0.1336,  ...,  0.3057,  0.3298, -0.3919],\n",
      "         [-0.0911,  0.0566,  0.6458,  ..., -0.1942, -0.0732,  0.5464],\n",
      "         ...,\n",
      "         [-0.0467, -0.7183,  0.6138,  ..., -0.0437,  0.1268, -1.0432],\n",
      "         [-0.3776, -0.2052,  0.1880,  ...,  0.2163, -0.0232, -0.3342],\n",
      "         [ 0.7332,  0.0542, -0.1112,  ...,  0.2360, -0.5283, -0.5455]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5662, -0.0577,  0.0054,  ...,  0.1241,  0.3104,  0.2456],\n",
      "         [ 0.2412,  0.0649,  0.2018,  ...,  0.2999,  0.7825,  1.0097],\n",
      "         [-0.1626,  0.7107,  0.3113,  ...,  0.3506, -0.0281,  0.0037],\n",
      "         ...,\n",
      "         [-1.0678,  0.2707,  0.5815,  ...,  0.4853,  0.0634, -0.1823],\n",
      "         [-0.6134, -0.0438,  0.4292,  ..., -0.0341,  0.3304, -0.0247],\n",
      "         [ 0.7745,  0.3671, -0.1467,  ...,  0.3866, -0.5789, -0.3601]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2583, -0.0299,  0.1022,  ...,  0.0718,  0.4381,  0.1487],\n",
      "         [ 0.0740, -0.1803,  0.1688,  ...,  0.3937,  0.2082, -0.3453],\n",
      "         [ 0.1891,  0.3091, -0.1075,  ...,  0.0375,  0.4336, -0.1887],\n",
      "         ...,\n",
      "         [-0.3973, -0.4430,  0.3714,  ...,  0.2914,  0.3822,  0.3929],\n",
      "         [-0.6156,  0.1123,  0.6266,  ..., -0.0321,  0.0686, -0.4059],\n",
      "         [ 0.7734,  0.2249, -0.0653,  ...,  0.2897, -0.3055, -0.3413]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2420, -0.1390,  0.1100,  ...,  0.1159,  0.2593,  0.1399],\n",
      "         [-0.5861, -0.0877, -0.1287,  ...,  0.3597,  0.8269, -0.3910],\n",
      "         [-0.5039, -0.3098,  0.1412,  ...,  0.1255,  0.2771, -0.2954],\n",
      "         ...,\n",
      "         [ 0.6376,  0.1308, -0.0447,  ...,  0.3520, -0.3878, -0.4309],\n",
      "         [-0.3090, -0.4660,  0.2988,  ...,  0.4155,  0.1786, -0.6585],\n",
      "         [ 0.5881,  0.3019, -0.0378,  ...,  0.0295, -0.4572, -0.4785]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1714, -0.1323,  0.0737,  ...,  0.1108,  0.2171,  0.4227],\n",
      "         [-0.4512, -0.1729,  0.3528,  ...,  0.1262,  0.1453, -0.2194],\n",
      "         [-0.6943,  0.1878,  0.2634,  ..., -0.0714,  0.0736, -0.5381],\n",
      "         ...,\n",
      "         [-0.6350,  0.2257,  0.2402,  ..., -0.1820,  0.1805, -0.8215],\n",
      "         [-0.6933, -0.1159,  0.0073,  ..., -0.0491, -0.2032, -0.8070],\n",
      "         [ 0.6829,  0.1217, -0.1666,  ...,  0.3928, -0.4498, -0.5582]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2332, -0.0196, -0.0401,  ..., -0.0669,  0.3929,  0.2835],\n",
      "         [ 0.0892,  0.4986,  0.5601,  ..., -0.2298,  0.2669,  0.3756],\n",
      "         [-0.5402, -0.1538,  0.2453,  ..., -0.0897,  0.1985,  0.0250],\n",
      "         ...,\n",
      "         [-0.2490,  0.1655,  0.3465,  ...,  0.0880,  0.6472, -0.2998],\n",
      "         [-0.6653,  0.1945,  0.0209,  ...,  0.3857,  0.1912, -0.7298],\n",
      "         [ 0.7115,  0.2925,  0.1042,  ...,  0.1490, -0.2693, -0.4200]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3430,  0.0290, -0.0384,  ..., -0.1903,  0.4947,  0.2350],\n",
      "         [-1.0069, -0.3650,  0.2656,  ..., -0.1774,  0.7461,  0.6558],\n",
      "         [-0.8384, -0.1243,  0.1062,  ..., -0.1623,  0.2555, -0.3136],\n",
      "         ...,\n",
      "         [-0.2386,  0.5788,  0.4699,  ..., -0.1890,  0.3687, -0.5589],\n",
      "         [-0.2260, -0.0066,  0.5486,  ..., -0.0764, -0.0958, -0.8746],\n",
      "         [ 0.7917,  0.1170, -0.1438,  ...,  0.1376, -0.4819, -0.2782]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4112, -0.0400,  0.0068,  ..., -0.1053,  0.3554,  0.2226],\n",
      "         [-0.5832,  0.2052,  0.2724,  ..., -0.0800,  0.1742, -0.0465],\n",
      "         [-0.5267,  0.0362,  0.4766,  ..., -0.3577,  0.0422, -0.3968],\n",
      "         ...,\n",
      "         [-0.9372,  0.1161, -0.0417,  ..., -0.0761,  0.3444,  0.1705],\n",
      "         [-0.2566,  0.1066, -0.1384,  ...,  0.1303,  0.0669, -0.7567],\n",
      "         [ 0.6064,  0.4305,  0.1521,  ...,  0.1155, -0.4515, -0.2485]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4712, -0.1310, -0.0408,  ..., -0.3128,  0.2761,  0.0897],\n",
      "         [-0.9256, -0.2278, -0.1590,  ..., -0.1157,  0.6413, -0.4183],\n",
      "         [-0.8676, -0.4627,  0.1843,  ..., -0.2906,  0.4033,  0.0010],\n",
      "         ...,\n",
      "         [-0.4413,  0.1296,  0.3009,  ..., -0.2059,  0.3107, -0.7218],\n",
      "         [-0.1396, -0.2922,  0.3834,  ..., -0.0634,  0.2575,  0.2671],\n",
      "         [ 0.5604,  0.2064,  0.0235,  ..., -0.1239, -0.2230, -0.5137]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2018, -0.0935, -0.0722,  ..., -0.1174,  0.4028,  0.2710],\n",
      "         [ 0.0125,  0.1850,  0.3973,  ...,  0.3009,  0.5806, -0.7318],\n",
      "         [-0.0510, -0.6348,  1.1036,  ...,  0.4527,  0.6183,  0.2896],\n",
      "         ...,\n",
      "         [ 0.3820,  0.5934,  0.4687,  ..., -0.0533,  0.1455, -1.0523],\n",
      "         [-0.2135,  0.1459,  0.1502,  ..., -0.0172,  0.0104, -0.9745],\n",
      "         [ 0.7751, -0.0195, -0.2542,  ...,  0.1833, -0.5323, -0.4592]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1426, -0.0386, -0.0464,  ..., -0.0258,  0.2663,  0.1410],\n",
      "         [-0.6954, -0.0404,  0.3363,  ...,  0.1416,  0.0890,  0.2209],\n",
      "         [-0.6264, -0.1719, -0.0358,  ...,  0.2818, -0.1033, -0.6547],\n",
      "         ...,\n",
      "         [-0.4513, -0.3422,  0.3847,  ...,  0.1524,  0.0782, -0.6382],\n",
      "         [-0.7084, -0.3221, -0.0429,  ...,  0.3542,  0.2081, -0.2722],\n",
      "         [ 0.6590, -0.0563,  0.1495,  ...,  0.0231, -0.1087, -0.5259]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2868,  0.0425, -0.1538,  ..., -0.1982,  0.4951,  0.1279],\n",
      "         [-0.8703,  0.3407,  0.0987,  ..., -0.3003,  0.5091, -0.0893],\n",
      "         [-0.3718,  0.0627,  0.2473,  ..., -0.2337,  0.2124,  0.1008],\n",
      "         ...,\n",
      "         [-0.0617,  0.1950, -0.2301,  ..., -0.0234,  0.1557, -0.2957],\n",
      "         [-0.8495,  0.1359, -0.3152,  ...,  0.1248,  0.2873, -0.4908],\n",
      "         [ 0.7621,  0.2813,  0.0490,  ...,  0.2468, -0.4905, -0.3731]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2736, -0.2108,  0.1221,  ...,  0.0968,  0.3594,  0.4721],\n",
      "         [-0.4319, -0.2044, -0.3329,  ..., -0.0955,  0.3621, -0.4850],\n",
      "         [ 0.0494, -0.5602,  0.7235,  ...,  0.3301,  0.1836,  0.3537],\n",
      "         ...,\n",
      "         [-0.2709, -0.4047,  0.6511,  ...,  0.1140, -0.0024, -0.4109],\n",
      "         [ 0.1096, -0.7309,  0.5400,  ...,  0.2884, -0.3972, -0.3398],\n",
      "         [ 0.8289,  0.3369, -0.0321,  ...,  0.2921, -0.4877, -0.2266]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2082, -0.1484,  0.0318,  ..., -0.0780,  0.3470,  0.1275],\n",
      "         [-0.7187, -0.0524,  0.1263,  ...,  0.2006,  0.4314, -0.8547],\n",
      "         [-0.9759, -0.0722,  0.3586,  ...,  0.1415,  0.0487, -0.5737],\n",
      "         ...,\n",
      "         [-0.1059, -0.1509,  0.4591,  ...,  0.1260,  0.3932, -0.6270],\n",
      "         [-0.1993, -0.3282,  0.0154,  ..., -0.0862,  0.0201, -1.0474],\n",
      "         [ 0.3183,  0.1919,  0.0082,  ...,  0.1647, -0.3552, -0.3291]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0982, -0.0086,  0.1123,  ..., -0.0657,  0.5384, -0.0680],\n",
      "         [-0.6535,  0.0621,  0.1134,  ...,  0.0601,  0.3691,  0.0790],\n",
      "         [-0.5360, -0.3487,  0.4271,  ..., -0.0289,  0.3571, -0.1639],\n",
      "         ...,\n",
      "         [ 0.5008, -0.0204,  0.1059,  ..., -0.1879,  0.1278, -1.0212],\n",
      "         [-0.3292, -0.4529,  0.4071,  ..., -0.2180,  0.0558, -0.3431],\n",
      "         [ 0.6383,  0.1151, -0.0155,  ...,  0.1214, -0.3988, -0.7200]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2676, -0.0234, -0.0953,  ...,  0.0870,  0.4226,  0.2061],\n",
      "         [-0.1560,  0.2758, -0.0778,  ...,  0.2903,  0.4010, -0.2833],\n",
      "         [-0.6382,  0.0678,  0.0968,  ...,  0.2753,  0.4912,  0.4200],\n",
      "         ...,\n",
      "         [-1.0465, -0.1462,  0.0459,  ...,  0.0257,  0.5072, -0.4965],\n",
      "         [ 0.0168, -0.2981,  0.1410,  ...,  0.3451,  0.1621, -0.5512],\n",
      "         [ 0.6243,  0.0781, -0.2059,  ..., -0.0276, -0.4361, -0.4772]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1463,  0.0329,  0.0677,  ...,  0.1127,  0.3706,  0.1900],\n",
      "         [-0.0277,  0.1262,  0.0369,  ...,  0.0598,  0.3770,  0.2285],\n",
      "         [-0.1892, -0.0327,  0.3004,  ..., -0.0742,  0.4199,  0.1823],\n",
      "         ...,\n",
      "         [ 0.1127,  0.4159,  0.0049,  ...,  0.2040,  0.1610, -0.0341],\n",
      "         [ 0.0888, -0.2694,  0.4349,  ...,  0.3307, -0.0133, -0.5457],\n",
      "         [ 0.2596,  0.1123, -0.0512,  ...,  0.2395, -0.4412, -0.6244]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4172, -0.0063, -0.0571,  ..., -0.0313,  0.5099,  0.0167],\n",
      "         [-0.3427, -0.1491,  0.0881,  ...,  0.2622,  0.5640,  0.0505],\n",
      "         [ 0.2243, -0.0444,  0.0810,  ..., -0.1560,  0.4070, -0.6818],\n",
      "         ...,\n",
      "         [ 0.6559,  0.0842,  0.4828,  ..., -0.2324,  0.0969, -0.9643],\n",
      "         [-0.3140,  0.3736,  0.2354,  ..., -0.3650, -0.1171, -1.1665],\n",
      "         [ 0.6134, -0.1195, -0.1088,  ...,  0.0654, -0.4911, -0.6271]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2018, -0.2121,  0.0417,  ..., -0.0199,  0.3712,  0.1241],\n",
      "         [-0.4107, -0.3633,  0.2833,  ...,  0.1887,  1.1635, -0.0332],\n",
      "         [-0.5051, -0.0320,  0.1540,  ...,  0.2535,  0.1964, -1.5104],\n",
      "         ...,\n",
      "         [ 0.2731, -0.3969,  0.9424,  ..., -0.1468,  0.6454, -0.1683],\n",
      "         [ 0.6060,  0.5076,  0.2624,  ...,  0.1528,  0.1642, -0.8629],\n",
      "         [ 0.7024,  0.2321, -0.2618,  ...,  0.2367, -0.5204, -0.4310]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1483, -0.2939,  0.0676,  ...,  0.1599,  0.3318, -0.0662],\n",
      "         [-0.2960, -0.1725, -0.3015,  ...,  0.0266,  0.5530, -0.4667],\n",
      "         [-0.6423, -0.0521, -0.4886,  ...,  0.3215,  0.6278, -0.7817],\n",
      "         ...,\n",
      "         [ 0.3468, -0.0159,  0.2911,  ...,  0.3424,  0.1268, -0.9187],\n",
      "         [-0.3125, -0.1652,  0.0279,  ...,  0.2868, -0.0118, -0.7377],\n",
      "         [ 0.6992,  0.1661, -0.0410,  ...,  0.1076, -0.3718, -0.5608]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3093, -0.1237, -0.0202,  ..., -0.0917,  0.3464,  0.0595],\n",
      "         [-0.5005, -0.0424,  0.4018,  ...,  0.2096,  0.0426, -0.0015],\n",
      "         [-0.3028, -0.1130,  0.2666,  ...,  0.1509,  0.1456, -0.4734],\n",
      "         ...,\n",
      "         [-0.5246,  0.1270,  0.1051,  ...,  0.2949,  0.4044, -0.6212],\n",
      "         [-0.3716, -0.6466,  0.4438,  ...,  0.0741, -0.1406, -0.8079],\n",
      "         [ 0.5706,  0.1257, -0.1874,  ...,  0.1263, -0.5893, -0.5682]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1721, -0.3239, -0.1649,  ...,  0.0065,  0.3069,  0.1725],\n",
      "         [-0.3230, -0.2185,  0.0758,  ...,  0.2281,  0.2707, -0.1621],\n",
      "         [-0.5780,  0.0684, -0.0479,  ...,  0.0848,  0.2883, -0.1739],\n",
      "         ...,\n",
      "         [-0.2968,  0.2730,  0.3444,  ...,  0.2072,  0.1232, -0.2795],\n",
      "         [ 0.0977, -0.1004,  0.4954,  ...,  0.1868,  0.0246, -0.5685],\n",
      "         [ 0.7640,  0.0744, -0.1002,  ...,  0.3143, -0.4405, -0.5159]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4060, -0.1500,  0.0269,  ..., -0.0216,  0.4122,  0.2096],\n",
      "         [-0.3274,  0.1881,  0.0683,  ...,  0.4598,  0.6280, -0.2165],\n",
      "         [-0.9628,  0.0954,  0.0377,  ...,  0.1209,  0.4074, -0.5519],\n",
      "         ...,\n",
      "         [-0.4796, -0.8074,  0.4825,  ..., -0.2589,  0.1168,  0.4045],\n",
      "         [-0.6524, -0.4618,  0.0968,  ...,  0.1259,  0.0500, -0.4275],\n",
      "         [ 0.6820, -0.0892,  0.0767,  ...,  0.1543, -0.3372, -0.3804]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4746,  0.0724, -0.0100,  ..., -0.0283,  0.2527,  0.1475],\n",
      "         [ 0.0763, -0.0123, -0.1386,  ...,  0.2553,  0.1644, -0.1784],\n",
      "         [ 0.0891, -0.4245,  0.0246,  ...,  0.0545,  0.2201,  0.5367],\n",
      "         ...,\n",
      "         [-0.2758,  0.5930, -0.0243,  ..., -0.2024,  0.0840, -0.3375],\n",
      "         [-0.7373, -0.5639, -0.2306,  ...,  0.2228,  0.5993, -0.5630],\n",
      "         [ 0.3081,  0.3249, -0.3239,  ...,  0.2045, -0.5497, -0.2295]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3835, -0.1448,  0.1147,  ..., -0.3367,  0.3426,  0.1261],\n",
      "         [-0.8518,  0.0638, -0.2324,  ..., -0.4098,  0.5641, -0.1571],\n",
      "         [-0.4943, -0.0128,  0.0182,  ..., -0.2593,  0.2155, -0.3129],\n",
      "         ...,\n",
      "         [ 0.9808,  0.6499,  0.2460,  ..., -0.2418, -0.2966, -0.5384],\n",
      "         [-0.5459,  0.0052,  0.1193,  ..., -0.0935,  0.1231, -0.6530],\n",
      "         [ 0.7356,  0.1936, -0.1054,  ...,  0.0852, -0.4601, -0.4370]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3843, -0.1447,  0.0123,  ..., -0.0626,  0.3806,  0.0333],\n",
      "         [-0.6888,  0.1441,  0.3157,  ..., -0.1421,  0.8268, -0.2976],\n",
      "         [-0.3436,  0.0977,  0.4918,  ..., -0.1695,  0.1170, -0.1254],\n",
      "         ...,\n",
      "         [-0.7623, -0.0612,  0.1917,  ...,  0.2069,  0.4010, -0.0571],\n",
      "         [ 0.0819, -0.1886,  0.4398,  ..., -0.0419,  0.4262, -0.5554],\n",
      "         [ 0.3260,  0.2241, -0.0250,  ...,  0.0713, -0.0783, -0.2819]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1423, -0.1842, -0.0316,  ...,  0.2010,  0.2840,  0.0449],\n",
      "         [-0.3972,  0.0595,  0.1794,  ...,  0.1404,  0.3923, -0.1288],\n",
      "         [-0.5700, -0.1291,  0.2818,  ...,  0.3073,  0.0619, -0.8686],\n",
      "         ...,\n",
      "         [ 0.7860,  0.3611,  0.3659,  ...,  0.6884,  0.3652, -0.7457],\n",
      "         [-0.6786,  0.2650,  0.2095,  ...,  0.7752,  0.1000, -0.5569],\n",
      "         [ 0.7187, -0.0056, -0.1787,  ...,  0.1597, -0.3939, -0.6223]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1648, -0.1518,  0.0047,  ...,  0.1460,  0.3626, -0.0220],\n",
      "         [-0.3628, -0.0837,  0.1296,  ...,  0.1904,  0.1743, -0.1596],\n",
      "         [-0.3479,  0.1230, -0.1048,  ..., -0.0400,  0.0028, -0.6665],\n",
      "         ...,\n",
      "         [ 0.7565,  0.4956,  0.3320,  ...,  0.5509,  0.1830, -0.9513],\n",
      "         [-0.3012, -0.3739,  0.4401,  ...,  0.2743, -0.0116, -0.6190],\n",
      "         [ 0.6839,  0.0845,  0.0703,  ...,  0.1325, -0.3356, -0.5953]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2913, -0.0395,  0.1504,  ...,  0.0259,  0.3406,  0.0914],\n",
      "         [-0.4150,  0.2893,  0.5263,  ...,  0.2376,  0.7083,  0.2734],\n",
      "         [-0.0967, -0.1605,  0.2532,  ...,  0.0587,  0.2009, -0.2375],\n",
      "         ...,\n",
      "         [ 0.0560,  0.5758,  0.0393,  ...,  0.0861, -0.0141, -0.8558],\n",
      "         [-0.5839, -0.2623, -0.1614,  ...,  0.3271,  0.4536, -0.6230],\n",
      "         [ 0.6790,  0.2110, -0.1885,  ...,  0.0190, -0.4270, -0.4436]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3064, -0.0665, -0.1285,  ..., -0.0148,  0.3868,  0.0718],\n",
      "         [-0.5424, -0.2193,  0.1050,  ...,  0.2235,  0.4243, -0.3111],\n",
      "         [-0.0368, -0.2616,  0.1471,  ...,  0.1354, -0.2761, -0.1957],\n",
      "         ...,\n",
      "         [-0.3429,  0.0540,  0.0560,  ...,  0.0719, -0.2962, -0.1995],\n",
      "         [-0.7174,  0.1953, -0.5041,  ...,  0.0254, -0.2979, -1.1661],\n",
      "         [ 0.7536,  0.1375,  0.0440,  ...,  0.1392, -0.2934, -0.3486]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1495, -0.2155,  0.1503,  ..., -0.0285,  0.3054, -0.0806],\n",
      "         [ 0.4665, -0.2800,  0.2780,  ..., -0.5142,  0.4251,  0.3135],\n",
      "         [-0.4745, -0.5348,  0.9690,  ...,  0.1719,  0.2194,  0.2741],\n",
      "         ...,\n",
      "         [ 0.0362, -0.1949,  0.2509,  ...,  0.2388, -0.1409, -0.0780],\n",
      "         [-0.6045,  0.0098,  0.0531,  ...,  0.3344,  0.4444, -0.7461],\n",
      "         [ 0.6145,  0.0880,  0.0517,  ...,  0.0508, -0.3916, -0.5272]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1032,  0.0829, -0.1062,  ..., -0.0715,  0.4192,  0.0956],\n",
      "         [-0.1582, -0.0545, -0.2214,  ...,  0.3340,  0.5753, -0.4343],\n",
      "         [-0.8443, -0.0501,  0.0250,  ..., -0.1981,  0.5189,  0.5339],\n",
      "         ...,\n",
      "         [-0.1161, -0.2070, -0.1958,  ..., -0.3789,  0.0140, -0.8619],\n",
      "         [-0.1076, -0.1953, -0.1456,  ..., -0.3444, -0.1908, -0.7222],\n",
      "         [ 0.5079, -0.0865, -0.0215,  ...,  0.1476, -0.3591, -0.5260]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3552, -0.0692, -0.0070,  ..., -0.0545,  0.3248,  0.1119],\n",
      "         [-0.3435, -0.1718, -0.2310,  ...,  0.1617,  0.6873, -0.3848],\n",
      "         [-0.6681, -0.5487,  0.1701,  ...,  0.1171, -0.0967, -0.1557],\n",
      "         ...,\n",
      "         [ 0.1395, -0.3807,  0.1344,  ..., -0.2654, -0.1283, -0.7091],\n",
      "         [-0.1283,  0.1323,  0.1423,  ...,  0.3172,  0.1767, -0.6871],\n",
      "         [ 0.6927,  0.1156, -0.2287,  ...,  0.1212, -0.3551, -0.4915]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3739,  0.0307, -0.1865,  ..., -0.1356,  0.5895,  0.1254],\n",
      "         [-0.0802,  0.8674,  0.1022,  ...,  0.0241,  0.2596, -0.1834],\n",
      "         [-0.8334,  0.0340, -0.2727,  ..., -0.1528,  0.9990,  0.0063],\n",
      "         ...,\n",
      "         [ 0.2521,  0.5046, -0.0979,  ..., -0.2225,  0.1059, -0.5276],\n",
      "         [ 0.4078,  0.3288, -0.0977,  ..., -0.0862,  0.1579, -0.4653],\n",
      "         [ 0.5115,  0.0693, -0.0812,  ...,  0.0989, -0.2507, -0.4542]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2365, -0.0436,  0.0090,  ..., -0.0765,  0.3847,  0.1299],\n",
      "         [-0.5469, -0.0836, -0.1516,  ...,  0.2589,  0.9385, -0.4257],\n",
      "         [-0.9963,  0.1809,  0.2410,  ..., -0.1605,  0.6159,  0.0146],\n",
      "         ...,\n",
      "         [-0.3202,  0.2108,  0.0722,  ..., -0.1024, -0.2131, -0.7584],\n",
      "         [ 0.2172, -0.4105,  0.1551,  ..., -0.3464,  0.1773, -1.1046],\n",
      "         [ 0.7708,  0.2270, -0.1108,  ...,  0.0759, -0.5184, -0.3606]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1563, -0.0370,  0.1542,  ...,  0.0940,  0.3192,  0.0383],\n",
      "         [-0.4761, -0.0037, -0.0431,  ...,  0.0842,  0.4344, -0.5606],\n",
      "         [ 0.0582,  0.2327,  0.2341,  ..., -0.4671,  0.3821, -0.3957],\n",
      "         ...,\n",
      "         [-0.2836, -0.0690, -0.0672,  ...,  0.2485, -0.2439, -0.2472],\n",
      "         [-0.9052, -0.1821, -0.1250,  ...,  0.1459,  0.1705, -1.5503],\n",
      "         [ 0.6192,  0.2025,  0.1309,  ...,  0.0798, -0.3506, -0.4533]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3001,  0.0400, -0.0290,  ...,  0.0374,  0.4196,  0.2568],\n",
      "         [-0.6566,  0.2072,  0.2663,  ...,  0.2286,  0.5452, -0.0464],\n",
      "         [-0.9360,  0.3084,  0.0866,  ...,  0.2987,  0.9609,  0.1105],\n",
      "         ...,\n",
      "         [-0.4752,  0.0264,  0.2377,  ...,  0.1936,  0.3809, -0.1077],\n",
      "         [ 0.2616,  0.0909,  0.3006,  ...,  0.3023,  0.1378, -0.1968],\n",
      "         [ 0.6188,  0.3409, -0.1742,  ...,  0.1619, -0.2340, -0.5340]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1609, -0.0864, -0.0394,  ...,  0.0679,  0.3868,  0.1027],\n",
      "         [-0.7645, -0.3315,  0.5280,  ...,  0.1279,  1.2520, -0.3728],\n",
      "         [-0.3820,  0.0464,  0.3711,  ...,  0.0220, -0.1762, -0.4543],\n",
      "         ...,\n",
      "         [-0.1487, -0.4010,  0.5479,  ..., -0.0179,  0.0045,  0.2101],\n",
      "         [ 0.7570,  0.0509,  0.2079,  ..., -0.3209,  0.1263, -1.0607],\n",
      "         [ 0.5324,  0.3368,  0.1674,  ...,  0.0514, -0.2485, -0.5517]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3478, -0.1167, -0.0451,  ..., -0.0134,  0.3674,  0.2234],\n",
      "         [ 0.2212,  0.2726,  0.2106,  ...,  0.1217,  0.5109, -0.5841],\n",
      "         [-0.5921,  0.1356,  0.3070,  ...,  0.5679,  0.3242,  0.4512],\n",
      "         ...,\n",
      "         [ 0.7679,  0.6669,  0.3389,  ...,  0.1252, -0.0844, -1.1505],\n",
      "         [ 0.0143, -0.0227, -0.0839,  ..., -0.0605,  0.3132, -0.6757],\n",
      "         [ 0.7867,  0.3706, -0.0457,  ...,  0.1906, -0.4980, -0.2240]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2443, -0.1579, -0.0563,  ..., -0.0136,  0.4044,  0.1604],\n",
      "         [-0.1490,  0.0298, -0.0399,  ...,  0.0537,  0.2093, -0.0637],\n",
      "         [-0.1890, -0.1420, -0.2129,  ..., -0.1698,  0.3227, -0.2713],\n",
      "         ...,\n",
      "         [ 0.2791, -0.1461,  0.0758,  ...,  0.2901,  0.2371, -0.8973],\n",
      "         [-0.0152, -0.2602, -0.0358,  ...,  0.1181,  0.0228, -0.6619],\n",
      "         [ 0.3796,  0.4299,  0.0933,  ..., -0.1735, -0.3020, -0.3772]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3707, -0.2225, -0.1911,  ...,  0.0839,  0.3767,  0.0224],\n",
      "         [-0.5467, -0.0355,  0.0274,  ...,  0.2577,  0.3321, -0.3226],\n",
      "         [ 0.2812, -0.1914,  0.0792,  ..., -0.0081,  0.1564, -0.2502],\n",
      "         ...,\n",
      "         [ 0.4158,  0.3202, -0.2148,  ..., -0.0253,  0.0567, -0.6363],\n",
      "         [-0.3752, -0.0544,  0.1141,  ...,  0.0902,  0.2073, -0.9675],\n",
      "         [ 0.7333,  0.2648, -0.1813,  ...,  0.3083, -0.3848, -0.3852]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1892,  0.1529, -0.0349,  ..., -0.1449,  0.1360,  0.2518],\n",
      "         [-0.7737, -0.1912,  0.8338,  ..., -0.0153,  0.2273,  0.4890],\n",
      "         [-0.5997,  0.0194,  0.7809,  ...,  0.1627, -0.0237,  0.4943],\n",
      "         ...,\n",
      "         [ 0.1473,  0.2346,  0.6226,  ..., -0.3588,  0.0876, -0.6204],\n",
      "         [-0.9021, -0.0197, -0.0311,  ..., -0.0709, -0.1255, -0.5885],\n",
      "         [ 0.7873,  0.2129, -0.1604,  ...,  0.2282, -0.3643, -0.2653]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2024, -0.1004,  0.0351,  ..., -0.0317,  0.4540,  0.2774],\n",
      "         [-0.2588, -0.1641,  0.3111,  ...,  0.1898,  0.3772, -0.0276],\n",
      "         [-0.1465, -0.0283,  0.3796,  ..., -0.1175,  0.4615,  0.2141],\n",
      "         ...,\n",
      "         [-0.8632, -0.8011,  0.0653,  ...,  0.1879,  0.4490, -0.5223],\n",
      "         [-0.5452,  0.1123, -0.0301,  ..., -0.1838,  0.3571, -0.2473],\n",
      "         [ 0.8080,  0.0437, -0.0266,  ...,  0.3065, -0.4621, -0.5095]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3382, -0.1425, -0.0551,  ..., -0.3312,  0.2985,  0.1838],\n",
      "         [-0.2357,  0.1062,  0.1526,  ..., -0.3498,  0.4462, -0.3624],\n",
      "         [-0.3181, -0.2219,  0.3213,  ..., -0.3666,  0.5053, -0.3698],\n",
      "         ...,\n",
      "         [ 0.1407,  0.2391,  0.4506,  ..., -0.1508, -0.0477, -0.2586],\n",
      "         [-0.0882,  0.1009, -0.1201,  ...,  0.0241, -0.1692, -0.4577],\n",
      "         [ 0.5019,  0.1130, -0.0728,  ..., -0.2145, -0.3297, -0.4455]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3009,  0.1461, -0.1592,  ..., -0.0518,  0.5818,  0.0889],\n",
      "         [-0.7343, -0.0091, -0.1714,  ...,  0.2157,  0.9788, -0.1128],\n",
      "         [-0.3298, -0.0822,  0.4306,  ...,  0.0153,  0.5998,  0.2737],\n",
      "         ...,\n",
      "         [ 0.7853,  0.0597,  0.4666,  ..., -0.0083,  0.0017, -0.3842],\n",
      "         [ 0.1883, -0.2606,  0.0495,  ...,  0.0234,  0.2132, -0.2105],\n",
      "         [ 0.4786,  0.1885, -0.0130,  ..., -0.0066, -0.4004, -0.3259]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.0157e-01, -3.9061e-04, -1.7245e-01,  ...,  1.8808e-03,\n",
      "           3.6084e-01,  2.0311e-01],\n",
      "         [-6.2847e-01,  1.0556e-01, -6.5703e-02,  ...,  4.2075e-01,\n",
      "           7.4751e-01, -6.9631e-01],\n",
      "         [-7.8854e-01,  1.7125e-01, -1.0420e-01,  ...,  1.6003e-01,\n",
      "           5.8497e-01,  9.9563e-02],\n",
      "         ...,\n",
      "         [-1.0106e+00,  3.6682e-01, -2.0995e-01,  ..., -1.9007e-01,\n",
      "           2.9086e-01, -5.3071e-01],\n",
      "         [-3.0289e-01, -1.0459e-01, -1.3804e-01,  ...,  3.0882e-01,\n",
      "           1.9252e-01, -7.8222e-01],\n",
      "         [ 5.7414e-01,  1.1446e-01, -1.9701e-01,  ...,  1.0634e-01,\n",
      "          -3.3687e-01, -5.5868e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3987,  0.0508, -0.2229,  ...,  0.0237,  0.4508,  0.2423],\n",
      "         [-0.7249, -0.2387,  0.5128,  ..., -0.0911,  0.5689,  0.0894],\n",
      "         [ 0.3052, -0.0615,  0.1799,  ...,  0.0589,  0.2151,  0.5283],\n",
      "         ...,\n",
      "         [-0.1525,  0.0624,  0.4375,  ...,  0.3075,  0.1156,  0.0627],\n",
      "         [-0.0068, -0.1731,  0.1486,  ...,  0.2334,  0.1830, -0.7749],\n",
      "         [ 0.5081,  0.5232, -0.0921,  ..., -0.0167, -0.5992, -0.7020]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3866, -0.2276, -0.0725,  ...,  0.0453,  0.5082,  0.0864],\n",
      "         [-0.5887, -0.2160, -0.2556,  ...,  0.3366,  0.8197, -0.3553],\n",
      "         [-0.4406,  0.0362, -0.0295,  ..., -0.0181,  0.5526,  0.1191],\n",
      "         ...,\n",
      "         [-0.3773,  0.1505,  0.1242,  ..., -0.1186,  0.5662, -0.2525],\n",
      "         [ 0.2124,  0.0023,  0.1641,  ..., -0.0309,  0.1580, -0.3864],\n",
      "         [ 0.6817,  0.2481, -0.0216,  ..., -0.0300, -0.3219, -0.3352]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2964, -0.0363,  0.1638,  ..., -0.0518,  0.3694,  0.1100],\n",
      "         [-0.2086, -0.2413, -0.0795,  ...,  0.0645,  0.6139, -0.1668],\n",
      "         [-0.8866, -0.0444,  0.0636,  ...,  0.0198,  0.4320,  0.1992],\n",
      "         ...,\n",
      "         [ 0.6808, -0.2091,  1.2019,  ...,  0.3492,  0.5177, -0.7354],\n",
      "         [-0.3755, -0.4172,  0.5645,  ...,  0.0638, -0.0992, -0.1707],\n",
      "         [ 0.7077,  0.3935,  0.0788,  ...,  0.2746, -0.3571, -0.4530]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2325, -0.0319, -0.3511,  ..., -0.2123,  0.4448,  0.1554],\n",
      "         [-0.4920, -0.3863, -0.1452,  ..., -0.3933,  0.5609,  0.3490],\n",
      "         [-0.1639,  0.1410, -0.6594,  ..., -0.1643,  0.0736, -0.1362],\n",
      "         ...,\n",
      "         [-0.1602,  0.2562, -0.7621,  ..., -0.3699,  0.0924, -0.2976],\n",
      "         [-0.6732, -0.6353, -0.2420,  ..., -0.2135,  0.1096, -0.2730],\n",
      "         [ 0.4948,  0.0347,  0.0664,  ..., -0.0269, -0.2876, -0.4085]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2121, -0.1991, -0.2744,  ..., -0.3097,  0.6825,  0.2164],\n",
      "         [-0.1919,  0.1082, -0.4443,  ...,  0.1319,  0.3766, -0.0402],\n",
      "         [-0.7085,  0.0796, -0.0409,  ..., -0.1382,  0.1804,  0.5865],\n",
      "         ...,\n",
      "         [-1.1817, -0.1673, -0.2824,  ..., -0.4541,  0.3876, -0.5896],\n",
      "         [-0.4678, -0.4612,  0.3634,  ..., -0.1482,  0.3891, -0.5038],\n",
      "         [ 0.7210,  0.1983, -0.0758,  ...,  0.1212, -0.4882, -0.3528]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3009, -0.0210, -0.3652,  ..., -0.1334,  0.3707,  0.0848],\n",
      "         [-0.3587,  0.2050, -0.1062,  ..., -0.0271,  0.3140, -0.0993],\n",
      "         [ 0.1500,  0.8759,  0.1109,  ..., -0.1518,  0.0786, -0.3098],\n",
      "         ...,\n",
      "         [-0.1609, -0.4493,  0.2341,  ..., -0.3438, -0.0412, -0.3666],\n",
      "         [-0.2119, -0.3174,  0.1852,  ..., -0.0818,  0.1402, -0.6124],\n",
      "         [ 0.2240,  0.2052,  0.0038,  ..., -0.0984, -0.1894, -0.5535]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4111, -0.2452,  0.2023,  ..., -0.1012,  0.3089,  0.2820],\n",
      "         [-0.1864,  0.3737,  0.0655,  ...,  0.2701,  0.3944, -0.2239],\n",
      "         [-0.7653, -0.0527,  0.7869,  ..., -0.3012, -0.3253, -0.1454],\n",
      "         ...,\n",
      "         [ 0.2941,  0.1357,  0.4780,  ..., -0.2217, -0.2001, -0.2425],\n",
      "         [-0.2426,  0.2026,  0.7859,  ..., -0.3487, -0.3291, -0.1910],\n",
      "         [ 0.7443, -0.3373,  0.1805,  ...,  0.0890, -0.4656, -0.1491]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1697, -0.0540, -0.1508,  ..., -0.2639,  0.6123,  0.0707],\n",
      "         [-0.4213, -0.1086, -0.1047,  ..., -0.1562,  0.9791,  0.4060],\n",
      "         [-0.1718,  0.0716,  0.0503,  ..., -0.2058,  0.1389,  0.0393],\n",
      "         ...,\n",
      "         [ 0.7984,  0.7428,  0.2579,  ..., -0.2686,  0.0315, -0.3177],\n",
      "         [-0.6016,  0.0014, -0.1382,  ..., -0.1213,  0.1609, -1.0083],\n",
      "         [ 0.3495,  0.1912,  0.1319,  ..., -0.1558, -0.1041, -0.6189]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2600,  0.1077, -0.2822,  ..., -0.1954,  0.4798,  0.1316],\n",
      "         [-0.4633, -0.0849,  0.0577,  ...,  0.0579,  0.0930, -0.2235],\n",
      "         [-0.7405,  0.3685, -0.3097,  ..., -0.1741,  0.2665, -0.4238],\n",
      "         ...,\n",
      "         [ 0.2156,  0.2781, -0.3049,  ..., -0.0505,  0.0756, -0.2842],\n",
      "         [-0.3103, -0.2903, -0.2386,  ..., -0.0352,  0.2171, -0.7161],\n",
      "         [ 0.4449,  0.0146, -0.1221,  ..., -0.0136, -0.3731, -0.4649]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1184, -0.4531, -0.0808,  ...,  0.0954,  0.3152,  0.0457],\n",
      "         [-0.4069, -0.0504,  0.0810,  ...,  0.1764,  0.2753, -0.0943],\n",
      "         [-0.4895,  0.0655, -0.0770,  ...,  0.0220,  0.4114, -0.4131],\n",
      "         ...,\n",
      "         [ 0.2754,  0.1508,  0.6088,  ...,  0.2878, -0.1847, -1.2482],\n",
      "         [-0.5638, -0.4128, -0.0301,  ...,  0.4290,  0.3626, -0.6045],\n",
      "         [ 0.5720,  0.0323,  0.2234,  ..., -0.0158, -0.2597, -0.6062]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3035, -0.2179, -0.0901,  ..., -0.0314,  0.3859, -0.0323],\n",
      "         [-0.3727, -0.0207,  0.0177,  ...,  0.3345,  0.3240, -0.2893],\n",
      "         [ 0.2173,  0.2877,  0.0494,  ..., -0.0359,  0.5961,  0.0265],\n",
      "         ...,\n",
      "         [-0.4843, -0.1014, -0.3736,  ..., -0.1719, -0.1396, -1.1519],\n",
      "         [ 1.3418, -0.4339,  0.0921,  ...,  0.1320, -0.0827, -0.0097],\n",
      "         [ 0.8179,  0.2047, -0.2479,  ...,  0.2727, -0.3194, -0.4411]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2665, -0.2035,  0.2520,  ..., -0.0547,  0.3064,  0.1372],\n",
      "         [-0.2570, -0.0679, -0.2259,  ...,  0.2455,  0.1947, -0.3190],\n",
      "         [-0.8172, -0.2676, -0.0813,  ...,  0.1331,  0.2342,  0.4442],\n",
      "         ...,\n",
      "         [-0.8053,  0.0369,  0.9145,  ...,  0.0874, -0.0169,  0.1036],\n",
      "         [-1.5169,  0.3916,  0.2164,  ..., -0.1451,  0.1144, -0.6926],\n",
      "         [ 0.6129,  0.0220, -0.0359,  ...,  0.2593, -0.3800, -0.3757]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2813, -0.1499, -0.1490,  ...,  0.0959,  0.4099,  0.0416],\n",
      "         [-0.0064, -0.0348,  0.0729,  ...,  0.1657,  0.0816, -0.3684],\n",
      "         [-0.2578, -0.5963,  0.2091,  ...,  0.0237,  0.3633, -0.1605],\n",
      "         ...,\n",
      "         [-0.9933, -0.6201,  0.3657,  ..., -0.0067, -0.1871, -0.4479],\n",
      "         [-0.3604, -0.4312,  0.4739,  ...,  0.2515, -0.1553, -0.8738],\n",
      "         [ 0.6595,  0.3157, -0.0044,  ...,  0.2653, -0.3525, -0.3322]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2162, -0.0840, -0.0351,  ...,  0.2051,  0.4535,  0.0301],\n",
      "         [-0.1411,  0.0975, -0.1721,  ...,  0.3914,  0.5466, -0.4774],\n",
      "         [-0.7264, -0.1723,  0.5499,  ...,  0.0605,  0.1612, -0.0440],\n",
      "         ...,\n",
      "         [ 0.3914,  0.3892, -0.0613,  ...,  0.6999, -0.3915, -0.7235],\n",
      "         [-0.2203,  0.0351,  0.1000,  ...,  0.3902, -0.1852, -0.1431],\n",
      "         [ 0.7375,  0.1479,  0.1203,  ...,  0.1513, -0.4341, -0.4570]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2418, -0.2006, -0.1268,  ..., -0.0074,  0.2046,  0.0385],\n",
      "         [-0.6863,  0.1969,  0.1771,  ..., -0.1596,  0.6015, -0.2271],\n",
      "         [-0.4548, -0.3090,  0.2731,  ..., -0.1895,  0.1227, -0.3368],\n",
      "         ...,\n",
      "         [-0.4659, -0.4583,  0.1790,  ...,  0.3277,  0.2480, -0.9694],\n",
      "         [-0.5314, -0.4712,  0.2962,  ...,  0.2155,  0.3985, -0.4597],\n",
      "         [ 0.4838,  0.1036,  0.1759,  ...,  0.1708, -0.1710, -0.7967]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2445,  0.0618,  0.0315,  ...,  0.0787,  0.4030,  0.4921],\n",
      "         [-0.3238,  0.1236,  0.5393,  ...,  0.0397,  0.4355, -0.5474],\n",
      "         [-0.1642, -0.8482,  0.0700,  ...,  0.3957,  0.4124, -0.0504],\n",
      "         ...,\n",
      "         [-0.8914,  0.1488, -0.1090,  ...,  0.4865,  0.4851, -0.4129],\n",
      "         [-0.6902, -0.0808,  0.0023,  ...,  0.4376,  0.2896, -0.3548],\n",
      "         [ 0.5363,  0.2576, -0.1150,  ...,  0.2065, -0.3522, -0.2741]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3307, -0.0917, -0.1055,  ..., -0.0749,  0.3122,  0.2094],\n",
      "         [ 0.3708, -0.0209,  0.5898,  ...,  0.5167,  0.6654, -0.4005],\n",
      "         [-0.9270, -0.6862,  0.2365,  ...,  0.0056,  0.1700, -0.0142],\n",
      "         ...,\n",
      "         [-0.5246,  0.0609,  0.0452,  ...,  0.2852,  0.4383, -0.5121],\n",
      "         [-0.3249, -0.5152, -0.0801,  ...,  0.2688,  0.0021, -0.6197],\n",
      "         [ 0.5984,  0.3363, -0.0192,  ...,  0.1355, -0.3747, -0.2775]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3615, -0.2608,  0.1636,  ..., -0.1306,  0.2255,  0.0663],\n",
      "         [-0.2870,  0.0411,  0.1346,  ...,  0.1997,  0.3492, -0.3974],\n",
      "         [-0.3875,  0.1041, -0.1028,  ..., -0.4335,  0.6858, -0.1784],\n",
      "         ...,\n",
      "         [ 0.0985, -1.0073,  0.2683,  ...,  0.0106, -0.0985, -0.9700],\n",
      "         [-1.5703, -0.0590,  0.3104,  ...,  0.4220, -0.3686, -0.2575],\n",
      "         [ 0.7593, -0.0304, -0.0047,  ...,  0.0560, -0.5056, -0.1468]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3415, -0.0441, -0.4029,  ..., -0.0344,  0.5151, -0.0777],\n",
      "         [-0.5766, -0.1365, -0.1250,  ...,  0.0964,  0.7318, -0.2115],\n",
      "         [-0.5414, -0.2138,  0.1532,  ..., -0.2808,  0.1139,  0.2538],\n",
      "         ...,\n",
      "         [-0.4126,  0.1112, -0.3534,  ...,  0.5304,  0.3011, -0.8053],\n",
      "         [-1.1599,  0.2382, -0.2523,  ...,  0.1979,  0.1516, -0.7804],\n",
      "         [ 0.7741,  0.1282, -0.0999,  ...,  0.1325, -0.3103, -0.4200]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1064,  0.0093, -0.1270,  ..., -0.1786,  0.4073,  0.3395],\n",
      "         [-0.6834, -0.2037,  0.5276,  ...,  0.0385,  0.3659,  0.3341],\n",
      "         [-0.4622, -0.0545,  0.7915,  ...,  0.0980,  0.0779,  0.5536],\n",
      "         ...,\n",
      "         [ 0.1015,  0.3255,  0.5924,  ..., -0.3284,  0.0494, -0.6424],\n",
      "         [-0.9078, -0.0516, -0.1069,  ...,  0.0699, -0.1476, -0.6351],\n",
      "         [ 0.5981, -0.0290,  0.0043,  ...,  0.1634, -0.1534, -0.3160]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '##ogene boundary k – t boundary diamond lane in united states and canada diamond lane', 'score': tensor([0.6526], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6526], device='cuda:0')\n",
      "pred answer_text: ogene boundary k – t boundary diamond lane in united states and canada diamond lane\n",
      "sequence_output  tensor([[[-0.2468, -0.1190, -0.0787,  ..., -0.1093,  0.4344, -0.0488],\n",
      "         [-0.6368, -0.0727, -0.3113,  ...,  0.2978,  0.9916, -0.5856],\n",
      "         [-0.9117,  0.1305,  0.3562,  ..., -0.2309,  0.6592, -0.0533],\n",
      "         ...,\n",
      "         [-0.2118,  0.2779,  0.0808,  ..., -0.1035, -0.2629, -0.8213],\n",
      "         [-0.0872, -0.3319,  0.1439,  ..., -0.3620,  0.0706, -1.1696],\n",
      "         [ 0.6953,  0.2422, -0.0993,  ...,  0.0787, -0.4527, -0.4359]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '1926 is american writer lyricist actor', 'score': tensor([0.6203], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6203], device='cuda:0')\n",
      "pred answer_text: 1926 is american writer lyricist actor\n",
      "sequence_output  tensor([[[-0.2079, -0.0399, -0.2143,  ..., -0.1024,  0.2773, -0.0247],\n",
      "         [-0.1227,  0.2896, -0.2875,  ...,  0.3405,  0.9712, -0.1302],\n",
      "         [-1.0036,  0.4999,  0.1589,  ...,  0.0119,  0.6894,  0.8593],\n",
      "         ...,\n",
      "         [ 0.5893,  0.1303, -0.1828,  ...,  0.0594,  0.0966, -0.3796],\n",
      "         [ 0.2715, -0.1230,  0.0330,  ...,  0.3239,  0.2088, -0.6562],\n",
      "         [ 0.5011,  0.1897, -0.0182,  ...,  0.0303, -0.4554, -0.4419]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '2008 canadian drama film directed by atom egoyan', 'score': tensor([0.6420], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6420], device='cuda:0')\n",
      "pred answer_text: 2008 canadian drama film directed by atom egoyan\n",
      "sequence_output  tensor([[[-1.7790e-01, -1.2435e-01, -2.1398e-01,  ..., -4.0458e-02,\n",
      "           4.8618e-01,  1.3999e-01],\n",
      "         [ 1.8102e-01,  3.8561e-01,  4.0891e-01,  ..., -2.1891e-01,\n",
      "           4.1560e-01,  3.1131e-01],\n",
      "         [-3.9651e-01, -1.5135e-01,  2.1480e-01,  ..., -1.6862e-01,\n",
      "           2.4700e-01, -6.8733e-04],\n",
      "         ...,\n",
      "         [-3.8079e-01,  2.3434e-02,  4.1568e-01,  ...,  1.2712e-01,\n",
      "           6.0524e-01, -5.6342e-01],\n",
      "         [-8.3487e-01,  1.1984e-01,  4.8061e-02,  ...,  2.6786e-01,\n",
      "           2.8824e-01, -7.6290e-01],\n",
      "         [ 5.9937e-01,  1.4694e-01,  6.3311e-02,  ...,  1.0894e-01,\n",
      "          -2.4393e-01, -4.2598e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': 'universe and charts first contact between humanity and alien species title of novel is wordplay on biblical mote and beam para', 'score': tensor([0.6308], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6308], device='cuda:0')\n",
      "pred answer_text: universe and charts first contact between humanity and alien species title of novel is wordplay on biblical mote and beam para\n",
      "sequence_output  tensor([[[-0.2573, -0.0905, -0.2728,  ...,  0.0260,  0.4769,  0.0766],\n",
      "         [-0.2290, -0.0239, -0.1533,  ...,  0.3577,  0.4979, -0.2919],\n",
      "         [-0.6164,  0.0051,  0.1141,  ...,  0.2511,  0.3884,  0.4796],\n",
      "         ...,\n",
      "         [-0.9591, -0.2440, -0.0441,  ...,  0.0415,  0.4341, -0.5445],\n",
      "         [ 0.0870, -0.4990, -0.0609,  ...,  0.1968,  0.0540, -0.7356],\n",
      "         [ 0.5632,  0.0404, -0.1966,  ..., -0.0043, -0.3313, -0.5012]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '2016 ncaa division i fcs football season lumberjacks were led by third year head coach clint conque', 'score': tensor([0.6265], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6265], device='cuda:0')\n",
      "pred answer_text: 2016 ncaa division i fcs football season lumberjacks were led by third year head coach clint conque\n",
      "sequence_output  tensor([[[-0.4485, -0.2682, -0.1297,  ..., -0.4228,  0.4985,  0.0807],\n",
      "         [-0.8812, -0.2882, -0.1751,  ..., -0.0217,  0.5620, -0.5008],\n",
      "         [-0.7217, -0.3563, -0.0231,  ..., -0.2727,  0.3970, -0.1131],\n",
      "         ...,\n",
      "         [-0.4944,  0.0306,  0.2481,  ..., -0.2832,  0.2875, -0.6530],\n",
      "         [ 0.1795, -0.3932,  0.4570,  ..., -0.1288,  0.2011,  0.0515],\n",
      "         [ 0.4945,  0.1068, -0.0081,  ..., -0.1551, -0.1801, -0.5041]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'of pomerania his seal combined coats of arms of norway center as inescutcheon upon cross over all denmark in dexter chief sweden folkung lion in dexter', 'score': tensor([0.6277], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6277], device='cuda:0')\n",
      "pred answer_text: of pomerania his seal combined coats of arms of norway center as inescutcheon upon cross over all denmark in dexter chief sweden folkung lion in dexter\n",
      "sequence_output  tensor([[[-0.1425, -0.0733, -0.2847,  ...,  0.1624,  0.5089, -0.0124],\n",
      "         [ 0.0034,  0.0276,  0.0470,  ...,  0.0635,  0.7394,  0.2118],\n",
      "         [-0.1742, -0.1156,  0.2119,  ..., -0.1346,  0.5283,  0.0555],\n",
      "         ...,\n",
      "         [ 0.1869,  0.2625,  0.0394,  ...,  0.1340, -0.0209,  0.1206],\n",
      "         [ 0.0664, -0.4452,  0.3408,  ...,  0.2576, -0.1624, -0.2923],\n",
      "         [ 0.4829,  0.0660, -0.1469,  ...,  0.2372, -0.4321, -0.5829]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '##s band is instantly recognizable both because of their unique unpredictable style and because of vocalist ottesens distinctive jæren dialect shared by four', 'score': tensor([0.6368], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6368], device='cuda:0')\n",
      "pred answer_text: s band is instantly recognizable both because of their unique unpredictable style and because of vocalist ottesens distinctive jæren dialect shared by four\n",
      "sequence_output  tensor([[[-0.4558, -0.3165,  0.1787,  ..., -0.1688,  0.3407,  0.2496],\n",
      "         [-0.2118,  0.2886,  0.0256,  ...,  0.2511,  0.3374, -0.1822],\n",
      "         [-0.7178, -0.0598,  0.7359,  ..., -0.2995, -0.3523, -0.1742],\n",
      "         ...,\n",
      "         [ 0.2795,  0.2029,  0.5247,  ..., -0.1048, -0.2302, -0.1993],\n",
      "         [ 0.0377,  0.0974,  0.8496,  ..., -0.3543, -0.3279, -0.1584],\n",
      "         [ 0.7552, -0.2991,  0.1025,  ...,  0.1549, -0.4753, -0.2641]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'massa', 'score': tensor([0.6223], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6223], device='cuda:0')\n",
      "pred answer_text: massa\n",
      "sequence_output  tensor([[[-0.3660, -0.2030, -0.0957,  ..., -0.0256,  0.5211,  0.0605],\n",
      "         [-0.0859,  0.2869, -0.0746,  ..., -0.0298,  0.7011,  0.0105],\n",
      "         [-0.6053, -0.0984,  0.2856,  ..., -0.0223, -0.0460, -0.4849],\n",
      "         ...,\n",
      "         [-1.2260,  0.1518,  0.5138,  ..., -0.0868,  0.6533, -1.0170],\n",
      "         [-1.1514, -0.0989, -0.0891,  ..., -0.0027,  0.0131, -0.9334],\n",
      "         [ 0.5889, -0.0330,  0.0101,  ...,  0.0476, -0.2249, -0.4564]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'awarded based', 'score': tensor([0.6261], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6261], device='cuda:0')\n",
      "pred answer_text: awarded based\n",
      "sequence_output  tensor([[[-0.3301, -0.1213, -0.1192,  ..., -0.1095,  0.3286,  0.2253],\n",
      "         [ 0.3292,  0.0339,  0.5135,  ...,  0.6385,  0.7030, -0.4487],\n",
      "         [-0.8595, -0.5710,  0.1642,  ...,  0.0787,  0.2064, -0.0652],\n",
      "         ...,\n",
      "         [-0.5957,  0.0474,  0.0062,  ...,  0.3513,  0.4605, -0.5067],\n",
      "         [-0.2840, -0.3768, -0.1521,  ...,  0.1981,  0.0101, -0.6176],\n",
      "         [ 0.5583,  0.3474, -0.0173,  ...,  0.1433, -0.3657, -0.3260]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': '##rnett written by c graham baker gene towne and oliver hp garrett and starring george raft', 'score': tensor([0.6331], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.6331], device='cuda:0')\n",
      "pred answer_text: rnett written by c graham baker gene towne and oliver hp garrett and starring george raft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.053793103992939 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00001: avg_val_f1 reached 0.05379 (best 0.05379), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer/_ckpt_epoch_1.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(11.6121, device='cuda:0')\tavg_answer_loss:  tensor(5.6370, device='cuda:0')\tavg_type_loss:  tensor(1.1950, device='cuda:0')\tavg_val_f1:  0.053793103992939\tavg_val_em:  0.0\tavg_val_prec:  0.03269230797886848\tavg_val_recall:  0.16666666865348817\n",
      "sequence_output  tensor([[[-0.2837, -0.3624, -0.0693,  ...,  0.2166,  0.3794, -0.0158],\n",
      "         [-0.5876, -0.0808, -0.1747,  ...,  0.3653,  0.9778, -0.4419],\n",
      "         [-0.4010, -0.2829,  0.2468,  ...,  0.0854,  0.1937, -0.4552],\n",
      "         ...,\n",
      "         [ 0.6938,  0.2359,  0.0887,  ...,  0.3146, -0.4342, -0.4103],\n",
      "         [-0.4096, -0.4491,  0.3858,  ...,  0.4593,  0.1678, -0.6420],\n",
      "         [ 0.5455,  0.1785,  0.1600,  ..., -0.0373, -0.2306, -0.4130]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2169, -0.1078,  0.0966,  ...,  0.0054,  0.3884,  0.0034],\n",
      "         [-0.6319, -0.3517,  0.2257,  ...,  0.2312,  0.3874, -1.2658],\n",
      "         [-1.0517, -0.1481,  0.5358,  ...,  0.0647,  0.0194, -0.4517],\n",
      "         ...,\n",
      "         [-0.2252, -0.1858,  0.6615,  ...,  0.0116,  0.5032, -0.4940],\n",
      "         [-0.2684, -0.3997,  0.0691,  ..., -0.0887,  0.0035, -1.1361],\n",
      "         [ 0.7268,  0.1617, -0.0964,  ...,  0.0792, -0.3191, -0.4352]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5590, -0.2621, -0.2456,  ..., -0.0631,  0.6585, -0.0932],\n",
      "         [-0.5549, -0.0695, -0.0562,  ...,  0.3692,  0.6589, -0.1973],\n",
      "         [-0.1942,  0.3845, -0.1697,  ..., -0.1622,  0.5093, -0.0360],\n",
      "         ...,\n",
      "         [ 0.6442,  0.1488,  0.1558,  ...,  0.2699,  0.3415, -0.3890],\n",
      "         [-0.9228, -0.3930, -0.2074,  ...,  0.5870,  0.1639, -0.9088],\n",
      "         [ 0.4385, -0.0843, -0.1362,  ...,  0.0910, -0.1254, -0.4683]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2410, -0.1931, -0.0502,  ...,  0.3236,  0.4919, -0.0605],\n",
      "         [-0.2351, -0.0439, -0.1081,  ...,  0.5176,  0.2714, -0.4756],\n",
      "         [-0.5453, -0.1737,  0.4550,  ...,  0.1861,  0.1837, -0.1384],\n",
      "         ...,\n",
      "         [ 0.7469,  0.3866, -0.1007,  ...,  0.5778, -0.2326, -0.8369],\n",
      "         [-0.1481, -0.1275,  0.2201,  ...,  0.2137, -0.2824, -0.4611],\n",
      "         [ 0.7406,  0.0955,  0.0161,  ...,  0.1330, -0.3778, -0.2960]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4277, -0.1186,  0.0401,  ..., -0.0990,  0.4154,  0.1425],\n",
      "         [-0.7969,  0.0803, -0.1720,  ...,  0.1063,  0.5529, -0.0965],\n",
      "         [-0.7050, -0.3533, -0.1107,  ...,  0.0193,  0.4446,  0.0598],\n",
      "         ...,\n",
      "         [ 0.0014, -0.4915,  0.3401,  ...,  0.2112,  0.2178, -0.9255],\n",
      "         [-1.0054,  0.3648, -0.1683,  ...,  0.0758, -0.2502, -0.4263],\n",
      "         [ 0.6872, -0.0617, -0.1349,  ...,  0.1390, -0.2838, -0.6724]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4565, -0.0025, -0.1844,  ..., -0.0744,  0.3579,  0.1122],\n",
      "         [-0.2094, -0.1853, -0.0987,  ...,  0.1696,  0.2085,  0.0284],\n",
      "         [-0.1452, -0.3740,  0.1451,  ...,  0.0415,  0.4134,  0.7272],\n",
      "         ...,\n",
      "         [-0.2670,  0.6616, -0.0213,  ..., -0.1512,  0.2045, -0.5703],\n",
      "         [-0.7726, -0.6585, -0.4280,  ...,  0.1592,  0.7575, -0.6652],\n",
      "         [ 0.3766,  0.3109, -0.2521,  ...,  0.0111, -0.3725, -0.2583]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.6145e-01, -1.1629e-01, -3.3398e-01,  ...,  1.4256e-01,\n",
      "           6.7700e-01, -2.5478e-02],\n",
      "         [-4.8454e-03, -1.6776e-02, -1.9545e-02,  ..., -5.4304e-02,\n",
      "           7.9485e-01,  6.5083e-02],\n",
      "         [-1.6162e-01, -1.5630e-01,  3.0817e-01,  ..., -5.2457e-02,\n",
      "           4.9634e-01, -4.2971e-02],\n",
      "         ...,\n",
      "         [ 9.4962e-02,  2.1412e-01,  9.0789e-02,  ...,  1.8052e-01,\n",
      "           7.9557e-02, -2.4007e-02],\n",
      "         [ 5.9788e-04, -3.0725e-01,  3.7248e-01,  ...,  4.1093e-01,\n",
      "          -1.5439e-01, -2.0500e-01],\n",
      "         [ 4.9363e-01,  3.0289e-01, -7.4884e-02,  ...,  2.5098e-01,\n",
      "          -3.6267e-01, -5.5561e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1237, -0.1910, -0.1981,  ...,  0.2284,  0.3455, -0.2251],\n",
      "         [-0.3713, -0.1539,  0.1364,  ...,  0.1177,  0.2356, -0.3912],\n",
      "         [-0.2482,  0.1672, -0.0741,  ..., -0.1116,  0.1850, -0.5498],\n",
      "         ...,\n",
      "         [ 0.9659,  0.2724,  0.4537,  ...,  0.4142, -0.0755, -1.2263],\n",
      "         [-0.1883, -0.4019,  0.2159,  ...,  0.2470,  0.0225, -0.5842],\n",
      "         [ 0.6235,  0.0760, -0.1165,  ...,  0.0604, -0.4748, -0.6522]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1894, -0.3305, -0.0520,  ...,  0.1186,  0.3613, -0.4626],\n",
      "         [ 0.5703, -0.2407,  0.3199,  ..., -0.4081,  0.4642,  0.2229],\n",
      "         [-0.4893, -0.6540,  0.9089,  ...,  0.3516,  0.2360,  0.2309],\n",
      "         ...,\n",
      "         [ 0.0873, -0.2474,  0.1533,  ...,  0.2375, -0.0391, -0.1017],\n",
      "         [-0.5783, -0.0860, -0.0520,  ...,  0.4203,  0.2842, -0.8487],\n",
      "         [ 0.7529,  0.0538, -0.1599,  ...,  0.1682, -0.4304, -0.5284]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3178,  0.1163, -0.4344,  ..., -0.0974,  0.6998,  0.0374],\n",
      "         [-0.7375, -0.0312, -0.3463,  ...,  0.2102,  0.9018, -0.3701],\n",
      "         [-0.3376, -0.1145,  0.4673,  ..., -0.0100,  0.5315,  0.1937],\n",
      "         ...,\n",
      "         [ 0.4511,  0.1310,  0.5349,  ..., -0.0134,  0.1993, -0.3424],\n",
      "         [ 0.2893, -0.1193,  0.2752,  ...,  0.0717, -0.0777, -0.2611],\n",
      "         [ 0.6401,  0.4084, -0.0748,  ...,  0.1255, -0.2018, -0.3821]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1318, -0.2275, -0.2955,  ..., -0.0734,  0.5517, -0.0286],\n",
      "         [-0.6345, -0.2350,  0.4248,  ...,  0.2438,  1.1890, -0.4705],\n",
      "         [-0.3581,  0.1461,  0.4348,  ...,  0.0382, -0.0172, -0.7596],\n",
      "         ...,\n",
      "         [ 0.0163, -0.4202,  0.4269,  ..., -0.0510, -0.1469,  0.0783],\n",
      "         [ 0.7816,  0.1876,  0.1910,  ..., -0.3567,  0.1063, -1.2205],\n",
      "         [ 0.6927, -0.0114,  0.0263,  ...,  0.1064, -0.3020, -0.4744]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1647, -0.2525, -0.1060,  ..., -0.0781,  0.5118, -0.1633],\n",
      "         [-0.4161, -0.5043,  0.3194,  ...,  0.4004,  0.9443, -0.2777],\n",
      "         [-0.5034, -0.1940,  0.1633,  ...,  0.4160,  0.2554, -1.4059],\n",
      "         ...,\n",
      "         [ 0.5526, -0.5024,  0.6243,  ..., -0.2908,  0.5826, -0.3663],\n",
      "         [ 0.1118,  0.5774,  0.0617,  ..., -0.0235,  0.2596, -1.0763],\n",
      "         [ 0.3985,  0.2389, -0.0795,  ...,  0.1940, -0.2611, -0.4412]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1700, -0.0497, -0.3622,  ..., -0.1910,  0.8031, -0.0824],\n",
      "         [ 0.1079,  0.7931, -0.0822,  ..., -0.0628,  0.3962, -0.4022],\n",
      "         [-0.8619, -0.0314, -0.2514,  ..., -0.1786,  1.0219, -0.1459],\n",
      "         ...,\n",
      "         [ 0.1606,  0.5614, -0.2864,  ..., -0.2670,  0.0175, -0.6316],\n",
      "         [ 0.2147,  0.1867, -0.1658,  ..., -0.1218,  0.1252, -0.4222],\n",
      "         [ 0.4321, -0.0325, -0.1159,  ...,  0.0367, -0.1519, -0.4770]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0530, -0.5034, -0.2832,  ...,  0.0268,  0.3623, -0.0170],\n",
      "         [-0.2279, -0.2541, -0.0905,  ...,  0.4020,  0.2765, -0.5396],\n",
      "         [-0.7467, -0.0729,  0.0684,  ...,  0.0377,  0.3675, -0.3192],\n",
      "         ...,\n",
      "         [-0.4506,  0.4930,  0.2025,  ...,  0.1256,  0.0410, -0.5833],\n",
      "         [ 0.3546, -0.0762,  0.5670,  ...,  0.2917,  0.0280, -0.8874],\n",
      "         [ 0.1694,  0.2017, -0.0473,  ...,  0.0041, -0.2419, -0.5791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0442, -0.1477, -0.3396,  ..., -0.3852,  0.7741,  0.0500],\n",
      "         [-0.0420,  0.1260, -0.5764,  ...,  0.1158,  0.4560,  0.0287],\n",
      "         [-0.6039,  0.0467, -0.0183,  ..., -0.2442,  0.3236,  0.4920],\n",
      "         ...,\n",
      "         [-1.0780, -0.6502, -0.2291,  ..., -0.2231,  0.4602, -0.6576],\n",
      "         [-0.7560, -0.3621,  0.2612,  ..., -0.0929,  0.3360, -0.7969],\n",
      "         [ 0.6477,  0.0957, -0.0395,  ..., -0.0530, -0.3596, -0.4168]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2001, -0.0380, -0.4927,  ..., -0.1762,  0.5110, -0.0919],\n",
      "         [-0.4943,  0.0801, -0.1113,  ..., -0.0846,  0.4308, -0.2441],\n",
      "         [ 0.1698,  0.7593,  0.2476,  ..., -0.3792,  0.2223, -0.3662],\n",
      "         ...,\n",
      "         [-0.3841, -0.6917,  0.2239,  ..., -0.3156,  0.1164, -0.5354],\n",
      "         [-0.3828, -0.3032,  0.2307,  ..., -0.0902,  0.3716, -0.6769],\n",
      "         [ 0.2972,  0.1918, -0.1968,  ..., -0.2070, -0.3229, -0.5746]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2386, -0.2073, -0.1949,  ..., -0.4988,  0.5538,  0.0210],\n",
      "         [-0.0760,  0.0107, -0.0996,  ..., -0.1565,  0.6373, -0.3595],\n",
      "         [-0.4189, -0.2722,  0.2744,  ..., -0.3861,  0.6711, -0.5908],\n",
      "         ...,\n",
      "         [ 0.1005,  0.2093,  0.4070,  ...,  0.0021, -0.1647, -0.2850],\n",
      "         [ 0.2029,  0.0281, -0.0455,  ...,  0.0620, -0.2173, -0.7121],\n",
      "         [ 0.4640,  0.0682, -0.2059,  ..., -0.1976, -0.2651, -0.5066]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4245, -0.2688, -0.2102,  ..., -0.1275,  0.6182, -0.1963],\n",
      "         [-0.0327,  0.1597,  0.1101,  ..., -0.0646,  0.7679, -0.1974],\n",
      "         [-0.7484, -0.0499,  0.3493,  ..., -0.0517, -0.0947, -0.6272],\n",
      "         ...,\n",
      "         [-1.2509,  0.0951,  0.3063,  ..., -0.1665,  0.4560, -0.9450],\n",
      "         [-1.2778, -0.0992, -0.0611,  ..., -0.1636,  0.4011, -0.9192],\n",
      "         [ 0.4102,  0.0399,  0.1252,  ..., -0.1681, -0.0597, -0.4246]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2147, -0.1279, -0.2541,  ...,  0.0611,  0.3821,  0.0379],\n",
      "         [ 0.0011,  0.0287,  0.1740,  ...,  0.1884,  0.1126, -0.4579],\n",
      "         [-0.1954, -0.4435,  0.1646,  ..., -0.0278,  0.3899, -0.1444],\n",
      "         ...,\n",
      "         [-1.0221, -0.6430,  0.5269,  ...,  0.0381, -0.2203, -0.1459],\n",
      "         [-0.4498, -0.5139,  0.3249,  ...,  0.5483, -0.1044, -0.7440],\n",
      "         [ 0.6207,  0.1816,  0.0992,  ...,  0.0946, -0.3074, -0.3055]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2760, -0.1951, -0.2485,  ..., -0.0508,  0.4064,  0.0326],\n",
      "         [-0.0167, -0.0185, -0.1661,  ...,  0.0958,  0.2714,  0.0526],\n",
      "         [-0.3773, -0.3490, -0.1496,  ..., -0.2781,  0.3608, -0.4331],\n",
      "         ...,\n",
      "         [ 0.2195, -0.1617,  0.0185,  ...,  0.1226,  0.1764, -0.9497],\n",
      "         [-0.1041, -0.1440, -0.0873,  ..., -0.0352,  0.0433, -0.9639],\n",
      "         [ 0.6424,  0.0661, -0.1954,  ...,  0.0339, -0.2906, -0.2427]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2576, -0.1528, -0.3498,  ...,  0.0600,  0.6630, -0.1630],\n",
      "         [-0.2350, -0.0834, -0.3189,  ...,  0.4774,  0.6632, -0.3367],\n",
      "         [-0.8137, -0.0267, -0.0683,  ...,  0.2880,  0.4665,  0.4823],\n",
      "         ...,\n",
      "         [-0.8272, -0.2219, -0.1681,  ..., -0.0198,  0.3962, -0.6088],\n",
      "         [ 0.1830, -0.4288, -0.0014,  ...,  0.1674,  0.0770, -0.8626],\n",
      "         [ 0.2920, -0.0120, -0.1311,  ..., -0.0554, -0.2900, -0.5155]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.3065e-01, -3.2929e-01,  1.1779e-01,  ...,  2.4860e-01,\n",
      "           6.7877e-01,  3.8259e-01],\n",
      "         [-4.7055e-01, -9.7406e-02, -3.8059e-01,  ..., -7.0206e-02,\n",
      "           2.0291e-01, -5.8653e-01],\n",
      "         [-1.7852e-01, -7.0988e-01,  7.5914e-01,  ...,  2.0265e-01,\n",
      "           2.1639e-01,  3.3662e-01],\n",
      "         ...,\n",
      "         [-3.4915e-01, -4.5949e-01,  3.9673e-01,  ..., -8.1040e-02,\n",
      "           1.5418e-01, -2.8075e-01],\n",
      "         [-5.9795e-02, -8.9793e-01,  5.0521e-01,  ...,  3.1164e-01,\n",
      "          -2.1469e-01, -3.4523e-01],\n",
      "         [ 3.8690e-01,  2.8710e-01, -2.6937e-04,  ...,  3.3346e-01,\n",
      "          -3.6151e-01, -2.0595e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1532, -0.0699, -0.1891,  ...,  0.0402,  0.6325, -0.1238],\n",
      "         [-0.3165, -0.1695,  0.0417,  ...,  0.3750,  1.0974, -0.6560],\n",
      "         [-0.3761,  0.3674, -0.1586,  ..., -0.3455,  0.3314, -0.0063],\n",
      "         ...,\n",
      "         [-1.0624,  0.0202,  0.4600,  ..., -0.1910,  0.4903,  0.0172],\n",
      "         [ 0.2323, -0.2290,  0.3201,  ...,  0.1751,  0.2875, -0.4119],\n",
      "         [ 0.7138, -0.0190, -0.2234,  ...,  0.2406, -0.3491, -0.4394]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4733,  0.0153, -0.3280,  ..., -0.0718,  0.5085, -0.0635],\n",
      "         [ 0.3495,  0.1573,  0.2824,  ...,  0.2549,  0.8361, -0.1784],\n",
      "         [-0.4178,  0.1363, -0.1417,  ..., -0.3295,  0.2774, -0.2838],\n",
      "         ...,\n",
      "         [ 0.8173, -0.2473,  0.4917,  ...,  0.2506, -0.0812, -0.6567],\n",
      "         [-0.4662,  0.1268,  0.2902,  ...,  0.0120, -0.0270, -0.8627],\n",
      "         [ 0.6871,  0.1638, -0.0861,  ...,  0.1522, -0.3092, -0.3596]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5091, -0.1268, -0.2120,  ..., -0.1584,  0.6048, -0.1405],\n",
      "         [-0.3127, -0.1595, -0.1706,  ...,  0.1678,  0.6700, -0.5516],\n",
      "         [-0.9211, -0.5247,  0.1143,  ..., -0.0444,  0.0761, -0.2733],\n",
      "         ...,\n",
      "         [ 0.1953, -0.2068,  0.1632,  ..., -0.3837,  0.1464, -0.5296],\n",
      "         [-0.5046,  0.3377,  0.2661,  ...,  0.5113,  0.3123, -1.0075],\n",
      "         [ 0.5324,  0.0415, -0.0528,  ...,  0.0337, -0.2418, -0.5220]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2473, -0.1696, -0.1251,  ..., -0.2321,  0.5188, -0.1802],\n",
      "         [-0.6653, -0.0987, -0.3193,  ...,  0.3256,  1.2688, -0.5578],\n",
      "         [-0.9023,  0.1162,  0.3435,  ..., -0.2885,  0.7091, -0.1786],\n",
      "         ...,\n",
      "         [-0.3493,  0.2851,  0.0842,  ..., -0.1972, -0.2783, -0.9153],\n",
      "         [-0.0800, -0.4530,  0.1469,  ..., -0.4878, -0.0022, -1.2368],\n",
      "         [ 0.6943,  0.1844, -0.0380,  ...,  0.0189, -0.4186, -0.4860]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3757, -0.0759, -0.2362,  ...,  0.0771,  0.6509,  0.0619],\n",
      "         [-0.6375,  0.0581,  0.1322,  ...,  0.0566,  0.7475, -0.1848],\n",
      "         [-1.0764,  0.2975, -0.2980,  ...,  0.2150,  1.0018,  0.0037],\n",
      "         ...,\n",
      "         [-0.6003, -0.1322,  0.0338,  ...,  0.1060,  0.2606, -0.3315],\n",
      "         [-0.0550,  0.0496,  0.2807,  ...,  0.2952,  0.1890, -0.2706],\n",
      "         [ 0.7868,  0.0437, -0.2825,  ...,  0.1418, -0.2177, -0.4692]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0593, -0.1297, -0.1108,  ..., -0.0606,  0.5319, -0.1796],\n",
      "         [-0.5650, -0.0696, -0.2125,  ...,  0.1919,  0.8910, -0.5046],\n",
      "         [-0.0792,  0.1880,  0.2442,  ..., -0.4534,  0.3740, -0.2217],\n",
      "         ...,\n",
      "         [-0.4045,  0.0167, -0.0580,  ...,  0.1423,  0.0642, -0.3963],\n",
      "         [-1.0964, -0.0817, -0.2149,  ...,  0.0718,  0.2895, -1.7778],\n",
      "         [ 0.5176,  0.1477,  0.1670,  ..., -0.0177, -0.1850, -0.4611]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3800, -0.1158, -0.5461,  ...,  0.1843,  0.5079, -0.0670],\n",
      "         [-0.1210,  0.3426,  0.0316,  ...,  0.1563,  0.2396,  0.1124],\n",
      "         [-0.0072, -0.1162,  0.3203,  ..., -0.0239,  0.1774,  0.0323],\n",
      "         ...,\n",
      "         [ 0.7733, -0.0817, -0.3617,  ..., -0.0660, -0.2186, -0.4712],\n",
      "         [ 0.0762,  0.3227, -0.2566,  ...,  0.3163, -0.3494, -0.3946],\n",
      "         [ 0.7005,  0.0184, -0.2245,  ...,  0.2428, -0.3657, -0.4609]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3316, -0.2972, -0.3990,  ...,  0.1094,  0.5441, -0.2347],\n",
      "         [-0.5818, -0.1402, -0.2295,  ...,  0.3487,  0.8452, -0.6451],\n",
      "         [ 0.3699, -0.2122,  0.0898,  ..., -0.0163,  0.1705, -0.2882],\n",
      "         ...,\n",
      "         [ 0.4010,  0.2544, -0.3667,  ...,  0.1672,  0.0798, -0.8741],\n",
      "         [-0.3429, -0.0527, -0.1111,  ..., -0.0647,  0.0245, -0.9447],\n",
      "         [ 0.7144,  0.1363, -0.2179,  ...,  0.1518, -0.3295, -0.4232]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0336, -0.1859, -0.2225,  ..., -0.1475,  0.8238, -0.4665],\n",
      "         [-0.7096,  0.0369, -0.0336,  ...,  0.0890,  0.6494, -0.3157],\n",
      "         [-0.7413, -0.4747,  0.0091,  ..., -0.1113,  0.3704, -0.3928],\n",
      "         ...,\n",
      "         [ 0.5797, -0.2537,  0.3322,  ..., -0.2013, -0.0074, -1.1244],\n",
      "         [-0.4918, -0.3719,  0.2443,  ..., -0.2260,  0.0441, -0.4929],\n",
      "         [ 0.6131, -0.0700, -0.2318,  ...,  0.0723, -0.1422, -0.5417]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2547, -0.0240, -0.1636,  ..., -0.2207,  0.6435, -0.1117],\n",
      "         [-0.8030,  0.1257,  0.1192,  ..., -0.1931,  0.8586, -0.4955],\n",
      "         [-0.5458,  0.1035,  0.3030,  ..., -0.2843,  0.1223, -0.1417],\n",
      "         ...,\n",
      "         [-0.7495, -0.1340,  0.1013,  ...,  0.2417,  0.2733, -0.2585],\n",
      "         [ 0.5210, -0.3324,  0.0697,  ..., -0.2064,  0.1967, -0.6898],\n",
      "         [ 0.7142,  0.0118, -0.0320,  ..., -0.0800, -0.2867, -0.3452]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-5.4118e-01, -3.2532e-01, -5.4430e-02,  ..., -5.4619e-03,\n",
      "           3.1597e-01, -4.0148e-02],\n",
      "         [-9.8158e-01, -4.8720e-02,  3.9041e-01,  ..., -2.8944e-04,\n",
      "           6.7024e-01,  3.2563e-01],\n",
      "         [ 1.5898e-01,  6.5692e-01,  2.9661e-01,  ...,  2.7032e-01,\n",
      "          -5.1283e-02,  3.0834e-01],\n",
      "         ...,\n",
      "         [-5.7987e-01, -2.7851e-01,  3.7632e-01,  ..., -9.5173e-02,\n",
      "          -8.1703e-03, -1.1084e+00],\n",
      "         [-2.6436e-01, -4.6351e-01,  4.9232e-02,  ..., -1.5957e-01,\n",
      "          -1.8621e-01, -1.4296e+00],\n",
      "         [ 5.2505e-01, -8.9491e-02, -1.7906e-01,  ...,  7.1775e-02,\n",
      "          -3.4502e-01, -3.1490e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.4801, -0.2921, -0.3502,  ..., -0.2067,  0.6795, -0.0056],\n",
      "         [-0.6697, -0.7581,  0.0565,  ..., -0.4831,  0.5078, -0.1436],\n",
      "         [-0.6271,  0.1291,  0.4044,  ..., -0.1815,  0.3159,  0.1536],\n",
      "         ...,\n",
      "         [-0.6663,  0.7707,  0.1743,  ..., -0.1044,  0.1037, -0.5690],\n",
      "         [-1.0535, -0.0802, -0.1673,  ...,  0.0829,  0.3395, -0.7136],\n",
      "         [ 0.5742, -0.0290, -0.0570,  ..., -0.1257, -0.1099, -0.4227]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0183, -0.3764, -0.3543,  ...,  0.2391,  0.3997, -0.2361],\n",
      "         [-0.2994, -0.1347,  0.0113,  ...,  0.1718,  0.5077, -0.5976],\n",
      "         [-0.4141, -0.1858,  0.2753,  ...,  0.2104,  0.0455, -1.0052],\n",
      "         ...,\n",
      "         [ 1.0194,  0.3087,  0.4311,  ...,  0.4925,  0.2018, -0.8140],\n",
      "         [-0.6067,  0.3228,  0.1936,  ...,  0.6800,  0.2725, -0.9272],\n",
      "         [ 0.6079, -0.1530,  0.0490,  ...,  0.2196, -0.1953, -0.6058]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4592, -0.2368, -0.1307,  ...,  0.0634,  0.5384,  0.0221],\n",
      "         [ 0.1041,  0.0962,  0.3861,  ...,  0.2426,  0.8040,  0.9022],\n",
      "         [-0.2358,  0.6757,  0.4701,  ...,  0.3998,  0.2134, -0.0683],\n",
      "         ...,\n",
      "         [-0.9064,  0.1930,  0.3391,  ...,  0.5322,  0.1881, -0.4163],\n",
      "         [-0.7807, -0.3483,  0.0849,  ..., -0.1102,  0.2486, -0.3553],\n",
      "         [ 0.8056,  0.0516, -0.2254,  ...,  0.2576, -0.3860, -0.3665]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3908, -0.2058, -0.2368,  ...,  0.1113,  0.7286, -0.2474],\n",
      "         [ 0.2011, -0.3189,  0.0783,  ...,  0.4511,  0.4403, -0.5957],\n",
      "         [ 0.1573,  0.3737, -0.1760,  ...,  0.1058,  0.3446, -0.2899],\n",
      "         ...,\n",
      "         [-0.8841, -0.7578,  0.3481,  ...,  0.1645,  0.1670, -0.0541],\n",
      "         [-0.6140,  0.0397,  0.4517,  ...,  0.0604, -0.1054, -0.7623],\n",
      "         [ 0.3425,  0.1078, -0.0248,  ...,  0.1341, -0.2455, -0.3623]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2516, -0.2860, -0.5827,  ..., -0.3837,  0.7929, -0.0830],\n",
      "         [-0.6446, -0.0841, -0.0302,  ..., -0.3553,  0.6098,  0.3592],\n",
      "         [-0.1672,  0.0372, -0.8563,  ..., -0.2573,  0.1751, -0.4504],\n",
      "         ...,\n",
      "         [-0.3187,  0.1730, -0.8200,  ..., -0.5517,  0.1498, -0.4242],\n",
      "         [-0.6889, -1.0682, -0.4292,  ..., -0.2000,  0.2647, -0.7536],\n",
      "         [ 0.2393, -0.0915,  0.0248,  ..., -0.0683, -0.2786, -0.4223]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3954, -0.1867, -0.1453,  ..., -0.2443,  0.6744,  0.0012],\n",
      "         [-0.4175,  0.2230,  0.2989,  ..., -0.0338,  0.2904, -0.3945],\n",
      "         [-0.3137, -0.2041,  0.6358,  ..., -0.5785, -0.0015, -0.3904],\n",
      "         ...,\n",
      "         [-0.9418, -0.0881, -0.1079,  ..., -0.2797,  0.3776,  0.1303],\n",
      "         [-0.3831, -0.1068, -0.0548,  ..., -0.0823,  0.0315, -0.8581],\n",
      "         [ 0.5397,  0.1890,  0.0215,  ..., -0.0175, -0.3194, -0.3214]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3412, -0.2688, -0.3313,  ..., -0.7453,  0.4465, -0.1981],\n",
      "         [-0.9109, -0.1407, -0.1835,  ..., -0.0718,  0.7250, -0.5463],\n",
      "         [-0.5865, -0.4391, -0.1564,  ..., -0.3629,  0.6266, -0.3890],\n",
      "         ...,\n",
      "         [-0.5753,  0.0986,  0.2454,  ..., -0.1630,  0.3594, -1.0516],\n",
      "         [-0.1484, -0.5179,  0.2233,  ..., -0.2273,  0.3423, -0.3333],\n",
      "         [ 0.5980, -0.0275, -0.0219,  ..., -0.0909, -0.0064, -0.4979]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2423, -0.0840, -0.0767,  ..., -0.0132,  0.4054, -0.1805],\n",
      "         [-0.3017,  0.2388,  0.3345,  ...,  0.3460,  0.7857,  0.1706],\n",
      "         [-0.0363, -0.2159, -0.0312,  ...,  0.0423,  0.0718, -0.3888],\n",
      "         ...,\n",
      "         [-0.1649,  0.5384, -0.1055,  ..., -0.0582, -0.0447, -1.2487],\n",
      "         [-0.9395, -0.4371, -0.1829,  ...,  0.1936,  0.3248, -0.8147],\n",
      "         [ 0.2292,  0.2582, -0.0890,  ...,  0.0171, -0.2089, -0.4432]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.8420, -0.3394,  0.2243,  ..., -0.0750,  0.5230, -0.0380],\n",
      "         [-0.7342, -0.0148, -0.0419,  ...,  0.1548,  0.5592, -0.5389],\n",
      "         [-0.4999, -0.0418, -0.0321,  ...,  0.1365,  0.1323, -0.2620],\n",
      "         ...,\n",
      "         [-0.3923, -0.0902,  1.0102,  ...,  0.2664,  0.3710, -1.0853],\n",
      "         [-0.2465, -0.1000,  1.0157,  ...,  0.4476, -0.1976, -0.7948],\n",
      "         [ 0.5222,  0.2326, -0.1761,  ...,  0.2771, -0.4467, -0.6344]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5076, -0.1913, -0.1917,  ..., -0.4591,  0.6345, -0.2566],\n",
      "         [-0.9280,  0.1199, -0.3914,  ..., -0.3660,  0.8682, -0.2645],\n",
      "         [-0.5893, -0.0556, -0.0795,  ..., -0.3437,  0.3510, -0.4823],\n",
      "         ...,\n",
      "         [ 0.6555,  0.8168,  0.2779,  ..., -0.3689, -0.1390, -0.9034],\n",
      "         [-0.8701, -0.0893, -0.1572,  ..., -0.3335,  0.0191, -0.9759],\n",
      "         [ 0.6855,  0.2387,  0.0034,  ...,  0.0169, -0.2358, -0.4278]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1318, -0.1311, -0.0882,  ..., -0.1608,  0.6348, -0.2876],\n",
      "         [-0.4249, -0.2270,  0.2088,  ...,  0.0927,  0.6361, -0.1769],\n",
      "         [-0.2489,  0.0569,  0.1172,  ..., -0.1098,  0.6998, -0.1423],\n",
      "         ...,\n",
      "         [-0.8595, -0.9591, -0.1795,  ...,  0.0381,  0.7145, -0.4907],\n",
      "         [-0.6622, -0.1005,  0.1350,  ..., -0.1742,  0.4632, -0.4802],\n",
      "         [ 0.8588, -0.0904,  0.0526,  ...,  0.2219, -0.2653, -0.4972]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2052, -0.2967, -0.1718,  ..., -0.2001,  0.3022, -0.2812],\n",
      "         [-0.9332,  0.2348,  0.0413,  ..., -0.0644,  0.6247, -0.4476],\n",
      "         [-0.6936, -0.6392,  0.2694,  ...,  0.0774,  0.3484, -0.3899],\n",
      "         ...,\n",
      "         [-0.3443, -0.5569,  0.1460,  ...,  0.2344,  0.4074, -0.7933],\n",
      "         [-0.9993, -0.6448, -0.0847,  ...,  0.0016,  0.6286, -0.6924],\n",
      "         [ 0.5209, -0.1069,  0.0778,  ...,  0.0925, -0.1885, -0.5116]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2041, -0.0410, -0.4812,  ..., -0.1944,  0.8437, -0.1698],\n",
      "         [-0.4857,  0.0517,  0.1886,  ..., -0.0570,  1.0753, -0.3707],\n",
      "         [-0.6235,  0.2612, -0.7014,  ..., -0.5390,  0.4753,  0.0464],\n",
      "         ...,\n",
      "         [ 0.5648,  0.6507,  0.1416,  ..., -0.6259,  0.2916, -0.1377],\n",
      "         [-0.9353, -0.4027, -0.3506,  ..., -0.0812,  0.2988, -0.6607],\n",
      "         [ 0.2919,  0.0556, -0.0715,  ...,  0.1168, -0.2621, -0.2779]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2826, -0.2457, -0.1501,  ..., -0.2332,  0.2852, -0.4518],\n",
      "         [-0.6139, -0.2460,  0.1666,  ...,  0.0766,  0.3769, -0.2046],\n",
      "         [-0.6887, -0.3671,  0.0400,  ...,  0.2618,  0.0367, -1.0077],\n",
      "         ...,\n",
      "         [-0.3491, -0.6453,  0.5113,  ...,  0.1811, -0.0977, -0.8889],\n",
      "         [-0.5731, -0.6944, -0.0856,  ...,  0.0725,  0.5569, -0.4994],\n",
      "         [ 0.6087, -0.1005, -0.0971,  ..., -0.0146, -0.1966, -0.5212]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1755, -0.2878,  0.2486,  ..., -0.1437,  0.5607, -0.1257],\n",
      "         [-0.2055, -0.2407, -0.3948,  ...,  0.3325,  0.6016, -0.6772],\n",
      "         [-0.9430, -0.1912, -0.0667,  ...,  0.0842,  0.6562, -0.0738],\n",
      "         ...,\n",
      "         [ 0.7760, -0.1824,  1.0414,  ...,  0.2534,  0.5348, -0.6160],\n",
      "         [-0.5269, -0.5609,  0.3158,  ..., -0.1375,  0.0305, -0.4467],\n",
      "         [ 0.6359,  0.0599,  0.0604,  ...,  0.1309, -0.1867, -0.4755]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3071, -0.1427, -0.7167,  ..., -0.0345,  0.6189, -0.5529],\n",
      "         [-0.6714, -0.3263,  0.2444,  ...,  0.0952,  0.1962, -0.1678],\n",
      "         [-0.5949, -0.3341,  0.2372,  ..., -0.2653,  0.1165,  0.3807],\n",
      "         ...,\n",
      "         [-0.7638, -0.0696, -0.1981,  ...,  0.5379,  0.2686, -1.1131],\n",
      "         [-1.3725,  0.0794, -0.3428,  ...,  0.3536,  0.0717, -1.3659],\n",
      "         [ 0.6668, -0.1287, -0.0513,  ...,  0.0245, -0.1659, -0.4280]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2167,  0.0233, -0.4639,  ..., -0.0099,  0.6404, -0.2618],\n",
      "         [-0.6705, -0.1083, -0.3838,  ...,  0.1706,  1.0155, -0.8556],\n",
      "         [-0.8555, -0.0531, -0.1915,  ...,  0.0308,  0.6154, -0.2469],\n",
      "         ...,\n",
      "         [-1.3889, -0.0924, -0.3491,  ..., -0.2663,  0.5805, -0.9259],\n",
      "         [-0.1626, -0.2418, -0.1815,  ...,  0.0973,  0.3548, -1.0541],\n",
      "         [ 0.4197, -0.0451, -0.1781,  ...,  0.0258, -0.2584, -0.5946]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2553, -0.2339, -0.6368,  ..., -0.0844,  0.2985, -0.4107],\n",
      "         [-0.2621,  0.3067, -0.1303,  ...,  0.2533,  0.6893, -0.2929],\n",
      "         [-0.6331,  0.4648, -0.0312,  ..., -0.0826,  0.6322,  0.5871],\n",
      "         ...,\n",
      "         [ 0.6373,  0.1740, -0.1150,  ..., -0.1255,  0.2150, -0.3927],\n",
      "         [ 0.2996, -0.0669,  0.0060,  ...,  0.2817,  0.1157, -0.9645],\n",
      "         [ 0.4641,  0.0669,  0.0287,  ...,  0.0365, -0.2733, -0.3721]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.5542, -0.4414, -0.3683,  ..., -0.1374,  0.8714, -0.4770],\n",
      "         [-0.4237, -0.0932,  0.0655,  ...,  0.1994,  0.3046, -0.3370],\n",
      "         [ 0.1676, -0.1852, -0.0322,  ..., -0.0716,  0.5691, -0.6122],\n",
      "         ...,\n",
      "         [ 0.5705,  0.0261,  0.4062,  ..., -0.3172, -0.0024, -1.3172],\n",
      "         [-0.4551,  0.2150, -0.0222,  ..., -0.4777, -0.0924, -1.6081],\n",
      "         [ 0.5076,  0.0431, -0.0143,  ..., -0.0199, -0.2679, -0.5469]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0223, -0.5824, -0.2243,  ..., -0.0414,  0.3125, -0.3820],\n",
      "         [-0.7627, -0.1550, -0.0220,  ...,  0.1298,  0.5308, -0.2522],\n",
      "         [-0.8869,  0.0895,  0.0184,  ..., -0.0935,  0.6043, -0.7357],\n",
      "         ...,\n",
      "         [ 0.4009,  0.2782,  0.7967,  ...,  0.1182, -0.1210, -1.3474],\n",
      "         [-0.6018, -0.4789, -0.1053,  ..., -0.0054,  0.3262, -0.8641],\n",
      "         [ 0.6538, -0.1493,  0.0367,  ...,  0.0586, -0.1289, -0.4213]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5844, -0.0955, -0.4970,  ..., -0.3028,  0.7273, -0.4639],\n",
      "         [-0.8730, -0.4102,  0.1046,  ..., -0.2480,  0.8163,  0.5501],\n",
      "         [-1.1347, -0.1390, -0.2107,  ..., -0.2036,  0.3157, -0.6207],\n",
      "         ...,\n",
      "         [-0.2909,  0.6939,  0.5945,  ..., -0.0361,  0.3757, -1.3204],\n",
      "         [-0.4045, -0.0768,  0.4277,  ..., -0.1094, -0.0543, -1.5197],\n",
      "         [ 0.7451, -0.0860, -0.2134,  ...,  0.1016, -0.2018, -0.3118]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2910, -0.2423, -0.2280,  ...,  0.0709,  0.4823, -0.1345],\n",
      "         [-0.2381,  0.0623,  0.2271,  ...,  0.2561,  0.1358, -0.3647],\n",
      "         [-0.6553,  0.2106,  0.2332,  ..., -0.2283, -0.0629, -0.8293],\n",
      "         ...,\n",
      "         [-0.4305,  0.3248,  0.0730,  ..., -0.6671,  0.0924, -1.0729],\n",
      "         [-0.7702, -0.3196, -0.0451,  ..., -0.1793, -0.1716, -1.2321],\n",
      "         [ 0.6660, -0.1375, -0.1978,  ...,  0.2639, -0.1350, -0.4185]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0594, -0.4429, -0.3469,  ..., -0.5740,  0.4022, -0.4307],\n",
      "         [ 0.0656,  0.1671,  0.1265,  ...,  0.2846,  0.7943, -1.1685],\n",
      "         [ 0.1034, -0.7662,  0.9702,  ...,  0.2549,  0.7071,  0.0774],\n",
      "         ...,\n",
      "         [ 0.2131,  0.6849,  0.2927,  ..., -0.1177,  0.0742, -1.4511],\n",
      "         [-0.3154,  0.1333, -0.2275,  ...,  0.1856,  0.0192, -1.2699],\n",
      "         [ 0.6178, -0.2091, -0.3142,  ...,  0.0353, -0.2337, -0.4474]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2161, -0.0159, -0.4757,  ..., -0.5319,  0.7098, -0.3903],\n",
      "         [-0.5826, -0.1000,  0.0732,  ...,  0.2007,  0.3447, -0.5581],\n",
      "         [-0.7371,  0.2816, -0.2626,  ..., -0.3707,  0.4211, -0.6504],\n",
      "         ...,\n",
      "         [ 0.3392,  0.0967, -0.0483,  ..., -0.0599,  0.1711, -0.1444],\n",
      "         [-0.5296, -0.3989, -0.2753,  ...,  0.0897,  0.2756, -1.0827],\n",
      "         [ 0.4538, -0.1676, -0.0795,  ..., -0.0071, -0.0645, -0.3671]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0917, -0.2401, -0.1406,  ...,  0.0726,  0.3529, -0.7475],\n",
      "         [-0.3585, -0.3115, -0.3792,  ..., -0.2070,  0.6576, -0.4418],\n",
      "         [-0.5991, -0.2065, -0.6204,  ...,  0.2088,  0.7946, -0.9080],\n",
      "         ...,\n",
      "         [ 0.4339, -0.0215,  0.4623,  ...,  0.1721, -0.0723, -1.0685],\n",
      "         [-0.6159, -0.2954,  0.1045,  ...,  0.0489, -0.0514, -1.2027],\n",
      "         [ 0.5381, -0.1312, -0.0517,  ..., -0.0585, -0.1636, -0.5191]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1189, -0.0578, -0.6253,  ..., -0.6400,  0.7263, -0.7360],\n",
      "         [-1.0396,  0.3108, -0.0582,  ..., -0.1746,  0.7915, -0.2850],\n",
      "         [-0.5158, -0.2347,  0.3453,  ..., -0.2758,  0.2753, -0.5504],\n",
      "         ...,\n",
      "         [-0.1786,  0.2107, -0.0969,  ..., -0.1670,  0.0389, -0.7786],\n",
      "         [-1.2323, -0.0803, -0.3851,  ..., -0.1235,  0.3716, -0.7449],\n",
      "         [ 0.6024,  0.0436, -0.1564,  ...,  0.0574, -0.3316, -0.6276]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3203, -0.0819, -0.3763,  ..., -0.1974,  0.5690, -0.6830],\n",
      "         [-0.5591, -0.2567, -0.1617,  ...,  0.1237,  0.1657, -0.7261],\n",
      "         [ 0.0147, -0.5210,  0.3195,  ...,  0.0211, -0.1090, -0.6670],\n",
      "         ...,\n",
      "         [-0.3307,  0.0031,  0.1818,  ..., -0.2074, -0.5474, -0.5007],\n",
      "         [-0.9744,  0.3775, -0.5431,  ..., -0.1582, -0.2334, -1.7888],\n",
      "         [ 0.6197, -0.1432, -0.0186,  ...,  0.0789,  0.0109, -0.3191]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1603, -0.3940, -0.4782,  ..., -0.3153,  0.5897, -0.5274],\n",
      "         [-0.5301,  0.0876,  0.5117,  ...,  0.2864,  0.1013, -0.1272],\n",
      "         [-0.1545, -0.0590,  0.5150,  ...,  0.1689,  0.1644, -0.8177],\n",
      "         ...,\n",
      "         [-0.5071,  0.0740,  0.1015,  ...,  0.1032,  0.3907, -1.2196],\n",
      "         [-0.2226, -0.6523,  0.3346,  ...,  0.0722, -0.0991, -1.6257],\n",
      "         [ 0.4685, -0.1729,  0.0535,  ...,  0.0408, -0.3341, -0.4791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3346, -0.3974, -0.5393,  ..., -0.2804,  0.4669, -0.2629],\n",
      "         [ 0.5128, -0.0601,  0.3782,  ...,  0.1899,  0.8535, -0.7038],\n",
      "         [-0.9325, -0.5605,  0.1353,  ...,  0.0918,  0.1567, -0.1938],\n",
      "         ...,\n",
      "         [-0.6064,  0.0350, -0.0034,  ...,  0.1645,  0.4629, -0.8777],\n",
      "         [-0.2002, -0.5259, -0.1814,  ..., -0.0174, -0.0296, -0.8875],\n",
      "         [ 0.5030,  0.1123,  0.0408,  ..., -0.0781, -0.1154, -0.3032]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2919, -0.4017, -0.3263,  ..., -0.4007,  0.3921, -0.7363],\n",
      "         [-0.3061, -0.4792,  0.0672,  ...,  0.1615,  0.7534, -0.7262],\n",
      "         [ 0.1030,  0.2902,  0.0757,  ..., -0.4182,  0.6552, -0.3069],\n",
      "         ...,\n",
      "         [-0.9207, -0.5498, -0.5871,  ..., -0.4587, -0.3163, -1.5986],\n",
      "         [ 1.1818, -0.4820,  0.1572,  ..., -0.1706, -0.0067, -0.4411],\n",
      "         [ 0.5060,  0.0492, -0.1164,  ..., -0.0234, -0.1437, -0.4825]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4840, -0.3636, -0.1965,  ..., -0.1861,  0.6265, -0.0995],\n",
      "         [-0.3178, -0.1189,  0.0442,  ...,  0.1969,  0.1183, -0.7904],\n",
      "         [ 0.0435, -0.0361,  0.4042,  ..., -0.2699,  0.0396,  0.1940],\n",
      "         ...,\n",
      "         [-0.1384, -0.5374,  0.3397,  ..., -0.2796,  0.1306, -0.9819],\n",
      "         [-0.4015, -0.2932, -0.0264,  ...,  0.1024, -0.0399, -0.7482],\n",
      "         [ 0.6266, -0.2462, -0.0584,  ...,  0.0657, -0.1485, -0.4553]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 5.1007e-04, -5.1846e-01,  4.2159e-02,  ..., -2.3339e-01,\n",
      "           4.4870e-01, -5.3192e-01],\n",
      "         [-3.6432e-01, -2.8854e-01, -4.1807e-01,  ...,  2.4810e-01,\n",
      "           3.6845e-01, -8.2426e-01],\n",
      "         [-9.8825e-01, -4.4631e-01, -2.6635e-01,  ..., -4.5440e-02,\n",
      "           3.5781e-01,  1.1172e-01],\n",
      "         ...,\n",
      "         [-7.6476e-01,  4.0050e-02,  8.5233e-01,  ..., -3.7277e-01,\n",
      "           8.3140e-03, -6.5432e-01],\n",
      "         [-1.8130e+00, -9.5065e-02, -4.9309e-02,  ..., -2.2116e-01,\n",
      "           2.8318e-03, -1.3082e+00],\n",
      "         [ 4.7306e-01, -7.7220e-02,  1.6059e-01,  ...,  1.2236e-02,\n",
      "           1.3381e-01, -5.6383e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2137, -0.2062, -0.3937,  ..., -0.0088,  0.4051,  0.0721],\n",
      "         [-0.1447, -0.1051,  0.1532,  ...,  0.0890,  0.5726, -0.9262],\n",
      "         [-0.3246, -0.9807,  0.2290,  ...,  0.0943,  0.4977, -0.2888],\n",
      "         ...,\n",
      "         [-0.8282, -0.0316, -0.1837,  ...,  0.3574,  0.3852, -0.9466],\n",
      "         [-1.0849, -0.1621, -0.1340,  ...,  0.2667,  0.1667, -0.4012],\n",
      "         [ 0.5547, -0.0106, -0.0123,  ...,  0.1223, -0.2025, -0.3671]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3017, -0.5260, -0.2793,  ..., -0.3692,  0.5557, -0.2460],\n",
      "         [-0.4330, -0.1176,  0.0942,  ...,  0.2163,  0.1770, -0.4242],\n",
      "         [-0.2893,  0.0043,  0.4350,  ...,  0.0456,  0.0492, -0.6885],\n",
      "         ...,\n",
      "         [-0.8836, -0.1537,  0.5023,  ..., -0.5381,  0.6918, -1.2442],\n",
      "         [-0.4408, -0.4486, -0.0480,  ...,  0.1056,  0.2384, -1.0949],\n",
      "         [ 0.5472,  0.0701, -0.1693,  ..., -0.0498, -0.2147, -0.4633]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.0696, -0.3915, -0.3302,  ..., -0.2922,  0.7909, -0.6345],\n",
      "         [-0.1957, -0.1747, -0.0159,  ..., -0.0985,  0.8854, -0.6955],\n",
      "         [-0.1381, -0.3641, -0.0157,  ..., -0.1503,  0.3680, -0.1356],\n",
      "         ...,\n",
      "         [ 0.0255,  0.4618, -0.1129,  ..., -0.0448, -0.2017, -1.2479],\n",
      "         [-0.6643, -0.6288, -0.0750,  ...,  0.2006, -0.1334, -0.8560],\n",
      "         [ 0.4100, -0.0417, -0.0456,  ..., -0.1969, -0.0693, -0.4915]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2773, -0.4690, -0.3443,  ..., -0.1295,  0.4174, -0.3823],\n",
      "         [ 0.2186,  0.2706, -0.0997,  ...,  0.0933,  0.6220, -0.6535],\n",
      "         [-0.5614, -0.1304,  0.1973,  ...,  0.4575,  0.3768,  0.5665],\n",
      "         ...,\n",
      "         [ 0.7777,  0.7961,  0.3947,  ...,  0.0772, -0.0988, -1.2737],\n",
      "         [-0.1090, -0.2039, -0.3824,  ..., -0.0784,  0.2852, -1.5295],\n",
      "         [ 0.5554,  0.1351, -0.0979,  ...,  0.0122, -0.1867, -0.2667]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2530, -0.7604, -0.0069,  ..., -0.3930,  0.4407, -0.0699],\n",
      "         [-0.1285,  0.2720,  0.5200,  ...,  0.1879,  0.5403, -0.4646],\n",
      "         [-0.5108, -0.2486,  0.6952,  ..., -0.4837, -0.1897, -0.3758],\n",
      "         ...,\n",
      "         [ 0.4973,  0.1514,  0.5305,  ..., -0.2774, -0.1135, -0.4636],\n",
      "         [ 0.0248, -0.0107,  0.8736,  ..., -0.4815, -0.2454, -0.4699],\n",
      "         [ 0.7077, -0.4541,  0.0257,  ...,  0.1627, -0.2224, -0.3086]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0240, -0.1548, -0.1645,  ..., -0.3280,  0.5055, -0.7672],\n",
      "         [ 0.2841,  0.3181,  0.1179,  ..., -0.2962,  0.6577, -0.0932],\n",
      "         [-0.5710, -0.1643,  0.1123,  ..., -0.3663,  0.2991, -0.3277],\n",
      "         ...,\n",
      "         [-0.4235,  0.0708,  0.3447,  ..., -0.0898,  0.4202, -0.8694],\n",
      "         [-1.1277, -0.0703,  0.0861,  ...,  0.0460,  0.0960, -0.9951],\n",
      "         [ 0.4871, -0.0570,  0.0908,  ..., -0.0424, -0.0223, -0.4561]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4531,  0.0271, -0.3710,  ..., -0.2375,  0.6072, -0.6006],\n",
      "         [-0.6891, -0.2766,  0.4094,  ..., -0.2315,  0.7404, -0.1816],\n",
      "         [ 0.3180, -0.2087,  0.0094,  ..., -0.0899,  0.4493,  0.2807],\n",
      "         ...,\n",
      "         [-0.3306,  0.1309,  0.3588,  ...,  0.1430, -0.0673, -0.2896],\n",
      "         [ 0.0247, -0.0072, -0.1630,  ...,  0.1906,  0.0665, -1.2705],\n",
      "         [ 0.6215,  0.0192,  0.0464,  ..., -0.0067, -0.3059, -0.3791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3313, -0.6081, -0.1556,  ..., -0.2296,  0.3191, -0.5159],\n",
      "         [-0.3378, -0.0833,  0.3731,  ..., -0.1390,  1.0968, -0.5858],\n",
      "         [-0.5755,  0.0797, -0.2249,  ..., -0.6937,  0.8431, -0.3548],\n",
      "         ...,\n",
      "         [ 0.3202, -0.8437,  0.3985,  ..., -0.0845, -0.1792, -1.0253],\n",
      "         [-1.7727, -0.2377,  0.0629,  ...,  0.2142, -0.3778, -1.1790],\n",
      "         [ 0.6941, -0.2595, -0.1209,  ...,  0.0117, -0.2062, -0.2585]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2110, -0.5847, -0.4261,  ..., -0.3775,  0.5228, -0.7050],\n",
      "         [-0.4662, -0.1953,  0.1852,  ...,  0.2139,  0.9723, -0.5008],\n",
      "         [-1.1126, -0.2710, -0.0168,  ..., -0.0360,  0.5638, -0.7923],\n",
      "         ...,\n",
      "         [-0.3835, -0.7945,  0.3950,  ..., -0.3321, -0.0718,  0.4284],\n",
      "         [-0.4623, -0.4241, -0.0465,  ...,  0.0957, -0.1597, -1.0837],\n",
      "         [ 0.5874,  0.0398,  0.0497,  ...,  0.0381, -0.0706, -0.3909]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1567,  0.0490, -0.3936,  ..., -0.5210,  0.2769, -0.7788],\n",
      "         [-0.1555,  0.4752,  0.3679,  ..., -0.0671,  0.7344, -0.1188],\n",
      "         [-0.3208,  0.0691,  0.4726,  ...,  0.2912,  0.2790,  0.6561],\n",
      "         ...,\n",
      "         [ 0.4301,  0.6818,  0.6702,  ..., -0.2978, -0.0410, -0.8736],\n",
      "         [-0.0095, -0.3258, -0.0037,  ..., -0.0492,  0.4907, -1.3783],\n",
      "         [ 0.5742,  0.0387,  0.0138,  ...,  0.0953, -0.0475, -0.3505]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3486, -0.5451, -0.4184,  ..., -0.2235,  0.7372, -0.6979],\n",
      "         [-0.8907, -0.4319, -0.3625,  ...,  0.2757,  1.1064, -0.7518],\n",
      "         [-0.8493, -0.0243, -0.0683,  ..., -0.1000,  0.8626, -0.3298],\n",
      "         ...,\n",
      "         [-0.6592, -0.1027, -0.3988,  ..., -0.0689,  0.5295, -0.7560],\n",
      "         [ 0.0860, -0.2415,  0.1292,  ..., -0.3719,  0.0922, -1.0826],\n",
      "         [ 0.6248,  0.0822, -0.0643,  ..., -0.0699, -0.2576, -0.3457]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2218, -0.2545, -0.5039,  ..., -0.8387,  0.6650, -0.8309],\n",
      "         [-0.5574, -0.4688, -0.3094,  ..., -0.3443,  1.3850,  0.0331],\n",
      "         [-0.1761,  0.0186, -0.0145,  ..., -0.4197,  0.1467, -0.3729],\n",
      "         ...,\n",
      "         [ 0.4315,  0.4864,  0.1073,  ..., -0.5013,  0.0528, -0.8167],\n",
      "         [-1.0013, -0.4205, -0.2438,  ..., -0.4812,  0.2857, -1.4773],\n",
      "         [ 0.2940, -0.0226, -0.0360,  ..., -0.2900,  0.1749, -0.5954]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1690,  0.0415, -0.4494,  ..., -0.6753,  0.5337, -0.1119],\n",
      "         [-0.8073, -0.3497,  0.3616,  ..., -0.0640,  0.4246,  0.1216],\n",
      "         [-0.5561, -0.0025,  0.7656,  ..., -0.0215,  0.0930,  0.0853],\n",
      "         ...,\n",
      "         [-0.0055,  0.4287,  0.4731,  ..., -0.6511, -0.1067, -0.6539],\n",
      "         [-0.8920, -0.2723, -0.4162,  ..., -0.1835,  0.0932, -1.1964],\n",
      "         [ 0.4061, -0.0776, -0.0211,  ..., -0.0714,  0.0493, -0.3868]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0457, -0.3240, -0.8904,  ..., -0.4389,  0.6573, -0.5656],\n",
      "         [-0.0922, -0.1101, -0.4911,  ...,  0.3801,  0.7820, -0.5469],\n",
      "         [-1.0215, -0.3796, -0.0406,  ..., -0.2255,  0.7036,  0.1652],\n",
      "         ...,\n",
      "         [-0.2731, -0.5848, -0.3969,  ..., -0.4221,  0.1713, -1.1373],\n",
      "         [-0.0258, -0.1135, -0.3632,  ..., -0.6277, -0.2844, -1.2853],\n",
      "         [ 0.3702, -0.2477, -0.0968,  ..., -0.0569, -0.0800, -0.4219]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.0098, -0.0340, -0.4037,  ..., -0.6050,  0.5388, -0.4687],\n",
      "         [-0.8559, -0.2970,  0.5245,  ..., -0.1114,  0.3721,  0.2542],\n",
      "         [-0.5456, -0.1413,  0.9159,  ..., -0.0021,  0.0602,  0.4592],\n",
      "         ...,\n",
      "         [-0.2220,  0.3392,  0.4244,  ..., -0.7295, -0.0455, -1.0475],\n",
      "         [-1.2006, -0.2564, -0.5077,  ..., -0.2763, -0.0903, -1.0731],\n",
      "         [ 0.3884, -0.1472, -0.0171,  ..., -0.0228,  0.0299, -0.3548]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7564], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7564], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2589, -0.2828, -0.3291,  ..., -0.3856,  0.2978, -0.9599],\n",
      "         [-0.7552, -0.2618, -0.6417,  ...,  0.1036,  1.1627, -1.1163],\n",
      "         [-1.0261,  0.0047,  0.0714,  ..., -0.3654,  0.7630, -0.3495],\n",
      "         ...,\n",
      "         [-0.3759,  0.3281,  0.0216,  ..., -0.2558, -0.1816, -1.1257],\n",
      "         [-0.2538, -0.4450, -0.0257,  ..., -0.5863,  0.0247, -1.6046],\n",
      "         [ 0.4924,  0.0104, -0.0748,  ..., -0.1020, -0.2041, -0.4530]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7846], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7846], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1631, -0.0576, -0.7000,  ..., -0.2686,  0.2943, -0.8231],\n",
      "         [-0.0953,  0.1921, -0.4136,  ...,  0.3658,  1.1475, -0.3825],\n",
      "         [-0.9577,  0.3876,  0.0717,  ..., -0.1285,  0.7205,  0.5892],\n",
      "         ...,\n",
      "         [ 0.7811,  0.1798, -0.2791,  ..., -0.1427,  0.1227, -0.7757],\n",
      "         [ 0.2563, -0.1649, -0.0287,  ...,  0.1646,  0.1329, -1.0573],\n",
      "         [ 0.3957, -0.0061, -0.0154,  ..., -0.0613, -0.2431, -0.4138]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7881], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7881], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1239, -0.1743, -0.4627,  ..., -0.3057,  0.5189, -0.7323],\n",
      "         [ 0.2201,  0.3717,  0.2520,  ..., -0.3391,  0.6215, -0.0454],\n",
      "         [-0.4700, -0.2060, -0.0394,  ..., -0.3259,  0.3878, -0.3196],\n",
      "         ...,\n",
      "         [-0.5158, -0.0520,  0.1544,  ..., -0.1328,  0.5185, -0.9270],\n",
      "         [-1.2013, -0.0394, -0.0706,  ...,  0.0390,  0.2100, -1.2002],\n",
      "         [ 0.4535, -0.0532,  0.0478,  ..., -0.0407, -0.0421, -0.4129]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7905], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7905], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3520, -0.1070, -0.5529,  ..., -0.3116,  0.4779, -1.0375],\n",
      "         [-0.2539, -0.1871, -0.3025,  ...,  0.3497,  0.6030, -0.6136],\n",
      "         [-0.7537, -0.3069, -0.0098,  ...,  0.2814,  0.4827,  0.2662],\n",
      "         ...,\n",
      "         [-1.1148, -0.3891, -0.4318,  ..., -0.1408,  0.4572, -0.8599],\n",
      "         [-0.0894, -0.5478, -0.3413,  ..., -0.0273, -0.1098, -1.1398],\n",
      "         [ 0.4211, -0.1512, -0.0975,  ..., -0.0904, -0.0997, -0.4692]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7916], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7916], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2051, -0.4470, -0.5147,  ..., -0.8589,  0.6120, -0.5836],\n",
      "         [-0.9452, -0.5333, -0.4689,  ..., -0.1599,  0.8196, -1.0036],\n",
      "         [-0.7755, -0.5290, -0.1457,  ..., -0.5318,  0.5092, -0.4695],\n",
      "         ...,\n",
      "         [-0.6858, -0.0314,  0.2183,  ..., -0.4086,  0.3275, -1.2123],\n",
      "         [-0.0817, -0.6152,  0.2961,  ..., -0.2354,  0.1383, -0.5372],\n",
      "         [ 0.4126, -0.0831, -0.0088,  ..., -0.2597,  0.0136, -0.4796]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7678], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7678], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[ 0.0950, -0.2568, -0.7871,  ...,  0.1142,  0.7103, -0.5884],\n",
      "         [-0.0040, -0.2040, -0.0680,  ...,  0.0587,  0.9331, -0.0038],\n",
      "         [-0.1618, -0.2318,  0.1768,  ..., -0.2505,  0.6189, -0.0923],\n",
      "         ...,\n",
      "         [-0.0910,  0.2108, -0.0199,  ..., -0.0623, -0.0572, -0.0256],\n",
      "         [-0.1361, -0.5160,  0.3791,  ...,  0.1261, -0.2548, -0.7384],\n",
      "         [ 0.4111, -0.0961, -0.1213,  ...,  0.0733, -0.2038, -0.5338]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7835], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7835], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2749, -0.7855, -0.0494,  ..., -0.3800,  0.3833, -0.1323],\n",
      "         [-0.1998,  0.2278, -0.1007,  ...,  0.2135,  0.4446, -0.3511],\n",
      "         [-0.7170, -0.1267,  0.6288,  ..., -0.4378, -0.3130, -0.3622],\n",
      "         ...,\n",
      "         [ 0.3059,  0.2622,  0.4413,  ..., -0.2027, -0.1694, -0.4332],\n",
      "         [ 0.1020, -0.0475,  0.7542,  ..., -0.5271, -0.3710, -0.4012],\n",
      "         [ 0.6504, -0.3832,  0.0340,  ...,  0.0992, -0.2219, -0.2834]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7472], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7472], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4915, -0.3817, -0.3918,  ..., -0.2762,  0.5522, -0.8157],\n",
      "         [-0.1098,  0.0746, -0.1638,  ..., -0.0176,  0.9423, -0.3348],\n",
      "         [-0.7043, -0.1772,  0.2329,  ..., -0.1102,  0.0197, -0.7577],\n",
      "         ...,\n",
      "         [-1.2948,  0.0115,  0.2022,  ..., -0.2934,  0.5988, -1.3290],\n",
      "         [-1.4319, -0.4251, -0.3770,  ..., -0.1280,  0.0884, -1.4298],\n",
      "         [ 0.4421, -0.1361,  0.0159,  ..., -0.0595, -0.0213, -0.4525]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7875], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7875], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2713, -0.4636, -0.4746,  ..., -0.4682,  0.2037, -0.4137],\n",
      "         [ 0.3712, -0.0885,  0.5032,  ...,  0.6284,  0.7805, -0.6519],\n",
      "         [-0.9152, -0.6800,  0.0115,  ..., -0.0744,  0.2772, -0.3087],\n",
      "         ...,\n",
      "         [-0.6166, -0.0482, -0.0898,  ...,  0.0618,  0.4067, -0.9564],\n",
      "         [-0.2923, -0.5714, -0.1754,  ...,  0.0328, -0.0759, -0.9942],\n",
      "         [ 0.4318,  0.1133, -0.0376,  ..., -0.0248, -0.1539, -0.3313]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.7793], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.7793], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: avg_val_f1 reached 0.00000 (best 0.05379), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer/_ckpt_epoch_2.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(11.7400, device='cuda:0')\tavg_answer_loss:  tensor(5.2624, device='cuda:0')\tavg_type_loss:  tensor(1.2955, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "sequence_output  tensor([[[-5.2943e-01, -2.2508e-01, -3.0818e-01,  ..., -1.5436e-01,\n",
      "           6.2144e-01, -7.9278e-01],\n",
      "         [-2.4497e-02, -5.6332e-03, -1.2186e-01,  ..., -1.2674e-01,\n",
      "           1.0537e+00, -4.9116e-01],\n",
      "         [-7.0380e-01, -1.5595e-01,  3.0040e-01,  ..., -1.9665e-01,\n",
      "           1.1966e-02, -8.8252e-01],\n",
      "         ...,\n",
      "         [-1.3243e+00, -4.9380e-02, -4.7367e-04,  ..., -3.4511e-01,\n",
      "           5.2995e-01, -1.1659e+00],\n",
      "         [-1.1896e+00, -2.8210e-01, -1.9088e-01,  ..., -2.9462e-01,\n",
      "           1.4714e-01, -1.4338e+00],\n",
      "         [ 4.5689e-01, -1.5369e-01,  3.2684e-02,  ..., -1.7775e-02,\n",
      "           2.0958e-02, -4.3129e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4116, -0.1011, -0.3695,  ..., -0.3008,  0.4873, -1.3312],\n",
      "         [-0.2481, -0.2283, -0.3380,  ..., -0.0185,  0.8578, -0.6068],\n",
      "         [-0.5914, -0.3473, -0.0725,  ...,  0.3668,  0.5412,  0.1451],\n",
      "         ...,\n",
      "         [-1.1720, -0.2009, -0.3550,  ..., -0.2007,  0.3786, -0.7943],\n",
      "         [-0.2537, -0.4636, -0.2865,  ..., -0.0092,  0.1274, -1.1489],\n",
      "         [ 0.3479, -0.1518, -0.0050,  ..., -0.1111, -0.0803, -0.4890]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4882, -0.5870, -0.4926,  ..., -0.0115,  0.4183, -0.4496],\n",
      "         [-0.1468, -0.0088,  0.3777,  ...,  0.2005,  0.7131,  0.8356],\n",
      "         [-0.1979,  0.4793,  0.2249,  ...,  0.3272, -0.0273, -0.4176],\n",
      "         ...,\n",
      "         [-0.8628,  0.0809,  0.2100,  ...,  0.4704,  0.2661, -0.8233],\n",
      "         [-0.8110, -0.4363, -0.0587,  ..., -0.2148,  0.0877, -0.4247],\n",
      "         [ 0.7337, -0.0199, -0.0809,  ...,  0.1666, -0.2140, -0.2990]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0029, -0.0530, -0.2994,  ..., -0.6457,  0.4120, -0.3580],\n",
      "         [-0.9918, -0.4163,  0.7115,  ..., -0.0558,  0.2758,  0.2079],\n",
      "         [-0.6541, -0.3611,  0.8597,  ...,  0.0043, -0.0774,  0.4587],\n",
      "         ...,\n",
      "         [-0.1277,  0.2995,  0.4711,  ..., -0.4511, -0.1665, -0.8700],\n",
      "         [-1.3373, -0.4710, -0.5544,  ..., -0.1490, -0.0954, -1.2571],\n",
      "         [ 0.4230, -0.2199, -0.0915,  ..., -0.2709,  0.1029, -0.3432]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1988, -0.1111, -0.6901,  ..., -0.8823,  0.6842, -0.7928],\n",
      "         [-0.4302, -0.1840, -0.0351,  ...,  0.0633,  0.4356, -0.4276],\n",
      "         [-1.0208,  0.2247, -0.5105,  ..., -0.8393,  0.6240, -0.9047],\n",
      "         ...,\n",
      "         [ 0.0096,  0.1235, -0.1192,  ...,  0.0533,  0.2267, -0.6316],\n",
      "         [-0.8245, -0.3480, -0.3203,  ..., -0.2172,  0.3269, -0.9177],\n",
      "         [ 0.3723, -0.2585, -0.1967,  ..., -0.1321, -0.0328, -0.4170]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6305, -0.2387, -0.4169,  ..., -0.6581,  0.6777, -0.6598],\n",
      "         [-0.9352, -0.5325,  0.3166,  ..., -0.4473,  1.0010,  0.4631],\n",
      "         [-1.1656,  0.0026,  0.0859,  ..., -0.4301,  0.3696, -0.5140],\n",
      "         ...,\n",
      "         [-0.3325,  0.4141,  0.4970,  ..., -0.0926,  0.4529, -1.7165],\n",
      "         [-0.4334, -0.0307,  0.3173,  ..., -0.2256, -0.0856, -1.9261],\n",
      "         [ 0.5874, -0.1215, -0.1457,  ..., -0.0277, -0.1036, -0.3044]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2626, -0.3395, -0.7053,  ..., -0.3988,  0.5626, -0.6315],\n",
      "         [-0.5198, -0.1187, -0.1423,  ..., -0.2600,  0.3448, -0.6219],\n",
      "         [ 0.1847,  0.7787,  0.0634,  ..., -0.4779,  0.1209, -0.5503],\n",
      "         ...,\n",
      "         [-0.3787, -0.6794, -0.1994,  ..., -0.6492, -0.1148, -0.8711],\n",
      "         [-0.6292, -0.5244,  0.0560,  ..., -0.2456,  0.1474, -1.0476],\n",
      "         [ 0.0694, -0.0152,  0.0679,  ..., -0.2240,  0.0852, -0.5061]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 4.4768e-02, -3.8325e-01, -2.0161e-01,  ..., -7.3615e-03,\n",
      "           8.6173e-02, -1.0118e+00],\n",
      "         [ 5.2951e-01, -4.8446e-01,  2.8933e-01,  ..., -4.2312e-01,\n",
      "           5.1419e-01, -7.9597e-02],\n",
      "         [-5.3961e-01, -8.2209e-01,  4.7127e-01,  ...,  2.6909e-01,\n",
      "           2.2430e-01, -1.7608e-01],\n",
      "         ...,\n",
      "         [-7.4809e-02, -7.7853e-02,  3.1575e-01,  ...,  3.9055e-02,\n",
      "          -1.5295e-01, -3.9435e-01],\n",
      "         [-7.7357e-01, -2.3421e-01, -4.6445e-01,  ..., -1.4487e-02,\n",
      "           3.4138e-01, -1.3844e+00],\n",
      "         [ 5.1533e-01, -1.5278e-01,  9.6447e-05,  ..., -8.5140e-02,\n",
      "          -1.1235e-01, -4.3006e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3203, -0.4082, -0.3060,  ..., -0.4757,  0.6976, -0.5366],\n",
      "         [-0.3986,  0.0667,  0.0871,  ..., -0.0662,  0.3913, -0.5729],\n",
      "         [-0.3169, -0.4595,  0.3991,  ..., -0.5902,  0.2047, -0.6395],\n",
      "         ...,\n",
      "         [-1.1138, -0.2860, -0.2641,  ..., -0.2067,  0.3156, -0.1173],\n",
      "         [-0.7187, -0.1004, -0.0077,  ..., -0.3820,  0.0859, -1.2796],\n",
      "         [ 0.4945,  0.0168,  0.0349,  ..., -0.0298, -0.1027, -0.2449]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3873, -0.3133,  0.0293,  ...,  0.0056,  0.1735, -0.2273],\n",
      "         [-0.3203, -0.1041,  0.0324,  ...,  0.1148,  0.5531, -0.8448],\n",
      "         [-0.2911, -0.8443,  0.0685,  ...,  0.1886,  0.3540, -0.1805],\n",
      "         ...,\n",
      "         [-0.8179, -0.0018, -0.3956,  ..., -0.1988,  0.3804, -1.0149],\n",
      "         [-1.2654, -0.2149, -0.3333,  ...,  0.1717,  0.2232, -0.4807],\n",
      "         [ 0.5259, -0.0448, -0.0439,  ...,  0.0732, -0.0765, -0.2887]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0457, -0.6685, -0.5068,  ..., -0.3009,  0.3369, -0.7261],\n",
      "         [-0.1745, -0.2480, -0.2651,  ...,  0.3675,  0.3450, -0.7155],\n",
      "         [-0.7320, -0.0810, -0.0354,  ..., -0.0095,  0.2661, -0.6262],\n",
      "         ...,\n",
      "         [-0.4678,  0.2721,  0.1545,  ...,  0.0791,  0.0363, -0.9776],\n",
      "         [ 0.5066, -0.3363,  0.4050,  ...,  0.0453, -0.1890, -1.3310],\n",
      "         [ 0.4719, -0.1227, -0.1901,  ..., -0.0459, -0.1077, -0.4350]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4570, -0.6803, -0.0460,  ..., -0.2973,  0.2700, -0.0942],\n",
      "         [-0.0430,  0.2755, -0.0781,  ...,  0.2031,  0.4653, -0.3810],\n",
      "         [-0.6447, -0.1902,  0.6739,  ..., -0.3488, -0.1224, -0.2996],\n",
      "         ...,\n",
      "         [ 0.2035,  0.2107,  0.3969,  ..., -0.0064, -0.1759, -0.6197],\n",
      "         [-0.0974, -0.0471,  0.5372,  ..., -0.3659,  0.0425, -0.4476],\n",
      "         [ 0.6695, -0.4063,  0.0676,  ...,  0.0229, -0.2083, -0.2514]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1047, -0.5348, -0.4984,  ..., -0.3862,  0.5695, -0.5586],\n",
      "         [-0.3314, -0.4090,  0.3912,  ...,  0.2339,  0.4031, -0.2876],\n",
      "         [-0.3005, -0.2505,  0.4963,  ...,  0.1034,  0.1584, -0.9479],\n",
      "         ...,\n",
      "         [-1.2335, -0.2049,  0.3570,  ..., -0.6569,  0.5843, -0.9463],\n",
      "         [-0.6657, -0.6527, -0.1322,  ...,  0.0878,  0.2062, -1.4196],\n",
      "         [ 0.5352, -0.0979, -0.0517,  ..., -0.1184, -0.1607, -0.4007]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0741, -0.2837, -0.2985,  ..., -0.0679,  0.3461, -0.4601],\n",
      "         [-0.3020, -0.1893,  0.1758,  ...,  0.1273,  0.5559, -0.5224],\n",
      "         [-0.7213,  0.0891,  0.2821,  ..., -0.3617,  0.1125, -1.0409],\n",
      "         ...,\n",
      "         [-0.7014,  0.3408,  0.1703,  ..., -0.6522,  0.1394, -1.3682],\n",
      "         [-0.6293, -0.3840, -0.2887,  ..., -0.1060, -0.0572, -1.5138],\n",
      "         [ 0.6133, -0.2389, -0.1503,  ...,  0.2103, -0.2124, -0.3720]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2285, -0.6917, -0.3859,  ..., -0.2718,  0.4563, -0.7873],\n",
      "         [-0.7111,  0.2089,  0.0324,  ..., -0.0520,  0.3948, -0.5627],\n",
      "         [-0.5810, -0.5704,  0.1621,  ..., -0.0215,  0.1217, -0.6293],\n",
      "         ...,\n",
      "         [-0.6565, -0.5218, -0.0522,  ...,  0.1101,  0.4969, -1.4182],\n",
      "         [-1.0656, -0.7044, -0.3973,  ...,  0.1822,  0.2746, -0.9762],\n",
      "         [ 0.4785, -0.1762, -0.0038,  ..., -0.0613, -0.0897, -0.4475]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0406, -0.2185, -0.3211,  ...,  0.1919,  0.2748, -0.9292],\n",
      "         [-0.2269, -0.3646, -0.3012,  ...,  0.5682,  0.7088, -0.6151],\n",
      "         [-0.8666, -0.1747,  0.3523,  ...,  0.1155,  0.2415, -0.2999],\n",
      "         ...,\n",
      "         [ 0.9173,  0.4303, -0.1740,  ...,  0.5302, -0.4182, -0.8340],\n",
      "         [-0.2579, -0.1221,  0.0995,  ...,  0.3065, -0.2069, -0.9621],\n",
      "         [ 0.7004, -0.1451, -0.0384,  ...,  0.0960, -0.2581, -0.3505]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4320, -0.3978, -0.6122,  ..., -0.2684,  0.6362, -0.6502],\n",
      "         [-1.0524,  0.0280, -0.3255,  ...,  0.2218,  0.4238, -0.3027],\n",
      "         [-0.8949, -0.5921, -0.1563,  ...,  0.1307,  0.4864, -0.5916],\n",
      "         ...,\n",
      "         [-0.2142, -0.7101,  0.4386,  ...,  0.0843,  0.2998, -1.0601],\n",
      "         [-1.3699,  0.2621, -0.0840,  ..., -0.0646, -0.2313, -0.4574],\n",
      "         [ 0.6443, -0.2179, -0.0772,  ...,  0.0710, -0.2947, -0.6002]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[ 0.1422, -0.5626, -0.5776,  ..., -0.0637,  0.4318, -0.7640],\n",
      "         [-0.6205, -0.3893, -0.1601,  ...,  0.1935,  0.2667, -0.3912],\n",
      "         [-0.9331, -0.0359, -0.1593,  ..., -0.1049,  0.5207, -1.1380],\n",
      "         ...,\n",
      "         [ 0.0830,  0.1139,  0.6141,  ...,  0.1042, -0.1812, -1.6795],\n",
      "         [-0.6280, -0.6617, -0.3802,  ...,  0.4625,  0.2630, -1.1406],\n",
      "         [ 0.3482, -0.2814, -0.1905,  ..., -0.0750, -0.0351, -0.4641]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1772, -0.4733, -0.5537,  ..., -0.9123,  0.4966, -0.6036],\n",
      "         [-0.3572,  0.1091,  0.1022,  ..., -0.2433,  0.4244, -0.4701],\n",
      "         [-0.3306, -0.5033, -0.0254,  ..., -0.3943,  0.6908, -0.7536],\n",
      "         ...,\n",
      "         [ 0.2781,  0.2394,  0.6626,  ..., -0.1082, -0.1941, -0.4655],\n",
      "         [ 0.0148, -0.0907, -0.0635,  ..., -0.2336, -0.2387, -0.7029],\n",
      "         [ 0.4104, -0.1174, -0.0410,  ..., -0.3800, -0.0426, -0.4263]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0289, -0.0796, -0.5729,  ..., -0.0374,  0.3417, -0.7498],\n",
      "         [-0.1487, -0.1042, -0.1625,  ...,  0.2126,  0.6208, -0.7991],\n",
      "         [-0.3444, -0.3794,  0.1216,  ..., -0.0319,  0.4720, -0.6066],\n",
      "         ...,\n",
      "         [-1.4950, -0.8808,  0.2272,  ...,  0.0288, -0.0758, -0.8164],\n",
      "         [-0.7681, -0.4927,  0.1106,  ...,  0.3905, -0.0025, -1.5412],\n",
      "         [ 0.5859, -0.0468,  0.0136,  ...,  0.0664, -0.0733, -0.2868]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2658, -0.4021, -0.9095,  ...,  0.1708,  0.6877, -0.6969],\n",
      "         [-0.1432,  0.0687, -0.0462,  ...,  0.0542,  0.2218,  0.1145],\n",
      "         [-0.0821, -0.2089, -0.1167,  ..., -0.0807,  0.0887,  0.1275],\n",
      "         ...,\n",
      "         [-0.9134, -0.0818,  0.2813,  ...,  0.8452, -0.0978, -1.5177],\n",
      "         [-0.1808,  0.3430, -0.1053,  ...,  0.1977, -0.4838, -0.5087],\n",
      "         [ 0.6914, -0.2208, -0.2956,  ...,  0.1698, -0.1702, -0.3716]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2227, -0.4467, -0.5649,  ..., -0.5472,  0.7342, -0.8933],\n",
      "         [-0.1172, -0.1346, -0.1700,  ..., -0.0244,  0.9724, -0.8083],\n",
      "         [-0.1321, -0.1176,  0.0558,  ..., -0.2167,  0.4151, -0.2836],\n",
      "         ...,\n",
      "         [-0.2955,  0.4228, -0.0804,  ..., -0.2466, -0.1368, -0.9538],\n",
      "         [-0.7341, -0.4753, -0.2534,  ...,  0.2473, -0.1513, -0.9153],\n",
      "         [ 0.6402, -0.2362, -0.1370,  ..., -0.1091,  0.0561, -0.4964]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1577, -0.1888, -0.5259,  ..., -0.0728,  0.8421, -0.5653],\n",
      "         [-0.7433, -0.1492,  0.0157,  ...,  0.2420,  0.8414, -0.4241],\n",
      "         [-1.1919,  0.0861, -0.3399,  ...,  0.1803,  1.0573, -0.2007],\n",
      "         ...,\n",
      "         [-0.7045, -0.2854,  0.0954,  ..., -0.0286,  0.3325, -0.3186],\n",
      "         [ 0.5026, -0.0037,  0.1115,  ...,  0.0379, -0.1234, -0.6914],\n",
      "         [ 0.6691, -0.1990, -0.2586,  ...,  0.0181,  0.0191, -0.4421]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1698, -0.1698, -0.6888,  ..., -0.1674,  0.2404, -0.6801],\n",
      "         [-0.1537,  0.1199, -0.5870,  ...,  0.4859,  1.2967, -0.4174],\n",
      "         [-1.0096,  0.2923,  0.0741,  ...,  0.1037,  0.7942,  0.4814],\n",
      "         ...,\n",
      "         [ 0.7114,  0.2295, -0.4005,  ..., -0.0885,  0.2426, -0.9889],\n",
      "         [ 0.1206, -0.3126, -0.0033,  ...,  0.1954,  0.1404, -1.2615],\n",
      "         [ 0.4779, -0.0095, -0.0322,  ..., -0.0250, -0.2177, -0.4044]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3268, -0.3822, -0.4534,  ..., -0.5153,  0.1935, -0.9317],\n",
      "         [-0.8032, -0.1980, -0.8506,  ...,  0.2053,  1.2030, -1.2112],\n",
      "         [-1.1620,  0.0083,  0.1750,  ..., -0.4373,  0.8162, -0.2716],\n",
      "         ...,\n",
      "         [-0.3845,  0.2774, -0.0459,  ..., -0.1532, -0.1746, -0.9249],\n",
      "         [-0.2981, -0.4159, -0.1807,  ..., -0.6552,  0.0654, -1.7169],\n",
      "         [ 0.4895, -0.0760, -0.2076,  ..., -0.0916, -0.1513, -0.3651]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1416, -0.5830, -0.3189,  ..., -0.8365,  0.6229, -0.4993],\n",
      "         [-1.0564, -0.6099, -0.4733,  ..., -0.1065,  1.0040, -0.6675],\n",
      "         [-0.7138, -0.7398,  0.0164,  ..., -0.3032,  0.6412, -0.6572],\n",
      "         ...,\n",
      "         [-0.7223, -0.0763,  0.2216,  ..., -0.4516,  0.2769, -1.2578],\n",
      "         [-0.1150, -0.6493,  0.0824,  ...,  0.0440,  0.3087, -0.7172],\n",
      "         [ 0.1797, -0.2019, -0.0091,  ..., -0.2291,  0.0292, -0.3388]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-9.7988e-02, -6.9144e-01, -5.2975e-01,  ..., -3.1091e-01,\n",
      "           1.7362e-01, -3.8372e-01],\n",
      "         [ 3.8988e-01,  1.9871e-02,  5.9281e-01,  ...,  4.2981e-01,\n",
      "           8.0222e-01, -6.6316e-01],\n",
      "         [-1.1174e+00, -7.5693e-01, -1.4615e-02,  ..., -5.0494e-02,\n",
      "           2.7765e-01, -1.3600e-01],\n",
      "         ...,\n",
      "         [-7.1532e-01, -3.2302e-02, -2.4766e-01,  ..., -7.2026e-02,\n",
      "           1.9971e-01, -8.0812e-01],\n",
      "         [-4.8131e-01, -7.8810e-01, -1.3482e-01,  ...,  7.2384e-02,\n",
      "          -1.4054e-01, -1.1333e+00],\n",
      "         [ 1.1912e-01,  3.7930e-03, -1.1925e-01,  ...,  5.1727e-04,\n",
      "          -9.3236e-02, -2.5054e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2344, -0.2090, -0.3451,  ..., -0.5362,  0.5690, -0.6974],\n",
      "         [-0.6887, -0.2211,  0.7831,  ...,  0.3495,  0.2242, -0.3151],\n",
      "         [-0.1869, -0.3114,  0.3748,  ..., -0.0316,  0.0407, -1.0114],\n",
      "         ...,\n",
      "         [-0.8162,  0.0531, -0.0550,  ...,  0.3262,  0.3536, -1.1393],\n",
      "         [-0.1456, -0.7028,  0.3117,  ..., -0.0356, -0.0858, -1.6118],\n",
      "         [ 0.3749, -0.2521, -0.0542,  ..., -0.0607, -0.2007, -0.3821]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2692, -0.3075, -0.5879,  ..., -0.2149,  0.3674, -0.9028],\n",
      "         [ 0.2900,  0.0618,  0.0350,  ..., -0.1994,  0.4035, -0.1536],\n",
      "         [-0.5666, -0.2177, -0.0711,  ...,  0.1397,  0.3244, -0.3283],\n",
      "         ...,\n",
      "         [-0.6121, -0.0399,  0.1761,  ..., -0.1563,  0.3096, -1.0762],\n",
      "         [-1.2881, -0.1964, -0.1702,  ...,  0.0319,  0.2751, -1.1767],\n",
      "         [ 0.3684, -0.1512, -0.2783,  ..., -0.1876,  0.2134, -0.7059]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0300, -0.2525, -0.3984,  ..., -0.2901,  0.6517, -0.8039],\n",
      "         [-0.4405,  0.1978,  0.2909,  ...,  0.3640,  0.9284,  0.0545],\n",
      "         [ 0.0958, -0.0192, -0.3390,  ..., -0.1891,  0.3255, -0.4536],\n",
      "         ...,\n",
      "         [-0.6753,  0.6267, -0.4464,  ..., -0.3497,  0.2358, -1.7571],\n",
      "         [-1.3086, -0.5383, -0.5885,  ...,  0.0055,  0.5005, -1.0600],\n",
      "         [ 0.6011, -0.1525, -0.2180,  ...,  0.0042, -0.0725, -0.4123]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3962, -0.3552, -0.3379,  ..., -0.5093,  0.4324, -0.5131],\n",
      "         [-1.0130, -0.0203, -0.2239,  ..., -0.1161,  0.9422, -0.6054],\n",
      "         [-0.5214, -0.1487,  0.2907,  ..., -0.4035,  0.3188, -0.6457],\n",
      "         ...,\n",
      "         [-1.5157, -0.2889, -0.3576,  ...,  0.0553,  0.3302, -0.2884],\n",
      "         [-0.0443, -0.6828,  0.3207,  ..., -0.0917,  0.4021, -0.8620],\n",
      "         [ 0.5504,  0.0333, -0.2413,  ..., -0.0659,  0.0385, -0.3030]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6941, -0.7075, -0.6786,  ..., -0.3061,  0.4596, -0.9450],\n",
      "         [-0.8313, -0.5512, -0.0678,  ...,  0.3625,  1.0405, -0.5498],\n",
      "         [-0.3140,  0.3520, -0.3540,  ..., -0.1831,  0.5385, -0.1436],\n",
      "         ...,\n",
      "         [ 0.4961, -0.1886,  0.1778,  ...,  0.1394,  0.1464, -0.9958],\n",
      "         [-1.1658, -0.7508, -0.2581,  ...,  0.4083,  0.2074, -1.4733],\n",
      "         [ 0.5249, -0.2195, -0.0892,  ...,  0.0480,  0.0200, -0.3791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0799, -0.7251, -0.5102,  ..., -0.4849,  0.2019, -0.6775],\n",
      "         [ 0.2731,  0.1045,  0.0879,  ...,  0.2413,  0.9094, -1.3618],\n",
      "         [ 0.0099, -0.9451,  0.8148,  ...,  0.2363,  0.6109, -0.2030],\n",
      "         ...,\n",
      "         [-0.0175,  0.6835,  0.1473,  ..., -0.2168,  0.0539, -1.3925],\n",
      "         [-0.4880,  0.0724, -0.0991,  ...,  0.0084,  0.0857, -1.4965],\n",
      "         [ 0.5933, -0.2686, -0.2406,  ..., -0.0763, -0.1888, -0.3599]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2142, -0.6965, -0.0949,  ..., -0.6059,  0.7216, -0.8085],\n",
      "         [-0.2220, -0.3633, -0.7225,  ...,  0.4585,  0.7937, -0.4243],\n",
      "         [-0.9758, -0.5188, -0.2075,  ..., -0.0724,  0.5260, -0.0966],\n",
      "         ...,\n",
      "         [ 0.7435, -0.3530,  1.1994,  ...,  0.0751,  0.4984, -0.8827],\n",
      "         [-0.5153, -0.6742,  0.4893,  ..., -0.1764, -0.2428, -0.5366],\n",
      "         [ 0.7317, -0.0581, -0.0269,  ...,  0.1377, -0.1417, -0.4293]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0424, -0.3943, -0.4451,  ..., -0.6030,  0.6537, -0.9998],\n",
      "         [-1.1352,  0.2424, -0.2795,  ..., -0.2147,  0.7642, -0.4021],\n",
      "         [-0.5907, -0.3756, -0.2108,  ..., -0.1587,  0.6065, -0.7640],\n",
      "         ...,\n",
      "         [-0.7262,  0.3727, -0.2908,  ..., -0.2122,  0.0356, -0.8363],\n",
      "         [-1.2732, -0.4161, -0.5987,  ..., -0.2416,  0.2569, -0.9618],\n",
      "         [ 0.5528, -0.1928,  0.0031,  ...,  0.1049, -0.0838, -0.3276]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-5.2468e-01, -6.5067e-01, -2.0432e-01,  ..., -1.3870e-01,\n",
      "           1.5310e-01, -5.5398e-01],\n",
      "         [-1.3213e+00, -4.8161e-02,  2.1044e-01,  ...,  4.0413e-02,\n",
      "           7.6031e-01,  4.6499e-02],\n",
      "         [ 2.0702e-01,  2.9371e-01, -1.0184e-01,  ...,  1.0647e-01,\n",
      "           3.9473e-02,  1.6596e-01],\n",
      "         ...,\n",
      "         [-3.1974e-01, -3.6084e-01,  5.7662e-01,  ..., -3.5105e-01,\n",
      "           3.9200e-04, -1.2906e+00],\n",
      "         [-6.3687e-02, -5.5608e-01, -1.6940e-01,  ..., -3.0884e-01,\n",
      "           8.2705e-02, -1.5289e+00],\n",
      "         [ 3.0549e-01, -1.1716e-01, -2.2851e-01,  ..., -9.7096e-02,\n",
      "          -2.0153e-01, -4.9202e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.9121e-01, -2.7896e-01, -5.4275e-01,  ..., -3.7359e-01,\n",
      "           1.9577e-01, -5.3941e-01],\n",
      "         [-8.4162e-01, -3.7988e-01, -4.1624e-01,  ...,  1.1336e-01,\n",
      "           7.3638e-01, -8.3411e-01],\n",
      "         [-2.4737e-02, -5.9625e-01,  3.7425e-01,  ..., -8.4766e-02,\n",
      "          -2.8066e-01, -9.9857e-01],\n",
      "         ...,\n",
      "         [-4.5954e-01,  7.5968e-02,  1.0812e-01,  ..., -2.6533e-01,\n",
      "          -5.0053e-01, -1.4224e-01],\n",
      "         [-7.6873e-01,  1.6891e-01, -7.8623e-01,  ..., -2.9992e-01,\n",
      "          -3.0859e-01, -1.9757e+00],\n",
      "         [ 6.5024e-01, -2.6946e-01,  1.9017e-03,  ...,  3.0810e-02,\n",
      "           7.4665e-02, -3.1146e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1364, -0.3846, -0.1896,  ..., -0.2203, -0.0109, -0.6207],\n",
      "         [-0.8944, -0.4006, -0.3006,  ...,  0.0369,  0.4214, -1.0961],\n",
      "         [ 0.0139, -0.0735,  0.0425,  ..., -0.6575,  0.3751, -0.7166],\n",
      "         ...,\n",
      "         [-0.8407, -0.2618, -0.1426,  ...,  0.1163, -0.1003, -0.7821],\n",
      "         [-1.0841, -0.2593, -0.6232,  ...,  0.1123,  0.2264, -1.8253],\n",
      "         [ 0.5349, -0.0333, -0.0975,  ...,  0.0082, -0.3072, -0.3149]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3998, -0.6529, -0.4802,  ..., -0.5966,  0.6501, -0.7365],\n",
      "         [-1.2422, -0.2206, -0.8826,  ..., -0.6306,  0.9989, -0.6487],\n",
      "         [-0.7721, -0.3641, -0.1275,  ..., -0.4682,  0.2926, -0.5700],\n",
      "         ...,\n",
      "         [ 0.8389,  1.0397,  0.2529,  ..., -0.4784, -0.2406, -0.9507],\n",
      "         [-1.0219, -0.3206, -0.4992,  ..., -0.2990,  0.0587, -1.1648],\n",
      "         [ 0.7188, -0.2645, -0.0545,  ..., -0.0077, -0.1097, -0.2475]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1560, -0.5811, -0.2703,  ...,  0.2187,  0.1983, -0.7482],\n",
      "         [-0.6262, -0.3202, -0.5385,  ...,  0.4458,  1.1102, -1.0361],\n",
      "         [-0.7021, -0.6020, -0.0075,  ...,  0.1636,  0.4246, -0.7413],\n",
      "         ...,\n",
      "         [ 0.6743,  0.1310, -0.0018,  ...,  0.0239, -0.1893, -0.8561],\n",
      "         [-0.6253, -0.8091,  0.1858,  ...,  0.0995,  0.1147, -0.6725],\n",
      "         [ 0.4960, -0.1887, -0.0671,  ...,  0.0314, -0.0356, -0.3490]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1724, -0.2970, -0.9554,  ..., -0.5824,  0.5845, -0.9174],\n",
      "         [-0.7636, -0.3818, -0.1590,  ...,  0.3402,  0.7239, -0.5352],\n",
      "         [-0.7614, -0.0510,  0.1454,  ..., -0.5049, -0.0426,  0.1983],\n",
      "         ...,\n",
      "         [-1.0841, -0.0393, -0.6574,  ...,  0.2337,  0.0951, -1.3926],\n",
      "         [-1.8294, -0.2261, -0.4648,  ...,  0.2604,  0.0930, -1.3533],\n",
      "         [ 0.5147, -0.2491, -0.1927,  ..., -0.0422,  0.0167, -0.2681]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2562, -0.6291, -0.8083,  ..., -0.2724,  0.8239, -0.8416],\n",
      "         [-0.1989, -0.2370, -0.7153,  ...,  0.5597,  0.8581, -0.8764],\n",
      "         [-1.2170, -0.5501, -0.4766,  ..., -0.2760,  0.7026,  0.1016],\n",
      "         ...,\n",
      "         [-0.2142, -0.2820, -0.3802,  ..., -0.3972,  0.2044, -0.9966],\n",
      "         [-0.2774, -0.2471, -0.3801,  ..., -0.6975, -0.1683, -1.6057],\n",
      "         [ 0.5390, -0.3630, -0.1027,  ...,  0.0949, -0.0765, -0.3571]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1543, -0.6819, -0.5844,  ..., -0.0720,  0.3158, -0.8712],\n",
      "         [-0.6633, -0.1413,  0.0542,  ...,  0.0379,  0.3589, -1.4389],\n",
      "         [-0.9530, -0.0795,  0.3096,  ...,  0.0849, -0.0166, -0.5925],\n",
      "         ...,\n",
      "         [-0.1467, -0.5025, -0.0123,  ..., -0.1713,  0.3542, -0.9928],\n",
      "         [-0.7790, -0.5063, -0.3948,  ..., -0.2472, -0.1747, -2.1127],\n",
      "         [ 0.5213, -0.1123,  0.0464,  ..., -0.0307, -0.2066, -0.3470]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1111, -0.7497, -0.1814,  ..., -0.1969,  0.3179, -0.5372],\n",
      "         [-0.4256, -0.4511, -0.7606,  ...,  0.2695,  0.7470, -0.7296],\n",
      "         [-1.1896, -0.3919, -0.1740,  ..., -0.2583,  0.4294,  0.1023],\n",
      "         ...,\n",
      "         [-0.8049, -0.1211,  0.5375,  ..., -0.4877, -0.0566, -0.7702],\n",
      "         [-1.7023,  0.0556, -0.2841,  ..., -0.3143, -0.0303, -1.5752],\n",
      "         [ 0.2923, -0.1698,  0.0891,  ...,  0.0668, -0.0673, -0.3293]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5548, -0.4186, -0.3315,  ...,  0.0768,  0.6803, -0.7158],\n",
      "         [ 0.1822, -0.3806, -0.1484,  ...,  0.6010,  0.6951, -0.8536],\n",
      "         [ 0.1532,  0.2983, -0.4027,  ..., -0.2354,  0.3188, -0.9123],\n",
      "         ...,\n",
      "         [-0.3136, -0.7671,  0.2383,  ...,  0.4041,  0.1266,  0.0401],\n",
      "         [-1.1901, -0.2499,  0.4962,  ..., -0.1753,  0.1578, -0.6809],\n",
      "         [ 0.6240, -0.1729, -0.1029,  ..., -0.0021, -0.0889, -0.3248]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0137, -0.4610, -0.4279,  ...,  0.2651,  0.1395, -0.8242],\n",
      "         [-0.3096, -0.2280,  0.0554,  ...,  0.4734,  0.2957, -0.5444],\n",
      "         [-0.4581,  0.1799, -0.2267,  ..., -0.3087,  0.3216, -0.7844],\n",
      "         ...,\n",
      "         [ 0.5352,  0.2662,  0.2238,  ...,  0.2805,  0.0970, -1.4574],\n",
      "         [-0.6412, -0.8621,  0.0781,  ...,  0.0536, -0.1454, -0.7902],\n",
      "         [ 0.5652, -0.2628, -0.0460,  ...,  0.0260, -0.1765, -0.4203]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2626, -0.7270, -0.4340,  ...,  0.0737,  0.6135, -0.4256],\n",
      "         [-0.9563, -0.3614, -0.6186,  ...,  0.0705,  1.0602, -0.5864],\n",
      "         [-0.6955,  0.0786, -0.0546,  ...,  0.0316,  0.6192, -0.1386],\n",
      "         ...,\n",
      "         [-0.9557, -0.4578, -0.3625,  ..., -0.0984,  0.8818, -0.6395],\n",
      "         [-0.0737,  0.0041,  0.3145,  ..., -0.5355,  0.1751, -1.3395],\n",
      "         [ 0.7108, -0.1751, -0.1432,  ..., -0.0040, -0.1336, -0.2249]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1857, -0.5805, -0.5114,  ..., -0.2727,  0.4966, -0.5687],\n",
      "         [-0.8335, -0.4670, -0.8053,  ...,  0.3471,  0.9600, -0.8101],\n",
      "         [-1.1541, -1.0058, -0.2977,  ..., -0.0307,  0.2689, -0.7808],\n",
      "         ...,\n",
      "         [-0.3719, -0.5845,  0.1414,  ..., -0.8458, -0.0395, -0.4446],\n",
      "         [-1.2150,  0.0428,  0.2348,  ...,  0.4694,  0.2673, -1.4982],\n",
      "         [ 0.5135, -0.2187, -0.0774,  ...,  0.0826, -0.1322, -0.4259]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5445, -0.2549, -0.7938,  ..., -0.3510,  0.8094, -0.2594],\n",
      "         [-1.1819, -0.4534, -0.2918,  ...,  0.1102,  1.0453, -0.7780],\n",
      "         [-0.4951, -0.3533,  0.5761,  ..., -0.2707,  0.5754,  0.1881],\n",
      "         ...,\n",
      "         [ 0.6073, -0.2588,  0.5241,  ..., -0.2645,  0.0310, -0.5774],\n",
      "         [-0.1911, -0.7101, -0.2384,  ..., -0.3854,  0.2106, -0.7277],\n",
      "         [ 0.4143, -0.2598, -0.3802,  ...,  0.0362, -0.2150, -0.3628]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3203, -0.5206, -0.5250,  ..., -0.3250,  1.0023, -0.5311],\n",
      "         [-0.6327, -0.2078,  0.0251,  ...,  0.0584,  1.1056, -0.8034],\n",
      "         [-0.7304, -0.1026, -0.6996,  ..., -0.6404,  0.4002,  0.1603],\n",
      "         ...,\n",
      "         [ 1.1248,  0.5074,  0.5381,  ..., -0.6624,  0.1950, -0.3866],\n",
      "         [-1.1512, -0.4679, -0.6912,  ..., -0.3062,  0.1109, -0.7244],\n",
      "         [ 0.7579, -0.2553, -0.1760,  ...,  0.0974, -0.0456, -0.2875]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1946, -0.5831, -0.6982,  ...,  0.0164,  0.5896, -0.7729],\n",
      "         [-0.6333, -0.3915, -0.3855,  ...,  0.3058,  1.2401, -0.9386],\n",
      "         [-0.8124,  0.1772, -0.1120,  ..., -0.6765,  0.4486, -0.4420],\n",
      "         ...,\n",
      "         [-1.4674,  0.1325,  0.3696,  ..., -0.5077,  0.3556, -0.2399],\n",
      "         [ 0.1032, -0.6872, -0.1325,  ..., -0.0201, -0.0729, -0.7250],\n",
      "         [ 0.4968, -0.0271, -0.1036,  ...,  0.0788,  0.0066, -0.3284]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0128, -0.5402, -0.6041,  ..., -0.0368,  0.4058, -0.6987],\n",
      "         [-0.7883, -0.4188, -0.3719,  ...,  0.2696,  0.9592, -0.5274],\n",
      "         [ 0.0721,  0.2892, -0.0130,  ..., -0.4992,  0.7725, -0.0205],\n",
      "         ...,\n",
      "         [-1.1044, -0.5521, -0.6410,  ..., -0.3183, -0.5150, -1.2037],\n",
      "         [ 0.8225, -0.6707,  0.0410,  ..., -0.2618, -0.1150, -0.4709],\n",
      "         [ 0.5875, -0.2148, -0.1375,  ..., -0.0606, -0.1663, -0.2932]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.9796, -0.7514, -0.0845,  ..., -0.1026,  0.3850, -0.2785],\n",
      "         [-0.9084, -0.1780, -0.1953,  ...,  0.2676,  1.1542, -0.7960],\n",
      "         [-0.7530, -0.0228,  0.1262,  ...,  0.2452,  0.1599, -0.6747],\n",
      "         ...,\n",
      "         [-0.4300, -0.1893,  0.6701,  ...,  0.0029,  0.3833, -1.3947],\n",
      "         [-0.5695, -0.2855,  0.6267,  ...,  0.1116, -0.4201, -0.8217],\n",
      "         [ 0.5543, -0.2458, -0.1573,  ...,  0.2170, -0.3070, -0.4040]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2719, -0.6900, -0.5264,  ..., -0.0660,  0.4162, -0.5179],\n",
      "         [-0.3717, -0.2129, -0.1256,  ...,  0.4817,  0.9215, -0.3577],\n",
      "         [-1.1334, -0.3430,  0.0028,  ..., -0.0689,  0.1874, -0.6108],\n",
      "         ...,\n",
      "         [-0.7423, -1.0082,  0.2395,  ..., -0.3259, -0.2002,  0.0357],\n",
      "         [-0.9991, -0.3883, -0.3534,  ...,  0.2089, -0.2271, -1.4099],\n",
      "         [ 0.6049, -0.0249,  0.1107,  ...,  0.0704, -0.1893, -0.3142]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2493, -0.5812, -0.9575,  ..., -0.1053, -0.0726, -0.6746],\n",
      "         [-0.3558, -0.5595,  0.2476,  ...,  0.1502,  0.1637, -0.4114],\n",
      "         [-0.8219, -0.4984,  0.0248,  ...,  0.1223,  0.1221, -1.4013],\n",
      "         ...,\n",
      "         [-0.7706, -0.7328,  0.3823,  ..., -0.1126, -0.0014, -0.9623],\n",
      "         [-1.2656, -1.0289, -0.2910,  ..., -0.0808,  0.4903, -0.8779],\n",
      "         [ 0.2185, -0.2964,  0.1893,  ...,  0.0385,  0.1099, -0.3548]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0073, -0.6775, -0.4879,  ...,  0.0582,  0.0047, -0.7972],\n",
      "         [-0.5917, -0.3535, -0.5763,  ...,  0.2099,  0.4383, -0.6730],\n",
      "         [-0.6303, -0.3732, -0.7232,  ...,  0.0745,  0.8911, -0.8349],\n",
      "         ...,\n",
      "         [ 0.0520,  0.0447,  0.2271,  ..., -0.0461, -0.0744, -1.6646],\n",
      "         [-0.7581, -0.2957, -0.0753,  ..., -0.1570, -0.1989, -1.5850],\n",
      "         [ 0.5797, -0.2601, -0.0096,  ...,  0.0895,  0.0086, -0.3883]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3906, -0.8744, -0.6165,  ..., -0.3906,  0.6643, -0.4605],\n",
      "         [-0.7887, -0.6968,  0.1761,  ..., -0.5721,  0.7989, -0.2643],\n",
      "         [-0.5844, -0.1269,  0.2984,  ..., -0.5099,  0.4794, -0.2431],\n",
      "         ...,\n",
      "         [-0.8615,  0.6660,  0.1663,  ..., -0.2814, -0.0022, -0.6929],\n",
      "         [-1.2712, -0.6703, -0.6251,  ..., -0.0430,  0.4036, -1.0121],\n",
      "         [ 0.5180, -0.0874, -0.3844,  ..., -0.0854, -0.0143, -0.3412]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3547, -0.4663, -0.7116,  ..., -0.2225,  0.8747, -0.7108],\n",
      "         [-0.2088,  0.7930, -0.1894,  ..., -0.2474,  0.6245, -0.8590],\n",
      "         [-1.2603, -0.0738, -0.2457,  ..., -0.4263,  1.0643, -0.3211],\n",
      "         ...,\n",
      "         [ 0.0859,  0.2465, -0.4135,  ..., -0.5288,  0.0457, -0.9990],\n",
      "         [-0.0099,  0.3368, -0.5464,  ..., -0.2853, -0.0413, -0.8023],\n",
      "         [ 0.6140, -0.3400, -0.0835,  ...,  0.0626, -0.0940, -0.3632]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3682, -0.6732, -0.1560,  ..., -0.1424,  0.2719, -0.3127],\n",
      "         [-0.5680, -0.2621,  0.1209,  ...,  0.1874,  0.6442, -0.9010],\n",
      "         [-0.4859,  0.3528, -0.1841,  ..., -0.8827,  0.6754, -0.4622],\n",
      "         ...,\n",
      "         [ 0.1634, -1.1591,  0.5503,  ..., -0.4155, -0.3450, -1.1723],\n",
      "         [-1.9541, -0.5788,  0.0367,  ..., -0.0162, -0.0933, -1.2816],\n",
      "         [ 0.6693, -0.2549, -0.0733,  ...,  0.0112, -0.3678, -0.0109]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3729, -0.4191, -0.6109,  ..., -0.0316,  0.5732, -0.8323],\n",
      "         [ 0.1972,  0.2695,  0.2208,  ...,  0.2483,  0.6549, -0.3223],\n",
      "         [-0.7759, -0.2911, -0.2463,  ..., -0.3818,  0.1446, -0.5154],\n",
      "         ...,\n",
      "         [ 0.6010, -0.0104,  0.3955,  ..., -0.0312, -0.1779, -1.2216],\n",
      "         [-1.2079, -0.0178,  0.2412,  ..., -0.3710, -0.2468, -1.0745],\n",
      "         [ 0.5438, -0.3879, -0.1983,  ...,  0.0805, -0.1527, -0.2959]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0483, -0.3700, -0.6608,  ...,  0.0318,  0.6304, -0.5458],\n",
      "         [-0.7757, -0.4576, -0.1327,  ...,  0.3012,  0.4567, -0.3939],\n",
      "         [-0.3136, -0.2354, -0.0808,  ..., -0.3811,  0.6490, -0.2561],\n",
      "         ...,\n",
      "         [-0.7799, -1.3085, -0.6592,  ..., -0.0659,  0.5132, -0.8850],\n",
      "         [-0.9946, -0.1496, -0.3065,  ..., -0.6726,  0.1586, -0.6049],\n",
      "         [ 0.2978, -0.2959, -0.1662,  ...,  0.0612, -0.1622, -0.4200]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5396, -0.5316, -0.5924,  ...,  0.0883,  0.1990, -0.9933],\n",
      "         [-0.8675, -0.5271, -0.4333,  ...,  0.3876,  1.2362, -1.0677],\n",
      "         [ 0.2872, -0.4041,  0.1993,  ..., -0.0827,  0.2299, -0.5136],\n",
      "         ...,\n",
      "         [-0.1054,  0.1280, -0.5618,  ..., -0.2460,  0.0422, -0.9402],\n",
      "         [-0.6196, -0.3044, -0.2976,  ..., -0.1835, -0.0722, -1.5232],\n",
      "         [ 0.6055, -0.0954, -0.1887,  ...,  0.0871, -0.2216, -0.3263]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1953, -0.1499, -0.7292,  ..., -0.0347,  0.7302, -0.3819],\n",
      "         [-0.7762, -0.3803, -0.2536,  ...,  0.4794,  1.2641, -1.2674],\n",
      "         [-0.9278, -0.1373, -0.2785,  ..., -0.0126,  0.6864, -0.3417],\n",
      "         ...,\n",
      "         [-1.6307, -0.2872, -0.5500,  ..., -0.1213,  0.4124, -0.9621],\n",
      "         [-0.6920, -0.4219, -0.3990,  ...,  0.0754,  0.2026, -1.8234],\n",
      "         [ 0.1632, -0.2554, -0.2116,  ...,  0.1165, -0.0962, -0.4388]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0264, -0.5708, -0.7238,  ..., -0.2226,  0.4708, -0.3274],\n",
      "         [-0.9284, -0.5219,  0.4287,  ...,  0.0862,  1.5174, -0.6895],\n",
      "         [-0.6668, -0.0057,  0.1222,  ..., -0.3915, -0.1817, -0.6202],\n",
      "         ...,\n",
      "         [-0.3322, -0.8748,  0.4143,  ..., -0.2165, -0.4376, -0.0822],\n",
      "         [ 0.5696,  0.0094,  0.1381,  ..., -0.5785, -0.0149, -1.7695],\n",
      "         [ 0.5855, -0.2102, -0.0983,  ..., -0.0650, -0.1880, -0.2672]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5100, -0.6147, -0.0756,  ...,  0.0770,  0.1941, -0.2573],\n",
      "         [-0.5456, -0.1014,  0.2310,  ...,  0.3792,  0.2306, -1.0029],\n",
      "         [-0.4274, -0.1677,  0.6938,  ..., -0.2597, -0.2194,  0.2690],\n",
      "         ...,\n",
      "         [-0.1381, -0.8648,  0.3172,  ..., -0.3123,  0.0137, -1.1957],\n",
      "         [-1.0670, -0.6470, -0.5876,  ...,  0.0416, -0.1444, -0.8936],\n",
      "         [ 0.6161, -0.2996, -0.0815,  ...,  0.1005, -0.1667, -0.3510]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.7793, -0.3789, -0.8118,  ..., -0.1807,  0.3408, -0.3215],\n",
      "         [ 0.1204, -0.5034, -0.3338,  ...,  0.4060,  0.6127, -0.1673],\n",
      "         [-0.0169, -0.6043,  0.0631,  ..., -0.1602,  0.2423,  0.5849],\n",
      "         ...,\n",
      "         [-0.6027,  0.2064, -0.1202,  ..., -0.3308,  0.1121, -0.9762],\n",
      "         [-1.1320, -1.0456, -0.9803,  ...,  0.1883,  0.8137, -0.7549],\n",
      "         [ 0.3895,  0.0133, -0.2076,  ..., -0.0440, -0.1769, -0.2042]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1992, -0.3739, -0.5731,  ..., -0.2428,  0.1403, -0.5356],\n",
      "         [-0.3062,  0.3126, -0.0546,  ...,  0.1125,  0.4871, -0.1286],\n",
      "         [-0.1066, -0.2289,  0.1969,  ...,  0.5496,  0.5712,  0.6770],\n",
      "         ...,\n",
      "         [ 0.5288,  0.4926,  0.6238,  ..., -0.5159, -0.1717, -1.1034],\n",
      "         [-0.3906, -0.2238,  0.1552,  ..., -0.1834,  0.3795, -1.4608],\n",
      "         [ 0.7207,  0.0232, -0.2863,  ...,  0.1853, -0.0955, -0.2217]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-6.9562e-01, -7.3697e-01, -4.2433e-01,  ...,  2.6819e-02,\n",
      "           7.1927e-01, -1.0582e+00],\n",
      "         [-3.1918e-01, -4.5563e-01, -1.8510e-01,  ...,  2.4476e-01,\n",
      "           6.3259e-01, -3.9758e-01],\n",
      "         [ 2.6137e-01, -2.6117e-01,  9.6146e-02,  ..., -2.9629e-01,\n",
      "           7.0257e-01, -8.7975e-01],\n",
      "         ...,\n",
      "         [ 5.0545e-01,  2.8600e-02,  2.7573e-01,  ..., -6.4532e-01,\n",
      "          -7.1977e-02, -1.2920e+00],\n",
      "         [-5.2314e-01,  3.7893e-05, -2.0800e-01,  ..., -6.1667e-01,\n",
      "          -4.0885e-01, -1.6809e+00],\n",
      "         [ 5.6675e-01, -2.9803e-01, -1.1788e-01,  ...,  6.4185e-02,\n",
      "          -3.8651e-01, -4.7522e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1198, -0.5407, -0.4741,  ..., -0.6377,  0.7314, -0.7094],\n",
      "         [-0.3979, -0.3640, -0.4732,  ..., -0.1366,  0.8117,  0.2686],\n",
      "         [-0.2026,  0.0817, -0.0700,  ..., -0.2709,  0.1444, -0.3708],\n",
      "         ...,\n",
      "         [ 0.5133,  0.5005,  0.1105,  ..., -0.7813, -0.0267, -0.9201],\n",
      "         [-1.0521, -0.6752, -0.5634,  ..., -0.5329,  0.1831, -1.0086],\n",
      "         [ 0.1650, -0.2336, -0.1495,  ..., -0.1758, -0.0345, -0.4908]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4028, -0.6293, -0.6424,  ..., -0.1629,  0.4866, -0.3734],\n",
      "         [ 0.0982,  0.2652,  0.2077,  ..., -0.0648,  0.2261, -0.9028],\n",
      "         [-0.7375, -0.1634,  0.0816,  ...,  0.4094,  0.2831,  0.4094],\n",
      "         ...,\n",
      "         [ 0.6074,  0.8217,  0.1788,  ..., -0.0192, -0.1327, -1.3577],\n",
      "         [-0.1211, -0.3540, -0.5161,  ..., -0.2127,  0.3605, -1.2852],\n",
      "         [ 0.7157,  0.0847, -0.1746,  ...,  0.1606, -0.2985, -0.1682]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1057, -0.8308, -0.5870,  ...,  0.4355,  0.6921,  0.1036],\n",
      "         [-0.5034, -0.3310, -0.4282,  ..., -0.1044,  0.3773, -0.7884],\n",
      "         [ 0.1988, -0.9189,  0.7185,  ...,  0.2316, -0.0087,  0.3676],\n",
      "         ...,\n",
      "         [-0.7785, -0.4332,  0.5885,  ..., -0.3046,  0.1934, -0.6664],\n",
      "         [-0.1614, -1.0177, -0.1311,  ...,  0.0226, -0.3195, -0.6567],\n",
      "         [ 0.7535,  0.1852, -0.0927,  ...,  0.3938, -0.3546, -0.2137]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0832, -0.7386, -0.7142,  ..., -0.3999,  0.8152, -0.4159],\n",
      "         [-0.3226, -0.3595, -0.9245,  ...,  0.1349,  0.5038, -0.6574],\n",
      "         [-1.0613, -0.5236, -0.1640,  ..., -0.2068,  0.5937,  0.5097],\n",
      "         ...,\n",
      "         [-1.0069, -1.1101, -0.5675,  ..., -0.1638,  0.4497, -0.7772],\n",
      "         [-1.2835, -0.7592,  0.3433,  ..., -0.5291,  0.2490, -1.4941],\n",
      "         [ 0.6792, -0.2042,  0.0304,  ...,  0.0089, -0.1294, -0.2148]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.9183e-01, -8.2496e-01, -7.2937e-01,  ..., -3.8463e-01,\n",
      "           6.7810e-01, -3.5522e-01],\n",
      "         [-9.2564e-01, -5.5219e-01, -7.3350e-02,  ..., -7.2369e-01,\n",
      "           9.2877e-01,  5.1303e-01],\n",
      "         [-4.3023e-01, -1.0842e-01, -7.3205e-01,  ..., -3.7642e-01,\n",
      "           9.5877e-02, -4.0272e-01],\n",
      "         ...,\n",
      "         [-4.3956e-01, -1.7082e-01, -6.0228e-01,  ..., -3.9962e-01,\n",
      "           1.0821e-01, -8.9115e-01],\n",
      "         [-1.0886e+00, -1.3700e+00, -7.5667e-01,  ..., -2.7804e-01,\n",
      "           2.7871e-01, -8.6928e-01],\n",
      "         [ 4.5585e-01, -5.0938e-02,  1.1425e-02,  ..., -2.0797e-04,\n",
      "          -1.3611e-01, -3.1127e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0726, -0.3066, -1.1141,  ...,  0.5741,  0.5706, -0.3043],\n",
      "         [ 0.0069, -0.1941,  0.1320,  ...,  0.0571,  0.6601, -0.0593],\n",
      "         [-0.2449, -0.2751,  0.2284,  ..., -0.2589,  0.4793, -0.0216],\n",
      "         ...,\n",
      "         [-0.1530,  0.2631,  0.0208,  ..., -0.1216,  0.1972, -0.0548],\n",
      "         [-0.1274, -0.6234,  0.4823,  ...,  0.1723, -0.4661, -0.8136],\n",
      "         [ 0.4418, -0.0713, -0.1008,  ...,  0.1281, -0.3249, -0.3567]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.2895, -0.6041, -0.3871,  ...,  0.1712,  0.0513, -0.5360],\n",
      "         [-0.3913, -0.4555, -0.2662,  ...,  0.2133,  0.3965, -0.8421],\n",
      "         [-0.6883, -0.1689,  0.2934,  ...,  0.1135,  0.2037, -1.2579],\n",
      "         ...,\n",
      "         [ 0.8432,  0.2297,  0.3461,  ...,  0.3170,  0.0746, -0.9885],\n",
      "         [-0.9426,  0.0657, -0.0079,  ...,  0.3627, -0.0417, -1.1674],\n",
      "         [ 0.6263, -0.2409, -0.0922,  ...,  0.1241, -0.3522, -0.3260]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2597, -0.1129, -0.7012,  ..., -0.0056,  0.5059, -0.3740],\n",
      "         [-0.7831, -0.4568,  0.4116,  ..., -0.0175,  1.0103, -0.4560],\n",
      "         [ 0.4247, -0.1654, -0.0443,  ..., -0.0643,  0.2880,  0.3691],\n",
      "         ...,\n",
      "         [-0.3584, -0.0279,  0.2322,  ...,  0.2740, -0.0740, -0.2014],\n",
      "         [-0.0454, -0.1826,  0.1577,  ...,  0.3030,  0.0037, -1.3912],\n",
      "         [ 0.6602, -0.0401, -0.0984,  ...,  0.0515, -0.4040, -0.1940]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3008, -0.6382, -0.2551,  ..., -0.2973,  0.5202, -0.8775],\n",
      "         [-0.9095, -0.3120, -0.1645,  ...,  0.0510,  0.6964, -0.2057],\n",
      "         [-0.4425, -0.9447,  0.2364,  ..., -0.0045,  0.6552, -0.7643],\n",
      "         ...,\n",
      "         [ 0.3283, -0.2048,  0.0403,  ..., -0.5705, -0.1476, -1.5371],\n",
      "         [-0.8086, -0.9361, -0.3819,  ..., -0.0343,  0.1602, -0.9123],\n",
      "         [ 0.5721, -0.2635, -0.0527,  ..., -0.0695, -0.1211, -0.4859]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2258, -0.6986, -0.5874,  ..., -0.0219,  0.2097, -0.6886],\n",
      "         [-0.5762, -0.5509,  0.2714,  ...,  0.1520,  0.6507, -0.5435],\n",
      "         [-0.6464,  0.1344,  0.0336,  ...,  0.3387,  0.3298, -1.9063],\n",
      "         ...,\n",
      "         [ 0.4694, -0.4421,  0.6777,  ..., -0.1334,  0.3821, -0.7058],\n",
      "         [ 0.4527,  0.4265, -0.4845,  ...,  0.1003, -0.1402, -1.4040],\n",
      "         [ 0.5699, -0.0170, -0.3023,  ...,  0.0717, -0.1846, -0.3496]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3522, -0.4473, -0.8852,  ..., -0.0182,  0.4528, -0.5212],\n",
      "         [-0.1335, -0.1882, -0.0670,  ...,  0.1871,  0.3038, -0.2293],\n",
      "         [-0.4535, -0.5812, -0.3302,  ..., -0.4067,  0.4384, -0.7849],\n",
      "         ...,\n",
      "         [-0.0512, -0.3520,  0.0087,  ...,  0.2850,  0.3461, -1.3308],\n",
      "         [-0.3504, -0.5823, -0.3243,  ...,  0.0106,  0.0357, -1.1012],\n",
      "         [ 0.6278, -0.0542, -0.0669,  ...,  0.0391, -0.2652, -0.2035]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[ 0.0531, -0.3915, -0.6148,  ..., -0.2553,  0.6133, -0.1590],\n",
      "         [-1.0276, -0.2643,  0.5243,  ..., -0.0814,  0.3341,  0.4026],\n",
      "         [-0.5999, -0.1534,  0.9548,  ...,  0.0349,  0.0717,  0.5674],\n",
      "         ...,\n",
      "         [-0.2684,  0.2853,  0.3515,  ..., -0.6482, -0.1771, -1.0857],\n",
      "         [-1.2990, -0.5197, -0.7140,  ..., -0.1929, -0.1578, -0.9268],\n",
      "         [ 0.5698, -0.2187, -0.0394,  ...,  0.1039, -0.1320, -0.2233]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9412], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9412], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-2.4575e-01, -5.0511e-01, -5.0196e-01,  ..., -2.1182e-01,\n",
      "           2.0391e-01, -8.3011e-01],\n",
      "         [-9.2706e-01, -3.6035e-01, -7.8222e-01,  ...,  3.1468e-01,\n",
      "           1.1649e+00, -1.0162e+00],\n",
      "         [-1.1542e+00, -5.3340e-02,  1.3290e-01,  ..., -2.7716e-01,\n",
      "           6.0405e-01, -1.8927e-01],\n",
      "         ...,\n",
      "         [-4.2138e-01,  2.8460e-01, -1.0318e-01,  ..., -2.3851e-01,\n",
      "          -1.9787e-01, -1.2417e+00],\n",
      "         [-3.4390e-01, -5.4261e-01, -1.8567e-01,  ..., -5.9306e-01,\n",
      "          -3.0671e-02, -1.7155e+00],\n",
      "         [ 6.3920e-01, -1.3261e-03, -1.4564e-01,  ...,  2.6799e-03,\n",
      "          -3.4942e-01, -3.0749e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9396], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9396], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-2.3724e-01, -1.9249e-01, -8.3124e-01,  ..., -1.9365e-01,\n",
      "           1.6067e-01, -6.6149e-01],\n",
      "         [-1.3576e-01,  1.6112e-01, -5.2051e-01,  ...,  4.4533e-01,\n",
      "           1.2036e+00, -4.4094e-01],\n",
      "         [-9.9062e-01,  3.9578e-01,  9.8715e-04,  ..., -1.2260e-01,\n",
      "           6.3797e-01,  5.8964e-01],\n",
      "         ...,\n",
      "         [ 8.1803e-01,  2.3318e-01, -3.2961e-01,  ..., -1.5938e-01,\n",
      "           1.4039e-01, -9.6220e-01],\n",
      "         [ 1.6163e-01, -1.4708e-01, -1.9063e-01,  ...,  1.2675e-01,\n",
      "           1.0084e-01, -1.1326e+00],\n",
      "         [ 5.2268e-01,  3.3299e-02, -5.6267e-02,  ..., -4.9027e-03,\n",
      "          -3.5905e-01, -2.9919e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9377], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9377], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.0410, -0.4494, -0.7787,  ..., -0.1522,  0.4125, -0.4137],\n",
      "         [ 0.2828,  0.4042,  0.2464,  ..., -0.1320,  0.5727,  0.2029],\n",
      "         [-0.5958, -0.3331, -0.0647,  ..., -0.1394,  0.2605, -0.1402],\n",
      "         ...,\n",
      "         [-0.6617, -0.2698,  0.1587,  ...,  0.0429,  0.4039, -0.8090],\n",
      "         [-1.4072, -0.1069, -0.2108,  ...,  0.1582,  0.1267, -1.2169],\n",
      "         [ 0.6255, -0.0595,  0.0153,  ...,  0.0531, -0.2362, -0.2638]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9452], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9452], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3469, -0.4068, -0.9160,  ..., -0.0471,  0.5243, -0.7306],\n",
      "         [-0.3369, -0.2513, -0.4191,  ...,  0.4704,  0.5752, -0.5890],\n",
      "         [-0.8635, -0.3260, -0.0229,  ...,  0.4337,  0.3519,  0.5159],\n",
      "         ...,\n",
      "         [-1.2961, -0.6657, -0.5504,  ...,  0.0245,  0.2809, -0.6115],\n",
      "         [-0.2164, -0.5785, -0.4930,  ..., -0.0185, -0.1276, -1.2010],\n",
      "         [ 0.5673, -0.1546, -0.1952,  ..., -0.0117, -0.2444, -0.3327]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9407], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9407], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1571, -0.8157, -0.6202,  ..., -0.6676,  0.6500, -0.3117],\n",
      "         [-1.0194, -0.6401, -0.5748,  ...,  0.0730,  0.8457, -0.9128],\n",
      "         [-0.8059, -0.6934, -0.0147,  ..., -0.3557,  0.4132, -0.3099],\n",
      "         ...,\n",
      "         [-0.6958, -0.1866,  0.1144,  ..., -0.3339,  0.2281, -1.2050],\n",
      "         [-0.0473, -0.7070,  0.1170,  ..., -0.1275, -0.0437, -0.3798],\n",
      "         [ 0.5585, -0.1365, -0.0262,  ..., -0.1118, -0.1321, -0.3409]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9395], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9395], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[ 0.1306, -0.3727, -1.0900,  ...,  0.3558,  0.6262, -0.2704],\n",
      "         [ 0.0180, -0.1916, -0.1248,  ...,  0.1127,  0.9576,  0.1256],\n",
      "         [-0.2345, -0.2756,  0.1659,  ..., -0.2142,  0.6772, -0.1221],\n",
      "         ...,\n",
      "         [-0.2389,  0.1984, -0.1073,  ..., -0.0867, -0.1346,  0.0138],\n",
      "         [-0.2628, -0.5067,  0.3469,  ...,  0.1154, -0.4052, -0.7411],\n",
      "         [ 0.5199, -0.1185, -0.1745,  ...,  0.1801, -0.3702, -0.3761]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9330], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9330], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2137, -0.8863, -0.2173,  ..., -0.2007,  0.2308,  0.1323],\n",
      "         [-0.2226,  0.2231, -0.1205,  ...,  0.2743,  0.3401, -0.2827],\n",
      "         [-0.8081, -0.2390,  0.6353,  ..., -0.4398, -0.3843, -0.4178],\n",
      "         ...,\n",
      "         [ 0.2997,  0.2852,  0.3999,  ..., -0.1339, -0.2139, -0.4304],\n",
      "         [ 0.1382, -0.1291,  0.7371,  ..., -0.5955, -0.5035, -0.3801],\n",
      "         [ 0.7017, -0.3673,  0.0135,  ...,  0.1759, -0.3384, -0.1873]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9158], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9158], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4980, -0.5556, -0.5615,  ..., -0.1102,  0.4985, -0.4819],\n",
      "         [-0.1091,  0.1254, -0.2342,  ...,  0.0623,  0.9360, -0.1652],\n",
      "         [-0.8461, -0.2062,  0.1632,  ..., -0.0564, -0.0065, -0.8419],\n",
      "         ...,\n",
      "         [-1.4619, -0.1847,  0.0880,  ..., -0.2232,  0.5204, -1.3049],\n",
      "         [-1.6796, -0.6021, -0.5016,  ..., -0.1514, -0.0247, -1.3377],\n",
      "         [ 0.5947, -0.1620, -0.0545,  ...,  0.0160, -0.2018, -0.3177]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9432], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9432], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2382, -0.6971, -0.5430,  ..., -0.2737,  0.1807, -0.2577],\n",
      "         [ 0.3649, -0.1261,  0.4869,  ...,  0.7200,  0.8467, -0.6711],\n",
      "         [-0.9134, -0.8156, -0.0018,  ...,  0.0979,  0.2474, -0.2996],\n",
      "         ...,\n",
      "         [-0.7583, -0.1462, -0.1299,  ...,  0.3017,  0.3144, -0.7960],\n",
      "         [-0.4129, -0.6781, -0.3083,  ...,  0.1586, -0.1157, -1.1322],\n",
      "         [ 0.5426,  0.1022, -0.0889,  ...,  0.0563, -0.2818, -0.2075]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9409], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9409], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: avg_val_f1 reached 0.00000 (best 0.05379), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer/_ckpt_epoch_3.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(10.7698, device='cuda:0')\tavg_answer_loss:  tensor(4.3736, device='cuda:0')\tavg_type_loss:  tensor(1.2792, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "sequence_output  tensor([[[-0.2230, -0.4302, -1.0218,  ...,  0.2894,  0.6808, -0.2896],\n",
      "         [ 0.0950,  0.0918, -0.0577,  ...,  0.4393,  0.1931, -0.0795],\n",
      "         [-0.1210, -0.3915,  0.0580,  ...,  0.1120,  0.1226, -0.1167],\n",
      "         ...,\n",
      "         [ 0.6924, -0.1539, -0.2355,  ..., -0.0300, -0.2195, -0.3335],\n",
      "         [-0.3216,  0.2299, -0.2583,  ...,  0.1876, -0.5170, -0.8177],\n",
      "         [ 0.6157, -0.0867, -0.2327,  ..., -0.0015, -0.2441, -0.3634]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0866, -0.8124, -0.6532,  ..., -0.2416,  0.3207, -0.4812],\n",
      "         [ 0.3352,  0.1333, -0.0107,  ...,  0.3415,  1.0242, -1.0266],\n",
      "         [ 0.1646, -0.8458,  0.8677,  ...,  0.2684,  0.6589,  0.0354],\n",
      "         ...,\n",
      "         [ 0.1914,  0.5308,  0.4231,  ..., -0.1825, -0.0033, -2.1219],\n",
      "         [-0.7304, -0.0362, -0.1194,  ...,  0.0576, -0.1586, -1.5351],\n",
      "         [ 0.7193,  0.0271, -0.2558,  ...,  0.0615, -0.3028, -0.2703]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 2.7353e-01, -1.2159e+00, -4.7745e-01,  ...,  4.6906e-02,\n",
      "          -6.0923e-02, -5.1960e-01],\n",
      "         [-4.6793e-01, -3.0692e-01, -1.0869e-01,  ...,  2.4550e-01,\n",
      "           4.5699e-01, -3.4705e-01],\n",
      "         [-7.6891e-01, -1.4678e-01, -1.7167e-01,  ...,  9.1402e-02,\n",
      "           5.7779e-01, -9.9571e-01],\n",
      "         ...,\n",
      "         [ 3.1378e-01,  1.8244e-01,  7.5181e-01,  ...,  1.7011e-01,\n",
      "          -2.8002e-01, -1.8022e+00],\n",
      "         [-9.1020e-01, -7.2008e-01, -4.8640e-01,  ...,  4.2257e-01,\n",
      "           2.5978e-01, -1.0159e+00],\n",
      "         [ 7.7509e-01, -2.5468e-01, -1.5064e-03,  ...,  7.9503e-02,\n",
      "          -1.6477e-01, -3.0572e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1292, -0.6118, -0.5195,  ...,  0.0257,  0.7194, -0.0926],\n",
      "         [-1.0236, -0.4786, -0.5664,  ...,  0.4437,  1.2700, -0.6056],\n",
      "         [-0.5060, -0.1050, -0.0542,  ...,  0.1855,  0.6007, -0.2550],\n",
      "         ...,\n",
      "         [-0.6672, -0.2697, -0.3698,  ...,  0.1042,  0.4894, -0.5424],\n",
      "         [-0.0184, -0.0887,  0.3045,  ..., -0.4696,  0.2614, -1.0087],\n",
      "         [ 0.7573, -0.0341, -0.1118,  ..., -0.0322, -0.2566, -0.1713]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1088, -0.6995, -0.2282,  ...,  0.3716, -0.0031, -0.2777],\n",
      "         [-0.6909, -0.3422, -0.3271,  ...,  0.5185,  0.5342, -0.8342],\n",
      "         [-0.5498, -0.3678,  0.0189,  ...,  0.2134,  0.0397, -0.5890],\n",
      "         ...,\n",
      "         [ 0.2910, -0.0222, -0.0855,  ..., -0.0630, -0.5291, -1.0632],\n",
      "         [-0.7122, -1.0953,  0.2486,  ...,  0.2585, -0.0493, -0.4269],\n",
      "         [ 0.6034, -0.1072, -0.0018,  ...,  0.1006, -0.1740, -0.2962]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0494, -0.5833, -0.6567,  ..., -0.6383,  0.7590, -0.5603],\n",
      "         [-0.5254, -0.5152, -0.5110,  ..., -0.2247,  1.0542,  0.5544],\n",
      "         [-0.2984,  0.0745, -0.0755,  ..., -0.2898,  0.1939, -0.1585],\n",
      "         ...,\n",
      "         [ 0.5079,  0.4209, -0.0034,  ..., -0.5021, -0.1542, -0.7910],\n",
      "         [-1.2789, -0.7824, -0.6670,  ..., -0.2613,  0.3669, -1.3775],\n",
      "         [ 0.5543, -0.1511, -0.0524,  ..., -0.0651, -0.1815, -0.3971]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1430, -0.8840, -0.2074,  ..., -0.0084,  0.1880,  0.1377],\n",
      "         [-0.1823,  0.2438, -0.5507,  ...,  0.2032,  0.4127, -0.2636],\n",
      "         [-0.7235, -0.3052,  0.6137,  ..., -0.3896, -0.2163, -0.3846],\n",
      "         ...,\n",
      "         [ 0.5922,  0.4456,  0.1723,  ..., -0.1065, -0.1035, -0.4533],\n",
      "         [ 0.2471, -0.0649,  0.5574,  ..., -0.3990, -0.2987, -0.6025],\n",
      "         [ 0.7584, -0.4341,  0.0336,  ...,  0.2631, -0.2767, -0.2244]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2968, -0.8714, -0.6338,  ..., -0.0905, -0.0612, -0.4899],\n",
      "         [-0.6510, -0.4704,  0.4056,  ...,  0.2272,  0.3382, -0.2615],\n",
      "         [-0.6330, -0.4980, -0.0385,  ...,  0.1099,  0.0569, -1.3590],\n",
      "         ...,\n",
      "         [-0.4981, -0.7709,  0.3815,  ...,  0.0160, -0.1074, -0.8265],\n",
      "         [-1.0410, -0.9565, -0.5440,  ...,  0.1654,  0.2527, -0.7861],\n",
      "         [ 0.3334, -0.2264, -0.0067,  ...,  0.0504, -0.0474, -0.3476]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2616, -0.7620, -0.6648,  ...,  0.3921,  0.3387,  0.0886],\n",
      "         [-0.3011, -0.3324,  0.2526,  ...,  0.2003,  0.3327, -0.1269],\n",
      "         [-0.5904, -0.0098,  0.2086,  ..., -0.3150,  0.0746, -0.7555],\n",
      "         ...,\n",
      "         [-0.9920,  0.0781, -0.1778,  ..., -0.3551,  0.0347, -1.1271],\n",
      "         [-1.0578, -0.3964, -0.6967,  ..., -0.2337, -0.2429, -1.1425],\n",
      "         [ 0.7717, -0.3169, -0.3192,  ...,  0.3971, -0.3156, -0.2950]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1253, -0.4681, -1.3131,  ...,  0.4928,  0.6786, -0.1214],\n",
      "         [ 0.0264, -0.2500, -0.0529,  ...,  0.0740,  0.8645,  0.0426],\n",
      "         [-0.1753, -0.3371,  0.1579,  ..., -0.3114,  0.6261, -0.1889],\n",
      "         ...,\n",
      "         [-0.1462,  0.2152, -0.1294,  ..., -0.0235,  0.3144, -0.3076],\n",
      "         [-0.2225, -0.5093,  0.3435,  ...,  0.2671, -0.6364, -0.8249],\n",
      "         [ 0.4605,  0.0718, -0.1446,  ...,  0.0909, -0.4517, -0.2993]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2435, -0.4901, -0.8630,  ..., -0.0339,  0.3126, -0.4841],\n",
      "         [-0.4135, -0.3843,  0.0520,  ...,  0.1942,  0.4459, -0.5471],\n",
      "         [-0.9237,  0.0642, -0.3226,  ..., -0.2077,  0.2623, -0.7660],\n",
      "         ...,\n",
      "         [-0.0626, -0.3538, -0.0323,  ..., -0.3464, -0.1109, -0.3905],\n",
      "         [-0.9992, -0.8019, -0.6096,  ..., -0.1042,  0.1672, -1.1102],\n",
      "         [ 0.1515, -0.3523, -0.0340,  ...,  0.0167, -0.4016, -0.2696]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0047, -0.4706, -0.6704,  ...,  0.2636,  0.3888, -0.3061],\n",
      "         [-0.1767, -0.1864, -0.1604,  ...,  0.5313,  0.6878, -0.6495],\n",
      "         [-0.3586, -0.7124,  0.0453,  ..., -0.0257,  0.5700, -0.7715],\n",
      "         ...,\n",
      "         [-1.0709, -0.9612,  0.3867,  ..., -0.0083, -0.3703, -0.6953],\n",
      "         [-0.5023, -0.3764,  0.1643,  ...,  0.2706, -0.3955, -0.8696],\n",
      "         [ 0.6489, -0.1386,  0.0526,  ...,  0.1119, -0.4045, -0.2808]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2298, -0.7720, -1.0144,  ...,  0.2269,  0.4950, -0.5742],\n",
      "         [-0.7745, -0.6054, -0.1959,  ...,  0.4989,  0.9379, -0.4918],\n",
      "         [ 0.0414,  0.3289, -0.2128,  ..., -0.4820,  0.7137, -0.1918],\n",
      "         ...,\n",
      "         [-0.7585, -0.7482, -0.6663,  ..., -0.1780, -0.4636, -1.4278],\n",
      "         [ 1.0875, -0.5595,  0.0450,  ...,  0.0079, -0.1974, -0.6719],\n",
      "         [ 0.7295, -0.1258, -0.1430,  ...,  0.1058, -0.2771, -0.3119]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 1.5126e-02, -1.0338e+00, -5.5772e-01,  ...,  6.5616e-02,\n",
      "           3.5612e-01, -5.5128e-01],\n",
      "         [-1.0504e+00, -6.6814e-02,  2.9187e-02,  ...,  8.3407e-02,\n",
      "           3.4615e-01, -1.5316e+00],\n",
      "         [-8.9705e-01, -1.9691e-01,  3.7768e-01,  ...,  6.8382e-02,\n",
      "           1.5971e-03, -8.3600e-01],\n",
      "         ...,\n",
      "         [-4.1435e-01, -7.1321e-01,  9.6007e-02,  ..., -1.7707e-02,\n",
      "           6.3960e-03, -4.4889e-01],\n",
      "         [-8.2974e-01, -5.3442e-01, -4.5472e-01,  ..., -3.3265e-01,\n",
      "          -1.6531e-01, -1.8877e+00],\n",
      "         [ 5.2169e-01, -1.5060e-01, -1.1631e-01,  ...,  1.1713e-01,\n",
      "          -2.3125e-01, -2.3326e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0657, -0.5709, -1.0282,  ..., -0.1378,  0.5249, -0.2588],\n",
      "         [-0.9095, -0.5679,  0.2888,  ...,  0.2163,  1.2784, -0.7673],\n",
      "         [-0.6430, -0.1422,  0.2499,  ...,  0.0327, -0.0940, -0.9448],\n",
      "         ...,\n",
      "         [-0.5603, -0.9551,  0.3104,  ..., -0.0714, -0.5293, -0.3512],\n",
      "         [ 0.4961, -0.1564, -0.0523,  ..., -0.6907, -0.3050, -1.6993],\n",
      "         [ 0.6558, -0.2900, -0.0348,  ...,  0.1300, -0.3898, -0.3290]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.4111e-02, -9.0961e-01, -2.7278e-01,  ...,  2.4331e-01,\n",
      "           1.5047e-01, -2.1425e-03],\n",
      "         [-5.1067e-01, -6.6500e-01, -5.0931e-01,  ...,  5.6430e-01,\n",
      "           6.3347e-01, -7.8912e-01],\n",
      "         [-1.3022e+00, -7.0610e-01, -1.5980e-01,  ..., -4.8717e-03,\n",
      "           1.5212e-01,  7.6559e-01],\n",
      "         ...,\n",
      "         [-7.7374e-01, -2.4005e-01,  6.1734e-01,  ..., -2.0743e-01,\n",
      "           2.3216e-04, -1.3531e-01],\n",
      "         [-1.7468e+00, -2.8382e-01, -1.2346e-01,  ..., -3.3629e-01,\n",
      "          -2.0089e-01, -1.1415e+00],\n",
      "         [ 7.6223e-01, -2.0862e-01, -1.7436e-01,  ...,  2.3171e-01,\n",
      "          -3.6174e-01, -1.9621e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1607, -0.5548, -0.5635,  ...,  0.2410,  0.6336, -0.3309],\n",
      "         [-0.7748, -0.5945, -0.5947,  ...,  0.5933,  1.3343, -0.8049],\n",
      "         [-0.9607,  0.1897, -0.3423,  ..., -0.5342,  0.4571, -0.0938],\n",
      "         ...,\n",
      "         [-1.6233, -0.1651,  0.4092,  ..., -0.2268,  0.1019, -0.2038],\n",
      "         [ 0.0787, -0.6657,  0.0074,  ...,  0.0188, -0.0904, -0.5753],\n",
      "         [ 0.7117, -0.4548, -0.1506,  ...,  0.1061, -0.3931, -0.3920]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1359, -0.8802, -0.6716,  ...,  0.1165,  0.1894, -0.1834],\n",
      "         [-0.4527, -0.6534,  0.0944,  ...,  0.2637,  0.5719, -0.8417],\n",
      "         [-1.0631, -0.0376, -0.2873,  ...,  0.0581,  0.2997, -1.0058],\n",
      "         ...,\n",
      "         [-0.7223,  0.3558,  0.1147,  ..., -0.0468, -0.3452, -0.7213],\n",
      "         [-0.1607, -0.4457,  0.4503,  ..., -0.1963, -0.5913, -0.9429],\n",
      "         [ 0.4158, -0.2065, -0.0325,  ...,  0.0320, -0.3710, -0.2847]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2545, -0.6254, -0.1701,  ...,  0.0257,  0.2543, -0.5827],\n",
      "         [-1.1485, -0.6509, -0.8642,  ...,  0.5414,  1.2785, -1.0907],\n",
      "         [-0.6908, -0.1987, -0.1289,  ..., -0.1197,  0.6680, -0.2297],\n",
      "         ...,\n",
      "         [-0.6258,  0.0606, -0.2670,  ..., -0.0946, -0.3406, -1.3461],\n",
      "         [-0.4532, -0.2928, -0.3919,  ..., -0.4236, -0.2123, -1.3224],\n",
      "         [ 0.6946, -0.0743, -0.1780,  ...,  0.0453, -0.4511, -0.3396]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.7996, -0.8980, -0.9036,  ...,  0.1466,  0.8167, -0.7717],\n",
      "         [-0.5722, -0.3149, -0.2279,  ...,  0.1848,  0.6193, -0.3283],\n",
      "         [ 0.1165,  0.1436,  0.2141,  ..., -0.2484,  0.4566, -1.0046],\n",
      "         ...,\n",
      "         [ 0.4350, -0.1308,  0.3114,  ..., -0.6222, -0.0418, -1.3749],\n",
      "         [-0.7725,  0.0603, -0.1736,  ..., -0.7063, -0.4457, -1.4888],\n",
      "         [ 0.3492, -0.3997, -0.1687,  ...,  0.1039, -0.3837, -0.3968]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0998, -0.4683, -1.2805,  ...,  0.0657,  0.5788, -0.2650],\n",
      "         [-0.5788,  0.1255, -0.3050,  ...,  0.1558,  0.6371, -0.4973],\n",
      "         [ 0.0490,  0.6937, -0.1353,  ..., -0.4542,  0.0215, -0.5390],\n",
      "         ...,\n",
      "         [-0.7207, -1.2168, -0.1654,  ..., -0.3515, -0.3730, -0.6708],\n",
      "         [-0.9304, -0.6017,  0.2580,  ..., -0.3166, -0.0844, -0.8425],\n",
      "         [ 0.0327, -0.1232,  0.0271,  ...,  0.0387, -0.1047, -0.4400]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1729, -0.5862, -0.8962,  ...,  0.1619,  0.7758, -0.3211],\n",
      "         [-0.2788, -0.1945, -0.8447,  ...,  0.6809,  0.6782, -0.7859],\n",
      "         [-0.6813, -0.5226, -0.2051,  ...,  0.0070,  0.5789,  0.4874],\n",
      "         ...,\n",
      "         [-0.4298, -0.3322, -0.2639,  ..., -0.3915, -0.2545, -1.2515],\n",
      "         [-0.5068, -0.2407, -0.5294,  ..., -0.8391, -0.4618, -1.6014],\n",
      "         [ 0.5865, -0.2780, -0.0608,  ...,  0.1570, -0.3550, -0.3146]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0148, -0.7685, -1.1563,  ..., -0.1122,  0.7102, -0.0029],\n",
      "         [-1.0468,  0.1851, -0.3322,  ..., -0.0368,  0.8096, -0.2218],\n",
      "         [-0.5927, -0.4221, -0.0626,  ..., -0.2972,  0.3082, -0.3765],\n",
      "         ...,\n",
      "         [-0.6909,  0.0602, -0.5385,  ..., -0.3855, -0.2237, -1.1876],\n",
      "         [-1.3506, -0.4467, -0.6295,  ..., -0.2283,  0.0810, -0.4670],\n",
      "         [ 0.7419, -0.2536,  0.0201,  ...,  0.1648, -0.3609, -0.2112]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0902, -0.4107, -0.6454,  ...,  0.2991,  0.4735, -0.4233],\n",
      "         [-0.8930, -0.2661, -0.2260,  ...,  0.0203,  1.0641, -1.0377],\n",
      "         [-0.3238,  0.0337,  0.2563,  ..., -0.5574,  0.4133, -0.5094],\n",
      "         ...,\n",
      "         [-0.9172, -0.5370, -0.1300,  ...,  0.1983, -0.1024, -0.3158],\n",
      "         [-1.4419, -0.5102, -0.5932,  ...,  0.0348, -0.3444, -1.4718],\n",
      "         [ 0.6023, -0.2376,  0.0336,  ...,  0.0432, -0.2565, -0.3437]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1593, -0.6184, -0.7812,  ...,  0.0607,  0.4438, -0.4728],\n",
      "         [-0.2509, -0.0079, -0.1198,  ...,  0.3998,  0.9974, -0.4200],\n",
      "         [-0.1240, -0.2945,  0.0200,  ..., -0.0628,  0.1755, -0.2004],\n",
      "         ...,\n",
      "         [-0.1756,  0.6267,  0.0708,  ..., -0.1004, -0.3619, -0.9026],\n",
      "         [-0.8183, -1.1280, -0.2293,  ...,  0.1715, -0.1060, -0.7181],\n",
      "         [ 0.9370, -0.4274, -0.3013,  ...,  0.1503, -0.5836, -0.4221]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5383, -0.3611, -0.7818,  ...,  0.1465,  0.5892, -0.3443],\n",
      "         [-0.7073, -0.5273,  0.2636,  ...,  0.0685,  0.9873,  0.0497],\n",
      "         [ 0.0922, -0.4266, -0.0020,  ..., -0.1455,  0.1193,  0.4852],\n",
      "         ...,\n",
      "         [-0.4464, -0.0957,  0.6095,  ...,  0.2287,  0.0142, -0.2524],\n",
      "         [-0.1457, -0.2631,  0.1385,  ...,  0.1428, -0.1206, -1.2299],\n",
      "         [ 0.7228,  0.0868, -0.0185,  ...,  0.0764, -0.4301, -0.3063]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4031, -0.8078, -0.3164,  ...,  0.2118,  0.6469, -0.2674],\n",
      "         [-0.0262,  0.1342, -0.3448,  ...,  0.1219,  1.0310, -0.2283],\n",
      "         [-1.0345, -0.3295,  0.1532,  ..., -0.0755, -0.1246, -0.9867],\n",
      "         ...,\n",
      "         [-1.4083, -0.3206,  0.1099,  ..., -0.1328,  0.3271, -1.3090],\n",
      "         [-1.7738, -0.6299, -0.3449,  ..., -0.4437, -0.1720, -0.9949],\n",
      "         [ 0.5993, -0.2770, -0.0704,  ...,  0.0691, -0.3117, -0.3732]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.6972e-01, -5.4322e-01, -8.7790e-01,  ..., -1.6622e-01,\n",
      "           7.2788e-01, -4.4864e-01],\n",
      "         [-1.3891e-01,  8.6363e-01, -7.1418e-02,  ...,  5.2952e-02,\n",
      "           5.4767e-01, -5.6810e-01],\n",
      "         [-1.2194e+00, -8.3207e-02, -1.6736e-01,  ..., -3.8841e-01,\n",
      "           9.4118e-01, -1.7871e-01],\n",
      "         ...,\n",
      "         [ 2.8287e-01,  1.4635e-01, -3.6372e-01,  ..., -5.1620e-01,\n",
      "           1.0797e-03, -9.0652e-01],\n",
      "         [ 2.2310e-01, -3.9266e-01, -2.8398e-01,  ..., -2.9876e-01,\n",
      "           1.0846e-01, -8.1734e-01],\n",
      "         [ 6.1048e-01, -2.8420e-01, -2.2363e-01,  ...,  2.8893e-01,\n",
      "          -2.9437e-01, -3.8461e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2736, -0.4319, -0.6882,  ...,  0.4070,  0.6943, -0.0487],\n",
      "         [-0.9523, -0.1659,  0.0578,  ...,  0.3533,  0.8092, -0.4399],\n",
      "         [-1.1899,  0.0271, -0.5002,  ...,  0.3829,  1.0722, -0.2118],\n",
      "         ...,\n",
      "         [-0.8911, -0.2494,  0.1116,  ..., -0.1643,  0.1598, -0.2691],\n",
      "         [-0.0995, -0.2564,  0.0166,  ..., -0.1447, -0.1393, -0.8569],\n",
      "         [ 0.7422, -0.3164, -0.1093,  ...,  0.1662, -0.3677, -0.3085]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4463, -0.6202, -0.5060,  ..., -0.1056,  0.7832, -0.0790],\n",
      "         [-0.5294, -0.0372,  0.2168,  ...,  0.0274,  0.0876, -0.2875],\n",
      "         [-0.6365, -0.3890,  0.4241,  ..., -0.7976,  0.0226, -0.4707],\n",
      "         ...,\n",
      "         [-1.3253, -0.7189,  0.0252,  ..., -0.2095, -0.0563,  0.2928],\n",
      "         [-1.2823, -0.1854, -0.4110,  ..., -0.0735, -0.0426, -1.8419],\n",
      "         [ 0.3836,  0.0053,  0.0715,  ...,  0.1378, -0.2938, -0.2142]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0267, -0.6644, -0.7174,  ...,  0.4490,  0.6386, -0.1684],\n",
      "         [-0.6271, -0.3504, -0.0522,  ...,  0.2504,  0.6032, -0.1732],\n",
      "         [-0.3776, -0.2358, -0.0691,  ..., -0.3529,  0.6908, -0.2599],\n",
      "         ...,\n",
      "         [-0.8900, -1.3748, -0.5319,  ...,  0.1863,  0.4123, -0.6531],\n",
      "         [-1.3124, -0.2392, -0.3169,  ..., -0.7072, -0.0934, -0.7527],\n",
      "         [ 0.8245, -0.4396, -0.0526,  ...,  0.2841, -0.3360, -0.3247]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1782, -0.6869, -0.9373,  ...,  0.2838,  0.3562, -0.3601],\n",
      "         [-0.0108, -0.5440, -0.1520,  ...,  0.4460,  0.4213, -0.2341],\n",
      "         [-0.4081, -0.6157, -0.5897,  ...,  0.0235,  0.5669, -0.4997],\n",
      "         ...,\n",
      "         [-0.2716, -0.5698, -0.2431,  ...,  0.2418,  0.1316, -1.4353],\n",
      "         [-0.4931, -0.6034, -0.4373,  ..., -0.0156, -0.2022, -1.3237],\n",
      "         [ 0.6877, -0.2347, -0.2348,  ...,  0.0340, -0.2769, -0.2565]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3586, -0.7939, -0.1621,  ...,  0.0576, -0.0851, -0.0711],\n",
      "         [-0.1613, -0.1961, -0.0836,  ...,  0.3375,  1.1663, -0.4563],\n",
      "         [-0.6368, -0.0315, -0.2210,  ..., -0.8119,  0.7281, -0.1187],\n",
      "         ...,\n",
      "         [ 0.0672, -1.4092,  0.6826,  ..., -0.1193, -0.1873, -1.2764],\n",
      "         [-2.1040, -0.5399, -0.2459,  ..., -0.1215, -0.4430, -0.9955],\n",
      "         [ 0.8150, -0.1928, -0.0771,  ...,  0.0652, -0.3802, -0.0971]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0949, -0.4066, -0.9040,  ...,  0.1302,  0.6816,  0.0657],\n",
      "         [-0.7027,  0.0649, -0.0177,  ...,  0.0549,  1.0879, -0.5484],\n",
      "         [-1.0235, -0.0527, -0.8173,  ..., -0.5951,  0.1588, -0.0026],\n",
      "         ...,\n",
      "         [ 0.8100,  0.3738,  0.1886,  ..., -0.6007, -0.0013, -0.3088],\n",
      "         [-0.7999, -0.3053, -0.3762,  ..., -0.3124,  0.1372, -0.6193],\n",
      "         [ 0.7084, -0.2570, -0.1176,  ...,  0.1916, -0.3471, -0.2182]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3302, -0.9146, -0.5443,  ..., -0.1463,  0.5253, -0.1694],\n",
      "         [-0.7410, -1.1839, -0.0587,  ..., -0.4577,  0.7612, -0.2111],\n",
      "         [-0.9263, -0.1888,  0.1926,  ..., -0.2712,  0.2280,  0.0661],\n",
      "         ...,\n",
      "         [-0.7471,  0.8210,  0.3179,  ..., -0.1289, -0.1281, -0.7998],\n",
      "         [-1.1740, -0.8273, -0.4431,  ...,  0.2341,  0.0440, -1.0584],\n",
      "         [ 0.7963, -0.2818, -0.2589,  ..., -0.0713, -0.2511, -0.2720]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4497, -0.1834, -0.7652,  ...,  0.4304,  0.2874,  0.2792],\n",
      "         [ 0.1222,  0.0118, -0.0644,  ...,  0.2418,  0.8207,  0.9235],\n",
      "         [-0.0648,  0.5271,  0.1098,  ...,  0.2850, -0.2447, -0.1261],\n",
      "         ...,\n",
      "         [-1.1175, -0.0684, -0.0977,  ...,  0.6911,  0.1588, -0.7930],\n",
      "         [-0.8853, -0.5435,  0.0111,  ..., -0.3466, -0.0042, -0.1147],\n",
      "         [ 0.9017, -0.2179, -0.1569,  ...,  0.2893, -0.3895, -0.2353]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1076, -0.7836, -0.3322,  ...,  0.4912,  0.2972, -0.3883],\n",
      "         [-0.3940, -0.3534,  0.5089,  ...,  0.0516, -0.0973, -0.3186],\n",
      "         [-0.0661,  0.0781, -0.4527,  ..., -0.3901,  0.3955, -1.0244],\n",
      "         ...,\n",
      "         [ 0.5873, -0.0266,  0.4190,  ...,  0.1925, -0.2073, -1.6275],\n",
      "         [-0.6411, -0.8720,  0.0849,  ...,  0.1122, -0.1007, -0.5537],\n",
      "         [ 0.3423, -0.2658, -0.0658,  ...,  0.0842, -0.3625, -0.4211]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.7099e-01, -6.0322e-01, -7.4475e-01,  ...,  2.3191e-02,\n",
      "           4.6539e-01, -2.2127e-01],\n",
      "         [ 1.3537e-03,  3.0055e-02, -7.5263e-02,  ...,  1.5740e-01,\n",
      "           6.5973e-01, -8.5678e-01],\n",
      "         [-7.4751e-01, -1.4243e-01,  2.2617e-01,  ...,  5.3964e-01,\n",
      "           7.6486e-01,  5.0810e-01],\n",
      "         ...,\n",
      "         [ 6.3607e-01,  7.2918e-01,  3.1245e-01,  ...,  1.1893e-01,\n",
      "          -3.4889e-01, -1.1115e+00],\n",
      "         [-3.5120e-01, -3.0234e-01, -6.5822e-01,  ..., -1.5744e-01,\n",
      "           2.8793e-01, -1.4901e+00],\n",
      "         [ 7.4338e-01,  1.1259e-01, -1.3378e-01,  ...,  1.7956e-01,\n",
      "          -4.5434e-01, -1.6254e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1996, -0.7464, -0.6155,  ...,  0.5729,  0.1688, -0.1490],\n",
      "         [-0.4511, -0.5882, -0.0488,  ...,  0.2180,  0.5560, -0.6631],\n",
      "         [-0.6217, -0.2767,  0.1578,  ...,  0.2039,  0.1163, -1.2920],\n",
      "         ...,\n",
      "         [ 0.9441,  0.4444,  0.3260,  ...,  0.3625,  0.2778, -1.1452],\n",
      "         [-0.9822, -0.2813,  0.0592,  ...,  0.4026, -0.2099, -0.8374],\n",
      "         [ 0.6292, -0.3377, -0.0733,  ...,  0.3019, -0.3029, -0.3339]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.0084e-01, -2.4354e-01, -8.4060e-01,  ..., -1.2976e-01,\n",
      "           6.1304e-01, -2.9310e-02],\n",
      "         [-1.1548e+00, -3.2985e-01, -2.2957e-01,  ...,  2.4888e-01,\n",
      "           1.1399e+00, -6.3499e-01],\n",
      "         [-4.2827e-01, -4.6153e-01,  5.6807e-01,  ..., -2.1852e-01,\n",
      "           6.9215e-01,  1.1568e-01],\n",
      "         ...,\n",
      "         [ 4.0919e-01, -3.8557e-01,  2.5880e-01,  ..., -1.8360e-01,\n",
      "          -2.1569e-01, -7.2291e-01],\n",
      "         [-5.7129e-01, -4.2162e-01, -1.0281e-03,  ..., -1.5818e-01,\n",
      "          -2.3939e-01, -3.3270e-01],\n",
      "         [ 6.1805e-01, -2.8777e-01, -1.6524e-01,  ...,  1.8771e-01,\n",
      "          -3.2529e-01, -2.5177e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2120, -0.9646, -0.3120,  ...,  0.1124,  0.3260, -0.2559],\n",
      "         [-0.5192, -0.4050, -0.0988,  ...,  0.6984,  0.3461, -0.4111],\n",
      "         [-1.1639, -0.2843, -0.3681,  ...,  0.2648, -0.0571, -0.7355],\n",
      "         ...,\n",
      "         [-0.5097, -1.1797,  0.3929,  ..., -0.2571, -0.1847,  0.2357],\n",
      "         [-0.8567, -0.8207, -0.4984,  ...,  0.1136, -0.4702, -1.5636],\n",
      "         [ 0.7283, -0.3094, -0.1065,  ...,  0.1171, -0.4863, -0.2957]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0272, -0.8707, -0.6548,  ...,  0.3749,  0.2668, -0.1569],\n",
      "         [-0.5901, -0.1350, -0.6694,  ...,  0.2340,  0.9695, -0.4118],\n",
      "         [-0.7252, -0.2274, -0.7640,  ...,  0.4580,  0.7723, -0.3901],\n",
      "         ...,\n",
      "         [ 0.2721, -0.0727,  0.2022,  ...,  0.1419, -0.0740, -1.7577],\n",
      "         [-0.5316, -0.4195,  0.0658,  ..., -0.0282, -0.2325, -1.8790],\n",
      "         [ 0.6154, -0.2977,  0.0506,  ...,  0.0425, -0.1292, -0.4322]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1863, -0.8615, -0.5662,  ...,  0.5187,  0.7114,  0.0498],\n",
      "         [-0.6156, -0.2106, -0.0656,  ..., -0.1197,  0.4776, -0.6935],\n",
      "         [ 0.1382, -0.9465,  0.5340,  ...,  0.1394,  0.3857,  0.0976],\n",
      "         ...,\n",
      "         [-0.7648, -0.6814,  0.0259,  ..., -0.2290,  0.0961, -0.6158],\n",
      "         [-0.1801, -0.9117,  0.0604,  ...,  0.0309, -0.1915, -0.6645],\n",
      "         [ 0.3234,  0.0861,  0.0204,  ...,  0.3682, -0.3697, -0.2274]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1054, -0.7932, -0.2384,  ..., -0.0201, -0.0043, -0.1647],\n",
      "         [-1.1827,  0.1299,  0.0480,  ...,  0.1590,  0.2179, -0.2478],\n",
      "         [-0.9955, -0.8569,  0.7092,  ...,  0.0932,  0.1446, -0.2697],\n",
      "         ...,\n",
      "         [-0.7042, -1.0458, -0.1430,  ...,  0.2465,  0.3272, -0.8696],\n",
      "         [-1.3156, -1.4054, -0.2608,  ...,  0.2265,  0.0796, -0.6103],\n",
      "         [ 0.7500, -0.4172,  0.0090,  ...,  0.1680, -0.3806, -0.2964]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5737, -0.9829, -0.3831,  ...,  0.4434,  0.1229, -0.0780],\n",
      "         [-1.4718, -0.0378,  0.2972,  ...,  0.0446,  0.5317,  0.3079],\n",
      "         [ 0.0756,  0.2902, -0.0620,  ...,  0.2540, -0.2745,  0.0998],\n",
      "         ...,\n",
      "         [-0.6823, -0.3209,  0.5648,  ..., -0.2780, -0.2727, -1.1279],\n",
      "         [-0.5391, -0.9444, -0.2942,  ..., -0.3027, -0.4882, -1.8305],\n",
      "         [ 0.3556, -0.4034, -0.1615,  ...,  0.0233, -0.1286, -0.5652]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.8576, -0.3527, -0.6514,  ...,  0.0802,  0.2233, -0.1016],\n",
      "         [-0.2151, -0.3460, -0.2801,  ...,  0.3609,  0.6544, -0.1206],\n",
      "         [-0.1967, -0.5784,  0.3777,  ..., -0.1278,  0.3925,  0.6696],\n",
      "         ...,\n",
      "         [-0.9391,  0.6418, -0.0806,  ..., -0.1064,  0.0411, -1.0829],\n",
      "         [-1.2468, -1.3259, -0.9610,  ...,  0.4793,  0.8161, -1.2365],\n",
      "         [ 0.4045,  0.0030, -0.2326,  ...,  0.0544, -0.2113, -0.1296]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0422, -0.6499, -0.5879,  ..., -0.0808,  0.6068,  0.0033],\n",
      "         [-0.6912, -0.3501,  0.5375,  ..., -0.1014,  0.1860,  0.5738],\n",
      "         [-0.7672, -0.2356,  0.9174,  ..., -0.1681,  0.1217,  0.3682],\n",
      "         ...,\n",
      "         [-0.0279,  0.2205,  0.5291,  ..., -0.3946, -0.3599, -1.1280],\n",
      "         [-1.3715, -0.6044, -0.3291,  ..., -0.3096, -0.1439, -0.9009],\n",
      "         [ 0.6252, -0.3601, -0.1085,  ...,  0.1005, -0.1607, -0.1565]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4822, -0.5624, -0.5657,  ..., -0.1242,  0.3519, -0.2212],\n",
      "         [-0.7123, -0.5265, -0.2183,  ...,  0.2118,  0.6846, -0.5306],\n",
      "         [-0.1431, -0.5520,  0.5572,  ...,  0.0793, -0.4804, -0.7381],\n",
      "         ...,\n",
      "         [-0.2941, -0.0639, -0.0533,  ..., -0.4590, -0.5567, -0.1546],\n",
      "         [-1.3031,  0.1507, -0.6752,  ..., -0.1022, -0.6348, -1.8539],\n",
      "         [ 0.5607, -0.2636,  0.0119,  ...,  0.0433, -0.2482, -0.3758]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1316, -0.6555, -0.4048,  ..., -0.0863,  0.3877, -0.5348],\n",
      "         [-0.7631,  0.0166,  0.3146,  ...,  0.3513,  0.8413, -0.0546],\n",
      "         [-0.1799, -0.3749, -0.2022,  ..., -0.1970,  0.3073, -0.5309],\n",
      "         ...,\n",
      "         [-0.8253,  0.7553, -0.3538,  ..., -0.4235,  0.0319, -1.8193],\n",
      "         [-1.2177, -0.5420, -0.4327,  ...,  0.1030,  0.2286, -1.0922],\n",
      "         [ 0.7583, -0.2065, -0.1028,  ...,  0.0928, -0.2497, -0.2969]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4518, -0.6540, -0.8575,  ..., -0.2576,  0.4391, -0.4232],\n",
      "         [-0.5925, -0.3829, -0.4247,  ...,  0.4425,  0.8847, -0.6292],\n",
      "         [-0.8635, -0.3639,  0.0097,  ...,  0.2495,  0.3348,  0.6808],\n",
      "         ...,\n",
      "         [-0.9575, -0.7535, -0.4056,  ...,  0.0262,  0.1523, -0.4966],\n",
      "         [-0.2407, -0.7068, -0.3819,  ..., -0.1662, -0.4667, -1.2350],\n",
      "         [ 0.6100, -0.3250, -0.2111,  ..., -0.0340, -0.2736, -0.3272]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2624, -0.5020, -0.3625,  ...,  0.2879,  0.4430, -0.2385],\n",
      "         [ 0.2588, -0.4237, -0.2977,  ...,  0.7047,  0.3420, -0.6277],\n",
      "         [ 0.3025, -0.1447, -0.3689,  ...,  0.0097,  0.5114, -0.4254],\n",
      "         ...,\n",
      "         [-0.6778, -0.8225, -0.0226,  ...,  0.0259, -0.0241,  0.2945],\n",
      "         [-0.8812, -0.2262,  0.6015,  ..., -0.0688, -0.0756, -0.6841],\n",
      "         [ 0.7167, -0.0761, -0.1340,  ...,  0.1292, -0.2683, -0.3696]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.5135, -0.4833, -0.4193,  ..., -0.3382,  0.3546, -0.1665],\n",
      "         [-1.0554, -0.1730,  0.0704,  ...,  0.3471,  0.2318, -0.4014],\n",
      "         [-1.0264, -0.7637, -0.0439,  ...,  0.0964,  0.2491, -0.1638],\n",
      "         ...,\n",
      "         [-0.6570, -1.0329,  0.1609,  ..., -0.1638,  0.2879, -0.9178],\n",
      "         [-1.0088,  0.1965, -0.0342,  ..., -0.2824, -0.1270, -0.4482],\n",
      "         [ 0.6812, -0.0805, -0.0265,  ...,  0.1785, -0.1569, -0.4241]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1716, -0.7125, -0.4817,  ...,  0.0050,  0.3259, -0.5514],\n",
      "         [-0.7292, -0.7465,  0.5199,  ...,  0.0099,  0.8779, -0.2616],\n",
      "         [-0.6883, -0.3266,  0.0343,  ...,  0.5106,  0.3985, -1.9848],\n",
      "         ...,\n",
      "         [ 0.5671, -0.3167,  0.4504,  ..., -0.8674,  0.3312, -0.6533],\n",
      "         [ 0.4020,  0.3966, -0.5212,  ...,  0.0115, -0.3160, -1.7223],\n",
      "         [ 0.6516, -0.1630, -0.1951,  ...,  0.1439, -0.2125, -0.3532]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3264, -0.3703, -0.6526,  ...,  0.0249,  0.5345, -0.5723],\n",
      "         [-0.0490, -0.1586,  0.1142,  ...,  0.2815,  0.8986, -0.6962],\n",
      "         [-0.7272, -0.3695, -0.3492,  ..., -0.4469,  0.2856, -0.3316],\n",
      "         ...,\n",
      "         [ 0.5020, -0.4537,  0.3689,  ..., -0.0548, -0.2034, -0.7433],\n",
      "         [-1.4008,  0.0274, -0.0162,  ..., -0.1707, -0.2647, -1.1350],\n",
      "         [ 0.1324, -0.3408, -0.0740,  ...,  0.1520, -0.2359, -0.2571]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-3.6212e-01, -1.6050e-01, -7.1574e-01,  ..., -1.4917e-01,\n",
      "          -2.3213e-03, -4.5280e-01],\n",
      "         [-2.1531e-01,  2.5028e-01, -5.2676e-01,  ...,  4.6610e-01,\n",
      "           1.2295e+00, -5.8446e-01],\n",
      "         [-8.5008e-01,  5.0682e-01,  1.3968e-01,  ..., -1.6686e-01,\n",
      "           1.0206e+00,  6.9176e-01],\n",
      "         ...,\n",
      "         [ 6.7014e-01, -2.1455e-01, -3.3162e-01,  ...,  2.6847e-02,\n",
      "           1.1382e-01, -8.9122e-01],\n",
      "         [ 7.8090e-02,  2.9880e-02, -1.1227e-01,  ...,  2.6906e-01,\n",
      "           8.4661e-02, -1.2115e+00],\n",
      "         [ 5.0950e-01,  5.4529e-04, -1.4644e-01,  ...,  3.3516e-02,\n",
      "          -3.7692e-01, -3.3045e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3857, -0.6998, -0.5595,  ..., -0.7017,  0.4548, -0.1517],\n",
      "         [-0.6152,  0.1401, -0.1863,  ..., -0.3381,  0.8617, -0.6671],\n",
      "         [-0.4706, -0.4680,  0.0863,  ..., -0.4017,  0.6276, -0.6604],\n",
      "         ...,\n",
      "         [ 0.2976,  0.2356,  0.6422,  ..., -0.2534, -0.2043, -0.6151],\n",
      "         [-0.1708, -0.3786, -0.2798,  ...,  0.0432, -0.4347, -0.7739],\n",
      "         [ 0.4143, -0.0521, -0.1823,  ..., -0.2575, -0.2542, -0.4093]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4595, -0.6517, -0.1768,  ..., -0.3938,  0.2427, -0.4162],\n",
      "         [-0.9221, -0.0562,  0.1076,  ..., -0.4031,  1.1130, -0.5418],\n",
      "         [-0.7607, -0.0234,  0.3498,  ..., -0.3811,  0.0438, -0.6286],\n",
      "         ...,\n",
      "         [-1.3760, -0.4081,  0.0667,  ...,  0.2125,  0.2028, -0.4878],\n",
      "         [ 0.2392, -0.7025, -0.0152,  ..., -0.3988,  0.2503, -0.7912],\n",
      "         [ 0.6930, -0.1963, -0.1897,  ...,  0.0342,  0.0644, -0.2447]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0626, -0.6985, -0.5360,  ..., -0.0889,  0.4629, -0.7611],\n",
      "         [-0.7623, -0.6032, -0.1892,  ...,  0.2250,  0.5769, -0.6070],\n",
      "         [-0.6112, -0.9374,  0.2414,  ..., -0.0252,  0.3429, -0.2904],\n",
      "         ...,\n",
      "         [ 0.4406, -0.2103,  0.1767,  ..., -0.5931, -0.2009, -1.3975],\n",
      "         [-0.8865, -1.1212, -0.1330,  ..., -0.4334, -0.2614, -0.4891],\n",
      "         [ 0.5473, -0.2857, -0.2185,  ...,  0.2207, -0.1530, -0.4728]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3303, -0.7884, -0.4488,  ..., -0.6109,  0.3897, -0.3153],\n",
      "         [-0.9546, -0.1316, -0.7134,  ..., -0.4579,  0.7410, -0.4613],\n",
      "         [-0.8377, -0.3120, -0.2345,  ..., -0.5255,  0.2255, -0.4202],\n",
      "         ...,\n",
      "         [ 0.9012,  0.8112, -0.0340,  ..., -0.6650, -0.2261, -0.9945],\n",
      "         [-0.7341, -0.1977, -0.3683,  ..., -0.5912, -0.0769, -0.7595],\n",
      "         [ 0.6487, -0.0730, -0.1496,  ...,  0.0843, -0.3427, -0.3443]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4020, -0.4642, -0.9274,  ..., -0.1669,  0.4273, -0.5355],\n",
      "         [-0.7767, -0.4362, -0.0656,  ...,  0.0433,  0.2780, -0.4023],\n",
      "         [-0.8545, -0.4826,  0.1989,  ..., -0.5775, -0.2813,  0.2730],\n",
      "         ...,\n",
      "         [-0.9929, -0.2241, -0.6821,  ...,  0.4842,  0.1592, -0.9902],\n",
      "         [-1.8380, -0.3244, -0.6933,  ..., -0.0624, -0.0678, -1.2231],\n",
      "         [ 0.2749, -0.2272, -0.2250,  ..., -0.0287, -0.1816, -0.3316]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3176, -0.8530, -0.3234,  ..., -0.7994,  0.4132, -0.1264],\n",
      "         [-1.0779, -0.6210, -0.5399,  ..., -0.2884,  0.8543, -1.1470],\n",
      "         [-0.9211, -0.5669,  0.1242,  ..., -0.8240,  0.4790, -0.3725],\n",
      "         ...,\n",
      "         [-0.9804, -0.2764,  0.0614,  ..., -0.7074,  0.2582, -1.1032],\n",
      "         [-0.5695, -0.7940,  0.1785,  ..., -0.3380, -0.1024, -0.4999],\n",
      "         [ 0.5436, -0.1977, -0.0616,  ..., -0.1177, -0.0542, -0.3277]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3786, -0.6367, -0.5443,  ..., -0.3406,  0.1697, -0.4887],\n",
      "         [-0.5072,  0.0588,  0.6074,  ...,  0.4314,  0.0256, -0.2444],\n",
      "         [-0.4721, -0.3329,  0.4687,  ...,  0.1598,  0.0741, -1.0866],\n",
      "         ...,\n",
      "         [-0.6506,  0.2052,  0.1164,  ...,  0.2317,  0.2039, -1.1718],\n",
      "         [-0.4891, -0.9228,  0.5022,  ..., -0.0589, -0.3425, -1.4942],\n",
      "         [ 0.5937, -0.1371, -0.1218,  ...,  0.1203, -0.2113, -0.3750]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.1438, -0.5084,  0.3821,  ..., -0.1027,  0.3770, -0.4207],\n",
      "         [-1.1243, -0.2765, -0.0022,  ...,  0.1574,  0.9542, -0.6847],\n",
      "         [-1.0250, -0.1367,  0.3542,  ..., -0.0128,  0.0378, -0.6179],\n",
      "         ...,\n",
      "         [-0.5915, -0.2256,  1.0676,  ...,  0.0317,  0.2975, -1.3137],\n",
      "         [-0.5425, -0.0730,  0.9111,  ...,  0.0114, -0.1991, -0.8695],\n",
      "         [ 0.5667, -0.1138, -0.1267,  ...,  0.1620, -0.3393, -0.3603]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6197, -0.2049, -0.3122,  ..., -0.4041,  0.4024, -0.3322],\n",
      "         [-1.1939, -0.4863,  0.0626,  ..., -0.5852,  1.0665,  0.6776],\n",
      "         [-0.9236, -0.0115, -0.3131,  ..., -0.6050,  0.3549, -0.8480],\n",
      "         ...,\n",
      "         [-0.2815,  0.2173,  0.5949,  ..., -0.3182,  0.4080, -1.3521],\n",
      "         [-0.6578, -0.1147,  0.2273,  ..., -0.4688, -0.1926, -1.7909],\n",
      "         [ 0.5451, -0.1284, -0.0541,  ...,  0.0343, -0.4927, -0.1498]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5249, -0.5307, -0.4164,  ..., -0.3129,  0.4653, -0.3840],\n",
      "         [-0.8050, -0.4078, -0.8103,  ...,  0.2587,  0.9538, -0.9679],\n",
      "         [-1.2562, -0.7685, -0.2038,  ..., -0.1516, -0.1048, -0.8919],\n",
      "         ...,\n",
      "         [-0.4150, -0.4859,  0.3099,  ..., -0.6962,  0.0822, -0.8494],\n",
      "         [-0.9436, -0.1213, -0.2018,  ...,  0.2310,  0.0406, -1.4301],\n",
      "         [ 0.4614, -0.1605, -0.0362,  ..., -0.0435, -0.1660, -0.3413]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.7166, -0.7550,  0.3035,  ..., -0.2226,  0.2906,  0.0570],\n",
      "         [-0.2328, -0.2789, -0.0628,  ...,  0.5460,  0.1496, -0.8860],\n",
      "         [-0.4208, -0.0540,  0.6394,  ..., -0.3579, -0.3068,  0.2842],\n",
      "         ...,\n",
      "         [-0.4621, -1.1011,  0.3080,  ..., -0.3369,  0.0118, -1.3247],\n",
      "         [-1.0958, -0.6974, -0.2995,  ..., -0.0505, -0.2907, -0.7478],\n",
      "         [ 0.2960, -0.2855, -0.1534,  ...,  0.1735, -0.1624, -0.2862]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1245, -0.9293, -0.2087,  ..., -0.0081, -0.2250, -0.9956],\n",
      "         [ 0.4111, -0.4304,  0.2370,  ..., -0.4606,  0.7607,  0.1643],\n",
      "         [-0.5447, -0.9677,  0.7934,  ...,  0.0967,  0.1744,  0.0117],\n",
      "         ...,\n",
      "         [ 0.0119, -0.3125,  0.2283,  ...,  0.0533, -0.1552, -0.6101],\n",
      "         [-1.1395, -0.3053, -0.2282,  ..., -0.0338,  0.1580, -1.3210],\n",
      "         [ 0.5595, -0.1773,  0.0830,  ...,  0.1236, -0.1962, -0.4114]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5165, -0.3139, -0.3870,  ..., -0.0888, -0.2103,  0.0130],\n",
      "         [-0.2277, -0.0683,  0.2183,  ..., -0.1359,  0.2685, -0.9831],\n",
      "         [-0.3126, -1.0362, -0.1434,  ...,  0.2415,  0.3063, -0.3095],\n",
      "         ...,\n",
      "         [-1.0118, -0.0971, -0.4992,  ...,  0.1344, -0.1473, -0.8695],\n",
      "         [-1.4564, -0.2239, -0.2790,  ...,  0.2152, -0.0941, -0.3201],\n",
      "         [ 0.4783,  0.0824, -0.1738,  ...,  0.1809, -0.2091, -0.2550]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4900, -0.6102, -0.4729,  ..., -0.4569,  0.1413, -0.0877],\n",
      "         [ 0.1089, -0.3550,  0.4915,  ...,  0.7075,  0.6385, -0.7720],\n",
      "         [-0.9485, -0.8756,  0.0609,  ...,  0.1701,  0.1905, -0.3798],\n",
      "         ...,\n",
      "         [-0.8329,  0.1151, -0.0459,  ...,  0.0414,  0.2934, -0.8871],\n",
      "         [-0.5233, -0.5521,  0.1118,  ...,  0.0468,  0.0344, -1.0966],\n",
      "         [ 0.4605,  0.2128, -0.0178,  ...,  0.0657, -0.1798, -0.1653]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.2048, -0.4737, -0.6595,  ..., -0.8255,  0.8111, -0.3213],\n",
      "         [-0.3844, -0.0864, -1.0763,  ..., -0.1464,  0.4730, -0.5765],\n",
      "         [-1.2010, -0.4436, -0.0397,  ..., -0.2838,  0.3324,  0.4613],\n",
      "         ...,\n",
      "         [-1.3796, -1.2259, -0.4894,  ..., -0.4786,  0.2471, -0.8123],\n",
      "         [-1.2565, -0.9441,  0.3051,  ..., -0.5093,  0.2427, -1.4688],\n",
      "         [ 0.5863, -0.0732, -0.0654,  ...,  0.0315, -0.2615, -0.2488]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1812, -0.5615, -0.9770,  ..., -0.5794,  0.7075, -0.3434],\n",
      "         [-0.8687, -0.4624, -0.3278,  ..., -0.7420,  0.6827,  0.2594],\n",
      "         [-0.3111,  0.0136, -0.9498,  ..., -0.3853, -0.0082, -0.6580],\n",
      "         ...,\n",
      "         [-0.4601, -0.1211, -0.7327,  ..., -0.4996,  0.1691, -0.9145],\n",
      "         [-0.9583, -1.3242, -0.8125,  ..., -0.8051,  0.0427, -0.6838],\n",
      "         [ 0.4185, -0.1809, -0.0111,  ..., -0.0151, -0.3680, -0.3007]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.7183e-01, -5.7858e-01, -6.9160e-01,  ..., -3.2115e-01,\n",
      "           3.3393e-01, -2.2410e-01],\n",
      "         [ 4.1725e-01,  5.4433e-01,  4.7890e-01,  ..., -2.7630e-01,\n",
      "           3.4521e-01,  2.2806e-01],\n",
      "         [-5.9296e-01, -2.6917e-01,  1.7138e-01,  ..., -2.5391e-01,\n",
      "           1.4640e-01, -3.0890e-02],\n",
      "         ...,\n",
      "         [-7.0945e-01,  1.1223e-03, -9.8504e-03,  ..., -3.6424e-01,\n",
      "           2.2489e-01, -7.7126e-01],\n",
      "         [-1.1984e+00, -3.4853e-02, -2.5514e-02,  ...,  8.7297e-02,\n",
      "           1.6362e-01, -9.9502e-01],\n",
      "         [ 6.4208e-01, -7.6934e-02, -4.2287e-02,  ...,  3.9823e-02,\n",
      "          -7.3340e-02, -3.6896e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0545,  0.0859, -0.5451,  ..., -0.6386, -0.0783, -0.5355],\n",
      "         [-0.3433,  0.3260, -0.0690,  ...,  0.0654,  0.8290, -0.1761],\n",
      "         [-0.2027, -0.1975,  0.2657,  ...,  0.2479,  0.3720,  0.8215],\n",
      "         ...,\n",
      "         [ 0.4842,  0.2068,  0.7850,  ..., -0.3484, -0.0753, -0.9613],\n",
      "         [-0.1310, -0.3513,  0.1803,  ..., -0.0841,  0.2741, -1.0741],\n",
      "         [ 0.5599,  0.1179, -0.1696,  ...,  0.1280, -0.1916, -0.2418]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5035, -0.1113, -0.4429,  ..., -0.2710,  0.4272, -0.6919],\n",
      "         [-0.9814, -0.2181, -0.3001,  ...,  0.2833,  1.1887, -1.1279],\n",
      "         [-1.1269,  0.0949, -0.2528,  ..., -0.3214,  0.5870, -0.3979],\n",
      "         ...,\n",
      "         [-1.5710, -0.0244, -0.4521,  ..., -0.6767,  0.1923, -0.8201],\n",
      "         [-0.3427, -0.3530, -0.3754,  ..., -0.0454,  0.3995, -1.8606],\n",
      "         [ 0.2945, -0.1317, -0.5164,  ...,  0.0814, -0.0811, -0.4456]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4206, -0.4938,  0.1501,  ..., -0.3192,  0.5641, -0.4729],\n",
      "         [-0.3666, -0.2342, -0.9428,  ...,  0.2465,  0.7086, -0.5460],\n",
      "         [-0.9371, -0.1899, -0.0986,  ..., -0.1990,  0.4760, -0.2880],\n",
      "         ...,\n",
      "         [ 0.6204, -0.3225,  1.1717,  ...,  0.1495,  0.4824, -1.1059],\n",
      "         [-0.7981, -0.6980,  0.3282,  ..., -0.1924,  0.2304, -0.5019],\n",
      "         [ 0.6377,  0.0388,  0.0425,  ..., -0.0518, -0.2181, -0.3240]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3573, -0.5524, -0.4067,  ..., -0.5952,  0.1760, -0.3071],\n",
      "         [-0.5691, -0.6946,  0.1353,  ...,  0.4017,  0.3577, -0.2728],\n",
      "         [-0.5787, -0.1824,  0.4232,  ...,  0.0633,  0.0393, -1.1338],\n",
      "         ...,\n",
      "         [-1.2583, -0.3863,  0.3455,  ..., -0.9055,  0.5033, -1.3163],\n",
      "         [-0.5658, -0.7048, -0.4150,  ..., -0.1484,  0.1250, -1.0565],\n",
      "         [ 0.6177, -0.1153, -0.2683,  ...,  0.0462, -0.2268, -0.2495]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1391, -0.1103, -0.2742,  ..., -0.0700, -0.0092, -0.6911],\n",
      "         [ 0.0343, -0.1892, -0.6853,  ...,  0.4921,  0.6565, -1.0401],\n",
      "         [-1.0301, -0.4481,  0.6416,  ..., -0.0120,  0.1576, -0.4770],\n",
      "         ...,\n",
      "         [ 0.5591,  0.1003, -0.3322,  ...,  0.4983, -0.4632, -0.7935],\n",
      "         [-0.6224, -0.4490, -0.0025,  ...,  0.4836, -0.4028, -0.7519],\n",
      "         [ 0.7128, -0.1178, -0.0016,  ...,  0.0808, -0.1588, -0.2249]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.2315e+00, -6.8482e-01, -6.9239e-01,  ..., -2.4304e-01,\n",
      "           5.2978e-01, -9.4101e-01],\n",
      "         [-9.2134e-01, -6.2165e-01, -7.0825e-01,  ...,  1.2612e-01,\n",
      "           1.1549e+00, -9.2592e-01],\n",
      "         [-9.2388e-01,  2.2073e-01, -4.8531e-01,  ..., -5.5441e-01,\n",
      "           4.8204e-01, -2.5795e-01],\n",
      "         ...,\n",
      "         [ 4.8762e-01, -7.5920e-02,  6.5007e-02,  ..., -4.1385e-04,\n",
      "           3.1482e-01, -5.6200e-01],\n",
      "         [-9.6661e-01, -8.0031e-01, -4.0256e-01,  ...,  6.9550e-01,\n",
      "           2.6079e-01, -1.6314e+00],\n",
      "         [ 4.2303e-01, -1.2556e-01, -4.0085e-02,  ...,  1.2876e-01,\n",
      "          -2.2995e-01, -3.2550e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-7.7503e-01, -6.7784e-01, -8.0086e-01,  ..., -2.6997e-01,\n",
      "           7.2599e-02, -9.1706e-01],\n",
      "         [-9.8495e-01, -4.6545e-01, -3.2268e-01,  ...,  4.1036e-01,\n",
      "           8.1249e-01, -7.5241e-01],\n",
      "         [ 2.0607e-01, -2.4451e-01, -2.8951e-02,  ..., -1.2064e-01,\n",
      "           1.0234e-01, -4.1066e-01],\n",
      "         ...,\n",
      "         [ 2.1347e-01,  1.3325e-01, -7.1848e-01,  ...,  1.4488e-03,\n",
      "           1.5192e-01, -1.2364e+00],\n",
      "         [-3.0472e-01, -1.5650e-01, -5.4537e-01,  ..., -2.6440e-01,\n",
      "           3.0964e-02, -1.4784e+00],\n",
      "         [ 1.5538e-01,  3.8881e-02, -8.8607e-02,  ...,  9.6409e-02,\n",
      "          -1.7681e-01, -3.7565e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.1405, -0.2184, -0.4429,  ..., -0.6852,  0.3567, -0.3073],\n",
      "         [-1.0725, -0.2737,  0.4919,  ..., -0.2175,  0.2839,  0.3469],\n",
      "         [-0.6214, -0.1456,  0.9732,  ..., -0.0354,  0.0373,  0.5067],\n",
      "         ...,\n",
      "         [-0.3540,  0.2176,  0.2236,  ..., -0.9440, -0.3183, -1.0453],\n",
      "         [-1.2369, -0.4540, -0.6519,  ..., -0.4619, -0.2527, -0.8446],\n",
      "         [ 0.4713, -0.1969,  0.0038,  ...,  0.0508, -0.0131, -0.2012]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9648], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9648], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3573, -0.4401, -0.3422,  ..., -0.4858, -0.0588, -0.8957],\n",
      "         [-1.0371, -0.2944, -0.8231,  ..., -0.0212,  1.0305, -1.1127],\n",
      "         [-1.1943,  0.0060,  0.0548,  ..., -0.4671,  0.5438, -0.2429],\n",
      "         ...,\n",
      "         [-0.3800,  0.1825, -0.0526,  ..., -0.3836, -0.2300, -1.2778],\n",
      "         [-0.3628, -0.5433, -0.1983,  ..., -0.7510, -0.0716, -1.7209],\n",
      "         [ 0.5198,  0.0483, -0.1065,  ..., -0.0471, -0.2659, -0.3526]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9654], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9654], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4507, -0.0825, -0.7068,  ..., -0.4229, -0.0018, -0.8290],\n",
      "         [-0.1407,  0.0950, -0.5375,  ...,  0.4497,  1.1796, -0.4377],\n",
      "         [-0.9832,  0.4272, -0.0061,  ..., -0.1934,  0.5991,  0.6298],\n",
      "         ...,\n",
      "         [ 0.7175,  0.0513, -0.3184,  ..., -0.0703,  0.1492, -0.7997],\n",
      "         [ 0.3322, -0.1522, -0.1419,  ...,  0.0995,  0.1058, -1.0978],\n",
      "         [ 0.4178,  0.1023,  0.0307,  ..., -0.0061, -0.3192, -0.2962]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9735], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9735], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1835, -0.2892, -0.6169,  ..., -0.4029,  0.1815, -0.5197],\n",
      "         [ 0.3148,  0.4343,  0.2909,  ..., -0.3240,  0.5152,  0.0906],\n",
      "         [-0.5944, -0.2969, -0.0692,  ..., -0.3994,  0.2359, -0.1163],\n",
      "         ...,\n",
      "         [-0.6854, -0.1509,  0.1292,  ..., -0.2056,  0.3702, -0.7754],\n",
      "         [-1.3934, -0.1303, -0.1626,  ...,  0.0227,  0.0846, -1.1533],\n",
      "         [ 0.5180, -0.0151,  0.0961,  ...,  0.0242, -0.1370, -0.2714]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9744], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9744], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4741, -0.2727, -0.5808,  ..., -0.4834,  0.2181, -0.9980],\n",
      "         [-0.4014, -0.2969, -0.3465,  ...,  0.2782,  0.5160, -0.4788],\n",
      "         [-0.8930, -0.2558,  0.0635,  ...,  0.2355,  0.2933,  0.4947],\n",
      "         ...,\n",
      "         [-1.1769, -0.4922, -0.4803,  ..., -0.2245,  0.2386, -0.6077],\n",
      "         [-0.1985, -0.6975, -0.4963,  ..., -0.1110, -0.1342, -1.2302],\n",
      "         [ 0.4196, -0.0793, -0.1449,  ..., -0.0650, -0.1342, -0.3808]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9743], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9743], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.3412, -0.6983, -0.4686,  ..., -0.9999,  0.3923, -0.3522],\n",
      "         [-0.9820, -0.6360, -0.6316,  ..., -0.2754,  0.6554, -1.0692],\n",
      "         [-0.7775, -0.5503, -0.0272,  ..., -0.7261,  0.2939, -0.3469],\n",
      "         ...,\n",
      "         [-0.7713, -0.2444,  0.1053,  ..., -0.6388,  0.0714, -1.1358],\n",
      "         [-0.2075, -0.6959,  0.0649,  ..., -0.3658, -0.0746, -0.5249],\n",
      "         [ 0.4290, -0.0989, -0.0101,  ..., -0.2130, -0.0547, -0.3650]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9712], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9712], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[ 0.0613, -0.1743, -1.1091,  ...,  0.1740,  0.5471, -0.2193],\n",
      "         [ 0.0507, -0.2123, -0.0836,  ..., -0.0020,  0.9356,  0.0667],\n",
      "         [-0.2412, -0.2879,  0.2191,  ..., -0.2914,  0.7295, -0.1129],\n",
      "         ...,\n",
      "         [-0.1718,  0.1203, -0.0717,  ..., -0.1480, -0.2374, -0.0501],\n",
      "         [-0.2352, -0.5221,  0.2655,  ...,  0.0215, -0.4032, -0.7908],\n",
      "         [ 0.4334, -0.0811, -0.1911,  ...,  0.1951, -0.3253, -0.3714]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9529], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9529], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.2943, -0.8583, -0.0177,  ..., -0.5714, -0.0750,  0.0535],\n",
      "         [-0.2870,  0.1574, -0.1087,  ...,  0.1700,  0.2597, -0.2981],\n",
      "         [-0.8564, -0.2593,  0.6934,  ..., -0.4917, -0.4045, -0.4701],\n",
      "         ...,\n",
      "         [ 0.2015,  0.1633,  0.4484,  ..., -0.2802, -0.2622, -0.4521],\n",
      "         [ 0.0412, -0.2323,  0.8529,  ..., -0.7307, -0.5751, -0.4059],\n",
      "         [ 0.6621, -0.4007,  0.1004,  ...,  0.2063, -0.2548, -0.1670]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9549], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9549], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.6721, -0.5648, -0.3307,  ..., -0.4232,  0.3608, -0.5433],\n",
      "         [-0.1241,  0.1214, -0.2296,  ..., -0.0836,  0.8454, -0.2453],\n",
      "         [-0.8951, -0.2823,  0.2356,  ..., -0.1443, -0.0531, -0.8809],\n",
      "         ...,\n",
      "         [-1.4630, -0.2287,  0.1946,  ..., -0.4870,  0.5145, -1.2891],\n",
      "         [-1.6751, -0.5641, -0.4507,  ..., -0.3335, -0.0404, -1.2124],\n",
      "         [ 0.4754, -0.1234,  0.0136,  ...,  0.0061, -0.0968, -0.3048]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9720], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9720], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4213, -0.6770, -0.4257,  ..., -0.4795,  0.0642, -0.3942],\n",
      "         [ 0.3646, -0.1669,  0.5276,  ...,  0.7003,  0.8076, -0.7193],\n",
      "         [-0.9286, -0.7987, -0.0341,  ...,  0.0050,  0.2066, -0.3596],\n",
      "         ...,\n",
      "         [-0.7257, -0.0651, -0.0778,  ...,  0.0669,  0.3127, -0.8479],\n",
      "         [-0.4082, -0.6729, -0.2232,  ...,  0.0813, -0.0819, -1.0449],\n",
      "         [ 0.4513,  0.1891, -0.0208,  ...,  0.0297, -0.2249, -0.2057]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9723], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9723], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: avg_val_f1 reached 0.00000 (best 0.05379), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer/_ckpt_epoch_4.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(9.6220, device='cuda:0')\tavg_answer_loss:  tensor(3.7297, device='cuda:0')\tavg_type_loss:  tensor(1.1785, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "sequence_output  tensor([[[-0.4733, -0.2872, -0.5575,  ..., -0.4216,  0.1344, -0.8811],\n",
      "         [-0.3529, -0.2950,  0.3916,  ...,  0.3723,  0.2071, -0.3958],\n",
      "         [-0.2756, -0.4427,  0.4420,  ...,  0.0380,  0.1455, -1.2500],\n",
      "         ...,\n",
      "         [-0.7585,  0.2415,  0.0578,  ...,  0.1952,  0.4168, -1.1911],\n",
      "         [-0.3033, -0.9531,  0.5011,  ..., -0.1382, -0.1374, -1.5810],\n",
      "         [ 0.4092, -0.1426,  0.1790,  ...,  0.0860, -0.1538, -0.3431]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.2258, -0.3463, -0.3774,  ...,  0.1152,  0.0514, -0.6960],\n",
      "         [-0.2846, -0.4825, -0.1035,  ...,  0.1954,  0.1749, -0.8756],\n",
      "         [-0.8744, -0.2003,  0.3069,  ...,  0.0391,  0.0142, -1.2012],\n",
      "         ...,\n",
      "         [ 1.3490,  0.2315,  0.0132,  ...,  0.1822,  0.2055, -0.9709],\n",
      "         [-0.5948, -0.0310,  0.0443,  ...,  0.3116,  0.0505, -1.1122],\n",
      "         [ 0.4311, -0.0716,  0.0911,  ...,  0.1018, -0.2420, -0.3754]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6535, -0.3760, -0.9094,  ..., -0.5109,  0.1928, -0.6301],\n",
      "         [ 0.1890,  0.0123, -0.1191,  ...,  0.0904,  0.1792, -0.0435],\n",
      "         [-0.0674, -0.3521,  0.2940,  ..., -0.3563, -0.0477,  0.1628],\n",
      "         ...,\n",
      "         [-0.9993, -0.1007,  0.2333,  ...,  0.7366, -0.3408, -1.5348],\n",
      "         [-0.1603,  0.1253, -0.2556,  ...,  0.3079, -0.5860, -0.8378],\n",
      "         [ 0.6865, -0.1340, -0.2783,  ...,  0.0339, -0.0455, -0.4393]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2284, -0.4473, -0.3100,  ..., -0.3129,  0.3676, -0.8866],\n",
      "         [-0.7248, -0.4497, -0.5859,  ...,  0.2759,  0.9881, -0.7306],\n",
      "         [-0.6685, -1.0409, -0.1258,  ..., -0.0589,  0.4284, -0.7716],\n",
      "         ...,\n",
      "         [-0.2706, -0.2392,  0.2053,  ..., -0.4635, -0.2127, -1.7514],\n",
      "         [-0.7962, -0.9576, -0.1600,  ..., -0.6936, -0.1867, -0.5191],\n",
      "         [ 0.3888, -0.0972, -0.0122,  ..., -0.0855, -0.0414, -0.6140]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6207, -0.0573, -0.0767,  ..., -0.2046,  0.2904, -0.8043],\n",
      "         [ 0.2185, -0.6591, -0.2284,  ...,  0.5924,  0.3277, -0.7494],\n",
      "         [ 0.3257,  0.1532, -0.4047,  ..., -0.0831,  0.4246, -0.7785],\n",
      "         ...,\n",
      "         [-0.7917, -0.6821,  0.1941,  ..., -0.2061,  0.1614, -0.0504],\n",
      "         [-1.0121,  0.0100, -0.0017,  ..., -0.2375, -0.2812, -1.3382],\n",
      "         [ 0.5924, -0.0279, -0.0223,  ...,  0.0387, -0.0789, -0.2357]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3760, -0.4636, -0.4552,  ..., -0.5393,  0.5000, -0.5651],\n",
      "         [-0.6380, -0.4022, -0.7185,  ..., -0.0341,  0.8697, -0.8791],\n",
      "         [-1.2035, -0.7176, -0.3213,  ..., -0.4308, -0.0875, -0.7600],\n",
      "         ...,\n",
      "         [-0.9871, -0.5680,  0.1939,  ..., -0.7410,  0.0107, -0.7289],\n",
      "         [-1.0622, -0.1303, -0.3017,  ...,  0.3169, -0.0018, -1.5027],\n",
      "         [ 0.4673, -0.1035,  0.0175,  ..., -0.0958, -0.0255, -0.3320]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.2104, -0.4375,  0.3992,  ..., -0.2740,  0.2800, -0.7460],\n",
      "         [-0.9833, -0.1134, -0.0219,  ..., -0.0018,  0.4659, -0.7624],\n",
      "         [-0.8500, -0.0420, -0.0562,  ...,  0.0729, -0.0477, -0.3864],\n",
      "         ...,\n",
      "         [-0.5877, -0.2808,  1.0629,  ...,  0.0757,  0.3567, -1.3280],\n",
      "         [-0.2774, -0.1985,  1.1758,  ...,  0.3651, -0.2231, -0.9125],\n",
      "         [ 0.4966, -0.0758, -0.0227,  ...,  0.2647, -0.2738, -0.3300]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3648, -0.4322,  0.3833,  ..., -0.1450, -0.0449, -0.1966],\n",
      "         [-0.4775, -0.2743,  0.3063,  ...,  0.0382,  0.1133, -0.2779],\n",
      "         [-0.8727,  0.1444,  0.2827,  ..., -0.5248,  0.1901, -1.0593],\n",
      "         ...,\n",
      "         [-0.9208,  0.0975,  0.0937,  ..., -0.7437,  0.1232, -1.1108],\n",
      "         [-0.8540, -0.3594, -0.4006,  ..., -0.2223, -0.2504, -1.0263],\n",
      "         [ 0.4325, -0.1931, -0.1065,  ...,  0.2987,  0.0691, -0.3310]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2460, -0.4260, -0.6451,  ..., -0.4114,  0.3528, -0.8501],\n",
      "         [-0.8459, -0.3609, -0.2128,  ..., -0.1929,  0.3067, -0.5480],\n",
      "         [-0.7907, -0.5467,  0.1119,  ..., -0.4394, -0.0031,  0.0835],\n",
      "         ...,\n",
      "         [-0.9968, -0.3398, -0.6071,  ...,  0.4123,  0.2699, -1.1740],\n",
      "         [-1.7779, -0.2508, -0.6254,  ..., -0.1655, -0.0784, -1.3500],\n",
      "         [ 0.7819, -0.1049, -0.3876,  ...,  0.0805, -0.2496, -0.3121]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3455, -0.4667, -0.6830,  ..., -0.6721,  0.7629, -0.8376],\n",
      "         [-0.0593,  0.5502, -0.4152,  ..., -0.4579,  0.7235, -0.9116],\n",
      "         [-1.3235, -0.3525, -0.4654,  ..., -0.5835,  1.0041, -0.4976],\n",
      "         ...,\n",
      "         [ 0.0967,  0.4932, -0.5119,  ..., -0.7492,  0.1951, -1.0156],\n",
      "         [ 0.2034,  0.0044, -0.2847,  ..., -0.6008, -0.1098, -0.9713],\n",
      "         [ 0.5146, -0.1718,  0.0531,  ...,  0.0591, -0.2154, -0.2993]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5186, -0.0635, -0.6795,  ..., -0.2248,  0.3662, -0.7503],\n",
      "         [-0.8613, -0.5147,  0.1117,  ..., -0.1222,  0.6828, -0.5223],\n",
      "         [ 0.3885, -0.2815, -0.0987,  ..., -0.2889,  0.1845,  0.2390],\n",
      "         ...,\n",
      "         [ 0.3325, -0.4091,  0.5038,  ..., -0.0051, -0.1024, -0.3375],\n",
      "         [-0.2669, -0.2878,  0.1808,  ...,  0.0848,  0.1536, -1.3408],\n",
      "         [ 0.2913,  0.1528,  0.0178,  ..., -0.0594, -0.3262, -0.2227]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5813, -0.1197, -0.7764,  ..., -0.3855,  0.2761, -0.7785],\n",
      "         [-0.2343, -0.0042, -0.6115,  ...,  0.4364,  1.2207,  0.0356],\n",
      "         [-0.9417,  0.1798, -0.0797,  ..., -0.3011,  0.6624,  0.5024],\n",
      "         ...,\n",
      "         [ 0.5021, -0.1502, -0.2880,  ..., -0.1650,  0.0469, -1.0013],\n",
      "         [ 0.2415, -0.1635, -0.2404,  ...,  0.0178,  0.2521, -1.1540],\n",
      "         [ 0.4866, -0.0138, -0.1072,  ..., -0.0546, -0.1763, -0.3862]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2186, -0.5558, -0.1427,  ..., -0.4746,  0.3874, -0.8609],\n",
      "         [-0.2254, -0.4693, -0.6854,  ...,  0.3642,  0.5325, -0.6172],\n",
      "         [-0.7884, -0.3001, -0.0241,  ..., -0.1791,  0.5454, -0.0565],\n",
      "         ...,\n",
      "         [ 0.4324, -0.5773,  1.1957,  ...,  0.0541,  0.5667, -0.9724],\n",
      "         [-0.6882, -0.8755,  0.3582,  ..., -0.1879,  0.0592, -0.5732],\n",
      "         [ 0.6739,  0.0611,  0.1082,  ...,  0.1555, -0.1576, -0.2866]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5448, -0.4884, -0.5670,  ..., -0.0566,  0.1226, -0.3860],\n",
      "         [-0.0954, -0.2210,  0.0818,  ..., -0.0134,  0.7540,  0.8162],\n",
      "         [-0.2148,  0.6281,  0.2801,  ...,  0.3611, -0.2010, -0.1407],\n",
      "         ...,\n",
      "         [-1.0929, -0.1101,  0.3200,  ...,  0.2227,  0.0467, -0.3731],\n",
      "         [-0.7726, -0.4359, -0.2095,  ..., -0.3458, -0.0856, -0.6427],\n",
      "         [ 0.7374, -0.0901, -0.1570,  ...,  0.2386, -0.2022, -0.1658]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0409, -0.4015, -0.2644,  ...,  0.1482,  0.0422, -0.6053],\n",
      "         [-0.0323, -0.5231, -0.5963,  ...,  0.7054,  0.7259, -0.9001],\n",
      "         [-0.6289, -0.3775,  0.5453,  ..., -0.0277,  0.0482, -0.3979],\n",
      "         ...,\n",
      "         [ 0.8131,  0.1706, -0.2776,  ...,  0.3067, -0.3065, -0.9767],\n",
      "         [-0.5763, -0.3765, -0.2003,  ...,  0.1385, -0.4507, -0.8328],\n",
      "         [ 0.7422, -0.2610,  0.1106,  ...,  0.0722, -0.1334, -0.1831]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3226, -0.4524, -0.1433,  ..., -0.3841, -0.0497, -0.8678],\n",
      "         [-0.9464, -0.3322, -0.8643,  ..., -0.1329,  0.9610, -1.1085],\n",
      "         [-1.1847, -0.2433, -0.1828,  ..., -0.6141,  0.6312, -0.3276],\n",
      "         ...,\n",
      "         [-0.5762,  0.2306, -0.3148,  ..., -0.4294, -0.3234, -1.2483],\n",
      "         [-0.3820, -0.4642,  0.0044,  ..., -0.5928, -0.1104, -1.8637],\n",
      "         [ 0.5569, -0.0970,  0.0286,  ..., -0.0773, -0.1769, -0.3164]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3707, -0.3665, -0.3961,  ..., -0.0412,  0.5556, -0.3658],\n",
      "         [-0.7881, -0.0658,  0.0604,  ...,  0.3499,  0.6699, -0.5659],\n",
      "         [-1.1834,  0.2154, -0.5605,  ...,  0.1669,  1.2521, -0.0844],\n",
      "         ...,\n",
      "         [-0.6646, -0.0550,  0.3919,  ..., -0.1657,  0.0659, -0.5849],\n",
      "         [ 0.1814, -0.1053,  0.1588,  ..., -0.2828, -0.2772, -0.6152],\n",
      "         [ 0.4935, -0.2352, -0.2670,  ...,  0.0061, -0.0813, -0.2447]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0616, -0.6609, -0.0354,  ..., -0.2149, -0.2394, -0.9087],\n",
      "         [ 0.4624, -0.2995,  0.3626,  ..., -0.6874,  0.6028,  0.0632],\n",
      "         [-0.6674, -0.9517,  0.8207,  ...,  0.0590,  0.1641, -0.0561],\n",
      "         ...,\n",
      "         [-0.1777, -0.4472,  0.1929,  ...,  0.0584, -0.3008, -0.1581],\n",
      "         [-0.5729, -0.4304, -0.4565,  ...,  0.0027,  0.1045, -1.2176],\n",
      "         [ 0.6112, -0.2690,  0.1715,  ...,  0.0672, -0.0817, -0.3178]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[ 0.1830, -0.4566, -0.5806,  ..., -0.9234,  0.4710, -0.6243],\n",
      "         [-0.7024, -0.8628, -0.5019,  ..., -0.2882,  1.1510,  0.4503],\n",
      "         [-0.3457, -0.2168, -0.1812,  ..., -0.4442,  0.2371, -0.4802],\n",
      "         ...,\n",
      "         [ 0.1303,  0.0291, -0.0127,  ..., -0.6489, -0.2689, -1.0671],\n",
      "         [-0.9356, -0.6536, -0.3960,  ..., -0.5595,  0.0975, -1.0277],\n",
      "         [ 0.4136, -0.2514,  0.1243,  ..., -0.3630,  0.1207, -0.4068]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3955, -0.6272, -0.1946,  ..., -0.2222,  0.5309, -0.2749],\n",
      "         [-1.0838, -0.5234, -0.6421,  ..., -0.0070,  1.0754, -1.0036],\n",
      "         [-0.6103, -0.1476,  0.2223,  ..., -0.3071,  0.3565, -0.2236],\n",
      "         ...,\n",
      "         [-0.7772, -0.5164, -0.3272,  ..., -0.4223,  0.3029, -0.5587],\n",
      "         [-0.1771, -0.4951,  0.6509,  ..., -0.2963,  0.1454, -1.0099],\n",
      "         [ 0.6454, -0.0953, -0.0771,  ...,  0.0271, -0.0275, -0.0914]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0521, -0.3903, -0.2440,  ...,  0.1309, -0.1133, -0.6429],\n",
      "         [-0.3808, -0.4996,  0.3121,  ...,  0.0381,  0.1968, -0.6542],\n",
      "         [-0.5087,  0.2213, -0.2855,  ..., -0.4128,  0.2380, -0.9749],\n",
      "         ...,\n",
      "         [ 0.5705, -0.0647,  0.3657,  ...,  0.0924, -0.1333, -1.5051],\n",
      "         [-0.7333, -0.9245,  0.0400,  ..., -0.1195, -0.2866, -0.6058],\n",
      "         [ 0.5669, -0.2916,  0.1125,  ...,  0.0113, -0.1529, -0.3785]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4579, -0.4831, -0.5963,  ..., -0.0702,  0.2586, -0.5638],\n",
      "         [ 0.3588,  0.1658,  0.3129,  ...,  0.0313,  0.9513, -0.5139],\n",
      "         [-0.4665, -0.2290, -0.1697,  ..., -0.4751,  0.1888, -0.6819],\n",
      "         ...,\n",
      "         [ 0.5785, -0.5612,  0.3916,  ..., -0.1152, -0.2279, -0.9392],\n",
      "         [-1.4976, -0.1210, -0.2461,  ..., -0.5679, -0.1261, -1.4012],\n",
      "         [ 0.5807, -0.1007, -0.0079,  ...,  0.1429, -0.0710, -0.2333]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3319, -0.4843, -0.3374,  ..., -0.6278,  0.3199, -0.2119],\n",
      "         [-1.1778, -0.4704,  0.4295,  ..., -0.3651,  0.4002,  0.4595],\n",
      "         [-0.7419,  0.0044,  0.7677,  ..., -0.1961,  0.1389,  0.3598],\n",
      "         ...,\n",
      "         [-0.3712, -0.3567,  0.0478,  ..., -1.1019, -0.3874, -1.2370],\n",
      "         [-1.1207, -0.8078, -0.5561,  ..., -0.5009, -0.0515, -0.9017],\n",
      "         [ 0.6225, -0.4642,  0.0867,  ...,  0.1243,  0.0872, -0.2045]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 2.0408e-02, -4.8444e-01, -6.8659e-01,  ..., -6.8162e-01,\n",
      "           4.0054e-01, -3.1707e-01],\n",
      "         [-8.9341e-01, -6.0915e-01, -5.8425e-01,  ..., -6.2832e-01,\n",
      "           8.5416e-01,  7.6815e-02],\n",
      "         [-1.5686e-02, -2.0379e-01, -4.7588e-01,  ..., -6.0151e-01,\n",
      "           1.4105e-01, -7.1133e-01],\n",
      "         ...,\n",
      "         [-4.6472e-01, -1.9648e-01, -7.5134e-01,  ..., -6.8592e-01,\n",
      "           1.1052e-01, -7.1525e-01],\n",
      "         [-1.0161e+00, -1.5212e+00, -6.6483e-01,  ..., -6.7136e-01,\n",
      "           2.1175e-01, -6.5672e-01],\n",
      "         [ 5.0550e-01, -3.2882e-01,  1.6102e-01,  ..., -7.5266e-02,\n",
      "          -1.0937e-03, -1.9790e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6192, -0.7617, -0.2182,  ..., -0.6523,  0.6648, -0.2708],\n",
      "         [-0.5288, -0.1825,  0.2033,  ..., -0.3196,  0.3724, -0.8748],\n",
      "         [-0.3684, -0.7253,  0.6869,  ..., -1.0606, -0.2620, -0.9543],\n",
      "         ...,\n",
      "         [-1.4460, -0.3492, -0.1262,  ..., -0.6283,  0.2256,  0.1337],\n",
      "         [-1.1042, -0.0248, -0.3814,  ..., -0.5436, -0.0128, -1.5919],\n",
      "         [ 0.4189, -0.0924,  0.1619,  ...,  0.1071, -0.0805, -0.1669]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3693, -0.5185, -0.4174,  ..., -0.9168,  0.2630, -0.6965],\n",
      "         [-1.2442, -0.2656, -0.4231,  ..., -0.4731,  0.2085, -0.4938],\n",
      "         [-0.6932, -0.4354, -0.5892,  ..., -0.2875,  0.2237, -0.4881],\n",
      "         ...,\n",
      "         [ 0.6870,  0.8672,  0.1495,  ..., -0.7912, -0.4842, -1.1875],\n",
      "         [-0.7345, -0.5884, -0.5415,  ..., -0.7109, -0.1918, -0.9149],\n",
      "         [ 0.7481, -0.3000,  0.0064,  ...,  0.2026, -0.0983, -0.3769]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0772, -0.9490, -0.1403,  ..., -0.0804, -0.3844, -0.7923],\n",
      "         [-0.7104, -0.3205, -0.6273,  ...,  0.0252,  0.5842, -0.5187],\n",
      "         [-0.5801, -0.5062, -0.7654,  ...,  0.0689,  0.4968, -0.8515],\n",
      "         ...,\n",
      "         [ 0.1028, -0.1427,  0.2397,  ...,  0.0497, -0.1779, -1.7870],\n",
      "         [-0.4909, -0.4647,  0.1337,  ..., -0.3687, -0.4430, -1.9006],\n",
      "         [ 0.5698, -0.0936, -0.0647,  ...,  0.1563,  0.0313, -0.4759]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 9.0875e-06, -6.7900e-01, -5.5161e-01,  ..., -1.1353e-01,\n",
      "           1.3895e-01, -6.3461e-01],\n",
      "         [-1.2424e-01, -4.4342e-01, -3.5482e-01,  ...,  8.1491e-03,\n",
      "           4.2272e-01, -6.4360e-01],\n",
      "         [-3.4289e-01, -6.0799e-01, -4.0946e-01,  ..., -4.5664e-01,\n",
      "           5.7897e-01, -8.6351e-01],\n",
      "         ...,\n",
      "         [-3.7081e-01, -5.6362e-01, -8.3794e-02,  ..., -3.0169e-02,\n",
      "           1.9915e-01, -1.5131e+00],\n",
      "         [-3.4030e-01, -8.7786e-01, -7.2079e-01,  ..., -1.4865e-01,\n",
      "          -2.1192e-01, -1.5569e+00],\n",
      "         [ 5.7532e-01, -1.8390e-01, -1.6848e-01,  ...,  1.2720e-01,\n",
      "          -5.7188e-02, -1.3156e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2495, -0.6743, -0.7219,  ..., -0.5566,  0.3298, -0.7345],\n",
      "         [-1.2125,  0.1123, -0.3381,  ..., -0.3112,  0.6100, -0.5257],\n",
      "         [-0.7903, -0.6900, -0.1258,  ..., -0.3851,  0.1754, -0.7069],\n",
      "         ...,\n",
      "         [-0.7270, -0.3810, -0.4616,  ..., -0.6428, -0.4561, -1.5493],\n",
      "         [-1.2167, -0.4750, -0.5710,  ..., -0.4961,  0.1652, -0.8164],\n",
      "         [ 0.6135, -0.3133,  0.1563,  ...,  0.1390, -0.0113, -0.2558]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3190, -0.9102, -0.1965,  ..., -0.4251, -0.3669, -0.7615],\n",
      "         [-1.1819, -0.3187, -0.1258,  ..., -0.0450,  0.8363, -0.6189],\n",
      "         [-1.1081, -0.6579,  0.3212,  ..., -0.2053,  0.1015, -0.7304],\n",
      "         ...,\n",
      "         [-0.5404, -0.9850,  0.0581,  ...,  0.0381,  0.2364, -0.9763],\n",
      "         [-1.2669, -1.2788, -0.2774,  ..., -0.1572,  0.1283, -0.8509],\n",
      "         [ 0.5475, -0.4307,  0.2599,  ...,  0.1471, -0.0950, -0.3123]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0634, -0.5131, -0.6821,  ..., -0.2323,  0.1397, -0.6290],\n",
      "         [-0.9350, -0.6855,  0.5058,  ..., -0.0248,  1.3737, -0.4823],\n",
      "         [-0.5860, -0.2341,  0.1697,  ..., -0.1613, -0.2387, -1.0342],\n",
      "         ...,\n",
      "         [-0.2434, -0.8322,  0.5779,  ..., -0.2178, -0.3324, -0.2478],\n",
      "         [ 0.2878, -0.1430, -0.2638,  ..., -0.8748, -0.2015, -1.6120],\n",
      "         [ 0.5835, -0.2900,  0.0893,  ...,  0.0704, -0.0483, -0.2833]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2348, -0.4124, -0.0369,  ..., -0.5650,  0.2853, -0.4754],\n",
      "         [-1.0638, -0.0651, -0.3376,  ..., -0.2371,  1.0027, -0.7084],\n",
      "         [-0.8682, -0.0926,  0.3932,  ..., -0.5967,  0.1029, -0.5775],\n",
      "         ...,\n",
      "         [-1.4990, -0.5143, -0.1480,  ..., -0.0159,  0.1726, -0.5658],\n",
      "         [-0.1761, -0.8308, -0.2456,  ..., -0.6010,  0.0025, -1.0923],\n",
      "         [ 0.6990, -0.3275, -0.2576,  ...,  0.0300,  0.1324, -0.1576]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1471, -0.8750, -0.3269,  ..., -0.5081,  0.1669, -0.4898],\n",
      "         [ 0.2363,  0.0835,  0.1187,  ...,  0.0943,  0.4700, -1.3562],\n",
      "         [ 0.1214, -1.1346,  0.7064,  ...,  0.1946,  0.5689, -0.0628],\n",
      "         ...,\n",
      "         [ 0.0324,  0.3322,  0.2326,  ..., -0.0452, -0.2008, -1.9973],\n",
      "         [-0.7226, -0.1782, -0.4023,  ..., -0.1490, -0.1296, -1.5229],\n",
      "         [ 0.5987, -0.1210, -0.0196,  ...,  0.1284, -0.2560, -0.2790]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.0533, -1.0795, -0.6652,  ...,  0.2428,  0.6227,  0.0462],\n",
      "         [-0.6216, -0.1360, -0.5724,  ..., -0.3809,  0.2295, -0.9130],\n",
      "         [-0.0384, -1.1207,  0.4653,  ...,  0.1199,  0.3192,  0.0653],\n",
      "         ...,\n",
      "         [-0.6843, -0.6930,  0.0813,  ..., -0.2568,  0.1897, -0.5686],\n",
      "         [-0.1979, -0.9577, -0.0470,  ..., -0.0022, -0.4078, -0.8407],\n",
      "         [ 0.7835,  0.0754,  0.0471,  ...,  0.3146, -0.2831, -0.1173]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0176, -0.4222, -0.7574,  ..., -0.5782,  0.5442, -0.2642],\n",
      "         [-0.3418, -0.6328, -1.1609,  ...,  0.1012,  0.4471, -0.7450],\n",
      "         [-0.9866, -0.6033, -0.3712,  ..., -0.1600,  0.3390,  0.4867],\n",
      "         ...,\n",
      "         [-1.4196, -1.1445, -0.6903,  ..., -0.3410,  0.0714, -0.5679],\n",
      "         [-0.9991, -1.0480,  0.4409,  ..., -0.3077,  0.1433, -1.4238],\n",
      "         [ 0.6742, -0.2950,  0.1374,  ...,  0.1118, -0.0357, -0.2356]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.2134, -0.2806, -1.4398,  ...,  0.3692,  0.4409, -0.0090],\n",
      "         [-0.0869, -0.2998,  0.1159,  ..., -0.1340,  0.8267,  0.1056],\n",
      "         [-0.4919, -0.3329,  0.2891,  ..., -0.3232,  0.7592, -0.0293],\n",
      "         ...,\n",
      "         [-0.2967, -0.0845, -0.0498,  ..., -0.0493, -0.0075, -0.5241],\n",
      "         [-0.2371, -0.6986,  0.2310,  ...,  0.1046, -0.5867, -0.7782],\n",
      "         [ 0.4508,  0.0677, -0.0659,  ...,  0.1908, -0.2398, -0.4299]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.9269, -0.8835, -0.4556,  ...,  0.0565,  0.4322, -0.8453],\n",
      "         [-0.4134, -0.5710, -0.4999,  ...,  0.3145,  0.8882, -0.8665],\n",
      "         [-0.6662,  0.1267, -0.5279,  ..., -0.4905,  0.2871, -0.1601],\n",
      "         ...,\n",
      "         [ 0.8315, -0.0981, -0.0782,  ...,  0.0672,  0.1978, -0.7992],\n",
      "         [-1.2141, -0.6829, -0.3107,  ...,  0.7775,  0.0434, -1.7514],\n",
      "         [ 0.5918, -0.2550,  0.0110,  ...,  0.2036, -0.0944, -0.4227]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.2559, -0.4033, -0.5959,  ...,  0.0664,  0.2956, -0.3364],\n",
      "         [-0.2711, -0.1178, -0.0427,  ...,  0.3073,  0.5680, -0.7740],\n",
      "         [-0.2853, -0.7176, -0.0678,  ...,  0.0479,  0.3943, -0.8505],\n",
      "         ...,\n",
      "         [-1.3245, -0.4358,  0.0668,  ..., -0.2095, -0.5816, -0.2223],\n",
      "         [-0.6155, -0.6429,  0.2421,  ...,  0.2912, -0.2347, -1.2566],\n",
      "         [ 0.2714, -0.1629,  0.1396,  ...,  0.1748, -0.0975, -0.1260]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.0248, -0.2746, -0.7499,  ..., -0.4592,  0.1444, -0.6918],\n",
      "         [-0.1041, -0.5941, -0.1110,  ...,  0.2542,  0.5586, -0.1601],\n",
      "         [-0.1194, -0.8727,  0.1576,  ..., -0.1493,  0.3183,  0.6773],\n",
      "         ...,\n",
      "         [-0.4967,  0.5392,  0.1169,  ..., -0.7124,  0.0285, -0.6943],\n",
      "         [-1.0261, -1.0539, -0.8793,  ...,  0.1734,  0.8143, -1.0428],\n",
      "         [ 0.3514,  0.0838, -0.1727,  ...,  0.1534, -0.3339, -0.0579]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.7652, -0.5635, -0.3272,  ..., -0.6225,  0.3124, -0.3508],\n",
      "         [-0.9858, -1.0908, -0.1227,  ..., -0.6281,  0.6382, -0.2218],\n",
      "         [-0.9316, -0.2117,  0.4419,  ..., -0.4491,  0.6160, -0.0282],\n",
      "         ...,\n",
      "         [-0.4111,  0.8798,  0.2209,  ..., -0.3203,  0.0401, -0.9528],\n",
      "         [-1.4814, -0.7301, -0.4783,  ..., -0.0681,  0.1025, -0.8876],\n",
      "         [ 0.3773, -0.1903, -0.2137,  ...,  0.1055, -0.0862, -0.2003]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0141, -0.4276, -0.6822,  ..., -0.4357,  0.1153, -0.4547],\n",
      "         [ 0.1781,  0.2143,  0.3431,  ..., -0.2640,  0.4876,  0.2752],\n",
      "         [-0.6739, -0.3165,  0.1237,  ..., -0.2488,  0.0107, -0.1392],\n",
      "         ...,\n",
      "         [-0.7696, -0.2360, -0.0037,  ...,  0.0173,  0.1408, -0.6195],\n",
      "         [-1.4113, -0.0211, -0.0889,  ..., -0.2454, -0.0752, -1.4147],\n",
      "         [ 0.7181, -0.2069,  0.1175,  ...,  0.1363, -0.1738, -0.3237]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0373, -0.9349, -0.4329,  ..., -0.7455,  0.3474, -0.4579],\n",
      "         [-0.4522, -0.0302, -0.1807,  ..., -0.3442,  0.7214, -0.6235],\n",
      "         [-0.5827, -0.7206, -0.0497,  ..., -0.5934,  0.4226, -0.7789],\n",
      "         ...,\n",
      "         [ 0.2288,  0.4146,  0.6477,  ..., -0.1383, -0.3208, -0.6614],\n",
      "         [-0.1941, -0.3706, -0.4919,  ..., -0.0725, -0.5330, -1.0457],\n",
      "         [ 0.5057, -0.0908, -0.0042,  ..., -0.0481, -0.2185, -0.2204]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.6060, -0.7641, -0.8538,  ..., -0.2542,  0.3310, -0.8848],\n",
      "         [-0.7906, -0.5861,  0.2676,  ...,  0.3207,  0.6514, -0.8393],\n",
      "         [-0.0196,  0.3858, -0.1469,  ..., -0.3021,  0.7717, -0.0968],\n",
      "         ...,\n",
      "         [-1.0277, -0.5005, -0.7451,  ..., -0.5044, -0.1290, -1.3000],\n",
      "         [ 0.6897, -0.6885,  0.0227,  ...,  0.0551, -0.0348, -0.4995],\n",
      "         [ 0.6499,  0.0743, -0.0291,  ...,  0.1280, -0.3233, -0.2258]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.3602, -1.2809, -0.2600,  ...,  0.2441, -0.0234, -0.0361],\n",
      "         [-0.5745, -0.6394,  0.0633,  ...,  0.3018,  0.0868, -0.4277],\n",
      "         [-1.0476, -0.3668,  0.0684,  ..., -0.0575,  0.1286, -1.1614],\n",
      "         ...,\n",
      "         [ 0.1323, -0.1762,  0.7510,  ...,  0.5236, -0.3511, -1.6667],\n",
      "         [-1.0854, -0.9685, -0.4801,  ...,  0.5211,  0.1815, -1.3816],\n",
      "         [ 0.8393, -0.3898, -0.1113,  ...,  0.2465, -0.1462, -0.2823]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1490, -0.2128, -0.9613,  ...,  0.1442,  0.7086, -0.1072],\n",
      "         [-1.0590, -0.4732, -0.5187,  ...,  0.1432,  1.0528, -0.8581],\n",
      "         [-0.2538, -0.4497,  0.5976,  ..., -0.2370,  0.6364,  0.1923],\n",
      "         ...,\n",
      "         [ 0.6595,  0.2867,  0.3443,  ..., -0.0248, -0.2060, -0.4601],\n",
      "         [-0.0410, -0.2206,  0.0574,  ..., -0.1345, -0.0581, -0.4484],\n",
      "         [ 0.5940, -0.1264,  0.0297,  ...,  0.2105, -0.3771, -0.2296]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0362, -1.0162, -0.1688,  ...,  0.1573,  0.0019, -0.3278],\n",
      "         [-0.6506, -0.6934, -0.5276,  ...,  0.5607,  0.5443, -0.8449],\n",
      "         [-0.7301, -0.8155, -0.0919,  ...,  0.3736,  0.0970, -0.4485],\n",
      "         ...,\n",
      "         [ 0.2925,  0.0354, -0.2175,  ...,  0.1528, -0.5829, -0.8648],\n",
      "         [-0.7236, -0.9712,  0.0914,  ...,  0.4043, -0.0984, -0.4304],\n",
      "         [ 0.4717, -0.2012,  0.2298,  ...,  0.2600, -0.1555, -0.2963]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4399, -0.7836, -0.5078,  ..., -0.2669,  0.1347, -0.1753],\n",
      "         [ 0.3024,  0.0186,  0.4390,  ...,  0.7515,  0.9394, -0.6657],\n",
      "         [-1.0355, -0.8099,  0.1667,  ...,  0.1728,  0.2539, -0.1426],\n",
      "         ...,\n",
      "         [-0.6564, -0.1484, -0.1205,  ...,  0.3959,  0.2605, -0.6805],\n",
      "         [-0.2317, -0.6747, -0.1338,  ...,  0.2399, -0.1640, -1.1788],\n",
      "         [ 0.6912,  0.0226, -0.1177,  ...,  0.1214, -0.2216, -0.1592]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1825, -0.8950, -0.3989,  ..., -0.0383,  0.1971, -0.4993],\n",
      "         [-0.3098, -0.5212, -0.1742,  ...,  0.6159,  0.8802, -0.5168],\n",
      "         [-1.2462, -0.2444, -0.3408,  ...,  0.3443,  0.0192, -0.8347],\n",
      "         ...,\n",
      "         [-0.4408, -0.9352,  0.3776,  ..., -0.2756, -0.2541,  0.0767],\n",
      "         [-0.9841, -1.0379, -0.3635,  ...,  0.0096, -0.0429, -1.2785],\n",
      "         [ 0.7417,  0.0212,  0.0807,  ...,  0.1690, -0.2876, -0.1675]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2338, -0.3038, -1.4364,  ..., -0.0073,  0.5139, -0.5447],\n",
      "         [-0.7726, -0.1260, -0.2558,  ...,  0.1666,  0.3713, -0.3305],\n",
      "         [ 0.0038,  0.7331,  0.1436,  ..., -0.3118,  0.0186, -0.4868],\n",
      "         ...,\n",
      "         [-0.6575, -1.3748,  0.0222,  ..., -0.2212, -0.3995, -0.3421],\n",
      "         [-0.7876, -0.7910,  0.2201,  ..., -0.3569, -0.1221, -0.8984],\n",
      "         [ 0.2909, -0.1922, -0.0038,  ...,  0.1473, -0.1535, -0.3219]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2785, -0.6802, -1.1910,  ..., -0.6940,  0.7863, -0.6268],\n",
      "         [-0.3961, -0.1477, -0.9618,  ...,  0.5168,  0.7573, -0.5594],\n",
      "         [-1.0609, -0.6957, -0.1604,  ..., -0.2013,  0.1518,  0.9218],\n",
      "         ...,\n",
      "         [-0.0449, -0.4599, -0.2263,  ..., -0.3909, -0.0759, -1.2288],\n",
      "         [-0.3281, -0.4323, -0.5833,  ..., -0.8454, -0.3202, -1.5547],\n",
      "         [ 0.6772, -0.4078, -0.0845,  ...,  0.4021, -0.2674, -0.2860]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.8237, -0.7858, -0.1773,  ..., -0.3996,  0.2805, -0.2103],\n",
      "         [-0.7720,  0.0766, -0.1377,  ...,  0.4038,  0.5401, -0.3779],\n",
      "         [-1.0185, -0.8369, -0.2836,  ...,  0.1200,  0.4292, -0.0616],\n",
      "         ...,\n",
      "         [-0.4452, -0.8545,  0.2994,  ...,  0.1660,  0.1009, -1.0140],\n",
      "         [-1.2558,  0.3323, -0.3397,  ..., -0.1735, -0.2809, -1.0021],\n",
      "         [ 0.7322, -0.3297, -0.1304,  ...,  0.2604, -0.3104, -0.4132]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[-0.6069, -0.7591, -1.0863,  ...,  0.3228,  0.0205, -0.6579],\n",
      "         [-0.8023, -0.4275, -0.4244,  ...,  0.2744,  0.9916, -0.9213],\n",
      "         [ 0.3551, -0.3139,  0.0725,  ..., -0.2190,  0.0387, -0.4980],\n",
      "         ...,\n",
      "         [-0.0651,  0.0265, -0.3010,  ..., -0.0608,  0.1276, -1.0101],\n",
      "         [-0.7021, -0.0879, -0.4694,  ..., -0.0668, -0.0751, -1.3132],\n",
      "         [ 0.6336, -0.1210, -0.0493,  ...,  0.3179, -0.2913, -0.2442]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0118, -0.6190, -0.6088,  ...,  0.1610,  0.3264, -0.2079],\n",
      "         [-0.6025, -0.4829, -0.3612,  ...,  0.3668,  0.8866, -0.3116],\n",
      "         [-0.3096, -0.1584,  0.0861,  ..., -0.3272,  0.7010, -0.2821],\n",
      "         ...,\n",
      "         [-1.0234, -1.4450, -0.6065,  ...,  0.2718,  0.2347, -0.6489],\n",
      "         [-1.2237, -0.4438, -0.3752,  ..., -0.5821, -0.0149, -0.2350],\n",
      "         [ 0.8586, -0.3821,  0.0042,  ...,  0.4056, -0.3372, -0.2593]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0342, -0.4983, -0.9908,  ..., -0.6565,  0.3375, -0.4246],\n",
      "         [-0.3239, -0.4846, -0.1137,  ...,  0.2694,  0.2473, -0.4784],\n",
      "         [-0.8301, -0.1967, -0.5565,  ...,  0.0187,  0.2061, -0.8146],\n",
      "         ...,\n",
      "         [-0.0277, -0.1527, -0.1898,  ..., -0.2799, -0.0906, -0.4673],\n",
      "         [-0.5970, -0.7874, -0.5005,  ...,  0.1982,  0.1502, -1.2713],\n",
      "         [ 0.6368, -0.3946, -0.1211,  ...,  0.1979, -0.2988, -0.0688]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4501, -0.8580, -0.0122,  ...,  0.5072,  0.3542,  0.3090],\n",
      "         [-0.5460, -0.1500, -0.2781,  ...,  0.7151,  0.4605, -0.6890],\n",
      "         [-0.4940,  0.0110,  0.5319,  ..., -0.2389, -0.1140,  0.2818],\n",
      "         ...,\n",
      "         [-0.0864, -1.0441,  0.1951,  ..., -0.2176, -0.0698, -1.0587],\n",
      "         [-1.0022, -0.5642, -0.4120,  ..., -0.1635,  0.0159, -0.6024],\n",
      "         [ 0.8366, -0.3620, -0.2189,  ...,  0.4297, -0.3101, -0.2312]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5353, -0.3392, -0.5338,  ..., -0.1922,  0.1052, -0.6222],\n",
      "         [-0.7663, -0.5419, -0.2805,  ...,  0.3947,  0.9127, -0.3062],\n",
      "         [ 0.0812, -0.5087,  0.2119,  ...,  0.0175, -0.2183, -0.6718],\n",
      "         ...,\n",
      "         [-0.5642,  0.1096, -0.0577,  ..., -0.3375, -0.6153, -0.6680],\n",
      "         [-1.1855,  0.0929, -0.9514,  ..., -0.0334, -0.6244, -1.7533],\n",
      "         [ 0.7893, -0.2024, -0.0130,  ...,  0.1962, -0.2266, -0.1211]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.4681, -0.5031, -0.1415,  ..., -0.0182,  0.5094, -0.2583],\n",
      "         [-0.5599, -0.4281,  0.0485,  ...,  0.2101,  0.8070, -0.7107],\n",
      "         [ 0.1375, -0.1314, -0.1174,  ..., -0.3601,  0.6306, -0.3562],\n",
      "         ...,\n",
      "         [-1.0408, -0.5945, -0.0379,  ...,  0.1574, -0.2516, -0.2701],\n",
      "         [-1.3232, -0.1417, -0.8538,  ...,  0.4567, -0.1154, -1.8752],\n",
      "         [ 0.6850, -0.1927, -0.1683,  ...,  0.2944, -0.2981, -0.1778]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0034, -0.8228, -0.6853,  ...,  0.2262,  0.3225, -0.6723],\n",
      "         [-0.5873,  0.2196,  0.4900,  ...,  0.4927,  0.9828, -0.0339],\n",
      "         [-0.3798, -0.4148,  0.0196,  ..., -0.0641,  0.2257, -0.6162],\n",
      "         ...,\n",
      "         [-0.7136,  0.7532, -0.5556,  ..., -0.4310,  0.0635, -1.9076],\n",
      "         [-1.5777, -0.8657, -0.8661,  ...,  0.1371,  0.0859, -0.8753],\n",
      "         [ 0.8141, -0.2048, -0.1186,  ...,  0.1091, -0.4014, -0.1504]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3868, -0.2987, -0.8897,  ...,  0.2981,  0.3417, -0.9604],\n",
      "         [-0.5318, -0.2633, -0.5244,  ...,  0.5098,  0.1685, -0.6176],\n",
      "         [-1.2790, -0.4450,  0.1309,  ...,  0.1177,  0.3951,  0.4847],\n",
      "         ...,\n",
      "         [-1.1812, -0.9899, -0.7293,  ...,  0.0839,  0.1293, -0.3898],\n",
      "         [-0.7171, -0.5993, -0.3338,  ..., -0.2187, -0.1312, -1.0241],\n",
      "         [ 0.6086, -0.3433, -0.0967,  ...,  0.2364, -0.1663, -0.2589]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.0753, -1.0637, -0.8053,  ...,  0.2288, -0.0170, -0.2214],\n",
      "         [-0.3913, -0.2957,  0.2175,  ...,  0.2779,  0.1099, -0.0956],\n",
      "         [-0.5018, -0.6927, -0.4687,  ...,  0.1653,  0.1877, -1.0137],\n",
      "         ...,\n",
      "         [-0.9092, -0.6139,  0.5300,  ...,  0.0474, -0.0982, -0.9974],\n",
      "         [-1.1625, -1.1009, -0.5342,  ...,  0.1363,  0.4615, -0.6090],\n",
      "         [ 0.7632, -0.3679,  0.0562,  ...,  0.3663, -0.0869, -0.2518]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3823, -0.9298, -0.1301,  ..., -0.0526, -0.0623,  0.0843],\n",
      "         [-1.4351, -0.1520,  0.1501,  ...,  0.1407,  0.7550,  0.1865],\n",
      "         [-0.0297,  0.5330, -0.0106,  ...,  0.1940, -0.4130, -0.0279],\n",
      "         ...,\n",
      "         [-0.7373, -0.5326,  0.4997,  ..., -0.2266, -0.1581, -1.1174],\n",
      "         [-0.3882, -0.7615, -0.3363,  ..., -0.4096, -0.5919, -1.2019],\n",
      "         [ 0.6414, -0.1410, -0.1686,  ...,  0.1879, -0.2860, -0.3104]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1165, -0.8528, -0.7692,  ...,  0.2033,  0.2411, -0.0313],\n",
      "         [-0.4801, -0.6580,  0.0929,  ...,  0.4701,  0.6582, -0.2155],\n",
      "         [-0.2832, -0.0030,  0.3406,  ...,  0.0165,  0.1648, -1.0199],\n",
      "         ...,\n",
      "         [-1.3959, -0.6891,  0.1964,  ..., -0.2865,  0.2519, -1.0155],\n",
      "         [-0.4465, -0.7804, -0.0622,  ...,  0.0245, -0.0298, -1.1941],\n",
      "         [ 0.8230, -0.3091,  0.0437,  ...,  0.2674, -0.3158, -0.1906]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1045, -0.3895, -0.3371,  ...,  0.5222,  0.3146, -0.2915],\n",
      "         [-0.3033, -0.6383, -0.8209,  ...,  0.4550,  0.7619, -0.3254],\n",
      "         [-1.4275, -1.0567, -0.1266,  ..., -0.0407,  0.2662,  0.6659],\n",
      "         ...,\n",
      "         [-0.8956, -0.2237,  0.5841,  ..., -0.2842, -0.1798, -0.1541],\n",
      "         [-1.6670,  0.2354, -0.0751,  ..., -0.2591, -0.4141, -0.5985],\n",
      "         [ 0.7567, -0.2153,  0.1545,  ...,  0.3247, -0.5188, -0.1729]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4107, -0.8680, -0.4770,  ...,  0.0296,  0.3166, -0.3516],\n",
      "         [-0.1700,  0.2072, -0.1458,  ...,  0.3461,  0.4409, -1.0723],\n",
      "         [-0.9785, -0.2736,  0.2814,  ...,  0.7792,  0.6209,  0.8468],\n",
      "         ...,\n",
      "         [ 0.8461,  0.9303, -0.0679,  ...,  0.0573,  0.0520, -1.1941],\n",
      "         [-0.1133, -0.3240, -0.5196,  ..., -0.1553,  0.1900, -1.4688],\n",
      "         [ 0.7131,  0.1647, -0.0113,  ...,  0.2350, -0.3736, -0.1317]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.8503, -0.6006, -0.9177,  ...,  0.2028,  0.8211, -0.9317],\n",
      "         [-0.4326, -0.6074,  0.3566,  ...,  0.2104,  0.4117, -0.5120],\n",
      "         [ 0.0287, -0.2529, -0.0664,  ..., -0.0999,  0.7221, -1.3392],\n",
      "         ...,\n",
      "         [ 0.5308, -0.2866,  0.2197,  ..., -0.4947, -0.2081, -1.6976],\n",
      "         [-0.5273,  0.0677, -0.2639,  ..., -0.7229, -0.2758, -1.6028],\n",
      "         [ 0.7915, -0.4721, -0.2237,  ...,  0.2219, -0.4758, -0.4221]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.3348, -0.7303, -0.3780,  ...,  0.1935,  0.6796, -0.2522],\n",
      "         [-0.2546,  0.0498, -0.2347,  ...,  0.1629,  0.9844,  0.0121],\n",
      "         [-1.1962, -0.4394,  0.5369,  ...,  0.0446, -0.0416, -1.0359],\n",
      "         ...,\n",
      "         [-1.6266, -0.3895,  0.0846,  ...,  0.0391,  0.3106, -1.5594],\n",
      "         [-1.7504, -0.8119, -0.5356,  ..., -0.3017, -0.2710, -1.0123],\n",
      "         [ 0.7540, -0.3182,  0.0119,  ...,  0.1893, -0.2792, -0.2983]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2470, -0.4059, -0.1753,  ...,  0.2914,  0.9840, -0.2377],\n",
      "         [-1.2224, -0.7664,  0.1127,  ..., -0.4321,  0.8400,  0.9097],\n",
      "         [-1.4687, -0.2391, -0.4414,  ..., -0.5510,  0.2636, -0.6865],\n",
      "         ...,\n",
      "         [-0.7611,  0.5357,  0.3326,  ...,  0.1051,  0.0699, -1.2617],\n",
      "         [-0.5268,  0.0291,  0.3381,  ..., -0.1703, -0.2920, -1.6559],\n",
      "         [ 0.8871, -0.2497, -0.2773,  ...,  0.2963, -0.5897, -0.1219]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 3.2825e-04, -4.2962e-01, -7.4254e-01,  ...,  5.7705e-02,\n",
      "           7.1609e-01, -1.5653e-01],\n",
      "         [-7.5705e-01, -4.3560e-01, -6.6237e-01,  ...,  7.1791e-01,\n",
      "           1.4218e+00, -1.3306e+00],\n",
      "         [-1.1568e+00, -1.7065e-01,  2.7223e-02,  ...,  1.1776e-01,\n",
      "           6.0860e-01, -4.5087e-01],\n",
      "         ...,\n",
      "         [-1.6091e+00, -5.4877e-01, -4.9154e-01,  ..., -3.0105e-01,\n",
      "           1.8009e-01, -4.6502e-01],\n",
      "         [-3.7405e-01, -6.9951e-01, -5.0864e-01,  ..., -1.9475e-01,\n",
      "           1.6677e-01, -2.1635e+00],\n",
      "         [ 6.7424e-01, -3.0790e-01, -9.4951e-02,  ...,  2.4459e-01,\n",
      "          -2.6612e-01, -2.8027e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.4178, -0.2794, -0.7931,  ...,  0.5882,  0.0748,  0.2627],\n",
      "         [-0.6367,  0.0877,  0.1743,  ...,  0.0411,  0.4357, -1.1243],\n",
      "         [-0.4408, -1.1374,  0.2406,  ...,  0.4606,  0.3862, -0.0343],\n",
      "         ...,\n",
      "         [-1.4546, -0.2209, -0.4819,  ...,  0.3344, -0.1490, -0.6815],\n",
      "         [-1.8924, -0.2426, -0.3370,  ...,  0.1886, -0.2279, -0.0476],\n",
      "         [ 0.6257, -0.2011, -0.2378,  ...,  0.4348, -0.3044, -0.0754]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[ 0.1530, -0.6805, -0.6340,  ...,  0.4376,  0.3339, -0.5422],\n",
      "         [-1.1228, -0.8644,  0.3350,  ...,  0.2478,  1.1911, -0.4329],\n",
      "         [-0.7444, -0.4888,  0.0098,  ...,  0.6140,  0.3541, -1.9935],\n",
      "         ...,\n",
      "         [ 0.5419, -0.5966,  0.6048,  ..., -0.7611,  0.1599, -0.3887],\n",
      "         [ 0.4550,  0.6304, -0.3518,  ..., -0.1115, -0.5443, -1.1363],\n",
      "         [ 0.6680, -0.2213, -0.1907,  ...,  0.2883, -0.2629, -0.1465]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0247, -0.8225, -0.7019,  ...,  0.3138,  0.4227, -0.5151],\n",
      "         [-0.9415, -0.4425, -0.0989,  ...,  0.1451,  0.2926, -1.4248],\n",
      "         [-1.0571, -0.3129,  0.3378,  ...,  0.3150,  0.0848, -0.9195],\n",
      "         ...,\n",
      "         [-0.5535, -0.5682,  0.2976,  ...,  0.1229,  0.1467, -0.3482],\n",
      "         [-0.7822, -0.2969, -0.2737,  ..., -0.2449, -0.1625, -1.7627],\n",
      "         [ 0.6725, -0.0673,  0.1879,  ...,  0.2266, -0.3249, -0.2137]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.2685, -0.3440, -0.6509,  ...,  0.5732,  0.5415, -0.4692],\n",
      "         [-0.9716, -0.3905, -0.5080,  ...,  0.5580,  0.7461, -0.7761],\n",
      "         [-1.1093,  0.2272, -0.5361,  ..., -0.6062,  0.3875, -0.0297],\n",
      "         ...,\n",
      "         [-1.6627, -0.2351,  0.2104,  ..., -0.2784,  0.1590, -0.5169],\n",
      "         [-0.2635, -0.6908, -0.3229,  ..., -0.0988, -0.0025, -0.7670],\n",
      "         [ 0.4807, -0.4582, -0.0865,  ...,  0.2546, -0.1089, -0.2541]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1671, -1.2056, -0.8134,  ...,  0.4204,  0.2601, -0.1874],\n",
      "         [-0.3638, -0.4944,  0.0673,  ...,  0.6939,  0.4523, -0.7871],\n",
      "         [-1.1583,  0.0032,  0.0969,  ...,  0.1458,  0.3627, -1.0079],\n",
      "         ...,\n",
      "         [-1.2231, -0.0521,  0.0663,  ..., -0.0068, -0.2979, -1.0616],\n",
      "         [-0.0192, -0.2607,  0.4251,  ...,  0.3900, -0.2388, -1.2735],\n",
      "         [ 0.4343, -0.2467,  0.0505,  ...,  0.2369, -0.4714, -0.2096]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1390, -0.4514, -0.7784,  ...,  0.1502,  0.8478, -0.1216],\n",
      "         [-0.6751, -0.2058,  0.0669,  ...,  0.1086,  1.0064, -0.7984],\n",
      "         [-0.7916,  0.0043, -0.9093,  ..., -0.4992,  0.6343,  0.1010],\n",
      "         ...,\n",
      "         [ 1.3124,  0.4205,  0.3952,  ..., -0.3557,  0.1536, -0.5769],\n",
      "         [-0.4922, -0.4081, -0.5591,  ..., -0.5142,  0.3647, -0.4114],\n",
      "         [ 0.9807, -0.2101, -0.2725,  ...,  0.5299, -0.4162, -0.2239]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.0296, -0.8519, -0.0595,  ...,  0.1409,  0.1053,  0.2307],\n",
      "         [-0.2586,  0.0194, -0.1490,  ..., -0.0117,  0.2063, -0.2112],\n",
      "         [-1.1198, -0.3113,  0.7577,  ..., -0.3466, -0.5428, -0.4302],\n",
      "         ...,\n",
      "         [-0.0150,  0.2054,  0.2398,  ..., -0.0485, -0.2530, -0.3193],\n",
      "         [ 0.3022, -0.2264,  0.6173,  ..., -0.5490, -0.1787, -0.2849],\n",
      "         [ 0.6799, -0.5994,  0.1524,  ...,  0.3905, -0.3558, -0.1332]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[ 0.1312,  0.1163, -0.6718,  ...,  0.1718, -0.0403,  0.0273],\n",
      "         [-0.3977,  0.4327, -0.1644,  ...,  0.1695,  0.4852, -0.0869],\n",
      "         [-0.3326, -0.2119,  0.2034,  ...,  0.6480,  0.6601,  0.7283],\n",
      "         ...,\n",
      "         [ 0.7129,  0.3830,  0.7140,  ..., -0.5122, -0.2849, -1.2198],\n",
      "         [-0.4060,  0.2164,  0.2732,  ..., -0.1175,  0.1714, -1.1588],\n",
      "         [ 0.4239, -0.0200,  0.0455,  ...,  0.2837, -0.4092, -0.1156]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-1.2545e-02, -6.3583e-01, -1.0802e+00,  ...,  1.9974e-01,\n",
      "           8.9081e-01, -3.8245e-01],\n",
      "         [-3.6378e-01, -8.4567e-02, -3.0875e-02,  ...,  3.9450e-01,\n",
      "           8.5219e-01, -7.3441e-01],\n",
      "         [-5.1632e-01, -3.9585e-01,  6.5002e-02,  ..., -5.6336e-02,\n",
      "           1.0847e-01, -4.1815e-02],\n",
      "         ...,\n",
      "         [ 1.0678e-03,  2.4221e-01,  1.7107e-01,  ..., -1.4794e-01,\n",
      "          -2.3783e-01, -1.2690e+00],\n",
      "         [-8.3503e-01, -1.2386e+00,  2.3721e-02,  ...,  2.0541e-02,\n",
      "          -2.4004e-01, -6.2429e-01],\n",
      "         [ 7.9630e-01, -4.7337e-01,  2.1636e-02,  ...,  3.7866e-01,\n",
      "          -2.0910e-01, -3.4064e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.1391, -0.8963, -0.4958,  ..., -0.2663,  0.5716, -0.1352],\n",
      "         [-0.8067, -0.5469, -0.6381,  ...,  0.3032,  1.0262, -0.9111],\n",
      "         [-0.9695, -0.7159,  0.1114,  ..., -0.0435,  0.3678, -0.1388],\n",
      "         ...,\n",
      "         [-0.7239, -0.3851,  0.4282,  ..., -0.0428,  0.2251, -0.8310],\n",
      "         [-0.2325, -0.7104,  0.1409,  ..., -0.0422,  0.0561, -0.3489],\n",
      "         [ 0.5064, -0.3330,  0.0727,  ...,  0.1355, -0.1819, -0.2478]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output  tensor([[[-0.5377, -0.6225,  0.2350,  ..., -0.1358,  0.0373, -0.3619],\n",
      "         [-0.2619, -0.0566, -0.1142,  ...,  0.3398,  1.1190, -0.5100],\n",
      "         [-0.6452,  0.2231, -0.3476,  ..., -1.0531,  0.8687, -0.0623],\n",
      "         ...,\n",
      "         [ 0.0667, -1.1343,  0.6681,  ...,  0.0103, -0.2750, -1.0385],\n",
      "         [-1.8653, -0.4480,  0.1353,  ...,  0.4010, -0.5556, -1.2673],\n",
      "         [ 0.8015, -0.1655,  0.0107,  ...,  0.1005, -0.4561,  0.0413]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output  tensor([[[ 0.1371, -0.7355, -0.7611,  ...,  0.2172,  0.7229,  0.4144],\n",
      "         [-1.0908, -0.2585,  0.4821,  ...,  0.0480,  0.4255,  0.5271],\n",
      "         [-0.6043, -0.1051,  0.9984,  ...,  0.0835,  0.1237,  0.6393],\n",
      "         ...,\n",
      "         [-0.5240,  0.0725,  0.2854,  ..., -0.3695, -0.3517, -0.8445],\n",
      "         [-1.5426, -0.8256, -0.7887,  ...,  0.0276, -0.1862, -0.5750],\n",
      "         [ 0.7379, -0.4366, -0.0169,  ...,  0.3458, -0.2573, -0.1992]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([2.3413], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.0835, -0.7769, -0.8853,  ...,  0.2829,  0.0847, -0.3342],\n",
      "         [-1.0367, -0.3683, -0.9457,  ...,  0.6232,  1.3266, -0.9772],\n",
      "         [-1.2948, -0.0451, -0.0429,  ..., -0.0750,  0.5539, -0.0289],\n",
      "         ...,\n",
      "         [-0.4958,  0.1678, -0.1211,  ..., -0.0948, -0.3127, -1.4513],\n",
      "         [-0.3619, -0.5610, -0.2894,  ..., -0.6306, -0.1021, -1.7603],\n",
      "         [ 0.7948, -0.1350, -0.1848,  ...,  0.2416, -0.5014, -0.2545]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([2.2115], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4695, -0.1955, -0.7264,  ..., -0.2134,  0.0603, -0.4386],\n",
      "         [-0.2401,  0.1401, -0.5894,  ...,  0.4562,  1.2213, -0.3533],\n",
      "         [-1.1134,  0.3507,  0.0281,  ..., -0.1818,  0.6384,  0.7048],\n",
      "         ...,\n",
      "         [ 0.7747,  0.3106, -0.3364,  ..., -0.1087,  0.1143, -0.9956],\n",
      "         [ 0.0923, -0.1447, -0.2308,  ...,  0.1479,  0.1401, -1.2240],\n",
      "         [ 0.5638, -0.0308, -0.0366,  ...,  0.1418, -0.4498, -0.2705]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9639], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9639], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[ 1.6491e-01, -6.4867e-01, -8.0516e-01,  ...,  7.1193e-02,\n",
      "           4.9026e-01,  7.4607e-02],\n",
      "         [ 3.1647e-01,  4.6159e-01,  2.8772e-01,  ..., -6.0698e-02,\n",
      "           5.0521e-01,  4.9281e-01],\n",
      "         [-7.4640e-01, -3.9286e-01, -8.6214e-02,  ...,  6.8824e-03,\n",
      "           1.8896e-01,  1.0732e-01],\n",
      "         ...,\n",
      "         [-8.3612e-01, -4.7824e-01,  1.7912e-01,  ...,  1.3470e-01,\n",
      "           2.8461e-01, -3.4601e-01],\n",
      "         [-1.6535e+00, -3.7479e-02, -2.5678e-01,  ...,  2.7439e-01,\n",
      "          -1.0048e-03, -1.2665e+00],\n",
      "         [ 7.3601e-01, -1.5710e-01,  8.8969e-02,  ...,  2.3198e-01,\n",
      "          -3.6766e-01, -2.1946e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9604], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9604], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-3.3891e-01, -5.4386e-01, -9.8007e-01,  ...,  2.8589e-01,\n",
      "           8.0726e-01, -7.4222e-01],\n",
      "         [-4.1661e-01, -3.9427e-01, -5.1509e-01,  ...,  4.8383e-01,\n",
      "           5.7693e-01, -4.7119e-01],\n",
      "         [-1.0215e+00, -4.2944e-01,  8.4916e-04,  ...,  4.4497e-01,\n",
      "           3.7893e-01,  8.7421e-01],\n",
      "         ...,\n",
      "         [-1.5806e+00, -9.6257e-01, -6.4634e-01,  ...,  1.3726e-01,\n",
      "           1.5335e-01, -3.3670e-01],\n",
      "         [-3.2871e-01, -6.5937e-01, -6.0968e-01,  ..., -1.2433e-01,\n",
      "          -2.0939e-01, -1.0742e+00],\n",
      "         [ 6.2611e-01, -2.9765e-01, -1.9670e-01,  ...,  1.6185e-01,\n",
      "          -3.3926e-01, -2.8531e-01]]], device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9634], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9634], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1105, -0.8221, -0.5806,  ..., -0.4520,  0.6657, -0.0764],\n",
      "         [-1.0272, -0.6069, -0.6836,  ...,  0.2881,  0.8564, -0.7145],\n",
      "         [-0.8952, -0.7956, -0.0163,  ..., -0.1042,  0.4228, -0.1082],\n",
      "         ...,\n",
      "         [-0.7474, -0.2849,  0.1916,  ..., -0.1886,  0.2643, -0.9750],\n",
      "         [-0.0848, -0.6868,  0.0897,  ...,  0.0458, -0.2957, -0.1313],\n",
      "         [ 0.7126, -0.2975,  0.0252,  ...,  0.1672, -0.1905, -0.2686]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9596], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9596], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[ 0.1626, -0.1029, -1.0162,  ...,  0.6159,  0.4994,  0.4097],\n",
      "         [-0.0731, -0.1140, -0.3093,  ...,  0.1312,  0.9553,  0.4556],\n",
      "         [-0.3409, -0.2909,  0.2169,  ..., -0.2127,  0.7855, -0.0717],\n",
      "         ...,\n",
      "         [-0.4892, -0.0622, -0.2453,  ..., -0.0121, -0.2998, -0.0012],\n",
      "         [-0.4860, -0.3416,  0.3569,  ...,  0.1499, -0.5360, -0.6480],\n",
      "         [ 0.5143, -0.2599, -0.2338,  ...,  0.3556, -0.5084, -0.2993]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([1.0422], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1170, -0.9619, -0.1440,  ..., -0.0017,  0.0464,  0.3116],\n",
      "         [-0.2719,  0.0838, -0.0572,  ...,  0.3393,  0.2228, -0.2167],\n",
      "         [-0.9586, -0.2969,  0.6141,  ..., -0.3627, -0.4876, -0.4694],\n",
      "         ...,\n",
      "         [ 0.1240,  0.2729,  0.3166,  ..., -0.0980, -0.3036, -0.3389],\n",
      "         [ 0.0314, -0.2264,  0.6490,  ..., -0.6313, -0.6049, -0.2300],\n",
      "         [ 0.6628, -0.5442,  0.1576,  ...,  0.3794, -0.4241, -0.1149]]],\n",
      "       device='cuda:0')\n",
      "pred answer_score: tensor([1.6501], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.4513, -0.7264, -0.4809,  ...,  0.1096,  0.7841, -0.0972],\n",
      "         [-0.1902,  0.2292, -0.2775,  ...,  0.1750,  0.9151, -0.0427],\n",
      "         [-1.1015, -0.2444,  0.2463,  ...,  0.0110, -0.0107, -0.8606],\n",
      "         ...,\n",
      "         [-1.8141, -0.4719, -0.0053,  ...,  0.1142,  0.4372, -1.2508],\n",
      "         [-1.9183, -0.7787, -0.5037,  ..., -0.2568, -0.1587, -0.9605],\n",
      "         [ 0.6854, -0.3251, -0.0922,  ...,  0.2106, -0.3434, -0.2401]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9348], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9348], device='cuda:0')\n",
      "pred answer_text: null\n",
      "sequence_output  tensor([[[-0.1092, -0.9856, -0.7037,  ..., -0.1325,  0.3286,  0.0485],\n",
      "         [ 0.3310, -0.1162,  0.5331,  ...,  0.7633,  0.8866, -0.7035],\n",
      "         [-0.9918, -0.8876, -0.0734,  ...,  0.1767,  0.2122, -0.2052],\n",
      "         ...,\n",
      "         [-0.9811, -0.1787, -0.2445,  ...,  0.5253,  0.2429, -0.5018],\n",
      "         [-0.5536, -0.5663, -0.3267,  ...,  0.3241, -0.1497, -1.1891],\n",
      "         [ 0.6431,  0.0266, -0.0803,  ...,  0.2860, -0.3552, -0.1792]]],\n",
      "       device='cuda:0')\n",
      "pred answers: [{'text': 'null', 'score': tensor([0.9604], device='cuda:0')}]\n",
      "pred answer_score: tensor([0.9604], device='cuda:0')\n",
      "pred answer_text: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: avg_val_f1  was not in top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(7.5525, device='cuda:0')\tavg_answer_loss:  tensor(3.5354, device='cuda:0')\tavg_type_loss:  tensor(0.8034, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "    #     if not args.test: \n",
    "    trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_3.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 4)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 512)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', False)\n",
      "('model_path', None)\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n",
      "main\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json  --dev_dataset small_3.json  \\\n",
    "    --gpus 0 --num_workers 4 \\\n",
    "    --max_seq_len 512 \\\n",
    "    --save_prefix hotpotqa-longformer'\n",
    "# --train_dataset hotpot_train_v1.1.json  --dev_dataset hotpot_dev_distractor_v1.json  \\\n",
    "\n",
    "import argparse \n",
    "# if __name__ == \"__main__\":\n",
    "main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "args = parser.parse_args(shlex.split(argString)) \n",
    "for arg in vars(args):\n",
    "    print((arg, getattr(args, arg)))\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from transformers import DistilBertTokenizer, DistilBertModel\n",
    ">>> import torch\n",
    ">>> tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    ">>> model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
