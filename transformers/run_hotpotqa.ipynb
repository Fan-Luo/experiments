{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   \n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "# sys.path.insert(0, '/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.7/site-packages') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "#!pip install -U transformers\n",
    "#!pip install torch==1.6.0 torchvision==0.7.0\n",
    "import transformers \n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null\"\n",
    "\n",
    " \n",
    "def create_example_dict(context, answer, id, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answer\": answer,\n",
    "                \"id\": id,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tqdm \n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git \n",
    "# !python -m pip install pytorch-lightning==0.6.0\n",
    "# !python -m pip install jdc  \n",
    "# !wget https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/longformer-base-4096.tar.gz\n",
    "# !tar -xf longformer-base-4096.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )    \n",
    "        \n",
    "        for lst in raw_contexts:\n",
    "            lst[0] = _normalize_text(lst[0])\n",
    "            lst[1] = [_normalize_text(sent) for sent in lst[1]]\n",
    "        \n",
    "        contexts = [lst[0]  + ' '  + ' '.join(lst[1]) for lst in raw_contexts]    \n",
    "        # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space; call the _normalize_text() which is same as the one used during evaluation\n",
    "        \n",
    "#         context = \" </s> \".join(contexts)\n",
    "#         print(context)\n",
    "        \n",
    "#         exit(0)\n",
    "\n",
    "        \n",
    "        answer = _normalize_text(example[\"answer\"]) \n",
    "#         print(\"answer: \", answer)\n",
    "        if(len(answer) > 0):   # answer can be '' after normalize\n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=contexts,\n",
    "                        answer=answer,\n",
    "                        id = example[\"_id\"],\n",
    "                        question=_normalize_text(example[\"question\"]),\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def _normalize_text(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": [\n",
      "        \"dr seuss how grinch stole christmas dr seuss how grinch stole christmas is video game based on dr seuss book with same name but mostly based on film game was released on november 8 2007\",\n",
      "        \"lorax film lorax also known as dr seuss lorax is 2012 american 3d computeranimated musical fantasy\\u2013comedy film produced by illumination entertainment and based on dr seusss childrens book of same name film was released by universal pictures on march 2 2012 on 108th birthday of dr seuss second film adaptation of book following 1972 animated television special film builds on book by expanding story of ted previously unnamed boy who visits onceler cast includes danny devito as lorax ed helms as onceler and zac efron as ted new characters introduced in film are audrey voiced by taylor swift aloysius ohare rob riggle mrs wiggins teds mother jenny slate and grammy norma betty white\",\n",
      "        \"horton hears who tv special horton hears who is 1970 television special based on dr seuss book of same name horton hears who  it was produced and directed by chuck jones \\u2013 who previously produced seuss special how grinch stole christmas \\u2013 for mgm television and first broadcast march 19 1970 on cbs special contains songs with lyrics by seuss and music by eugene poddany who previously wrote songs for seuss book cat in hat song book\",\n",
      "        \"dr seuss memorial dr seuss national memorial sculpture garden is sculpture garden in springfield massachusetts that honors theodor seuss geisel better known to world as dr seuss located at quadrangle dr seuss national memorial sculpture garden honors author and illustrator who was born in springfield in 1904 monument was designed by lark grey dimondcates authors stepdaughter and created by sculptor and artist ron henson\",\n",
      "        \"dr seuss bibliography theodor seuss geisel better known as dr seuss published over 60 childrens books over course of his long career though most were published under his wellknown pseudonym dr seuss he also authored over dozen books as theo lesieg and one as rosetta stone as one of most popular childrens authors of all time geisels books have topped many bestseller lists sold over 222 million copies and been translated into more than 15 languages in 2000 when publishers weekly compiled their list of bestselling childrens books of all time 16 of top 100 hardcover books were written by geisel including green eggs and ham at number 4 cat in hat at number 9 and one fish two fish red fish blue fish at number 13 and dr seusss abc in years following his death in 1991 several additional books based on his sketches and notes were published including hooray for diffendoofer day and daisyhead mayzie although they were all published under name dr seuss only my many colored days originally written in 1973 was entirely by geisel\",\n",
      "        \"how grinch stole christmas 2018 film dr seuss how grinch stole christmas promoted theatrically as dr seuss grinch is upcoming american 3d computeranimated christmas musical comedy film produced by illumination entertainment it is based on 1957 dr seuss story of same name film will be released on november 9 2018 by universal pictures\",\n",
      "        \"do you know what im going to do next saturday do you know what im going to do next saturday is 1963 childrens book published by beginner books and written by helen palmer geisel first wife of theodor seuss geisel dr seuss unlike most of beginner books do you know what im going to do next saturday did not follow format of text with inline drawings being illustrated with blackandwhite photographs by lynn fayman featuring boy named rawli davis it is sometimes misattributed to dr seuss himself books cover features photograph of young boy sitting at breakfast table with huge pile of pancakes\",\n",
      "        \"wubbulous world of dr seuss wubbulous world of dr seuss is liveactionpuppet television series based on characters created by dr seuss produced by jim henson company it aired from october 13 1996 to december 28 1998 on nickelodeon it is notable for its use of live puppets with digitally animated backgrounds and in its first season for refashioning characters and themes from original dr seuss books into new stories that often retained much of flavor of dr seuss own works\",\n",
      "        \"cat in hat film dr seuss cat in hat is 2003 american family comedy film directed by bo welch it is based on 1957 dr seuss book of same name film stars mike myers in title role of cat in hat and dakota fanning as sally sallys brother who is unnamed in book and 1971 tv special conrad is portrayed by spencer breslin film is second featurelength dr seuss adaptation after 2000 holiday film how grinch stole christmas\",\n",
      "        \"kyle balda kyle balda is american animator and film director best known for codirecting animated films lorax 2012 with chris renaud and minions 2015 with pierre coffin he has also worked as animator on several films including jumanji toy story 2 and despicable me he has worked for pixar for years and now he is working for illumination entertainment\"\n",
      "      ],\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answer\": \"lorax\",\n",
      "          \"id\": \"5ab990925542996be2020553\",\n",
      "          \"question\": \"what film did kyle balda work on that was based on dr seuss book\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/Data')\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[76200:76280]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[37:50]' > small_dev.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[31:50]' > sample.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/reduced_questions/hotpot_reduced_context_04-08-2021-01:12:53/hotpot_dev_reduced_context_coref_fuzzy.json | ../../helper/jq-linux64 -c '.[6666:7000]' > small_dev.json\n",
    "# !cat small_dev.json | ../../helper/jq-linux64 -c '.[230:235]' > small_dev2.json\n",
    "        \n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "\n",
    "# !pip uninstall longformer -y\n",
    "# !python -m pip uninstall longformer -y\n",
    "# !pip install git+https://github.com/allenai/longformer.git \n",
    "# !python -m pip uninstall pytorch-lightning -y\n",
    "# !pip uninstall pytorch-lightning -y\n",
    "# !python -m pip install git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning\n",
    "#!pip install torch==1.6.0 torchvision==0.7.0\n",
    " \n",
    "\n",
    "\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "#!python -m pip install jdc \n",
    "# !pip install test-tube \n",
    "#!python -m pip install ipywidgets \n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        if(\"reduced_context\" not in self.file_path):\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "                \n",
    "        else:\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = json.load(f)['data']            \n",
    "                print(self.data_json[0])\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "\n",
    "#         print(tokenizer.all_special_tokens) \n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =                  {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        if(len(tensors_list) != 1):\n",
    "            print(\"tensors_list: \", tensors_list)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     106,
     122,
     147,
     162
    ]
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset):\n",
    "            # char offset to word offset\n",
    "            if(answer_start >= len(char_to_word_offset)):\n",
    "                print(\"answer_start: \", answer_start)\n",
    "                print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "            # char offset to word offset\n",
    "            start_word_position = char_to_word_offset[answer_start]\n",
    "            end_word_position = char_to_word_offset[answer_end-1] \n",
    "\n",
    "#             print(\"start_word_position: \", start_word_position)\n",
    "#             print(\"end_word_position: \", end_word_position)\n",
    "            # sub_tokens postion reletive to context\n",
    "            tok_start_position_in_doc = orig_to_tok_index[start_word_position]  \n",
    "            not_end_of_doc = int(end_word_position + 1 < len(orig_to_tok_index))\n",
    "            tok_end_position_in_doc = orig_to_tok_index[end_word_position + not_end_of_doc] - not_end_of_doc\n",
    "            \n",
    "            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                return (-1, -1) # this answer is outside the current slice                     \n",
    "            \n",
    "            # sub_tokens postion reletive to begining of all the tokens, including query sub tokens  \n",
    "            start_position = tok_start_position_in_doc + doc_offset  \n",
    "            end_position = tok_end_position_in_doc + doc_offset\n",
    "            \n",
    "            return (start_position, end_position)\n",
    "        \n",
    "#         print(\"idx: \", idx)\n",
    "#         print(\"len(example): \", \"len(example)\")\n",
    "        if(len(example[\"paragraphs\"])==0):\n",
    "            print(\"idx: \", idx, \"'s len(example[‘paragraphs’])==0\")\n",
    "\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            # print(\"for paragraph in example['paragraphs']: \") \n",
    "            context = self.tokenizer.sep_token + ' ' + (' ' + self.tokenizer.sep_token + ' ').join(paragraph[\"context\"] )   \n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in context:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            \n",
    "#             print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "#             print(\"char_to_word_offset: \", char_to_word_offset)\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "                # print(\"question text: \", question_text)  \n",
    "                # sp_sent = qa[\"is_sup_fact\"]\n",
    "                # sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None \n",
    "\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent) \n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    " \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                \n",
    "                   \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                 \n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                \n",
    "                # each original token in the context is tokenized to multiple sub_tokens\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "                \n",
    "                # all sub tokens, truncate up to limit\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len-7] \n",
    "\n",
    "                # The -7 accounts for CLS, QUESTION_START, QUESTION_END， [/par]， yes， no， </s>   \n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 7\n",
    "                if(max_tokens_per_doc_slice <= 0):\n",
    "                    print(\"(max_tokens_per_doc_slice <= 0)\")\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                # sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                # sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "                if(len(all_doc_tokens) == 0):\n",
    "                    print(\"idx: \", idx, \" len(all_doc_tokens) == 0\")\n",
    "#               \n",
    "                \n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                \n",
    "                    # print(\"slice_start in range\") \n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [self.tokenizer.cls_token] + [QUESTION_START] + query_tokens + [QUESTION_END] + doc_slice_tokens + [PAR] + self.tokenizer.tokenize(\"yes\") + self.tokenizer.tokenize(\"no\") + [self.tokenizer.eos_token]   \n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] * (len(doc_slice_tokens) + 4) \n",
    "#                     if(len(segment_ids) != len(tokens)):\n",
    "#                         print(\"len(segment_ids): \", len(segment_ids))\n",
    "#                         print(\"len(tokens): \", len(tokens))\n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    \n",
    "                    # ===== answer positions tensors  ============\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    " \n",
    "                    answer = qa[\"answer\"] \n",
    "                    # print(\"idx: \", idx, \" qa['id']: \", qa['id'], \" answer: \", answer)\n",
    "                    if answer == '':\n",
    "                        q_type = -1\n",
    "                        start_positions.append(-1)   \n",
    "                        end_positions.append(-1)           \n",
    "                    \n",
    "                    elif answer == 'yes':\n",
    "                        q_type = 1\n",
    "                        start_positions.append(len(tokens)-3)   \n",
    "                        end_positions.append(len(tokens)-3) \n",
    "                    elif answer == 'no':\n",
    "                        q_type = 2\n",
    "                        start_positions.append(len(tokens)-2)   \n",
    "                        end_positions.append(len(tokens)-2)  \n",
    "                    else:\n",
    "                        # keep all the occurences of answer in the context \n",
    "#                         for m in re.finditer(\"\\s?\".join(answer.split()), context):   # \"\\s?\".join(answer.split()) in order to match even with extra space in answer or context\n",
    "                        for m in re.finditer(_normalize_text(answer), context, re.IGNORECASE):\n",
    "                            answer_start, answer_end = m.span() \n",
    "                            start_position, end_position = map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset)\n",
    "                            if(start_position != -1):\n",
    "                                start_positions.append(start_position)   \n",
    "                                end_positions.append(end_position)\n",
    "                            \n",
    "                        if(len(start_positions) > 0): \n",
    "                            q_type = 0\n",
    "                        else: # answer not found in context\n",
    "                            q_type = -1\n",
    "                            start_positions.append(-1)   \n",
    "                            end_positions.append(-1) \n",
    "\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        \n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "                    \n",
    "                                         \n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "                        \n",
    "                        print(\"self.doc_stride >= 0\")\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len  \n",
    "                        \n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                    \n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                    #   torch.tensor([sp_sent_list]),  torch.tensor([sp_para_list]),\n",
    "                                     qa['id'], answer))     \n",
    "        return tensors_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 2  # qid and answer  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7f6ed80eec50>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    " \n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [TITLE_START, TITLE_END, SENT_MARKER_END, QUESTION_START , QUESTION_END, PAR]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "         \n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 3)   #  question type (yes/no/span/null) classification \n",
    "\n",
    "#         self.fnn_sp_sent = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "        \n",
    "#         self.fnn_sp_para = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "         \n",
    "        \n",
    "        self.train_dataloader_object = self.val_dataloader_object = self.test_dataloader_object = None\n",
    "        \n",
    " \n",
    "    def load_model(self):\n",
    "        \n",
    "        config = LongformerConfig.from_pretrained(self.args.model_path) \n",
    "        # choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n",
    "        # 'n2': for regular n2 attantion\n",
    "        # 'tvm': a custom CUDA kernel implementation of our sliding window attention\n",
    "        # 'sliding_chunks': a PyTorch implementation of our sliding window attention\n",
    "        config.attention_mode = 'sliding_chunks'\n",
    "        model = Longformer.from_pretrained(self.args.model_path, config=config)\n",
    "\n",
    "        print(\"self.args.model_path: \", self.args.model_path)\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "#%%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "###################################################### dataloaders ########################################################### \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl  \n",
    "        return self.train_dataloader_object\n",
    "    \n",
    " \n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "\n",
    "        \n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataloader_object is not None:\n",
    "            return self.test_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.test_dataloader_object = dl\n",
    "        return self.test_dataloader_object\n",
    "\n",
    "#%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type):  #, sp_sent, sp_para):\n",
    " \n",
    " \n",
    "        if 'longformer' in self.args.model_path:\n",
    "            \n",
    "            if(input_ids.size(0) != 1):\n",
    "                print(\"input_ids.size(0) != 1\")\n",
    "            assert(input_ids.size(0)==1)\n",
    "            # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "            # Make sure all rows have the same question length.\n",
    "            \n",
    "#             print(\"start_positions: \", start_positions)\n",
    "#             print(\"end_positions: \", end_positions)\n",
    "            # local attention everywhere\n",
    "            attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "            \n",
    "            # global attention for the cls and all question tokens\n",
    "            # print(\"question_end_index\")\n",
    "            question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "    #         if(question_end_index.size(0) == 1):\n",
    "    #             attention_mask[:,:question_end_index.item()] = 2  \n",
    "    #         else:\n",
    "            attention_mask[:,:question_end_index[0].item()+1] = 2  # from <cls> until </q>\n",
    "    #             print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "            \n",
    "            # global attention for the sentence and paragraph special tokens  \n",
    "            # print(\"sent_indexes\")\n",
    "            sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "            attention_mask[:, sent_indexes] = 2\n",
    "            \n",
    "            # print(\"para_indexes\")\n",
    "            para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "            attention_mask[:, para_indexes] = 2       \n",
    "             \n",
    "    \n",
    "            # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "            input_ids, attention_mask = pad_to_window_size(\n",
    "                input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "    \n",
    "            sequence_output = self.model(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask)[0]\n",
    "    #         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "    \n",
    "            # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "            # before computing loss and decoding.\n",
    "            padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "            if padding_len > 0:\n",
    "                sequence_output = sequence_output[:, :-padding_len]\n",
    "    #         print(\"size of sequence_output after removing padding: \" + str(sequence_output.size()))\n",
    "        else:\n",
    "            sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]      \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1) \n",
    "        start_logits = start_logits.squeeze(-1) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    " \n",
    "        ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "        type_logits = self.linear_type(sequence_output[:,0]) \n",
    "        \n",
    "        # ### 3. supporting paragraph classification ###  \n",
    "        # sp_para_output = sequence_output[:,para_indexes,:]  \n",
    "        # sp_para_output_t = self.fnn_sp_para(sp_para_output) \n",
    "\n",
    "        #  # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no.   \n",
    "        # # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "        # sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "        # predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous() \n",
    " \n",
    "        # ### 4. supporting fact classification ###     \n",
    "        # # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    " \n",
    "        # sp_sent_output = sequence_output[:,sent_indexes,:]  \n",
    "        # sp_sent_output_t = self.fnn_sp_sent(sp_sent_output)     \n",
    "        # sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "        # predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous() \n",
    "        \n",
    "        answer_loss, type_loss = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits)\n",
    "        # answer_loss, type_loss, sp_para_loss, sp_sent_loss  = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)\n",
    "         \n",
    " \n",
    "        return answer_loss, type_loss, start_logits, end_logits, type_logits\n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits):  #, sp_para, predict_support_para, sp_sent, predict_support_sent):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    " \n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "  \n",
    "\n",
    "            else: \n",
    "                start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = crossentropy(start_logits, start_positions[:, 0])\n",
    "                end_loss = crossentropy(end_logits, end_positions[:, 0])\n",
    "                \n",
    " \n",
    "            crossentropy = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            type_loss = crossentropy(type_logits, q_type)  \n",
    "            \n",
    "            # crossentropy_average = torch.nn.CrossEntropyLoss(reduction = 'mean', ignore_index=-1)      \n",
    "            # sp_para_loss = crossentropy_average(predict_support_para.view(-1, 2), sp_para.view(-1))\n",
    "            # sp_sent_loss = crossentropy_average(predict_support_sent.view(-1, 2), sp_sent.view(-1))      \n",
    " \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss, type_loss#, sp_para_loss, sp_sent_loss  \n",
    "\n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_tokens):\n",
    "        \n",
    "        if(input_ids.size(0)!=1):\n",
    "            print(\"input_ids.size(0): \", input_ids.size(0))\n",
    "            print(\"input_ids: \", input_ids)\n",
    "        \n",
    "        assert(input_ids.size(0)==1) \n",
    "        mask = input_ids != input_ids # initilaize \n",
    "        for special_token in special_tokens:\n",
    "            mask = torch.logical_or(mask, input_ids.eq(self.tokenizer.convert_tokens_to_ids(special_token))) \n",
    " \n",
    "        token_indices = torch.nonzero(mask, as_tuple=False)    \n",
    "         \n",
    " \n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    " \n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "    \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    " \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    " \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    " \n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    " \n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm) \n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "#         loss = torch.tanh(loss)\n",
    "#         print(\"final loss: \" + str(loss)) \n",
    "        return loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.nonzero(sp_sent, as_tuple=True)[1]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2\n",
    "\n",
    "\n",
    "# import torch\n",
    "# special_tokens = [1,2]\n",
    "# input_ids = torch.tensor([[ 1, 0, 2, 1, 0, 2]])\n",
    "\n",
    "# mask = input_ids != input_ids # initilaize \n",
    "# for special_token in special_tokens:\n",
    "#     mask = torch.logical_or(mask, input_ids.eq(special_token)) \n",
    "#     print(\"mask: \", mask)\n",
    "# torch.nonzero(mask)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # # print(tokenizer.all_special_tokens)    \n",
    "# # # # print(tokenizer.all_special_ids)     \n",
    "# # # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # # tokenizer.sep_token\n",
    "# print(tokenizer.tokenize(\"yes\"))\n",
    "# print(tokenizer.tokenize(\"no\"))\n",
    "# print(tokenizer.tokenize(\"null\"))\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "    return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch \n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch \n",
    "    # print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    answer_loss, type_loss = output[:2]\n",
    "    # answer_loss, type_loss = output[:4]\n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    # print(\"answer_loss: \", answer_loss)\n",
    "    # print(\"type_loss: \", type_loss)\n",
    "    # print(\"sp_para_loss: \", sp_para_loss)\n",
    "    # print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "\n",
    "#     loss  = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "#     print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it.  \n",
    "\n",
    "    tensorboard_logs = {'loss': loss, 'train_answer_loss': answer_loss, 'train_type_loss': type_loss, \n",
    "                        # 'train_sp_para_loss': sp_para_loss, 'train_sp_sent_loss': sp_sent_loss, \n",
    "                        'lr': lr #,\n",
    "                        # 'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) \n",
    "    }\n",
    "    return tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "    # # the function is called for each batch after every epoch is completed\n",
    "    # def training_end(self, output): \n",
    "    #     # print(\"training_end at epoch: \", self.current_epoch)\n",
    "    # #     print(\"len(outputs): \",len(outputs))\n",
    "    # #     print(\"output: \",output)\n",
    "    \n",
    "    #     # one batch only has one example\n",
    "    #     avg_loss = output['loss']    \n",
    "    #     avg_answer_loss = output['train_answer_loss']  \n",
    "    #     avg_type_loss = output['train_type_loss']    \n",
    "    #     avg_sp_para_loss = output['train_sp_para_loss']   \n",
    "    #     avg_sp_sent_loss = output['train_sp_sent_loss'] \n",
    "    #     avg_lr = output['lr']      \n",
    "         \n",
    "     \n",
    "    #     if self.trainer.use_ddp:\n",
    "    #         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_answer_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_type_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_para_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_sent_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_lr, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_lr /= self.trainer.world_size \n",
    "            \n",
    "     \n",
    "    #     tensorboard_logs = { #'avg_train_loss': avg_loss, \n",
    "    #             'avg_train_answer_loss': avg_answer_loss, 'avg_train_type_loss': avg_type_loss, 'avg_train_sp_para_loss': avg_sp_para_loss, 'avg_train_sp_sent_loss': avg_sp_sent_loss, 'lr': avg_lr\n",
    "    #           }\n",
    "    \n",
    "    #     return {'loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    print(\"validation_step\")\n",
    "    print(\"batch_nb: \", batch_nb)\n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch\n",
    "    print(\"qid: \", qid)\n",
    "    print(\"q_type: \", q_type)\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, start_logits, end_logits, type_logits = output \n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    if(q_type.item() != -1 ):\n",
    "    # answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "        answers_pred  = self.decode(input_ids, start_logits, end_logits, type_logits)\n",
    "    else:\n",
    "        answers_pred  = [{'text': '', 'score': -1000000, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': 1}]\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "#         print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "#         print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = _normalize_text(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "#         print(\"f1: \" + str(f1))\n",
    "#         print(\"prec: \" + str(prec))\n",
    "#         print(\"recall: \" + str(recall))\n",
    "#         print(\"em: \" + str(em))  \n",
    "\n",
    "#         if(len(sp_sent_pred) > 0):\n",
    "#             sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "#             sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "#             sp_sent_precision = torch.tensor(sp_sent_precision).type_as(loss)\n",
    "#             sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "#             sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "\n",
    "#   #         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "#   #         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "#   #         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "#   #         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "\n",
    "#             joint_prec = prec * sp_sent_precision\n",
    "#             joint_recall = recall * sp_sent_recall\n",
    "#             if joint_prec + joint_recall > 0:\n",
    "#                 joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "#             else:\n",
    "#                 joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "#             joint_em = em * sp_sent_em \n",
    "\n",
    "#         else:\n",
    "#             sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "#             joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "            # 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "               'answer_score': pre_answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em #,\n",
    "            #   'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "            #   'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall\n",
    "\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits): #, sp_para_logits, sp_sent_logits):\n",
    "    # print(\"decode\")\n",
    "\n",
    "    question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "\n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "    print(\"start_logits_indices: \", start_logits_indices)\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#         print(\"p_type: \", p_type)\n",
    "#         print(\"p_type_score: \", p_type_score)\n",
    "\n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#             print(\"sorted_answers: \" + str(sorted_answers))\n",
    "\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "\n",
    "            # remove [/sent], <t> and </t>\n",
    "\n",
    "            for special_token in [SENT_MARKER_END, TITLE_START, TITLE_END, self.tokenizer.sep_token]:\n",
    "                try:\n",
    "                    if(answer_tokens[0] == special_token):\n",
    "                        answer['start_logit'] = -2000000\n",
    "                    elif(answer_tokens[-1] == special_token):\n",
    "                        answer['end_logit'] = -2000000\n",
    "\n",
    "                    answer_tokens.remove(special_token)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens) \n",
    "            score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            answers.append({'text': text, 'score': score, 'start_logit': answer['start_logit'], 'end_logit': answer['end_logit'], 'p_type_score': p_type_score})\n",
    "\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score}) \n",
    "    else:\n",
    "        assert False \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = _normalize_text(prediction)\n",
    "    normalized_ground_truth = _normalize_text(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(_normalize_text(prediction) == _normalize_text(ground_truth))\n",
    "\n",
    "\n",
    "# def sp_metrics(self, prediction, gold): \n",
    "#     tp, fp, fn = 0, 0, 0\n",
    "#     for e in prediction:\n",
    "#         if e in gold:\n",
    "#             tp += 1\n",
    "#         else:\n",
    "#             fp += 1 \n",
    "#     for e in gold:\n",
    "#         if e not in prediction:\n",
    "#             fn += 1 \n",
    "#     prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "#     recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "#     f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "#     em = 1.0 if fp + fn == 0 else 0.0 \n",
    "#     return em, prec, recall, f1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_epoch_end(self, outputs):\n",
    "    print(\"validation_epoch_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs] \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "    # sp_sent_f1_scores = [x['sp_f1'] for x in outputs]   \n",
    "    # sp_sent_em_scores = [x['sp_em'] for x in outputs]   \n",
    "    # sp_sent_prec_scores = [x['sp_prec'] for x in outputs]   \n",
    "    # sp_sent_recall_scores = [x['sp_recall'] for x in outputs]   \n",
    "    # joint_f1_scores = [x['joint_f1'] for x in outputs]  \n",
    "    # joint_em_scores = [x['joint_em'] for x in outputs]  \n",
    "    # joint_prec_scores = [x['joint_prec'] for x in outputs]  \n",
    "    # joint_recall_scores = [x['joint_recall'] for x in outputs]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_para_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        # sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        # joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "        # joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "        # joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "        # joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_val_em = sum(em_scores) / len(em_scores)\n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)\n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores)\n",
    "    # avg_val_sp_sent_f1 = torch.tensor(sum(sp_sent_f1_scores) / len(sp_sent_f1_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_sp_sent_em = torch.tensor(sum(sp_sent_em_scores) / len(sp_sent_em_scores) ).type_as(avg_loss)    \n",
    "    # avg_val_sp_sent_prec = torch.tensor(sum(sp_sent_prec_scores) / len(sp_sent_prec_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_sp_sent_recall = torch.tensor(sum(sp_sent_recall_scores) / len(sp_sent_recall_scores) ).type_as(avg_loss)    \n",
    "    # avg_val_joint_f1 = torch.tensor(sum(joint_f1_scores) / len(joint_f1_scores) ).type_as(avg_loss)  \n",
    "    # avg_val_joint_em = torch.tensor(sum(joint_em_scores) / len(joint_em_scores) ).type_as(avg_loss)  \n",
    "    # avg_val_joint_prec = torch.tensor(sum(joint_prec_scores) / len(joint_prec_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_joint_recall = torch.tensor(sum(joint_recall_scores) / len(joint_recall_scores) ).type_as(avg_loss) \n",
    "\n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t')   \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t')   \n",
    "    # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss)   \n",
    "    print(\"avg_val_f1: \", avg_val_f1, end = '\\t')   \n",
    "    print(\"avg_val_em: \", avg_val_em, end = '\\t')   \n",
    "    print(\"avg_val_prec: \", avg_val_prec, end = '\\t')   \n",
    "    print(\"avg_val_recall: \", avg_val_recall)   \n",
    "    # print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1, end = '\\t')   \n",
    "    # print(\"avg_val_sp_sent_em: \" , avg_val_sp_sent_em, end = '\\t')  \n",
    "    # print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec, end = '\\t')   \n",
    "    # print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)   \n",
    "    # print(\"avg_val_joint_f1: \" , avg_val_joint_f1, end = '\\t')  \n",
    "    # print(\"avg_val_joint_em: \", avg_val_joint_em, end = '\\t')   \n",
    "    # print(\"avg_val_joint_prec: \", avg_val_joint_prec, end = '\\t')   \n",
    "    # print(\"avg_val_joint_recall: \", avg_val_joint_recall)   \n",
    "\n",
    "\n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, \n",
    "        # 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss,   \n",
    "        'avg_val_f1': avg_val_f1 , 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall #,    \n",
    "        # 'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,    \n",
    "        # 'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall \n",
    "    }   \n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch\n",
    "\n",
    "    print(\"test_step of qid: \", qid, end=\"\\t\") \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    answer_loss, type_loss, start_logits, end_logits, type_logits = output \n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    # answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "    answers_pred = self.decode(input_ids, start_logits, end_logits, type_logits)\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    start_logit = answers_pred[0]['start_logit']\n",
    "    end_logit = answers_pred[0]['end_logit']\n",
    "    type_score = answers_pred[0]['p_type_score']\n",
    "    pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "    # print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "    # print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = _normalize_text(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "\n",
    "    print(\"pre_answer:\\t\", pre_answer, \"\\tgold_answer:\\t\", gold_answer) #, \"\\tstart_logits:\\t\", start_logits.cpu(), \"\\tend_logits:\\t\", end_logits.cpu(), \"\\ttype_logits:\\t\", type_logits.cpu())\n",
    "\n",
    "    self.logger.log_metrics({'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "                                'answer_score': pre_answer_score, 'start_logit': start_logit, 'end_logit': end_logit,  \n",
    "                                'type_score': type_score,\n",
    "                                'f1': f1, 'prec':prec, 'recall':recall, 'em': em \n",
    "                            }) \n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "             'answer_score': pre_answer_score, 'start_logit': start_logit, 'end_logit': end_logit, 'type_score': type_score,\n",
    "             'f1': f1, 'prec':prec, 'recall':recall, 'em': em\n",
    "            # 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss, \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_epoch_end(self, outputs):\n",
    "    print(\"test_epoch_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_para_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_sent_loss /= self.trainer.world_size \n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "#         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}')\n",
    "    # print(\"answer_scores: \", answer_scores)\n",
    "    avg_test_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_test_em = sum(em_scores) / len(em_scores)\n",
    "    avg_test_prec = sum(prec_scores) / len(prec_scores)\n",
    "    avg_test_recall = sum(recall_scores) / len(recall_scores)     \n",
    "    # print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "    # print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    # print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "    # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "\n",
    "    logs = {'avg_test_loss': avg_loss, 'avg_test_answer_loss': avg_answer_loss, 'avg_test_type_loss': avg_type_loss, \n",
    "            'avg_test_f1': avg_test_f1 , 'avg_test_em': avg_test_em,  'avg_test_prec': avg_test_prec, 'avg_test_recall': avg_test_recall #,    \n",
    "            # 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss\n",
    "           }\n",
    "\n",
    "    return {'avg_test_loss': avg_loss, 'log': logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHECKPOINT_HYPER_PARAMS_KEY',\n",
       " 'CHECKPOINT_HYPER_PARAMS_NAME',\n",
       " 'CHECKPOINT_HYPER_PARAMS_TYPE',\n",
       " 'T_destination',\n",
       " '_LightningModule__get_hparams_assignment_variable',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_apply',\n",
       " '_auto_collect_arguments',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_init_slurm_connection',\n",
       " '_load_from_state_dict',\n",
       " '_load_model_state',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_hparams',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'amp_scale_loss',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'example_input_array',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'get_progress_bar_dict',\n",
       " 'get_tqdm_dict',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'hparams',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_fit_end',\n",
       " 'on_fit_start',\n",
       " 'on_gpu',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'optimizer_zero_grad',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'prepare_data',\n",
       " 'print',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'save_hyperparameters',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'teardown',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_epoch_end',\n",
       " 'test_step',\n",
       " 'test_step_end',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_epoch_end',\n",
       " 'training_step',\n",
       " 'training_step_end',\n",
       " 'transfer_batch_to_device',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_epoch_end',\n",
       " 'validation_step',\n",
       " 'validation_step_end',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHECKPOINT_HYPER_PARAMS_KEY', 'hyper_parameters'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_NAME', 'hparams_name'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_TYPE', 'hparams_type'),\n",
       " ('T_destination', ~T_destination),\n",
       " ('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__abstractmethods__', frozenset()),\n",
       " ('__annotations__',\n",
       "  {'_device': Ellipsis, '_dtype': typing.Union[str, torch.dtype]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'test_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset(),\n",
       "                '_abc_registry': <_weakrefset.WeakSet at 0x7f6e4c8d4c50>,\n",
       "                '_abc_cache': <_weakrefset.WeakSet at 0x7f6e4c8d4c88>,\n",
       "                '_abc_negative_cache': <_weakrefset.WeakSet at 0x7f6e4c8d4cf8>,\n",
       "                '_abc_negative_cache_version': 72,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'validation_epoch_end': <function __main__.validation_epoch_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'test_step': <function __main__.test_step(self, batch, batch_nb)>,\n",
       "                'test_epoch_end': <function __main__.test_epoch_end(self, outputs)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7f6e4c8a6550>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'ABC' objects>),\n",
       " ('_abc_cache', <_weakrefset.WeakSet at 0x7f6e4c8d4c88>),\n",
       " ('_abc_negative_cache', <_weakrefset.WeakSet at 0x7f6e4c8d4cf8>),\n",
       " ('_abc_negative_cache_version', 72),\n",
       " ('_abc_registry', <_weakrefset.WeakSet at 0x7f6e4c8d4c50>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_auto_collect_arguments',\n",
       "  <bound method LightningModule._auto_collect_arguments of <class '__main__.hotpotqa'>>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_load_model_state',\n",
       "  <bound method ModelIO._load_model_state of <class '__main__.hotpotqa'>>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('device', <property at 0x7f6e4eb47b38>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('dtype', <property at 0x7f6e4eb47a98>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('example_input_array', <property at 0x7f6e4eca6638>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('hparams', <property at 0x7f6e4eb55908>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method ModelIO.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method ModelIO.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_gpu', <property at 0x7f6e4eca60e8>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int, second_order_closure:Union[Callable, NoneType]=None, on_tpu:bool=False, using_native_amp:bool=False, using_lbfgs:bool=False) -> None>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end', <function __main__.test_epoch_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int, second_order_closure:Union[Callable, NoneType]=None, on_tpu:bool=False, using_native_amp:bool=False, using_lbfgs:bool=False) -> None>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end', <function __main__.test_epoch_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    if not args.test:     # if it needs to train, remove exsiting folder\n",
    "        import shutil\n",
    "        save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "        if os.path.exists(save_folder):\n",
    "            shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder \n",
    "        \n",
    "    import shutil\n",
    "    save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.args.model_path:  /xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096\n",
      "Loaded model with config:\n",
      "LongformerConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"sliding_chunks\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "#     model.to('cuda')    # this is necessary to use gpu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_size:  9.0\n",
      "num_devices:  1\n",
      ">>>>>>> #train_set_size: 90447.0, #steps: 271341.0,  #warmup steps: 1000, #epochs: 6, batch_size: 2 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "    train_set_size = 9 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size) \n",
    "\n",
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus!='' else None\n",
    "    num_devices = 1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "\n",
    "    train_set_size = 90447 * args.train_percent    # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps},  #warmup steps: {args.warmup}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Multi-processing is handled by Slurm.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                             track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                             accumulate_grad_batches=args.batch_size,\n",
    "                             train_percent_check = args.train_percent,\n",
    "        #                          val_check_interval=args.val_every,\n",
    "                             val_percent_check=args.val_percent_check,\n",
    "                             test_percent_check=args.val_percent_check,\n",
    "                             logger=logger if not args.disable_checkpointing else False,\n",
    "                             checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                             show_progress_bar=args.no_progress_bar,\n",
    "#                              use_amp=not args.fp32, \n",
    "                             amp_level='O2',\n",
    "#                              check_val_every_n_epoch=1\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=ddp\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | Longformer | 148 M \n",
      "1 | qa_outputs  | Linear     | 1 K   \n",
      "2 | linear_type | Linear     | 2 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: small.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  773,  885,  906, 1174,  143,  366,  208,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 426, 218, 352, 497, 238,\n",
      "        697, 281, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_epoch_end\n",
      "before sync --> sizes:  2, 2, 2\n",
      "after sync --> sizes: 2, 2, 2\n",
      "avg_loss:  tensor(14.2475, device='cuda:0')\tavg_answer_loss:  tensor(7.0615, device='cuda:0')\tavg_type_loss:  tensor(1.4372, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "reading file: small_dev2.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c566635738d41a08ef8623f84798153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  773,  885,  906, 1174,  143,  366,  208,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 426, 218, 352, 497, 238,\n",
      "        697, 281, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301,  220, 1382,  522], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([754, 245, 111, 805, 675, 841, 720, 953, 154, 662, 899, 759, 248, 702,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1218, 1311, 1285,  247, 1181,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 631, 124, 763, 623,\n",
      "        291, 247,  92, 726, 713, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 627, 348, 208, 595, 122, 220, 356,\n",
      "        893, 379, 649, 735, 162, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 665, 733,\n",
      "        296, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "        1139,  153, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  804,  372,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254,  749,  613, 1202,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  769,  917,  908,  771,  935,\n",
      "         195,  866,  561,  503,   94,  761,  282,  747], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 842, 396, 728, 382, 761, 685, 684, 513, 750, 240, 757, 725,\n",
      "        409, 692, 485, 190, 157, 117], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 145, 285, 243, 426, 296, 361, 248, 591, 735, 456,\n",
      "         71, 398, 239, 324, 297, 244], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532,  772, 1252,  355,  789, 1247, 1821,  699,\n",
      "         363, 1692, 1263,  812,  701,  683,  312,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 146, 304, 833, 629, 953, 696, 979,\n",
      "        746, 369, 758, 860, 384, 760], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  235,  605,  253,  376,  803,  154,  161,  208,  361,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 245, 685, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  297,  707,  213,  768,  244,  377,  336,  223, 1314, 1359,\n",
      "         737, 1159,  723,  508, 1087,  758,  466,  333], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240, 1020,  736,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 739, 761, 751, 795, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  133,  156, 1280,  738, 1160, 1297, 1281, 1121,\n",
      "        1007,  932,  734,  266,  267, 1222,   35,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([860, 848, 844, 291, 296, 454, 634, 245, 459,  12, 852, 926, 609, 631,\n",
      "        610, 228, 383, 371,  13, 240], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394,  280,  159, 1001,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         397,  235,  737,  101,  214,  954,   19,  301], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 139, 830, 147, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  468,  198,  362,  430,\n",
      "         254,  679,  424, 1718,  255, 1150,  680,  687], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 376, 326, 717, 775, 336,\n",
      "        305, 130, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  239,  681, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851,  752, 1380, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745,  248, 1173,  904,  118,  672,  679,  243,  338, 1182,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1239, 1220, 1200,  227,  697, 1169,  247,  281,  672,  273,  242,\n",
      "         270, 1185, 1007,  591,  268,  895, 1114,  286], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([220, 762, 240, 678, 717, 620, 798,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 715, 747, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  380,  358,  633,   95, 1269,  189,   97,  126,\n",
      "         706,  147,  156,  258,  764,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 132, 187,\n",
      "        537, 637, 362, 279, 131, 454], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,  277,   84,  203,  186,\n",
      "         621,  479,  262,  422,  872,  778,  721,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733, 1182,  123, 2068,  188,  189, 1882,\n",
      "        1646, 1892, 1395, 1394, 1771, 1583,  190, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 731, 807, 198, 872, 229, 662, 718, 657, 132, 879,\n",
      "        375, 607, 648, 311, 106, 808], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  148,  342,  739,   68,  773,   42, 1198], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "         149, 1232,  133,  255,  480,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 205, 629, 188, 127, 406,\n",
      "        126, 585, 359, 189,   8, 483], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 769, 742, 313, 590], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  781,  451,  313, 1130,\n",
      "         320,  316, 1112,  618,  430,  746,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,  130,   92,  148, 1153,  659, 1146,  306,  128,  119,\n",
      "         797, 1226,  622, 1179,  384,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873,  916, 1148, 1012,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  658,  509,  517,  137,  286,  815,  912,  251,  314,\n",
      "         179,  756,  711,  355,  758,  253,  174, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443,  455, 1172,  233,  660,  889,  851,  639,  945,  846,\n",
      "         259,  204, 1070,  745,  944,  633,  619,   79], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "        1169,  495,   21,  757,  187, 1172,  875,  504], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408, 1294,  232,  814,\n",
      "         137,  416, 1186,  889, 1042,  975, 1292,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  654,  358,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230,  40, 756, 323, 248,\n",
      "        987, 754, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311,  816, 1100, 1270, 1296,\n",
      "          31,  239,  808, 1306,  809,  521, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 424, 228, 240, 235, 161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  235,  558,  346,  316,  638,  624, 1119], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  697,  679,  658, 1306,  625,  360, 1323,\n",
      "        1255,  741, 1188,  339,  469,  808,  773,  906], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,  927,    8,  876, 1034,\n",
      "         578,  679,  301,  696,  347,  139,  216,  698], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  647,  335, 1181,  730,  661,  833,  904,  834,\n",
      "        1153,  810,  333,  665,  717,  648,  716,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 158, 189, 340, 737,  95, 680, 457, 373,\n",
      "        249,  52, 522, 460, 738,  30], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 242, 255, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855,  185,  152, 1017, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760,  755, 1236, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 185,  51, 580, 535, 626,\n",
      "        262, 284, 124, 240, 385, 560], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  598,\n",
      "         579,  920,  165,  680,  672,  677,  389,  651], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  372,  775,  769,  776,  263,  203,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 258, 515, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 137, 275,  83, 134, 295, 332,  66,\n",
      "        246, 173, 158, 229, 370, 292], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 154, 198,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1152, 1075,  237,  224, 1169, 1256,  888,\n",
      "         337,  648,  425,  238,  766,  263, 1298,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  908,  459,  649,  648,   58,  464,  203,  148,  721,  238,  859,\n",
      "         675,  660, 1105,  673,  686,  683,  802,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 870, 268, 891, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 661, 257, 945,  97, 191, 193, 184, 949, 791, 952,\n",
      "        871, 128, 183, 695, 194, 592], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  620,  414, 1002, 1036,  890], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 648, 163, 696, 839, 840, 804, 718, 725, 769, 244,\n",
      "        860, 661, 145, 109, 805, 133], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310,  56, 346, 313, 136, 855, 372, 241, 712,  50, 371,\n",
      "         30, 380, 850, 693, 657, 731], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 685, 710,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  134, 1146,  320,  146,  125,  131,  142,   81, 1158,   75,\n",
      "        1559,  266, 1147,  617,  162,  425,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  282,  643,  481,  334,  313,  383,  879,  629,\n",
      "        1163,  347,  261,  216, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.0 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00000: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_0.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.6445, device='cuda:0')\tavg_answer_loss:  tensor(6.1872, device='cuda:0')\tavg_type_loss:  tensor(1.4915, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  773,  885,  906, 1174,  143,  366,  208,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 218, 426, 352, 497, 238,\n",
      "        697, 281, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301,  220, 1382,  522], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([754, 245, 111, 805, 675, 841, 720, 953, 154, 662, 899, 759, 248, 702,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1218, 1311, 1285,  247, 1181,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 124, 631, 763, 623,\n",
      "        291, 247,  92, 726, 713, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 627, 348, 208, 595, 122, 220, 356,\n",
      "        893, 379, 649, 735, 162, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 665, 733,\n",
      "        296, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "        1139,  153, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  804,  372,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254,  749,  613, 1202,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  769,  917,  908,  771,  935,\n",
      "         195,  866,  561,  503,  761,   94,  282,  747], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 396, 842, 728, 382, 761, 685, 684, 513, 750, 240, 757, 725,\n",
      "        409, 485, 692, 190, 157, 117], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 285, 145, 243, 426, 296, 361, 248, 591, 735, 456,\n",
      "         71, 398, 239, 324, 297, 244], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532,  772, 1252,  355,  789, 1247, 1821,  699,\n",
      "        1692,  363, 1263,  812,  701,  683,  312,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 146, 304, 833, 629, 953, 696, 979,\n",
      "        746, 369, 758, 860, 760, 384], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  235,  605,  253,  376,  803,  161,  154,  208,  361,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 685, 245, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  297,  707,  213,  768,  244,  377,  336,  223, 1314, 1359,\n",
      "         737, 1159,  723, 1087,  508,  758,  333,  466], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240, 1020,  736,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 739, 761, 751, 795, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  133,  156, 1280,  738, 1160, 1297, 1281, 1121,\n",
      "        1007,  734,  932,  266,  267, 1222,   35,  262], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([860, 848, 844, 291, 296, 454, 634, 245, 459,  12, 852, 926, 609, 631,\n",
      "        610, 228, 383, 371,  13, 240], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394,  159, 1001,  280,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         235,  397,  737,  101,  214,  954,   19,  705], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 139, 830, 147, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  468,  198,  362,  430,\n",
      "         254,  679,  424, 1718,  255, 1150,  680,  687], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 376, 326, 717, 775, 336,\n",
      "        130, 305, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  239,  681, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851,  752, 1380, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745,  248, 1173,  904,  118,  672,  679,  243,  338, 1182,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1239, 1220, 1200,  227,  697, 1169,  247,  281,  672,  242,  273,\n",
      "         270, 1185, 1007,  591,  268,  895, 1114,  286], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([220, 762, 240, 678, 620, 717, 798,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 715, 747, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  380,  358,  633,   95, 1269,  189,  126,   97,\n",
      "         706,  147,  156,  258,  764,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 132, 187,\n",
      "        537, 637, 362, 279, 131, 454], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,  277,   84,  203,  186,\n",
      "         621,  479,  262,  422,  872,  721,  778,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733, 1182,  123, 2068,  188,  189, 1882,\n",
      "        1646, 1892, 1395, 1394, 1771, 1583,  190, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 731, 807, 198, 872, 229, 662, 718, 657, 132, 879,\n",
      "        375, 607, 648, 311, 106, 808], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  148,  342,  739,   68,   42,  773, 1198], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "         149, 1232,  133,  255,  480,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 205, 629, 188, 127, 406,\n",
      "        126, 585, 359, 189,   8, 483], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 769, 742, 313, 590], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  781,  451,  313, 1130,\n",
      "         320,  316, 1112,  618,  746,  430,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,  130,   92,  148, 1153,  659, 1146,  306,  128,  119,\n",
      "         797, 1226,  622, 1179,  384,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873,  916, 1148, 1012,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  658,  509,  517,  137,  286,  815,  912,  251,  314,\n",
      "         179,  756,  711,  355,  758,  253,  174, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443,  455, 1172,  233,  660,  889,  851,  639,  945,  846,\n",
      "         259,  204, 1070,  745,  944,  633,   79,  619], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "         495, 1169,   21,  757,  187, 1172,  875,  504], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408, 1294,  232,  814,\n",
      "         137,  416, 1186,  889, 1042,  975, 1292,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  654,  358,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230,  40, 756, 323, 248,\n",
      "        987, 754, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311,  816, 1100, 1270, 1296,\n",
      "          31,  239,  808, 1306,  521,  809, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 228, 424, 240, 235, 197], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  235,  558,  346,  316,  638,  624, 1119], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  679,  697,  658, 1306,  625,  360, 1323,\n",
      "        1255,  741, 1188,  339,  469,  808,  773,  906], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,  927,    8,  876, 1034,\n",
      "         578,  679,  301,  696,  347,  139,  698,  216], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  647,  335, 1181,  730,  661,  833,  904,  834,\n",
      "        1153,  810,  333,  717,  665,  648,  716,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 158, 189, 340, 737,  95, 680, 457, 373,\n",
      "        249,  52, 522, 460, 738,  30], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 242, 255, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855,  185, 1017,  152, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760,  755, 1236, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 185,  51, 580, 535, 626,\n",
      "        262, 284, 124, 240, 385, 560], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  598,\n",
      "         579,  920,  165,  680,  672,  677,  389,  651], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  372,  775,  769,  776,  263,  203,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 515, 258, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 275, 137,  83, 134, 295, 332,  66,\n",
      "        246, 173, 158, 229, 370, 292], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 154, 198,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1152, 1075,  237,  224, 1169, 1256,  888,\n",
      "         337,  648,  425,  238,  766,  263, 1298,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  908,  459,  649,  648,   58,  464,  203,  148,  721,  238,  859,\n",
      "         675,  660, 1105,  673,  686,  683,  802,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 870, 268, 891, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 661, 257, 945,  97, 191, 193, 184, 949, 791, 952,\n",
      "        871, 128, 183, 695, 194, 592], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  620,  414, 1002, 1036,  890], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 648, 163, 696, 839, 840, 804, 718, 725, 769, 244,\n",
      "        860, 661, 145, 109, 805, 133], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310,  56, 346, 313, 136, 855, 372, 241, 712,  50, 371,\n",
      "         30, 380, 850, 693, 657, 731], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 710, 685,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  134, 1146,  320,  146,  125,  131,  142,   81, 1158,   75,\n",
      "        1559,  266, 1147,  617,  162,  425,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  282,  643,  481,  334,  313,  383,  879,  629,\n",
      "        1163,  347,  216,  261, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_1.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.6348, device='cuda:0')\tavg_answer_loss:  tensor(6.1871, device='cuda:0')\tavg_type_loss:  tensor(1.4896, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  885,  773,  906, 1174,  143,  366,  208,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 218, 426, 352, 497, 238,\n",
      "        697, 281, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301, 1382,  220,  754], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([754, 245, 111, 805, 675, 841, 720, 953, 154, 662, 899, 759, 248, 702,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1311, 1218, 1285,  247, 1181,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 124, 631, 763, 623,\n",
      "        291, 247,  92, 726, 713, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 627, 348, 208, 595, 122, 220, 356,\n",
      "        893, 649, 379, 162, 735, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 665, 733,\n",
      "        296, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "         153, 1139, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  804,  372,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254,  749,  613, 1202,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  769,  917,  908,  771,  935,\n",
      "         195,  866,  561,  761,  503,   94,  282,  747], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 396, 842, 728, 382, 761, 685, 684, 513, 240, 750, 757, 725,\n",
      "        409, 485, 692, 190, 157, 117], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 285, 145, 243, 426, 296, 361, 248, 591, 735, 456,\n",
      "         71, 398, 239, 324, 297, 244], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532,  772, 1252,  355,  789, 1247, 1821,  699,\n",
      "        1692,  812, 1263,  363,  701,  683,  312,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 304, 146, 833, 629, 953, 696, 979,\n",
      "        746, 369, 758, 860, 384, 760], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  235,  605,  253,  376,  803,  161,  154,  208,  361,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 685, 245, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  297,  707,  213,  768,  244,  377,  336,  223, 1314, 1359,\n",
      "         737, 1159,  723, 1087,  508,  758,  333,  466], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240, 1020,  736,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 739, 761, 751, 795, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  133,  156,  738, 1280, 1160, 1297, 1281, 1121,\n",
      "        1007,  734,  266,  932,  267, 1222,   35,  262], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([860, 848, 844, 291, 296, 454, 634, 245, 459,  12, 852, 926, 609, 631,\n",
      "        610, 228, 383, 371,  13, 240], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394, 1001,  159,  280,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         235,  397,  737,  101,  214,  954,   19,  705], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 139, 147, 830, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  468,  198,  362,  430,\n",
      "         254,  679,  424, 1718,  255, 1150,  680,  687], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 376, 326, 717, 775, 336,\n",
      "        130, 305, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  239,  681, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851, 1380,  752, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745,  248, 1173,  904,  118,  672,  679,  243,  338, 1182,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1220, 1239, 1200,  227,  697, 1169,  247,  281,  672,  242,  273,\n",
      "         270, 1185, 1007,  591,  268,  895, 1114,  286], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 762, 240, 678, 620, 717, 798,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 715, 747, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  358,  380,  633,   95, 1269,  189,  126,   97,\n",
      "         706,  147,  156,  258,  764,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 132, 187,\n",
      "        537, 637, 362, 279, 131, 454], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,   84,  277,  203,  186,\n",
      "         621,  479,  262,  422,  872,  721,  778,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733, 1182,  123, 2068,  188,  189, 1882,\n",
      "        1646, 1892, 1395, 1394, 1771, 1583,  190, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 731, 807, 198, 872, 229, 662, 718, 657, 132, 879,\n",
      "        375, 607, 648, 311, 106, 895], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  148,  342,  739,   68,   42, 1198,  773], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "         149, 1232,  133,  255,  480,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 205, 629, 188, 127, 406,\n",
      "        126, 585, 359, 189,   8, 483], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 769, 742, 313, 590], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  781,  451,  313, 1130,\n",
      "         320,  316, 1112,  618,  746,  430,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,  130,   92,  148, 1153,  659, 1146,  306,  128,  119,\n",
      "         797, 1226,  622, 1179,  384,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873,  916, 1148,  187, 1012], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  658,  509,  517,  137,  286,  815,  912,  251,  314,\n",
      "         179,  756,  711,  355,  758,  253,  174, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443,  455, 1172,  233,  660,  889,  851,  945,  639,  846,\n",
      "         259,  204, 1070,  745,  944,  633,   79,  619], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "         495,   21, 1169,  757,  187, 1172,  875,  504], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408, 1294,  232,  814,\n",
      "         137,  416, 1186,  889, 1042,  975, 1292,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  654,  358,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230, 756,  40, 323, 248,\n",
      "        987, 754, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311,  816, 1100, 1270, 1296,\n",
      "          31,  239,  808, 1306,  521,  809, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 228, 424, 240, 235, 197], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  235,  558,  346,  316,  638,  624, 1119], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  679,  697,  658, 1306,  625,  360, 1323,\n",
      "        1255,  741, 1188,  339,  808,  469,  773,  906], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,  927,    8,  876, 1034,\n",
      "         578,  679,  301,  696,  347,  139,  698,  216], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  647,  335, 1181,  730,  661,  833,  904,  834,\n",
      "        1153,  810,  333,  717,  665,  648,  716,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 158, 189, 340, 737,  95, 680, 457, 373,\n",
      "        249,  52, 522, 460, 738,  30], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 242, 255, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855, 1017,  185,  152, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760,  755, 1236, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 185,  51, 580, 535, 626,\n",
      "        262, 284, 124, 240, 385, 560], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  598,\n",
      "         579,  920,  165,  680,  389,  672,  677,  651], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  372,  775,  769,  776,  263,  203,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 515, 258, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 137, 275,  83, 134, 295, 332,  66,\n",
      "        246, 173, 158, 229, 370, 211], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 154, 198,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1075, 1152,  237,  224, 1169,  888, 1256,\n",
      "         337,  648,  425,  238,  766,  263, 1298,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  908,  459,  649,  648,  464,   58,  203,  148,  721,  238,  859,\n",
      "         675,  660,  673, 1105,  686,  683,  802,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 870, 268, 891, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 661, 257, 945,  97, 191, 193, 184, 949, 791, 952,\n",
      "        871, 128, 183, 695, 194, 592], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  620,  414, 1036, 1002,  890], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 648, 163, 696, 839, 840, 804, 718, 725, 769, 244,\n",
      "        860, 661, 145, 109, 805, 738], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310,  56, 346, 313, 136, 855, 372, 241, 712,  50, 371,\n",
      "         30, 850, 380, 693, 657, 731], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 710, 685,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  134, 1146,  320,  146,  125,  131,  142,   81, 1158,   75,\n",
      "        1559,  266, 1147,  162,  617,  425,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  282,  643,  481,  334,  313,  383,  879,  629,\n",
      "        1163,  216,  347,  261, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_2.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.6193, device='cuda:0')\tavg_answer_loss:  tensor(6.1869, device='cuda:0')\tavg_type_loss:  tensor(1.4865, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  885,  773,  906, 1174,  143,  366,  208,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 218, 426, 352, 497, 238,\n",
      "        281, 697, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301, 1382,  220,  754], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([754, 245, 111, 805, 675, 720, 841, 953, 154, 662, 899, 759, 702, 248,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1311, 1218, 1285,  247, 1181,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 124, 631, 763, 623,\n",
      "        291, 247,  92, 726, 713, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 348, 627, 208, 595, 122, 220, 356,\n",
      "        893, 649, 379, 162, 735, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 665, 733,\n",
      "        296, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "         153, 1139, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  372,  804,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254,  749,  613, 1202,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  917,  769,  908,  771,  935,\n",
      "         195,  866,  561,  761,  503,   94,  282,  747], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 396, 842, 728, 382, 761, 685, 684, 513, 240, 750, 757, 725,\n",
      "        409, 485, 692, 190, 117, 157], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 285, 145, 243, 426, 296, 361, 591, 248, 735, 456,\n",
      "         71, 398, 239, 324, 297, 244], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532,  772, 1252,  355,  789, 1247, 1821,  699,\n",
      "         812, 1692,  363, 1263,  701,  312,  683,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 304, 146, 833, 629, 953, 696, 979,\n",
      "        746, 369, 758, 860, 384, 760], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  235,  605,  253,  376,  803,  161,  154,  208,  361,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 685, 245, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  297,  707,  213,  768,  244,  377,  336,  223, 1359, 1314,\n",
      "         737, 1159,  723, 1087,  508,  758,  333,  466], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240,  736, 1020,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 739, 761, 751, 795, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  133,  156,  738, 1280, 1160, 1297, 1121, 1281,\n",
      "        1007,  266,  734,  932,  267,   35, 1222,  262], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([860, 848, 844, 291, 454, 296, 634, 245, 459,  12, 852, 926, 609, 631,\n",
      "        610, 228, 383,  13, 371, 240], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394, 1001,  159,  280,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         235,  397,  737,  101,  214,   19,  954,  705], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 147, 139, 830, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  198,  468,  362,  430,\n",
      "         254,  679,  424, 1718, 1150,  255,  680,  687], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 326, 376, 717, 775, 336,\n",
      "        130, 305, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  681,  239, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851, 1380,  752, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745,  248, 1173,  904,  118,  672,  679,  243,  338, 1182,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1220, 1239, 1200,  227,  697, 1169,  247,  281,  672,  242,  273,\n",
      "         270, 1185, 1007,  591,  268,  895, 1114,  286], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 762, 240, 678, 620, 717, 798,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 715, 747, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  358,  380,  633,   95, 1269,  189,  126,   97,\n",
      "         706,  147,  156,  258,  764,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 132, 187,\n",
      "        537, 637, 362, 279, 131, 454], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,   84,  277,  203,  186,\n",
      "         621,  479,  262,  422,  872,  721,  778,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733,  123, 1182, 2068,  188,  189, 1646,\n",
      "        1882, 1892, 1395, 1394, 1771,  190, 1583, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 731, 807, 198, 872, 229, 662, 718, 657, 132, 879,\n",
      "        375, 607, 648, 311, 106, 895], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  148,  342,  739,   68,   42, 1198,  773], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "         149, 1232,  133,  255,  480,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 205, 629, 188, 127, 406,\n",
      "        585, 126, 359, 189,   8, 483], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 769, 742, 590, 313], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  781,  451,  313, 1130,\n",
      "         320, 1112,  316,  746,  618,  430,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,  130,   92,  148, 1153,  659,  306, 1146,  119,  128,\n",
      "         797, 1226,  622, 1179,  384,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873,  916, 1148,  187, 1012], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  658,  509,  517,  137,  286,  815,  912,  251,  314,\n",
      "         756,  179,  711,  355,  758,  253,  174, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443,  455, 1172,  233,  660,  889,  851,  945,  639,  846,\n",
      "         259,  204, 1070,  745,  944,  633,  619,   79], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "         495,   21, 1169,  757,  187,  875, 1172,  504], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408, 1294,  232,  814,\n",
      "         137,  416, 1186,  889, 1042,  975, 1292,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  654,  358,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230, 756,  40, 323, 248,\n",
      "        987, 754, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311, 1100,  816, 1270, 1296,\n",
      "          31,  239,  808,  521, 1306,  809, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 228, 424, 240, 235, 197], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  235,  558,  346,  316,  638, 1119,  624], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  679,  697,  658, 1306,  625,  360, 1323,\n",
      "        1255,  741, 1188,  339,  808,  469,  773,  906], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,  927,    8,  876, 1034,\n",
      "         578,  679,  301,  696,  347,  139,  698,  216], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  647,  335, 1181,  730,  661,  833,  904,  834,\n",
      "        1153,  810,  333,  665,  717,  648,  716,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 189, 158, 340, 737,  95, 680, 457, 373,\n",
      "        249,  52, 522, 460, 738,  30], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 242, 255, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855, 1017,  185,  152, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760, 1236,  755, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 185,  51, 580, 535, 626,\n",
      "        262, 284, 124, 240, 385, 560], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  598,\n",
      "         920,  579,  165,  680,  389,  672,  677,  139], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  372,  769,  775,  776,  263,  203,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 515, 258, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 137, 275,  83, 134, 295, 332,  66,\n",
      "        246, 173, 158, 229, 370, 211], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 198, 154,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1075, 1152,  237,  224, 1169,  888, 1256,\n",
      "         337,  648,  425,  238,  766,  263, 1298,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  459,  908,  649,  648,  464,   58,  203,  148,  721,  238,  859,\n",
      "         675,  660,  673, 1105,  686,  683,  802,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 870, 268, 891, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 257, 661, 945,  97, 191, 193, 184, 949, 791, 952,\n",
      "        871, 128, 183, 695, 194, 592], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  414,  620, 1036, 1002,  890], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 648, 163, 696, 839, 840, 804, 718, 725, 769, 244,\n",
      "        860, 661, 145, 109, 805, 738], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310,  56, 346, 313, 136, 855, 372, 241,  50, 712, 371,\n",
      "         30, 850, 380, 693, 657, 731], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 710, 685,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  320, 1146,  134,  146,  131,  125,   81,  142, 1158,   75,\n",
      "        1559,  266,  162,  617, 1147,  425,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  643,  282,  481,  334,  313,  879,  383,  629,\n",
      "        1163,  347,  216,  261, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_3.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.6017, device='cuda:0')\tavg_answer_loss:  tensor(6.1867, device='cuda:0')\tavg_type_loss:  tensor(1.4830, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  885,  773,  906, 1174,  143,  208,  366,  408,  363, 1084,\n",
      "         739,  370, 1238,  292,  917, 1243,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 426, 218, 352, 497, 238,\n",
      "        281, 697, 615, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301, 1382,  220,  754], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([245, 754, 111, 805, 675, 720, 841, 953, 154, 662, 899, 759, 702, 248,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1311, 1218, 1285, 1181,  247,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 124, 631, 763, 623,\n",
      "        291, 247,  92, 726, 713, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 348, 627, 208, 595, 122, 220, 356,\n",
      "        893, 649, 379, 162, 735, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 665, 733,\n",
      "        296, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "        1139,  153, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  372,  804,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254,  613, 1202,  749,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  917,  769,  908,  771,  935,\n",
      "         195,  866,  561,  761,  503,   94,  747,  282], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 396, 842, 728, 382, 761, 685, 684, 513, 240, 750, 757, 725,\n",
      "        409, 485, 190, 117, 692, 157], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 285, 145, 243, 426, 296, 361, 591, 248, 735, 456,\n",
      "         71, 239, 398, 324, 297, 666], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532, 1252,  772,  355,  789, 1247, 1821,  699,\n",
      "         812, 1692,  363, 1263,  701,  312,  683,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 304, 146, 833, 629, 953, 696, 979,\n",
      "        746, 758, 369, 384, 860, 760], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  605,  235,  253,  376,  803,  161,  361,  208,  154,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 685, 245, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  707,  297,  213,  768,  244,  377,  336,  223, 1359, 1314,\n",
      "         737, 1159,  723, 1087,  508,  758,  333,  466], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240,  736, 1020,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 761, 739, 795, 751, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  156,  133,  738, 1280, 1160, 1297, 1121, 1281,\n",
      "        1007,  734,  266,  267,  932,   35, 1222,  262], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([860, 848, 844, 291, 454, 296, 634, 245, 459,  12, 852, 926, 609, 631,\n",
      "        610, 228, 383,  13, 371, 240], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394, 1001,  159,  280,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         235,  737,  397,  101,  214,   19,  954,  705], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 147, 139, 830, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  198,  362,  468,  430,\n",
      "         254,  679,  424, 1150,  255, 1718,  680, 1053], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 326, 376, 717, 775, 130,\n",
      "        336, 305, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  681,  239, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851, 1380,  752, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745,  248, 1173,  904,  118,  672,  679,  243, 1182,  338,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1220, 1239, 1200,  227,  697, 1169,  247,  281,  242,  672,  273,\n",
      "         270, 1185, 1007,  591,  268,  895,  286, 1114], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 762, 240, 678, 620, 717, 798,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 715, 747, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  358,  380,  633,   95, 1269,  189,  126,   97,\n",
      "         706,  147,  156,  764,  258,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 132, 187,\n",
      "        537, 637, 362, 279, 131, 454], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,   84,  277,  203,  186,\n",
      "         621,  479,  262,  422,  721,  778,  872,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733, 1182,  123, 2068,  188,  189, 1646,\n",
      "        1882, 1892, 1395, 1394, 1771,  190,   18, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 807, 731, 872, 198, 229, 662, 718, 132, 657, 879,\n",
      "        375, 607, 648, 311, 106, 808], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  342,  148,   68,  739,   42, 1198,  773], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "         149, 1232,  133,  480,  255,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 205, 188, 629, 406, 127,\n",
      "        585, 126, 359, 189,   8, 483], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 769, 742, 590, 313], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  451,  781,  313,  320,\n",
      "        1130, 1112,  316,  746,  618,  430,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,   92,  130,  148, 1153,  659,  306, 1146,  119,  128,\n",
      "         797, 1226,  622, 1179,  384,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873,  916, 1148,  187, 1012], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  517,  658,  509,  137,  286,  815,  912,  251,  314,\n",
      "         756,  179,  711,  355,  758,  253,  174, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443, 1172,  455,  233,  660,  889,  851,  945,  846,  639,\n",
      "         259,  204, 1070,  944,  745,  619,   79,  633], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "          21,  495, 1169,  757,  187,  875, 1172,  504], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408, 1294,  232,  814,\n",
      "         137, 1186,  416,  889, 1042,  975, 1292,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  654,  358,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230, 756,  40, 323, 248,\n",
      "        987, 754, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311, 1100,  816, 1270,   31,\n",
      "        1296,  808,  239,  521,  809, 1306, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 228, 424, 240, 235, 197], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  235,  558,  316,  346,  638, 1119,  624], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  679,  697,  658, 1306,  625,  360, 1323,\n",
      "        1255,  741, 1188,  339,  808,  469,  773,  906], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,  927,    8, 1034,  876,\n",
      "         578,  679,  301,  696,  347,  139,  698,  216], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  647,  335, 1181,  730,  833,  661,  904,  834,\n",
      "        1153,  810,  333,  665,  717,  648,  716,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 189, 158, 737, 340,  95, 457, 680, 373,\n",
      "        249,  52, 522, 460, 738,  30], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 242, 255, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855, 1017,  185,  152, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760, 1236,  755, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 580, 535, 185,  51, 626,\n",
      "        262, 284, 124, 240, 385, 560], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  920,\n",
      "         598,  579,  165,  680,  389,  672,  677,  139], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  769,  372,  775,  776,  263,  203,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 515, 258, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 137, 275,  83, 134, 295, 332,  66,\n",
      "        246, 173, 158, 229, 211, 370], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 198, 154,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1075, 1152,  237,  224, 1169,  888, 1256,\n",
      "         337,  648,  425,  238,  766, 1298,  263,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  459,  908,  649,  648,  464,   58,  203,  148,  721,  859,  238,\n",
      "         675,  660,  673, 1105,  686,  683,  802,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 870, 891, 268, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 257, 661, 945,  97, 191, 193, 184, 949, 791, 952,\n",
      "        871, 128, 183, 695, 194, 592], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  414,  620, 1036, 1002,  890], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 163, 648, 696, 839, 840, 804, 718, 725, 244, 769,\n",
      "        860, 661, 145, 109, 805, 133], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310,  56, 346, 313, 136, 855, 372, 241,  50, 712, 371,\n",
      "         30, 850, 380, 693, 657, 731], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 710, 685,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  320, 1146,  134,  146,  131,  125,   81,  142, 1158,   75,\n",
      "        1559,  266,  162,  617,  425, 1147,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  643,  282,  481,  334,  313,  879,  383,  629,\n",
      "        1163,  347,  216,  261, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: avg_val_f1 reached 0.00000 (best 0.00000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_4.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.5797, device='cuda:0')\tavg_answer_loss:  tensor(6.1863, device='cuda:0')\tavg_type_loss:  tensor(1.4787, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5ac32c305542995ef918c159\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 737, 1245,  885,  773,  906, 1174,  143,  208,  366,  408,  363, 1084,\n",
      "         739,  370, 1238,  292, 1243,  917,  525,  411], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ab83cfc5542990e739ec884\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([755, 660, 279, 448, 221, 137, 138, 358, 665, 426, 218, 352, 497, 238,\n",
      "        281, 615, 697, 269, 359, 200], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5a731ba95542991f9a20c608\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1327,  302,  179,  690,  759, 1162,  602,  626,  620,  172, 1116, 1063,\n",
      "         775, 1147,  194, 1174,  301, 1382,  220,  754], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab990925542996be2020553\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([111, 245, 754, 805, 675, 720, 841, 953, 154, 662, 899, 759, 702, 248,\n",
      "        552, 840, 842, 661, 152, 679], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7b8d85554299294a54a9ee\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 294,  360,  290,  772, 1279, 1172,  307, 1311, 1218, 1285, 1181,  247,\n",
      "         403,  978,  396, 1104,  278, 1199, 1095, 1187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5a7e72555542991319bc94b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([848, 587, 830, 461,  57, 516, 683, 476, 622, 732, 124, 631, 763, 623,\n",
      "        291, 726, 247, 713,  92, 682], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5a8fafb15542992414482b3f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([232, 251, 731, 351, 742, 173, 139, 348, 627, 208, 595, 122, 220, 893,\n",
      "        356, 649, 379, 162, 735, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae52b015542990ba0bbb1df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([117, 337, 115, 370, 290, 680, 375, 368, 335, 730, 786, 301, 296, 665,\n",
      "        733, 221, 187, 195, 715, 351], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5ab29544554299545a2cf9ac\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 273,  145,  146,  138, 1152, 1026, 1191,  790, 1040,  140,  124,  973,\n",
      "        1139,  153, 1123,  247,  970,  956,  986,  721], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5a8da77e554299441c6ba02d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([279, 413, 380,  97, 386, 406, 758, 226, 415, 385, 242, 402, 282, 738,\n",
      "        692, 214, 856,  96, 383, 245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  10\n",
      "qid:  5ac00829554299012d1db57d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 258,  893,  874,  757, 1001,  240, 1112,  689,  236,  488,  377,  670,\n",
      "         235,  372,  804,  225,  700,  975,  836,  967], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  11\n",
      "qid:  5ae0e2865542993d6555ec7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 568,  490, 1487,  157,  485,  872,   90,  868, 1254, 1202,  613,  749,\n",
      "        1245,  721, 1203, 1275, 1269,  829, 1108, 1510], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  12\n",
      "qid:  5a8e332f5542995a26add481\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 996, 1081,  151, 1001,  346,  985,  123,  917,  769,  908,  771,  935,\n",
      "         195,  866,  761,  561,  503,   94,  747,  282], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  13\n",
      "qid:  5ae2086f5542997283cd233c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([260, 749, 396, 842, 728, 382, 761, 685, 684, 513, 750, 240, 757, 725,\n",
      "        409, 485, 117, 190, 157, 692], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  14\n",
      "qid:  5a7c2fce5542996dd594b8b6\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([265, 260, 219, 227, 285, 145, 243, 426, 296, 361, 591, 248, 735, 456,\n",
      "         71, 239, 398, 324, 297, 666], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  15\n",
      "qid:  5abb68e25542996cc5e49fe9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 766,  220,  777,  395,  532, 1252,  772,  355,  789, 1247, 1821,  699,\n",
      "         812, 1692,  363, 1263,  701,  312,  683,  770], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  16\n",
      "qid:  5ab3f3815542992ade7c6ef3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([355, 541, 230, 441, 656, 649, 501, 304, 146, 833, 629, 953, 696, 979,\n",
      "        746, 384, 758, 369, 860, 760], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  17\n",
      "qid:  5ab32e0e55429969a97a80f4\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 780,  855,  775,  605,  235,  253,  376,  803,  161,  361,  208,  154,\n",
      "         232,  801,  213, 1031,  695,  138,  796,   91], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  18\n",
      "qid:  5ac28da95542996366519a05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([835, 148, 268, 721, 238, 290, 771, 488, 785, 304, 786, 708, 229, 197,\n",
      "        882, 365, 685, 245, 739, 745], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  19\n",
      "qid:  5adbf05455429947ff173859\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 239,  871,  707,  297,  213,  768,  244,  377,  336,  223, 1359, 1314,\n",
      "         737, 1159,  723, 1087,  508,  758,  466,  333], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  20\n",
      "qid:  5ac530c15542994611c8b40c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1256,  783,  250, 1292,  241,  781,  246,  341,  300, 1179,  632,  379,\n",
      "        1254,  304,  631,  240,  736, 1020,  658,  611], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  21\n",
      "qid:  5a8b03c255429971feec4600\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([769, 734, 173, 900, 899, 940, 886, 909,  51, 877, 357, 908, 176, 126,\n",
      "        676, 739, 761, 795, 751, 300], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  22\n",
      "qid:  5a85d0e15542997175ce2040\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1099, 1095,  668,  672,  156,  738, 1280,  133, 1160, 1297, 1121, 1007,\n",
      "        1281,  734,  932,  267,  266, 1222,  262, 1156], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  23\n",
      "qid:  5a874a0c5542996432c57251\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860,  848,  844,  291,  454,  296,  634,  245,  459,   12,  852,  926,\n",
      "         609,  631,  610,  228,   13,  383,  371, 1027], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  24\n",
      "qid:  5a7221f055429971e9dc92b9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 209,  394, 1001,  159,  280,  305,  124,   91,  730,  264,  409, 1002,\n",
      "         235,  737,  397,  101,   19,  214,  954,  705], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  25\n",
      "qid:  5abcf46a55429965836004df\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([148, 696, 177, 886, 147, 139, 830, 203, 134, 806, 219, 770, 894, 795,\n",
      "        318, 138, 871, 309, 585, 733], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  26\n",
      "qid:  5ab92f83554299753720f77f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1706, 1231,  665,  639, 1191, 1229,  659,  780,  198,  362,  468,  430,\n",
      "         254,  679,  424,  255, 1150, 1718,  680, 1053], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  27\n",
      "qid:  5adfde0c55429925eb1afad8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 232,  835,  384,  843,  759,  878,  744,  380, 1206,  229,  241,  280,\n",
      "         627,  691, 1172, 1137,  662,  228,  694, 1101], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  28\n",
      "qid:  5ae0e16d5542993d6555ec7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([786, 234, 244, 431, 107, 480, 452, 782, 386, 326, 376, 717, 130, 775,\n",
      "        336, 305, 177, 699, 646, 118], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  29\n",
      "qid:  5ae7d64a554299540e5a565d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1377,  232, 1234, 1720, 1171,  681,  239, 1739,  361, 1806, 1591,   69,\n",
      "        1786, 1934,  177,  851, 1380,  752, 1723, 1396], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  30\n",
      "qid:  5a79b1b85542996c55b2dc37\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 316,  745, 1173,  248,  904,  118,  679,  672,  243, 1182,  338,  732,\n",
      "         341,  354, 1175, 1180,  377,  886, 1101,  149], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  31\n",
      "qid:  5a7d449d5542991319bc9380\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783, 1220, 1239, 1200,  227,  697,  247, 1169,  281,  242,  672,  273,\n",
      "         270, 1185, 1007,  591,  268,  286,  895, 1114], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  32\n",
      "qid:  5adce33b55429947343537bf\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 762, 240, 678, 620, 798, 717,  12, 775, 725, 231, 748, 828, 906,\n",
      "        682, 869, 747, 715, 306, 189], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  33\n",
      "qid:  5ae09e8a55429924de1b7115\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 141,  697,  705,  720,  732,  743,  468,  712,  323,  321,  737,  455,\n",
      "         481,  448,  428,  408, 1006,  757,  747,  398], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  34\n",
      "qid:  5a8b379055429949d91db4ea\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 152,  835, 1259,  201,  358,  380,  633,   95, 1269,  189,  126,   97,\n",
      "         706,  147,  156,  764,  258,  636, 1245,  161], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  35\n",
      "qid:  5abfedab5542997d6429594b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([344, 663, 745, 672, 228, 151, 304, 735, 699, 527, 660, 723, 187, 132,\n",
      "        537, 362, 637, 279, 454, 131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  36\n",
      "qid:  5ae823185542997ec272772c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1309,  257,  616,  591,  171,  345,  238,  634,   84,  277,  203,  186,\n",
      "         621,  479,  262,  721,  422,  778,  872,  187], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  37\n",
      "qid:  5adf452b5542995ec70e8fb0\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 755,  756,  754, 2298,  774,  733, 1182,  123, 2068,  188,  189, 1646,\n",
      "        1882, 1892, 1395, 1394,   18, 1771,  190, 1709], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  38\n",
      "qid:  5abbfb605542993f40c73c39\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([698, 781, 283, 736, 807, 872, 198, 229, 731, 662, 718, 879, 132, 375,\n",
      "        657, 607, 311, 648, 106, 895], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  39\n",
      "qid:  5a8b83355542997f31a41d59\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 119,  144,  179,   96, 1183,   27,   23,  476,  120,  371,  351,  465,\n",
      "         663,  342,  148,   68,  739,   42, 1198,  172], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  40\n",
      "qid:  5a773ff45542994aec3b7248\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 243,  803,  702,  114,  254,  560,  240, 1008,  167,  256,  225,  367,\n",
      "        1232,  149,  133,  480,  255,  162,  917,  465], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  41\n",
      "qid:  5add316c5542992ae4cec4e4\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([498, 105, 210,  49, 714, 365, 768, 706, 497, 188, 205, 629, 406, 585,\n",
      "        127, 126, 359, 189, 483,   8], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  42\n",
      "qid:  5a8c3f0a5542996e8ac88a2e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([901, 320, 767, 126, 669, 245, 753, 800, 745, 246, 647, 244, 748, 768,\n",
      "        310, 648, 742, 769, 590, 313], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  43\n",
      "qid:  5adfd81d55429906c02daa68\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([656, 739, 614, 683, 758, 625, 750, 229, 591, 158, 335, 142, 649, 709,\n",
      "        264, 660, 744, 116, 626, 230], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  44\n",
      "qid:  5a70f2275542994082a3e417\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 286,  533,  302,  377, 1003,  279,  246,  462,  451,  781,  313,  320,\n",
      "        1130, 1112,  316,  746,  618,  430,  297,  530], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  45\n",
      "qid:  5ae4ddef5542990ba0bbb182\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 661, 1209,  752,   92,  130,  148, 1153,  659,  306, 1146,  119,  128,\n",
      "         797, 1226,  622,  384, 1179,  273,  845,  805], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  46\n",
      "qid:  5a7a3d435542994f819ef184\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 783,  784,  683,  386,  889, 1137,  775,  786,  787,  211,  810,  789,\n",
      "         111,  915, 1153,  873, 1148,  916,  187, 1012], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  47\n",
      "qid:  5a79e2ac5542994bb9457128\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 867,  868,  797,  517,  658,  509,  137,  286,  815,  912,  251,  314,\n",
      "         756,  179,  711,  355,  758,  174,  253, 1131], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  48\n",
      "qid:  5ae74456554299572ea547a3\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 767,  774,  443, 1172,  233,  455,  660,  889,  851,  945,  846,  639,\n",
      "         259,  204, 1070,  944,  619,   79,  745,  633], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  49\n",
      "qid:  5a8a17e55542992d82986eab\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1232, 1155,   32, 1161,  877,   70,  750,  778,   38, 1191,  761, 1026,\n",
      "          21,  495,  757, 1169,  187,  875,  504, 1163], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  50\n",
      "qid:  5ab302f655429976abd1bc1e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 702,  253, 1044,  624, 1188,  982,   80,  767, 1408,  232, 1294,  814,\n",
      "         137, 1186,  889,  416, 1042, 1292,  975,  142], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  51\n",
      "qid:  5ab78ddb5542995dae37e958\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 659,  243,  345,  623,  615,  354,  765,  370,   49,  898,  358,  654,\n",
      "         903,  349,  743, 1134,  406,  182,  666,  237], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  52\n",
      "qid:  5ac2c410554299218029db2f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([319, 760, 239, 936, 249, 673, 661, 389, 139, 230, 756,  40, 323, 248,\n",
      "        754, 987, 221, 755, 669, 309], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  53\n",
      "qid:  5a7f78805542992097ad2f7d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 754,  726, 1282,  369, 1266, 1295,   20, 1311, 1100,  816, 1270,   31,\n",
      "        1296,  808,  239,  809,  521, 1306, 1267,   70], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  54\n",
      "qid:  5a7dc4ce5542997cc2c4749f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([169, 255, 296, 222, 136, 465, 379, 223, 265, 201, 370, 317, 155, 224,\n",
      "        142, 228, 424, 240, 235, 197], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  55\n",
      "qid:  5a837a64554299334474601b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 253,  272,  924,  389,  342,  405,  250,  147,  425,  460,  723,  754,\n",
      "         349,  558,  235,  316,  346, 1119,  638,  462], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  56\n",
      "qid:  5a83b55a554299123d8c2195\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 295,  659, 1234,  198, 1204,  679,  697, 1306,  658,  360,  625, 1323,\n",
      "        1255,  741, 1188,  339,  808,  469,  906,  773], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  57\n",
      "qid:  5a8cebfa554299441c6b9f7c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 270,  251,  545,  329,  701,  265,  264,   54,    8,  927, 1034,  578,\n",
      "         876,  679,  301,  696,  347,  139,  698,  216], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  58\n",
      "qid:  5ae764ef5542997b22f6a723\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 759,   49,  696,  771,  335,  647, 1181,  730,  833,  904,  661,  834,\n",
      "        1153,  810,  333,  665,  716,  717,  648,  220], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  59\n",
      "qid:  5a8f06b9554299458435d521\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([271, 253, 246, 728, 662,  12, 189, 158, 737, 340,  95, 457, 680, 373,\n",
      "        249,  52, 522, 460,  30, 738], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  60\n",
      "qid:  5adf45735542993344016c4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([758, 344, 345, 245, 821, 295, 823, 819, 255, 242, 750, 905, 115, 770,\n",
      "        730, 738, 191, 244, 253, 634], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  61\n",
      "qid:  5aba7a4955429955dce3ee53\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 860, 1237,  391,  855, 1017,  185,  152, 1148,  347,  202, 1160,  196,\n",
      "        1285,  182, 1187,  781,  760, 1236,  755, 1001], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  62\n",
      "qid:  5a8ae0335542992d82986fc8\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([519, 230, 335, 331, 263, 229, 134, 238, 548, 580, 535,  51, 185, 626,\n",
      "        262, 284, 124, 240, 385, 333], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  63\n",
      "qid:  5add72405542992ae4cec56b\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits_indices:  tensor([ 659, 1239, 1138,  796,  789, 1219,  249, 1196,  793, 1264, 1194,  920,\n",
      "         598,  579,  165,  680,  389,  672,  677,  139], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  64\n",
      "qid:  5a7729245542993569682d0b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 361,  161,  135,  101,  847,  769,  372,  775,  776,  203,  263,  129,\n",
      "         146, 1133,  746,   30,  803,  352,   33,  689], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  65\n",
      "qid:  5a7508be5542996c70cfae7f\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([524, 219, 515, 258, 237, 236, 776, 518, 871, 121, 977, 220,  12, 459,\n",
      "        629, 556, 207, 293, 769, 399], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  66\n",
      "qid:  5ab6b101554299110f219a4c\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([220, 345, 182, 258, 217, 106, 350, 137, 275,  83, 134, 295, 332, 246,\n",
      "        173,  66, 158, 229, 211, 370], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  67\n",
      "qid:  5ac282b855429963665199ef\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([376, 220,  14, 315, 140, 259,  73, 257, 486,  94, 246, 302, 198, 154,\n",
      "        452, 132, 117, 415, 267, 169], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  68\n",
      "qid:  5adf341a5542993a75d26421\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 278, 1176,  718, 1064,  417, 1075, 1152,  237,  224,  888, 1169, 1256,\n",
      "         337,  648,  766,  238,  425, 1298,  263,  245], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  69\n",
      "qid:  5ae802d55542993210984032\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([  30,  459,  908,  649,  648,  464,   58,  203,  148,  721,  859,  238,\n",
      "         675,  660,  673, 1105,  686,  802,  683,  726], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  70\n",
      "qid:  5ae300d15542991a06ce991d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([186, 307, 252, 650, 818, 812, 287, 110, 305, 919, 846, 709, 847, 691,\n",
      "        236, 339, 891, 870, 268, 273], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  71\n",
      "qid:  5ae244ac5542996483e6494e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([206, 747, 652, 771, 257, 661, 945,  97, 191, 193, 184, 949, 791, 871,\n",
      "        952, 128, 183, 194, 695, 106], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  72\n",
      "qid:  5a74c84d55429974ef308c60\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1393,  383, 1127, 1167, 1325,  627, 1208, 1276,  114, 1383,  850, 1280,\n",
      "        1359,  683,  119,  414,  620, 1036, 1002,  877], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  73\n",
      "qid:  5ab521fb5542991779162d8b\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([334, 299, 300, 209, 326, 204, 180, 247,  47, 146, 233, 188, 302, 242,\n",
      "        145, 305, 346, 284, 143, 255], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  74\n",
      "qid:  5ae725ad554299572ea54732\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 44, 775, 750, 252, 163, 696, 648, 839, 840, 804, 718, 725, 244, 769,\n",
      "        860, 661, 145, 109, 805, 133], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  75\n",
      "qid:  5a8052315542992bc0c4a6fe\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([770, 374, 876, 310, 346,  56, 313, 136, 855, 372, 241,  50, 712, 371,\n",
      "         30, 850, 380, 693, 731, 657], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  76\n",
      "qid:  5ae497475542995dadf24364\n",
      "q_type:  tensor([2], device='cuda:0')\n",
      "start_logits_indices:  tensor([721, 766, 159, 162, 663, 394, 613, 402, 603, 191, 183, 158, 710, 685,\n",
      "        675,  72, 671, 325, 362, 636], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  77\n",
      "qid:  5a8fc2a755429933b8a20452\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([1182, 1116,  320, 1146,  146,  134,  131,  125,   81,  142, 1158,   75,\n",
      "        1559,  266,  162,  617,  425, 1147,  148,  173], device='cuda:0')\n",
      "validation_step\n",
      "batch_nb:  78\n",
      "qid:  5a8840ec5542994846c1ce64\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "start_logits_indices:  tensor([ 802,  796,  183,  753,  643,  282,  481,  334,  313,  879,  383,  629,\n",
      "        1163,  216,  347,  261, 1066,  196,  322,  755], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: avg_val_f1  was not in top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  79, 79, 79\n",
      "after sync --> sizes: 79, 79, 79\n",
      "avg_loss:  tensor(13.5439, device='cuda:0')\tavg_answer_loss:  tensor(6.1859, device='cuda:0')\tavg_type_loss:  tensor(1.4716, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     if not args.test: \n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer_jupyter')\n",
      "('train_dataset', 'small_dev2.json')\n",
      "('dev_dataset', 'small.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 4)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', False)\n",
      "('model_path', '/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small_dev2.json --dev_dataset small.json  \\\n",
    "    --gpus 0 --num_workers 4 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer_jupyter  --model_path /xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096'\n",
    "# hotpot_dev_distractor_v1.json\n",
    "\n",
    "import argparse \n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    for arg in vars(args):\n",
    "        print((arg, getattr(args, arg)))\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "config = LongformerConfig.from_pretrained('/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096') \n",
    "# choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n",
    "# 'n2': for regular n2 attantion\n",
    "# 'tvm': a custom CUDA kernel implementation of our sliding window attention\n",
    "# 'sliding_chunks': a PyTorch implementation of our sliding window attention\n",
    "config.attention_mode = 'sliding_chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Longformer.from_pretrained('/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096', config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n",
    "\n",
    "# TVM code doesn't work on CPU. Uncomment this if `config.attention_mode = 'tvm'`\n",
    "model = model.cuda() \n",
    "input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n",
    "attention_mask[:, [1, 4, 21,]] =  2  # Set global attention based on the task. For example,\n",
    "                                     # classification: the <s> token\n",
    "                                     # QA: question tokens\n",
    "\n",
    "# padding seqlen to the nearest multiple of 512. Needed for the 'sliding_chunks' attention\n",
    "input_ids, attention_mask = pad_to_window_size(\n",
    "        input_ids, attention_mask, config.attention_window[0], tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
