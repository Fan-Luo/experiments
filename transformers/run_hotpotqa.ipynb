{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "import transformers\n",
    "import shap\n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null </s>\"\n",
    "\n",
    " \n",
    "def create_example_dict(context, answer, id, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answer\": answer,\n",
    "                \"id\": id,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )    \n",
    "        \n",
    "        for lst in raw_contexts:\n",
    "            lst[0] = normalize_answer(lst[0])\n",
    "            lst[1] = [normalize_answer(sent) for sent in lst[1]]\n",
    "        \n",
    "        contexts = [TITLE_START + ' ' + lst[0]  + ' ' + TITLE_END + ' ' + (' ' + SENT_MARKER_END +' ').join(lst[1]) + ' ' + SENT_MARKER_END for lst in raw_contexts]    \n",
    "        # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space; call the normalize_answer() which is same as the one used during evaluation\n",
    "   \n",
    "        context = \" \".join(contexts)\n",
    "#         print(context)\n",
    "        \n",
    "#         exit(0)\n",
    "\n",
    "        \n",
    "        answer = normalize_answer(example[\"answer\"]) \n",
    "#         print(\"answer: \", answer)\n",
    "        if(len(answer) > 0):   # answer can be '' after normalize\n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=context,\n",
    "                        answer=answer,\n",
    "                        id = example[\"_id\"],\n",
    "                        question=normalize_answer(example[\"question\"]),\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": \"<t> dr seuss how grinch stole christmas </t> dr seuss how grinch stole christmas [/sent] is video game based on dr seuss book with same name but mostly based on film [/sent] game was released on november 8 2007 [/sent] <t> lorax film </t> lorax also known as dr seuss lorax is 2012 american 3d computeranimated musical fantasy\\u2013comedy film produced by illumination entertainment and based on dr seusss childrens book of same name [/sent] film was released by universal pictures on march 2 2012 on 108th birthday of dr seuss [/sent] second film adaptation of book following 1972 animated television special film builds on book by expanding story of ted previously unnamed boy who visits onceler [/sent] cast includes danny devito as lorax ed helms as onceler and zac efron as ted [/sent] new characters introduced in film are audrey voiced by taylor swift aloysius ohare rob riggle mrs wiggins teds mother jenny slate and grammy norma betty white [/sent] <t> horton hears who tv special </t> horton hears who [/sent] is 1970 television special based on dr seuss book of same name horton hears who [/sent]  [/sent] it was produced and directed by chuck jones \\u2013 who previously produced seuss special how grinch stole christmas [/sent] \\u2013 for mgm television and first broadcast march 19 1970 on cbs [/sent] special contains songs with lyrics by seuss and music by eugene poddany who previously wrote songs for seuss book cat in hat song book [/sent] <t> dr seuss memorial </t> dr seuss national memorial sculpture garden is sculpture garden in springfield massachusetts that honors theodor seuss geisel better known to world as dr seuss [/sent] located at quadrangle dr seuss national memorial sculpture garden honors author and illustrator who was born in springfield in 1904 [/sent] monument was designed by lark grey dimondcates authors stepdaughter and created by sculptor and artist ron henson [/sent] <t> dr seuss bibliography </t> theodor seuss geisel better known as dr seuss published over 60 childrens books over course of his long career [/sent] though most were published under his wellknown pseudonym dr seuss he also authored over dozen books as theo [/sent] lesieg and one as rosetta stone [/sent] as one of most popular childrens authors of all time geisels books have topped many bestseller lists sold over 222 million copies and been translated into more than 15 languages [/sent] in 2000 when publishers weekly compiled their list of bestselling childrens books of all time 16 of top 100 hardcover books were written by geisel including green eggs and ham at number 4 cat in hat at number 9 and one fish two fish red fish blue fish at number 13 and dr seusss abc [/sent] in years following his death in 1991 several additional books based on his sketches and notes were published including hooray for diffendoofer day [/sent] and daisyhead mayzie [/sent] although they were all published under name dr seuss only my many colored days originally written in 1973 was entirely by geisel [/sent] <t> how grinch stole christmas 2018 film </t> dr seuss how grinch stole christmas promoted theatrically as dr seuss grinch is upcoming american 3d computeranimated christmas musical comedy film produced by illumination entertainment [/sent] it is based on 1957 dr seuss story of same name [/sent] film will be released on november 9 2018 by universal pictures [/sent] <t> do you know what im going to do next saturday </t> do you know what im going to do next saturday [/sent] is 1963 childrens book published by beginner books and written by helen palmer geisel first wife of theodor seuss geisel dr seuss [/sent] unlike most of beginner books do you know what im going to do next saturday [/sent] did not follow format of text with inline drawings being illustrated with blackandwhite photographs by lynn fayman featuring boy named rawli davis [/sent] it is sometimes misattributed to dr seuss himself [/sent] books cover features photograph of young boy sitting at breakfast table with huge pile of pancakes [/sent] <t> wubbulous world of dr seuss </t> wubbulous world of dr seuss is liveactionpuppet television series based on characters created by dr seuss produced by jim henson company [/sent] it aired from october 13 1996 to december 28 1998 on nickelodeon [/sent] it is notable for its use of live puppets with digitally animated backgrounds and in its first season for refashioning characters and themes from original dr seuss books into new stories that often retained much of flavor of dr seuss own works [/sent] <t> cat in hat film </t> dr seuss cat in hat is 2003 american family comedy film directed by bo welch [/sent] it is based on 1957 dr seuss book of same name [/sent] film stars mike myers in title role of cat in hat and dakota fanning as sally [/sent] sallys brother who is unnamed in book and 1971 tv special conrad is portrayed by spencer breslin [/sent] film is second featurelength dr seuss adaptation after 2000 holiday film how grinch stole christmas [/sent] <t> kyle balda </t> kyle balda is american animator and film director best known for codirecting animated films lorax 2012 with chris renaud and minions 2015 with pierre coffin [/sent] he has also worked as animator on several films including jumanji toy story 2 and despicable me [/sent] he has worked for pixar for years and now he is working for illumination entertainment [/sent]\",\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answer\": \"lorax\",\n",
      "          \"id\": \"5ab990925542996be2020553\",\n",
      "          \"question\": \"what film did kyle balda work on that was based on dr seuss book\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/Data')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[76200:76280]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[37:50]' > small_dev.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[31:50]' > sample.json\n",
    "\n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "# sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.8/site-packages')\n",
    "\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "#!python -m pip install jdc \n",
    "# !pip install test-tube \n",
    "# !conda install ipywidgets --yes\n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size \n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.8/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        if(\"reduced_context\" not in self.file_path):\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "                \n",
    "        else:\n",
    "            with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "                print(f'reading file: {self.file_path}')\n",
    "                self.data_json = json.load(f)['data']            \n",
    "                print(self.data_json[0])\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "\n",
    "#         print(tokenizer.all_special_tokens) \n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =                  {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        if(len(tensors_list) != 1):\n",
    "            print(\"tensors_list: \", tensors_list)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     106,
     122,
     147,
     162
    ]
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset):\n",
    "            \n",
    "            # char offset to word offset\n",
    "            start_word_position = char_to_word_offset[answer_start]\n",
    "            end_word_position = char_to_word_offset[answer_end-1] \n",
    "\n",
    "#             print(\"start_word_position: \", start_word_position)\n",
    "#             print(\"end_word_position: \", end_word_position)\n",
    "            # sub_tokens postion reletive to context\n",
    "            tok_start_position_in_doc = orig_to_tok_index[start_word_position]  \n",
    "            not_end_of_doc = int(end_word_position + 1 < len(orig_to_tok_index))\n",
    "            tok_end_position_in_doc = orig_to_tok_index[end_word_position + not_end_of_doc] - not_end_of_doc\n",
    "            \n",
    "            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                return (-1, -1) # this answer is outside the current slice                     \n",
    "            \n",
    "            # sub_tokens postion reletive to begining of all the tokens, including query sub tokens  \n",
    "            start_position = tok_start_position_in_doc + doc_offset  \n",
    "            end_position = tok_end_position_in_doc + doc_offset\n",
    "            \n",
    "            return (start_position, end_position)\n",
    "        \n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            context = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in context:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            \n",
    "#             print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "#             print(\"char_to_word_offset: \", char_to_word_offset)\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "#                 print(\"question text: \", question_text)  \n",
    "                sp_sent = qa[\"is_sup_fact\"]\n",
    "                sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None \n",
    "\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent) \n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    " \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                \n",
    "                   \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                 \n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                \n",
    "                # each original token in the context is tokenized to multiple sub_tokens\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "                \n",
    "                # all sub tokens, truncate up to limit\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len-8] \n",
    "\n",
    "                # The -8 accounts for CLS, QUESTION_START, QUESTION_END， [/par]， yes， no， null， </s>   \n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 8\n",
    "                if(max_tokens_per_doc_slice <= 0):\n",
    "                    print(\"(max_tokens_per_doc_slice <= 0)\")\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "#                 print(\"before for\")\n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [self.tokenizer.cls_token] + [QUESTION_START] + query_tokens + [QUESTION_END] + doc_slice_tokens + [PAR] + self.tokenizer.tokenize(\"yes\") + self.tokenizer.tokenize(\"no\") + self.tokenizer.tokenize(\"null\") +  [self.tokenizer.eos_token]   \n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] * (len(doc_slice_tokens) + 5) \n",
    "#                     if(len(segment_ids) != len(tokens)):\n",
    "#                         print(\"len(segment_ids): \", len(segment_ids))\n",
    "#                         print(\"len(tokens): \", len(tokens))\n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    \n",
    "                    # ===== answer positions tensors  ============\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    " \n",
    "                    answer = qa[\"answer\"] \n",
    "                    print(\"answer: \", answer)\n",
    "                    if answer == 'yes':\n",
    "                        q_type = 1\n",
    "                        start_positions.append(len(tokens)-4)   \n",
    "                        end_positions.append(len(tokens)-4) \n",
    "                    elif answer == 'no':\n",
    "                        q_type = 2\n",
    "                        start_positions.append(len(tokens)-3)   \n",
    "                        end_positions.append(len(tokens)-3)  \n",
    "                    else:\n",
    "                        # keep all the occurences of answer in the context \n",
    "#                         for m in re.finditer(\"\\s?\".join(answer.split()), context):   # \"\\s?\".join(answer.split()) in order to match even with extra space in answer or context\n",
    "                        for m in re.finditer(normalize_answer(answer), context, re.IGNORECASE):\n",
    "                            answer_start, answer_end = m.span() \n",
    "                            start_position, end_position = map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset)\n",
    "                            if(start_position != -1):\n",
    "                                start_positions.append(start_position)   \n",
    "                                end_positions.append(end_position)\n",
    "                            \n",
    "                        if(len(start_positions) > 0): \n",
    "                            q_type = 0\n",
    "                        else: # answer not found in context\n",
    "                            q_type = 3 \n",
    "                            start_positions.append(len(tokens)-2)   \n",
    "                            end_positions.append(len(tokens)-2)  \n",
    "\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        \n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "                        \n",
    "#                         # for debug\n",
    "#                         if(start_position != -1):\n",
    "#                             answer_token_ids = input_ids[start_position: end_position+1]\n",
    "#                             answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids)\n",
    "#                             answer_text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#                             print(\"answer_text: \", answer_text)\n",
    "                        \n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "                        \n",
    "                        print(\"self.doc_stride >= 0\")\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len  \n",
    "                        \n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                    \n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                      torch.tensor([sp_sent_list]),  torch.tensor([sp_para_list]),\n",
    "                                     qa['id'], answer))     \n",
    "        return tensors_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 2  # qid and answer  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7f31c84676d0>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  hotpotqaDataset(file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |  \n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    " \n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [TITLE_START, TITLE_END, SENT_MARKER_END, QUESTION_START , QUESTION_END, PAR]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "         \n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 4)   #  question type (yes/no/span/null) classification \n",
    "           \n",
    "       \n",
    "        self.fnn_sp_sent = torch.nn.Sequential(\n",
    "          torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "          torch.nn.GELU(),\n",
    "          torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "        )\n",
    "        \n",
    "        self.fnn_sp_para = torch.nn.Sequential(\n",
    "          torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "          torch.nn.GELU(),\n",
    "          torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "        )\n",
    "         \n",
    "        \n",
    "        self.train_dataloader_object = self.val_dataloader_object = self.test_dataloader_object = None\n",
    "        \n",
    " \n",
    "    def load_model(self):\n",
    "        \n",
    "        config = LongformerConfig.from_pretrained(self.args.model_path) \n",
    "        config.attention_mode = self.args.attention_mode\n",
    "        model = Longformer.from_pretrained(self.args.model_path, config=config)\n",
    "        # model = Longformer.from_pretrained(self.args.model_path) \n",
    "\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "#%%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "###################################################### dataloaders ########################################################### \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "        \n",
    "        dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, sampler=dist_sampler, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl  \n",
    "        return self.train_dataloader_object\n",
    "    \n",
    " \n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "\n",
    "        \n",
    "        \n",
    "        dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, sampler=dist_sampler, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataloader_object is not None:\n",
    "            return self.test_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "        dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, sampler=dist_sampler, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.test_dataloader_object = dl\n",
    "        return self.test_dataloader_object\n",
    "\n",
    "#%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para):\n",
    " \n",
    "        if(input_ids.size(0) > 1):\n",
    "            assert(\"multi rows per document\")\n",
    "        # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "        # Make sure all rows have the same question length.\n",
    "        \n",
    " \n",
    "        # local attention everywhere\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "        # global attention for the cls and all question tokens\n",
    "        question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "#         if(question_end_index.size(0) == 1):\n",
    "#             attention_mask[:,:question_end_index.item()] = 2  \n",
    "#         else:\n",
    "        attention_mask[:,:question_end_index[0].item()+1] = 2  # from <cls> until </q>\n",
    "#             print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "        \n",
    "        # global attention for the sentence and paragraph special tokens  \n",
    "        sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "        attention_mask[:, sent_indexes] = 2\n",
    "        \n",
    "        para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "        attention_mask[:, para_indexes] = 2       \n",
    "         \n",
    "\n",
    "        # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "        input_ids, attention_mask = pad_to_window_size(\n",
    "            input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "\n",
    "        sequence_output = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask)[0]\n",
    "#         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "\n",
    "        # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "        # before computing loss and decoding.\n",
    "        padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "        if padding_len > 0:\n",
    "            sequence_output = sequence_output[:, :-padding_len]\n",
    "#         print(\"size of sequence_output after removing padding: \" + str(sequence_output.size()))\n",
    "              \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1) \n",
    "        start_logits = start_logits.squeeze(-1) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    " \n",
    "        ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "        type_logits = self.linear_type(sequence_output[:,0]) \n",
    "        \n",
    "        ### 3. supporting paragraph classification ###  \n",
    "        sp_para_output = sequence_output[:,para_indexes,:]  \n",
    "        sp_para_output_t = self.fnn_sp_para(sp_para_output) \n",
    "\n",
    "         # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no.   \n",
    "        # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "        sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "        predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous() \n",
    " \n",
    "        ### 4. supporting fact classification ###     \n",
    "        # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    " \n",
    "        sp_sent_output = sequence_output[:,sent_indexes,:]  \n",
    "        sp_sent_output_t = self.fnn_sp_sent(sp_sent_output)     \n",
    "        sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "        predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous() \n",
    "        \n",
    "        outputs = (start_logits, end_logits, type_logits, sp_para_output_t, sp_sent_output_t)  \n",
    "        answer_loss, type_loss, sp_para_loss, sp_sent_loss  = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)\n",
    " \n",
    "        outputs = (answer_loss, type_loss, sp_para_loss, sp_sent_loss,) + outputs    \n",
    "    \n",
    "    \n",
    "        explainer = shap.GradientExplainer( (logits, sequence_output), self.qa_outputs(sequence_output))\n",
    "        print(self.explainer)\n",
    "\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    " \n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "  \n",
    "\n",
    "            else: \n",
    "                start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = crossentropy(start_logits, start_positions[:, 0])\n",
    "                end_loss = crossentropy(end_logits, end_positions[:, 0])\n",
    "                \n",
    " \n",
    "            crossentropy = torch.nn.CrossEntropyLoss()\n",
    "            type_loss = crossentropy(type_logits, q_type)  \n",
    "            \n",
    "            crossentropy_average = torch.nn.CrossEntropyLoss(reduction = 'mean', ignore_index=-1)      \n",
    "            sp_para_loss = crossentropy_average(predict_support_para.view(-1, 2), sp_para.view(-1))\n",
    "            sp_sent_loss = crossentropy_average(predict_support_sent.view(-1, 2), sp_sent.view(-1))      \n",
    " \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss, type_loss, sp_para_loss, sp_sent_loss  \n",
    "\n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_tokens):\n",
    "        assert(input_ids.size(0)==1) \n",
    "        mask = input_ids != input_ids # initilaize \n",
    "        for special_token in special_tokens:\n",
    "            mask = torch.logical_or(mask, input_ids.eq(self.tokenizer.convert_tokens_to_ids(special_token))) \n",
    " \n",
    "        token_indices = torch.nonzero(mask)    \n",
    "         \n",
    " \n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    " \n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "    \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    " \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    " \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    " \n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    " \n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm) \n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "#         loss = torch.tanh(loss)\n",
    "#         print(\"final loss: \" + str(loss)) \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.nonzero(sp_sent, as_tuple=True)[1]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2\n",
    "\n",
    "\n",
    "# import torch\n",
    "# special_tokens = [1,2]\n",
    "# input_ids = torch.tensor([[ 1, 0, 2, 1, 0, 2]])\n",
    "\n",
    "# mask = input_ids != input_ids # initilaize \n",
    "# for special_token in special_tokens:\n",
    "#     mask = torch.logical_or(mask, input_ids.eq(special_token)) \n",
    "#     print(\"mask: \", mask)\n",
    "# torch.nonzero(mask)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # # print(tokenizer.all_special_tokens)    \n",
    "# # # # print(tokenizer.all_special_ids)     \n",
    "# # # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # # tokenizer.sep_token\n",
    "# print(tokenizer.tokenize(\"yes\"))\n",
    "# print(tokenizer.tokenize(\"no\"))\n",
    "# print(tokenizer.tokenize(\"null\"))\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    self.scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)  # scheduler is not saved in the checkpoint, but global_step is, which is enough to restart\n",
    "    self.scheduler.step(self.global_step)\n",
    "    print(\"global step: \", self.global_step)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimizer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook to do a lot of non-standard training tricks such as learning-rate warm-up\n",
    "def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.scheduler.step(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch \n",
    "    # print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    \n",
    "    print(self.explainer(input_ids))\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    # print(\"answer_loss: \", answer_loss)\n",
    "    # print(\"type_loss: \", type_loss)\n",
    "    # print(\"sp_para_loss: \", sp_para_loss)\n",
    "    # print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "\n",
    "#     loss  = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    loss = answer_loss + 5*type_loss + 10*sp_para_loss + 10*sp_sent_loss\n",
    "#     print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it.  \n",
    "\n",
    "    tensorboard_logs = {'loss': loss, 'train_answer_loss': answer_loss, 'train_type_loss': type_loss, \n",
    "                        'train_sp_para_loss': sp_para_loss, 'train_sp_sent_loss': sp_sent_loss, \n",
    "                        'lr': lr,\n",
    "                        'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) }\n",
    "    return tensorboard_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "    # # the function is called for each batch after every epoch is completed\n",
    "    # def training_end(self, output): \n",
    "    #     # print(\"training_end at epoch: \", self.current_epoch)\n",
    "    # #     print(\"len(outputs): \",len(outputs))\n",
    "    # #     print(\"output: \",output)\n",
    "    \n",
    "    #     # one batch only has one example\n",
    "    #     avg_loss = output['loss']    \n",
    "    #     avg_answer_loss = output['train_answer_loss']  \n",
    "    #     avg_type_loss = output['train_type_loss']    \n",
    "    #     avg_sp_para_loss = output['train_sp_para_loss']   \n",
    "    #     avg_sp_sent_loss = output['train_sp_sent_loss'] \n",
    "    #     avg_lr = output['lr']      \n",
    "         \n",
    "     \n",
    "    #     if self.trainer.use_ddp:\n",
    "    #         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_answer_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_type_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_para_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_sent_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_lr, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_lr /= self.trainer.world_size \n",
    "            \n",
    "     \n",
    "    #     tensorboard_logs = { #'avg_train_loss': avg_loss, \n",
    "    #             'avg_train_answer_loss': avg_answer_loss, 'avg_train_type_loss': avg_type_loss, 'avg_train_sp_para_loss': avg_sp_para_loss, 'avg_train_sp_sent_loss': avg_sp_sent_loss, 'lr': avg_lr\n",
    "    #           }\n",
    "    \n",
    "    #     return {'loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss + 5*type_loss + 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    pre_answer = normalize_answer(answers_pred[0]['text'])\n",
    "#         print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "#         print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = normalize_answer(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "#         print(\"f1: \" + str(f1))\n",
    "#         print(\"prec: \" + str(prec))\n",
    "#         print(\"recall: \" + str(recall))\n",
    "#         print(\"em: \" + str(em))  \n",
    "\n",
    "    if(len(sp_sent_pred) > 0):\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "        sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "        sp_sent_precision = torch.tensor(sp_sent_precision).type_as(loss)\n",
    "        sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "        sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "\n",
    "#         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "#         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "#         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "#         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "\n",
    "        joint_prec = prec * sp_sent_precision\n",
    "        joint_recall = recall * sp_sent_recall\n",
    "        if joint_prec + joint_recall > 0:\n",
    "            joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "        else:\n",
    "            joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "        joint_em = em * sp_sent_em \n",
    "\n",
    "    else:\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "        joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "               'answer_score': pre_answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em,\n",
    "               'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "               'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits):\n",
    "#         print(\"decode\")\n",
    "\n",
    "    question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "\n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "#     print(\"start_logits_indices: \", start_logits_indices)\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#         print(\"p_type: \", p_type)\n",
    "#         print(\"p_type_score: \", p_type_score)\n",
    "\n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#             print(\"sorted_answers: \" + str(sorted_answers))\n",
    "\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "            # remove [/sent], <t> and </t>\n",
    "            for special_token in [SENT_MARKER_END, TITLE_START, TITLE_END]:\n",
    "                try:\n",
    "                    answer_tokens.remove(special_token)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#             score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            score = (torch.sigmoid(answer['start_logit']) + torch.sigmoid(answer['end_logit']) + torch.sigmoid(p_type_score)) / 3\n",
    "            answers.append({'text': text, 'score': score})\n",
    "#             print(\"answers: \" + str(answers))\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score})\n",
    "    elif p_type == 3:\n",
    "        answers.append({'text': 'null', 'score': p_type_score})\n",
    "    else:\n",
    "        assert False \n",
    "\n",
    "\n",
    "    sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "    para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "\n",
    "    s_to_p_map = []\n",
    "    for s in sent_indexes:\n",
    "        s_to_p = torch.where(torch.le(para_indexes, s))[0][-1]     # last para_index smaller or equal to s\n",
    "        s_to_p_map.append(s_to_p.item()) \n",
    "#         print(\"s_to_p_map: \" + str(s_to_p_map))\n",
    "\n",
    "#         print(\"sp_para_logits\", sp_para_logits)\n",
    "#         print(\"sp_sent_logits\", sp_sent_logits)\n",
    "\n",
    "    sp_para_top2 = sp_para_logits.squeeze().topk(k=2).indices\n",
    "    if(sp_sent_logits.squeeze().size(0) > 12):\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=12).indices\n",
    "    else:\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=sp_sent_logits.squeeze().size(0)).indices\n",
    "#         print(\"sp_para_top2\", sp_para_top2)\n",
    "#         print(\"sp_sent_top12\", sp_sent_top12)\n",
    "\n",
    "    sp_sent_pred = set()\n",
    "    sp_para_pred = set(sp_para_top2.tolist())\n",
    "    for sp_sent in sp_sent_top12:\n",
    "        sp_sent_to_para = s_to_p_map[sp_sent.item()]\n",
    "        if sp_sent_to_para in sp_para_top2:\n",
    "            sp_sent_pred.add(sp_sent.item())\n",
    "#             sp_para_pred.add(sp_sent_to_para) \n",
    "#         print(\"sp_sent_pred: \" + str(sp_sent_pred))\n",
    "#         print(\"sp_para_pred: \" + str(sp_para_pred))\n",
    "    return (answers, sp_sent_pred, sp_para_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# def normalize_answer(self, s):\n",
    "\n",
    "#     def remove_articles(text):\n",
    "#         return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "#     def white_space_fix(text):\n",
    "#         return ' '.join(text.split())\n",
    "\n",
    "#     def remove_punc(text):\n",
    "#         exclude = set(string.punctuation)\n",
    "#         return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "#     def lower(text):\n",
    "#         return text.lower()\n",
    "\n",
    "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def sp_metrics(self, prediction, gold): \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in prediction:\n",
    "        if e in gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "    for e in gold:\n",
    "        if e not in prediction:\n",
    "            fn += 1 \n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0 \n",
    "    return em, prec, recall, f1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_end(self, outputs):\n",
    "    print(\"validation_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs] \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "    sp_sent_f1_scores = [x['sp_f1'] for x in outputs]   \n",
    "    sp_sent_em_scores = [x['sp_em'] for x in outputs]   \n",
    "    sp_sent_prec_scores = [x['sp_prec'] for x in outputs]   \n",
    "    sp_sent_recall_scores = [x['sp_recall'] for x in outputs]   \n",
    "    joint_f1_scores = [x['joint_f1'] for x in outputs]  \n",
    "    joint_em_scores = [x['joint_em'] for x in outputs]  \n",
    "    joint_prec_scores = [x['joint_prec'] for x in outputs]  \n",
    "    joint_recall_scores = [x['joint_recall'] for x in outputs]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_para_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "        joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "        joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "        joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)    \n",
    "    avg_val_em = sum(em_scores) / len(em_scores)    \n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)  \n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores)    \n",
    "    avg_val_sp_sent_f1 = sum(sp_sent_f1_scores) / len(sp_sent_f1_scores)    \n",
    "    avg_val_sp_sent_em = sum(sp_sent_em_scores) / len(sp_sent_em_scores)    \n",
    "    avg_val_sp_sent_prec = sum(sp_sent_prec_scores) / len(sp_sent_prec_scores)  \n",
    "    avg_val_sp_sent_recall = sum(sp_sent_recall_scores) / len(sp_sent_recall_scores)    \n",
    "    avg_val_joint_f1 = sum(joint_f1_scores) / len(joint_f1_scores)  \n",
    "    avg_val_joint_em = sum(joint_em_scores) / len(joint_em_scores)  \n",
    "    avg_val_joint_prec = sum(joint_prec_scores) / len(joint_prec_scores)    \n",
    "    avg_val_joint_recall = sum(joint_recall_scores) / len(joint_recall_scores)  \n",
    "\n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t')   \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t')   \n",
    "    print(\"avg_sp_sent_loss: \", avg_sp_sent_loss)   \n",
    "    print(\"avg_val_f1: \", avg_val_f1, end = '\\t')   \n",
    "    print(\"avg_val_em: \", avg_val_em, end = '\\t')   \n",
    "    print(\"avg_val_prec: \", avg_val_prec, end = '\\t')   \n",
    "    print(\"avg_val_recall: \", avg_val_recall)   \n",
    "    print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1, end = '\\t')   \n",
    "    print(\"avg_val_sp_sent_em: \" , avg_val_sp_sent_em, end = '\\t')  \n",
    "    print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec, end = '\\t')   \n",
    "    print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)   \n",
    "    print(\"avg_val_joint_f1: \" , avg_val_joint_f1, end = '\\t')  \n",
    "    print(\"avg_val_joint_em: \", avg_val_joint_em, end = '\\t')   \n",
    "    print(\"avg_val_joint_prec: \", avg_val_joint_prec, end = '\\t')   \n",
    "    print(\"avg_val_joint_recall: \", avg_val_joint_recall)   \n",
    "\n",
    "\n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss,   \n",
    "    'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall,    \n",
    "    'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,    \n",
    "    'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall \n",
    "    }   \n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "\n",
    "    print(\"test_step of qid: \", qid, end=\"\\t\") \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss + 5*type_loss + 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    pre_answer = normalize_answer(answers_pred[0]['text'])\n",
    "    # print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "    # print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = normalize_answer(answer)\n",
    "\n",
    "    print(\"pre_answer:\\t\", pre_answer, \"\\tgold_answer:\\t\", gold_answer)\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss, 'answer_score': pre_answer_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_end(self, outputs):\n",
    "    print(\"test_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "\n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_para_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "#         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}')\n",
    "    # print(\"answer_scores: \", answer_scores)\n",
    "\n",
    "    # print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "    # print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    # print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "    # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "\n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss\n",
    "           }\n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHECKPOINT_HYPER_PARAMS_KEY',\n",
       " 'CHECKPOINT_HYPER_PARAMS_NAME',\n",
       " 'CHECKPOINT_HYPER_PARAMS_TYPE',\n",
       " 'T_destination',\n",
       " '_LightningModule__get_hparams_assignment_variable',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply',\n",
       " '_auto_collect_arguments',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_init_slurm_connection',\n",
       " '_load_from_state_dict',\n",
       " '_load_model_state',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_hparams',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'amp_scale_loss',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'example_input_array',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'get_progress_bar_dict',\n",
       " 'get_tqdm_dict',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'hparams',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_fit_end',\n",
       " 'on_fit_start',\n",
       " 'on_gpu',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'optimizer_zero_grad',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'prepare_data',\n",
       " 'print',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'save_hyperparameters',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'sp_metrics',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'teardown',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_epoch_end',\n",
       " 'test_step',\n",
       " 'test_step_end',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_epoch_end',\n",
       " 'training_step',\n",
       " 'training_step_end',\n",
       " 'transfer_batch_to_device',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_epoch_end',\n",
       " 'validation_step',\n",
       " 'validation_step_end',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHECKPOINT_HYPER_PARAMS_KEY', 'hyper_parameters'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_NAME', 'hparams_name'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_TYPE', 'hparams_type'),\n",
       " ('T_destination', ~T_destination),\n",
       " ('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__abstractmethods__', frozenset()),\n",
       " ('__annotations__',\n",
       "  {'_device': Ellipsis, '_dtype': typing.Union[str, torch.dtype]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'test_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset(),\n",
       "                '_abc_impl': <_abc_data at 0x7f31bff99ae0>,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'optimizer_step': <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'sp_metrics': <function __main__.sp_metrics(self, prediction, gold)>,\n",
       "                'validation_end': <function __main__.validation_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'test_step': <function __main__.test_step(self, batch, batch_nb)>,\n",
       "                'test_end': <function __main__.test_end(self, outputs)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7f31c815feb0>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__slots__', ()),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'LightningModule' objects>),\n",
       " ('_abc_impl', <_abc_data at 0x7f31bff99ae0>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_auto_collect_arguments',\n",
       "  <bound method LightningModule._auto_collect_arguments of <class '__main__.hotpotqa'>>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input: Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_load_model_state',\n",
       "  <bound method ModelIO._load_model_state of <class '__main__.hotpotqa'>>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp: Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name: str, module: 'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss: torch.Tensor, optimizer: torch.optim.optimizer.Optimizer, optimizer_idx: int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self: ~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp: object, model: 'LightningModule', optimizers: List[torch.optim.optimizer.Optimizer], amp_level: str) -> Tuple[ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device: Union[int, NoneType] = None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('device', <property at 0x7f31c86ac1d0>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('dtype', <property at 0x7f31c86ac180>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self: ~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('example_input_array', <property at 0x7f31c86ac9a0>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[int, str]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[int, str]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type: Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('hparams', <property at 0x7f31c86aca90>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank: int, world_size: int, is_slurm_managing_tasks: bool = True) -> None>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method ModelIO.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method ModelIO.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict: Dict[str, torch.Tensor], strict: bool = True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch: Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer: torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_gpu', <property at 0x7f31c8704d10>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: torch.optim.optimizer.Optimizer, optimizer_idx: int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name: str, tensor: torch.Tensor, persistent: bool = True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name: str, param: torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self: ~T, requires_grad: bool = True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage: str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self: ~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode: str = 'top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch: torch.Tensor, split_size: int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage: str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self: ~T, mode: bool = True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch: Any, device: torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type: Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input: Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp: Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name: str, module: 'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss: torch.Tensor, optimizer: torch.optim.optimizer.Optimizer, optimizer_idx: int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self: ~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp: object, model: 'LightningModule', optimizers: List[torch.optim.optimizer.Optimizer], amp_level: str) -> Tuple[ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device: Union[int, NoneType] = None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self: ~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[int, str]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[int, str]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type: Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank: int, world_size: int, is_slurm_managing_tasks: bool = True) -> None>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict: Dict[str, torch.Tensor], strict: bool = True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch: Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer: torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: torch.optim.optimizer.Optimizer, optimizer_idx: int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name: str, tensor: torch.Tensor, persistent: bool = True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name: str, param: torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self: ~T, requires_grad: bool = True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage: str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self: ~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode: str = 'top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch: torch.Tensor, split_size: int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage: str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self: ~T, mode: bool = True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch: Any, device: torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type: Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end', <function __main__.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_epoch_end(self, outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    import shutil\n",
    "    save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with config:\n",
      "LongformerConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"sliding_chunks\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5eeb0d598bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhotpotqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__abstractmethods__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhotpotqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# this is necessary to use gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-8bf7f72a7ee4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.8/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"partition\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'feature_names'"
     ]
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "    model.to('cuda')    # this is necessary to use gpu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_set_size = 9 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size) \n",
    "    \n",
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus is not \"\" else None\n",
    "    num_devices = len(args.gpus) #1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "    \n",
    "    train_set_size = 90447 * args.train_percent    # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps},  #warmup steps: {args.warmup}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                         track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                         accumulate_grad_batches=args.batch_size,\n",
    "                         train_percent_check = args.train_percent,\n",
    "#                          val_check_interval=args.val_every,\n",
    "                         val_percent_check=args.val_percent_check,\n",
    "                         test_percent_check=args.val_percent_check,\n",
    "                         logger=logger if not args.disable_checkpointing else False,\n",
    "                         checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                         show_progress_bar=args.no_progress_bar,\n",
    "                         use_amp=not args.fp32, amp_level='O1',\n",
    "                         check_val_every_n_epoch=1\n",
    "                         )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     if not args.test: \n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_dev.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 4)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', True)\n",
      "('model_path', '/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json --dev_dataset small_dev.json  \\\n",
    "    --gpus 0 --num_workers 4 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer  --model_path /xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096 --test '\n",
    "# hotpot_dev_distractor_v1.json\n",
    "\n",
    "import argparse \n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    for arg in vars(args):\n",
    "        print((arg, getattr(args, arg)))\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
