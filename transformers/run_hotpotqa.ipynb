{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def create_example_dict(context, answers, id, is_impossible, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answers\": answers,\n",
    "                \"id\": id,\n",
    "                \"is_impossible\": is_impossible,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        contexts = [\" <s> \".join(lst[1]) for lst in raw_contexts]    # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space \n",
    "        context = \" <p> \" + \" <p> \".join(contexts)\n",
    "        \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )\n",
    "\n",
    "\n",
    "        answer = example[\"answer\"].strip() \n",
    "        if answer.lower() == 'yes':\n",
    "            answers = [{\"answer_start\": -1, \"answer_end\": -1, \"text\": answer}] \n",
    "        elif answer.lower() == 'no':\n",
    "            answers = [{\"answer_start\": -2, \"answer_end\": -2, \"text\": answer}] \n",
    "        else:\n",
    "            answers = []          # keep all the occurences of answer in the context\n",
    "            for m in re.finditer(re.escape(answer), context):    \n",
    "                answer_start, answer_end = m.span() \n",
    "                answers.append({\"answer_start\": answer_start, \"answer_end\": answer_end, \"text\": answer})\n",
    "             \n",
    "        if(len(answers) > 0): \n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=context,\n",
    "                        answers=answers,\n",
    "                        id = example[\"_id\"],\n",
    "                        is_impossible=(answers == []),\n",
    "                        question=example[\"question\"],\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": \" <p> Radio City is India's first private FM radio station and was started on 3 July 2001. <s>  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). <s>  It plays Hindi, English and regional songs. <s>  It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. <s>  Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. <s>  The Radio station currently plays a mix of Hindi and Regional music. <s>  Abraham Thomas is the CEO of the company. <p> Football in Albania existed before the Albanian Football Federation (FSHF) was created. <s>  This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) . <s>  Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946. <s>  In 1932, Albania joined FIFA (during the 12\\u201316 June convention ) And in 1954 she was one of the founding members of UEFA. <p> Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California. <s>  Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016. <s>  Echosmith started first as \\\"Ready Set Go!\\\" <s>  until they signed to Warner Bros. <s>  Records in May 2012. <s>  They are best known for their hit song \\\"Cool Kids\\\", which reached number 13 on the \\\"Billboard\\\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia. <s>  The song was Warner Bros. <s>  Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold. <s>  The band's debut album, \\\"Talking Dreams\\\", was released on October 8, 2013. <p> Women's colleges in the Southern United States refers to undergraduate, bachelor's degree\\u2013granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States. <s>  Many started first as girls' seminaries or academies. <s>  Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women. <s>  Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level. <p> The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum. <p> Arthur's Magazine (1844\\u20131846) was an American literary periodical published in Philadelphia in the 19th century. <s>  Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. <s>  In May 1846 it was merged into \\\"Godey's Lady's Book\\\". <p> The 2014\\u201315 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship. <s>  Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues. <s>  Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014. <s>  The regular season included just 12 rounds, where all the teams went to the semifinals. <s>  In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk. <p> First for Women is a woman's magazine published by Bauer Media Group in the USA. <s>  The magazine was started in 1989. <s>  It is based in Englewood Cliffs, New Jersey. <s>  In 2011 the circulation of the magazine was 1,310,696 copies. <p> The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California. <s>  The fire started as two separate fires on November 15, 2008. <s>  The \\\"Freeway Fire\\\" started first shortly after 9am with the \\\"Landfill Fire\\\" igniting approximately 2 hours later. <s>  These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda. <p> William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. <s>  It is most known for their premium jeans. <s>  On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line. <s>  The label also produces other clothing items such as jackets and tops. <s>  The company started first as a denim line, later evolving into a men\\u2019s and women\\u2019s clothing line.\",\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answers\": [\n",
      "            {\n",
      "              \"answer_start\": 2990,\n",
      "              \"answer_end\": 3007,\n",
      "              \"text\": \"Arthur's Magazine\"\n",
      "            }\n",
      "          ],\n",
      "          \"id\": \"5a7a06935542990198eaf050\",\n",
      "          \"is_impossible\": false,\n",
      "          \"question\": \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[0:16]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[17:30]' > small_dev.json\n",
    "\n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "# !pip install jdc \n",
    "# !pip install test-tube \n",
    "# !conda install ipywidgets --yes\n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "\n",
    "from longformer.longformer import Longformer\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "            print(f'reading file: {self.file_path}')\n",
    "            self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "#             print(self.data_json[0])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "        print(tokenizer.all_special_tokens)\n",
    "        print(tokenizer.all_special_ids)\n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =  \\\n",
    "                {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     106,
     122,
     147,
     162
    ]
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "#                 print(\"question text: \", question_text)  \n",
    "                sp_sent = qa[\"is_sup_fact\"]\n",
    "                sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                \n",
    "                p_list = list(locate(doc_tokens , lambda x: x == \"<p>\")) \n",
    "                assert(len(p_list) == len(sp_para))\n",
    "                s_list = list(locate(doc_tokens , lambda x: x == \"<s>\"))\n",
    "#                 \n",
    "#                 if(len(s_list) + len(p_list) != len(sp_sent)):\n",
    "#                     print(\"len(s_list):\", len(s_list))\n",
    "#                     print(\"len(p_list):\", len(p_list))\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent)\n",
    "#                     print(\"paragraph_text\", paragraph_text)\n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    "                assert(len(s_list) + len(p_list) == len(sp_sent) )\n",
    "                \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                answer_spans = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "#                     print(\"orig_answer_text: \", orig_answer_text)\n",
    "                    answer_start = answer[\"answer_start\"]\n",
    "                    answer_end = answer[\"answer_end\"]  \n",
    "                    if(answer_start >= 0 and answer_end > 0):\n",
    "                        try:\n",
    "                            start_word_position = char_to_word_offset[answer_start]\n",
    "                            end_word_position = char_to_word_offset[answer_end-1]\n",
    "#                             print(\"answer by start_word_position and end_word_position: \", doc_tokens[start_word_position: end_word_position+1])\n",
    "                        except:\n",
    "                            print(f'error: Reading example {idx} failed')\n",
    "                            start_word_position = -3\n",
    "                            end_word_position = -3\n",
    "                            \n",
    "                    else:\n",
    "                        start_word_position = answer[\"answer_start\"]\n",
    "                        end_word_position = answer[\"answer_end\"]\n",
    "                    answer_spans.append({'start': start_word_position, 'end': end_word_position})\n",
    "\n",
    "                    \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                \n",
    "                # each original token in the context is tokenized to multiple sub_tokens\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "                \n",
    "                # all sub tokens, truncate up to limit\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len-3]\n",
    "\n",
    "                # The -3 accounts for [CLS], [q], [/q]  \n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 3\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [\"<cls>\"] + [\"<q>\"] + query_tokens + [\"</q>\"] + doc_slice_tokens   \n",
    "#                     print(\"tokens: \", tokens)\n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] *  len(doc_slice_tokens) \n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len\n",
    "\n",
    "                    # ===== answer positions tensors  ============\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    "                    q_type = None\n",
    "                    assert(len(answer_spans) > 0)\n",
    "                    for answer_span in answer_spans:\n",
    "                        start_position = answer_span['start']   # reletive to context\n",
    "                        end_position = answer_span['end']\n",
    "                        if(start_position >= 0):\n",
    "                            tok_start_position_in_doc = orig_to_tok_index[start_position]  # sub_tokens postion reletive to context\n",
    "                            not_end_of_doc = int(end_position + 1 < len(orig_to_tok_index))\n",
    "                            tok_end_position_in_doc = orig_to_tok_index[end_position + not_end_of_doc] - not_end_of_doc\n",
    "                            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                                assert(\"this answer is outside the current slice\")   # only has one slice with the large negative doc_stride\n",
    "                                continue                                \n",
    "                            start_positions.append(tok_start_position_in_doc + doc_offset)   # sub_tokens postion reletive to begining of all the tokens, including query sub tokens  \n",
    "                            end_positions.append(tok_end_position_in_doc + doc_offset)\n",
    "#                             print(\"answer by start_positions and end_positions: \", tokens[tok_start_position_in_doc + doc_offset: tok_end_position_in_doc + doc_offset+1])\n",
    "                            if(q_type != None and q_type != 0):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 0\n",
    "                \n",
    "                        elif(start_position == -1):\n",
    "                            if(q_type != None and q_type != 1):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 1\n",
    "                            start_positions.append(-1)  # -1 is the IGNORE_INDEX, will be ignored\n",
    "                            end_positions.append(-1)     \n",
    "                        elif(start_position == -2):\n",
    "                            if(q_type != None and q_type != 2):\n",
    "                                assert(\"inconsistance q_type\")\n",
    "                            q_type = 2\n",
    "                            start_positions.append(-1)\n",
    "                            end_positions.append(-1)     \n",
    "                        else:\n",
    "                            assert(\"unknown start_positions\")\n",
    "                            continue\n",
    "                    assert len(start_positions) == len(end_positions)\n",
    "                    \n",
    "                    \n",
    "                    if self.ignore_seq_with_no_answers and len(start_positions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "\n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                if (input_ids_list is None):\n",
    "                    print(\"input_ids_list is None\")\n",
    "                if (input_mask_list is None):\n",
    "                    print(\"input_mask_list is None\")\n",
    "                if (segment_ids_list is None):\n",
    "                    print(\"segment_ids_list is None\")\n",
    "                if (start_positions_list is None):\n",
    "                    print(\"start_positions_list is None\")\n",
    "                if (end_positions_list is None):\n",
    "                    print(\"end_positions_list is None\")\n",
    "                if (q_type_list is None):\n",
    "                    print(\"q_type_list is None\")\n",
    "                if (sp_sent_list is None):\n",
    "                    print(\"sp_sent_list is None\")\n",
    "                if (sp_para_list is None):\n",
    "                    print(\"sp_para_list is None\")\n",
    "                if (qa['id'] is None):\n",
    "                    print(\"qa['id'] is None\")\n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                      torch.tensor([sp_sent_list]),  torch.tensor([sp_para_list]),\n",
    "                                     qa['id']))    \n",
    "#                 tensors_list.append((doc_tokens))\n",
    "        return tensors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 1  # qids  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7f56a92df5f8>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<cls>\", \"<p>\", \"<q>\", \"</q>\"]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        \n",
    "        self.dense_type = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 3)   #  question type (yes/no/span) classification \n",
    "        self.dense_sp_sent = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_sp_sent = torch.nn.Linear(self.model.config.hidden_size, 1)    \n",
    "        self.dense_sp_para = torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.linear_sp_para = torch.nn.Linear(self.model.config.hidden_size, 1) \n",
    "        self.train_dataloader_object = self.val_dataloader_object  = None  # = self.test_dataloader_object = None\n",
    "    \n",
    "    def load_model(self):\n",
    "#         model = Longformer.from_pretrained(self.args.model_path)\n",
    "        model = Longformer.from_pretrained('longformer-base-4096')\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "#%%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "###################################################### dataloaders ########################################################### \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    " \n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl\n",
    "        return self.train_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    # @pl.data_loader\n",
    "    # def test_dataloader(self):\n",
    "    #     if self.test_dataloader_object is not None:\n",
    "    #         return self.test_dataloader_object\n",
    "    #     dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "    #                               max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "    #                               doc_stride=self.args.doc_stride,\n",
    "    #                               max_num_answers=self.args.max_num_answers,\n",
    "    #                               max_question_len=self.args.max_question_len,\n",
    "    #                               ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "    #     dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "    #                     num_workers=self.args.num_workers, sampler=None,\n",
    "    #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "    #     self.test_dataloader_object = dl\n",
    "    #     return self.test_dataloader_object\n",
    "\n",
    "#%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para):\n",
    "#         print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "#         print(\"size of attention_mask: \" + str(attention_mask.size()))\n",
    "#         print(\"size of segment_ids: \" + str(segment_ids.size()))\n",
    "#         print(\"size of start_positions: \" + str(start_positions.size()))\n",
    "#         print(\"size of end_positions:\" + str(end_positions.size()))\n",
    "#         print(\"q_type: \" + str(q_type))\n",
    "#         print(\"size of sp_sent: \" + str(sp_sent.size()))\n",
    "#         print(\"size of sp_para: \" + str(sp_para.size()))\n",
    "        if(input_ids.size(0) > 1):\n",
    "            assert(\"multi rows per document\")\n",
    "        # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "        # Make sure all rows have the same question length.\n",
    "        \n",
    "#         size of input_ids: torch.Size([1, 1495])\n",
    "#         size of attention_mask: torch.Size([1, 1495])\n",
    "#         size of segment_ids: torch.Size([1, 1495])\n",
    "#         size of start_positions: torch.Size([1, 64])   # multiple occurences of the same answer string, -1 padding up to self.max_num_answers\n",
    "#         size of end_positions: torch.Size([1, 64])\n",
    "#         size of q_type: torch.Size([1, 1])\n",
    "#         size of sp_sent: torch.Size([1, 40])           # number of sentences in context\n",
    "#         size of sp_para: torch.Size([1, 10])\n",
    "#         print(\"input: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "        # local attention everywhere\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "        # global attention for the cls and all question tokens\n",
    "        question_end_index = self._get_special_index(input_ids, \"</q>\")\n",
    "        if(question_end_index.size(0) == 1):\n",
    "            attention_mask[:,:question_end_index.item()] = 2  # from <cls> until </q>\n",
    "        else:\n",
    "            attention_mask[:,:question_end_index[0].item()] = 2\n",
    "            print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "        \n",
    "        # global attention for the sentence and paragraph special tokens  \n",
    "        p_index = self._get_special_index(input_ids, \"<p>\")\n",
    "#         print(\"size of p_index: \" + str(p_index.size()))\n",
    "        attention_mask[:, p_index] = 2\n",
    "              \n",
    "        s_index = self._get_special_index(input_ids, \"<s>\")\n",
    "#         print(\"size of s_index: \" + str(s_index.size()))\n",
    "        attention_mask[:, s_index] = 2\n",
    "        \n",
    "#         print(\"p_index:\", p_index) \n",
    "#         print(\"attention_mask: \", attention_mask)\n",
    "        \n",
    "\n",
    "        # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "        input_ids, attention_mask = pad_to_window_size(\n",
    "            input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "\n",
    "        sequence_output = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask)[0]\n",
    "\n",
    "        # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "        # before computing loss and decoding.\n",
    "        padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "        if padding_len > 0:\n",
    "            sequence_output = sequence_output[:, :-padding_len]\n",
    "#         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "              \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "#         print(\"size of logits: \" + str(logits.size())) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "#         print(\"size of start_logits: \" + str(start_logits.size())) \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "#         print(\"size of start_logits after squeeze: \" + str(start_logits.size())) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    " \n",
    "        ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "#         print(\"size of sequence_output[:,0]: \" + str(sequence_output[:,0].size()))\n",
    "        type_logits = self.dense_type(sequence_output[:,0])\n",
    "#         print(\"size of type_logits after dense: \" + str(type_logits.size()))\n",
    "        # Non-linearity\n",
    "        type_logits = torch.tanh(type_logits) \n",
    "#         print(\"size of type_logits after tanh: \" + str(type_logits.size()))\n",
    "        type_logits = self.linear_type(type_logits)\n",
    "#         print(\"size of type_logits: \" + str(type_logits.size()))\n",
    "        \n",
    "        ### 3. supporting paragraph classification ### \n",
    "        sp_para_output = sequence_output[:,p_index,:]\n",
    "        print(\"size of sp_para_output: \" + str(sp_para_output.size()))      \n",
    "              \n",
    "        sp_para_output_t = self.dense_sp_para(sp_para_output)\n",
    "#         print(\"size of sp_para_output_t after dense: \" + str(sp_para_output_t.size()))   \n",
    "        # Non-linearity\n",
    "        sp_para_output_t = torch.tanh(sp_para_output_t) \n",
    "#         print(\"size of sp_para_output_t after tanh: \" + str(sp_para_output_t.size()))\n",
    "        sp_para_output_t = self.linear_sp_para(sp_para_output_t)\n",
    "#         print(\"size of sp_para_output_t: \" + str(sp_para_output_t.size()))   \n",
    "        \n",
    "        # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no. \n",
    "        # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "        sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "#         print(\"size of sp_para_output_aux: \" + str(sp_para_output_aux.size()))   \n",
    "        predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous()\n",
    "#         print(\"size of predict_support_para: \" + str(predict_support_para.size()))              \n",
    "            \n",
    "        ### 4. supporting fact classification ###     \n",
    "        # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    "        sent_indexes = torch.sort(torch.cat((s_index, p_index)))[0] # torch.sort returns a 'torch.return_types.sort' object has 2 items: values, indices\n",
    "#         print(\"size of sent_indexes: \" + str(sent_indexes.size()))\n",
    "#         print(\"sent_indexes: \", sent_indexes)\n",
    "        sp_sent_output = sequence_output[:,sent_indexes,:]\n",
    "        print(\"size of sp_sent_output: \" + str(sp_sent_output.size()))      \n",
    "        \n",
    "        sp_sent_output_t = self.dense_sp_sent(sp_sent_output)\n",
    "#         print(\"size of sp_sent_output_t after dense: \" + str(sp_sent_output_t.size()))      \n",
    "        # Non-linearity\n",
    "        sp_sent_output_t = torch.tanh(sp_sent_output_t) \n",
    "#         print(\"size of sp_sent_output_t after tanh: \" + str(sp_sent_output_t.size()))        \n",
    "        sp_sent_output_t = self.linear_sp_sent(sp_sent_output_t)\n",
    "#         print(\"size of sp_sent_output_t: \" + str(sp_sent_output_t.size()))       \n",
    " \n",
    "        sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "#         print(\"size of sp_sent_output_aux: \" + str(sp_sent_output_aux.size()))  \n",
    "        predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous()\n",
    "#         print(\"size of predict_support_sent: \" + str(predict_support_sent.size()))  \n",
    "        \n",
    "        outputs = (start_logits, end_logits, type_logits, sp_para_output_t, sp_sent_output_t)  \n",
    "        #outputs = (torch.sigmoid(start_logits), torch.sigmoid(end_logits), torch.sigmoid(type_logits), torch.sigmoid(sp_para_output_t), torch.sigmoid(sp_sent_output_t))  \n",
    "        answer_loss, type_loss, sp_para_loss, sp_sent_loss  = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)\n",
    "#         print(\"answer_loss: \" + str(answer_loss))\n",
    "#         print(\"type_loss: \" + str(type_loss))\n",
    "#         print(\"sp_para_loss: \" + str(sp_para_loss))\n",
    "#         print(\"sp_sent_loss: \" + str(sp_sent_loss))\n",
    "        outputs = (answer_loss, type_loss, sp_para_loss, sp_sent_loss,) + outputs    \n",
    "        return outputs\n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    "#                 print(\"start_positions: \" + str(start_positions)) \n",
    "#                 print(\"start_loss: \" + str(start_loss)) \n",
    "                \n",
    "#                 # for debug: check is there any impact if remove -1s from start_positions, and turns out no impact at all\n",
    "#                 start_positions_debug = start_positions[:, torch.where(start_positions!=-1)[1]]\n",
    "#                 start_loss_debug = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions_debug, ignore_index=-1)\n",
    "#                 print(\"start_positions_debug: \" + str(start_positions_debug)) \n",
    "#                 print(\"start_loss_debug: \" + str(start_loss_debug)) \n",
    "                \n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "#                 print(\"end_positions: \" + str(end_positions)) \n",
    "#                 print(\"end_loss: \" + str(end_loss)) \n",
    "                \n",
    "#                 # for debug: check is there any impact if remove -1s from \n",
    "#                 end_positions_debug = end_positions[:, torch.where(end_positions!=-1)[1]]\n",
    "#                 end_loss_debug = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions_debug, ignore_index=-1)\n",
    "#                 print(\"end_positions_debug: \" + str(end_positions_debug)) \n",
    "#                 print(\"end_loss_debug: \" + str(end_loss_debug)) \n",
    "\n",
    "                type_loss = self.or_softmax_cross_entropy_loss_one_doc(type_logits, q_type.unsqueeze(0), ignore_index=-1)\n",
    "\n",
    "#                 binary_loss = torch.nn.BCELoss()\n",
    "# #                 print(\"sp_para_output_t.squeeze().type(): \", sp_para_output_t.squeeze().type())\n",
    "# #                 print(\"sp_para.to(dtype=torch.half, device=sp_para.device).type(): \", sp_para.to(dtype=torch.half, device=sp_para.device).type())\n",
    "#                 sp_para_loss = binary_loss(sp_para_output_t.squeeze(), sp_para.squeeze().to(dtype=torch.half, device=sp_para.device))\n",
    "#                 sp_sent_loss = binary_loss(sp_sent_output_t.squeeze(), sp_sent.squeeze().to(dtype=torch.half, device=sp_sent.device))\n",
    "                \n",
    "#                 sp_para_loss = torch.tensor([0.0], device = predict_support_para.device )\n",
    "# #                 print(\"predict_support_para.squeeze(): \", predict_support_para.squeeze())\n",
    "# #                 print(\"sp_para.squeeze(): \", sp_para.squeeze())\n",
    "#                 for para_predict, para_gold in zip(predict_support_para.squeeze(), sp_para.squeeze()):\n",
    "# #                     print(\"para_predict.unsqueeze(0): \", para_predict.unsqueeze(0))\n",
    "# #                     print(\" para_gold.unsqueeze(0): \",  para_gold.unsqueeze(0))\n",
    "\n",
    "                # only one example per batch, instead treating each example as a row, after squeeze, each para / sentence is a row\n",
    "                sp_para_loss = self.or_softmax_cross_entropy_loss_one_doc(predict_support_para.squeeze(), sp_para.squeeze().unsqueeze(-1), ignore_index=-1)\n",
    "                sp_sent_loss = self.or_softmax_cross_entropy_loss_one_doc(predict_support_sent.squeeze(), sp_sent.squeeze().unsqueeze(-1), ignore_index=-1)\n",
    "        \n",
    "            else:\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "                start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = loss_fct(start_logits, start_positions[:, 0])\n",
    "                end_loss = loss_fct(end_logits, end_positions[:, 0])\n",
    "                type_loss = loss_fct(type_logits, q_type)  \n",
    "                \n",
    "                nll_average = torch.nn.CrossEntropyLoss(size_average=True, ignore_index=-1)\n",
    "#                 print(\"predict_support_para.view(-1, 2).size()\", predict_support_para.view(-1, 2).size())\n",
    "#                 print(\"sp_para.view(-1).size()\", sp_para.view(-1).size())\n",
    "                sp_para_loss = nll_average(predict_support_para.view(-1, 2), sp_para.view(-1))\n",
    "                sp_sent_loss = nll_average(predict_support_sent.view(-1, 2), sp_sent.view(-1))\n",
    " \n",
    "                \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss, type_loss, sp_para_loss, sp_sent_loss  \n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_token):\n",
    "        assert(input_ids.size(0)==1)\n",
    "        token_indices =  torch.nonzero(input_ids == self.tokenizer.convert_tokens_to_ids(special_token))\n",
    "        ### FOR DEBUG ###\n",
    "        # input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "        # token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    "#         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "#         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "#         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#         print(\"target: \" + str(target)) \n",
    "\n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#         print(\"masked_target: \" + str(masked_target))     \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "#         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#         print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#         print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "        \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    "#         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#         print(\"log_score: \" + str(log_score)) \n",
    "        # denominator = log(sum(exp(logits)))\n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#         print(\"log_norm: \" + str(log_norm)) \n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm)\n",
    "        print(\"loss: \" + str(loss))\n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "       \n",
    "        print(\"final loss: \" + str(loss)) \n",
    "        return loss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.where(sp_sent.squeeze())[0]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # print(tokenizer.all_special_tokens)    \n",
    "# # # print(tokenizer.all_special_ids)     \n",
    "# # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # tokenizer.sep_token\n",
    "\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    self.scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)  # scheduler is not saved in the checkpoint, but global_step is, which is enough to restart\n",
    "    self.scheduler.step(self.global_step)\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimizer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook to do a lot of non-standard training tricks such as learning-rate warm-up\n",
    "def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.scheduler.step(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook\n",
    "def on_epoch_start(self):\n",
    "    print(\"Start epoch \", self.current_epoch)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch \n",
    "    print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    print(\"answer_loss: \", answer_loss)\n",
    "    print(\"type_loss: \", type_loss)\n",
    "    print(\"sp_para_loss: \", sp_para_loss)\n",
    "    print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "    \n",
    "#     loss  = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    loss = answer_loss + 20 * type_loss + 20 * sp_para_loss + 50 * sp_sent_loss\n",
    "    print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it. \n",
    "    if(q_type == 1 or q_type == 2 ):\n",
    "        print(\"answer_loss of q_type == 1 or q_type == 2: \", answer_loss)\n",
    "    print(\"lr: \", lr)    # lr will increading over time\n",
    "    tensorboard_logs = {'train_answer_loss': answer_loss, 'train_type_loss': type_loss, 'train_sp_para_loss': sp_para_loss, 'train_sp_sent_loss': sp_sent_loss, \n",
    "                        'lr': lr,\n",
    "                        'input_size': torch.tensor(input_ids.numel()).type_as(loss) ,\n",
    "                        'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) }\n",
    "    return {'loss': loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch\n",
    "    print(\"validation_step\")\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    print(\"answer_loss: \" + str(answer_loss))\n",
    "\n",
    "    answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "    print(\"answers_pred: \" + str(answers_pred))\n",
    "    \n",
    "    # answers_pred only contains the top one predicted answer['text', 'score']\n",
    "#     answers_pred = sorted(answers_pred, key=lambda x: x['score'], reverse=True)[0:1] # each batch is one document\n",
    "#     print(\"answers_pred after sorted: \" + str(answers_pred))\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "    answers_pred = answers_pred[0]\n",
    "\n",
    "    answer_score = answers_pred['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    print(\"pred answer_score: \" + str(answer_score))\n",
    "    \n",
    "    print(\"pred answer_text: \" + str(answers_pred['text'])) \n",
    "\n",
    "    if(q_type == 1):\n",
    "        answer_gold = 'yes'\n",
    "    elif(q_type == 2):\n",
    "        answer_gold = 'no' \n",
    "    else:\n",
    "        # even though there can be multiple gold start_postion (subword_start) and end_position(subword_end), the corresponing answer string are same\n",
    "        answer_gold_token_ids = input_ids[0, subword_starts[0][0]: subword_ends[0][0] + 1]\n",
    "        print(\"answer_gold_token_ids: \" + str(answer_gold_token_ids))\n",
    "        answer_gold_tokens = self.tokenizer.convert_ids_to_tokens(answer_gold_token_ids.tolist())\n",
    "        print(\"answer_gold_tokens: \" + str(answer_gold_tokens))\n",
    "        answer_gold = self.tokenizer.convert_tokens_to_string(answer_gold_tokens)\n",
    "    print(\"answer_gold: \" + str(answer_gold))\n",
    " \n",
    "    f1, prec, recall = self.f1_score(answers_pred['text'], answer_gold)\n",
    "    em = self.exact_match_score(answers_pred['text'], answer_gold) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"prec: \" + str(prec))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"em: \" + str(em)) \n",
    "\n",
    "    if(len(sp_sent_pred) > 0):\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "        sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "        sp_sent_precision = torch.tensor(sp_sent_em).type_as(loss)\n",
    "        sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "        sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "        \n",
    "        #         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "#         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "#         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "#         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "        \n",
    "        joint_prec = prec * sp_sent_precision\n",
    "        joint_recall = recall * sp_sent_recall\n",
    "        if joint_prec + joint_recall > 0:\n",
    "            joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "        else:\n",
    "            joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "        joint_em = em * sp_sent_em \n",
    "\n",
    "    else:\n",
    "        sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "        joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "         \n",
    "    print(\"return\") \n",
    "#     return {'qids': [qids], 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "#             'answer_score': [answer_score], 'f1': [f1], 'prec':[prec], 'recall':[recall], 'em': [em],\n",
    "#             'sp_em': [sp_sent_em], 'sp_f1': [sp_sent_f1], 'sp_prec': [sp_sent_precision], 'sp_recall': [sp_sent_recall],\n",
    "#             'joint_em': [joint_em], 'joint_f1': [joint_f1], 'joint_prec': [joint_prec], 'joint_recall': [joint_recall]}\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "                'answer_score': answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em,\n",
    "                'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "                'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits):\n",
    "    print(\"decode\")\n",
    "    \n",
    "    question_end_index = self._get_special_index(input_ids, \"</q>\")\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "    \n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "#     print(\"start_logits: \", start_logits)\n",
    "#     print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "#     print(\"start_logits_indices: \", start_logits_indices)\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#     print(\"p_type: \", p_type)\n",
    "#     print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#         print(\"sorted_answers: \" + str(sorted_answers))\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "#             score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            score = (torch.sigmoid(answer['start_logit']) + torch.sigmoid(answer['end_logit']) + torch.sigmoid(p_type_score)) / 3\n",
    "            answers.append({'text': text, 'score': score})\n",
    "            print(\"answers: \" + str(answers))\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score})\n",
    "    else:\n",
    "        assert False \n",
    "\n",
    "    p_index = self._get_special_index(input_ids, \"<p>\")\n",
    "#     print(\"p_index: \" + str(p_index))\n",
    "    s_index = self._get_special_index(input_ids, \"<s>\")\n",
    "#     print(\"s_index: \" + str(s_index))\n",
    "    sent_indexes = torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "    \n",
    "    s_to_p_map = []\n",
    "    for s in sent_indexes:\n",
    "        s_to_p = torch.where(torch.le(p_index, s))[0][-1]     # last p_index smaller or equal to s\n",
    "        s_to_p_map.append(s_to_p.item()) \n",
    "#     print(\"s_to_p_map: \" + str(s_to_p_map))\n",
    "    \n",
    "#     print(\"sp_para_logits\", sp_para_logits)\n",
    "#     print(\"sp_sent_logits\", sp_sent_logits)\n",
    "\n",
    "#     print(\"sp_para_logits.squeeze().size(0): \", sp_para_logits.squeeze().size(0))\n",
    "#     print(\"sp_sent_logits.squeeze().size(0): \", sp_sent_logits.squeeze().size(0))\n",
    "    sp_para_top2 = sp_para_logits.squeeze().topk(k=2).indices\n",
    "    if(sp_sent_logits.squeeze().size(0) > 12):\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=12).indices\n",
    "    else:\n",
    "        sp_sent_top12 = sp_sent_logits.squeeze().topk(k=sp_sent_logits.squeeze().size(0)).indices\n",
    "#     print(\"sp_para_top2\", sp_para_top2)\n",
    "#     print(\"sp_sent_top12\", sp_sent_top12)\n",
    "    \n",
    "    sp_sent_pred = set()\n",
    "    sp_para_pred = set()\n",
    "    for sp_sent in sp_sent_top12:\n",
    "        sp_sent_to_para = s_to_p_map[sp_sent.item()]\n",
    "        if sp_sent_to_para in sp_para_top2:\n",
    "            sp_sent_pred.add(sp_sent.item())\n",
    "            sp_para_pred.add(sp_sent_to_para) \n",
    "#     print(\"sp_sent_pred: \" + str(sp_sent_pred))\n",
    "#     print(\"sp_para_pred: \" + str(sp_para_pred))\n",
    "    return (answers, sp_sent_pred, sp_para_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def normalize_answer(self, s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = self.normalize_answer(prediction)\n",
    "    normalized_ground_truth = self.normalize_answer(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "    \n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def sp_metrics(self, prediction, gold):\n",
    "#     print(\"prediction: \", prediction)\n",
    "#     print(\"gold: \", gold)\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in prediction:\n",
    "        if e in gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "#             print(\"e: \", e)\n",
    "#             print(\"gold: \", gold)\n",
    "#             print(\"e not in gold!!!\")\n",
    "    for e in gold:\n",
    "        if e not in prediction:\n",
    "            fn += 1\n",
    "#             print(\"e: \", e)\n",
    "#             print(\"prediction: \", prediction)\n",
    "#             print(\"e not in prediction!!!\")\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "#     print(\"sp prec: \", prec)\n",
    "#     print(\"sp recall: \", recall)\n",
    "#     print(\"sp f1: \", f1)\n",
    "#     print(\"sp em: \", em)\n",
    "    return em, prec, recall, f1 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_epoch_end(self, outputs):\n",
    "    print(\"validation_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "        \n",
    "#     string_qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "#     int_qids = [self.val_dataloader_object.dataset.val_qid_string_to_int_map[qid] for qid in string_qids]\n",
    "    answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "    f1_scores = [x['f1'] for x in outputs] # [item for sublist in outputs for item in sublist['f1']] #torch.stack([x['f1'] for x in outputs]).mean()  #\n",
    "    em_scores = [x['em'] for x in outputs] # [item for sublist in outputs for item in sublist['em']] #torch.stack([x['em'] for x in outputs]).mean()  #\n",
    "    prec_scores =  [x['prec'] for x in outputs] #[item for sublist in outputs for item in sublist['prec']] #torch.stack([x['prec'] for x in outputs]).mean()  #\n",
    "    recall_scores = [x['recall'] for x in outputs] #[item for sublist in outputs for item in sublist['recall']]  #torch.stack([x['recall'] for x in outputs]).mean()  #\n",
    "    \n",
    "    sp_sent_f1_scores = [x['sp_f1'] for x in outputs] #[item for sublist in outputs for item in sublist['sp_f1']] #torch.stack([x['sp_f1'] for x in outputs]).mean() #\n",
    "    sp_sent_em_scores = [x['sp_em'] for x in outputs] # [item for sublist in outputs for item in sublist['sp_em']] #torch.stack([x['sp_em'] for x in outputs]).mean() #\n",
    "    sp_sent_prec_scores = [x['sp_prec'] for x in outputs]  #[item for sublist in outputs for item in sublist['sp_prec']] #torch.stack([x['sp_prec'] for x in outputs]).mean() #\n",
    "    sp_sent_recall_scores = [x['sp_recall'] for x in outputs] # [item for sublist in outputs for item in sublist['sp_recall']]  #torch.stack([x['sp_recall'] for x in outputs]).mean() #\n",
    "     \n",
    "    joint_f1_scores = [x['joint_f1'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_f1']] #torch.stack([x['joint_f1'] for x in outputs]).mean() #\n",
    "    joint_em_scores = [x['joint_em'] for x in outputs]   # [item for sublist in outputs for item in sublist['joint_em']] #torch.stack([x['joint_em'] for x in outputs]).mean() #\n",
    "    joint_prec_scores = [x['joint_prec'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_prec']] #torch.stack([x['joint_prec'] for x in outputs]).mean() #\n",
    "    joint_recall_scores = [x['joint_recall'] for x in outputs]   #[item for sublist in outputs for item in sublist['joint_recall']] #torch.stack([x['joint_recall'] for x in outputs]).mean() #     \n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_para_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "#         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "        sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "        joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "        joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "        joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "        \n",
    "        \n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    print(\"answer_scores: \", answer_scores)\n",
    "    print(\"f1_scores: \", f1_scores)\n",
    "    print(\"em_scores: \", em_scores)\n",
    "    \n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "    print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "        \n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    print(\"len(f1_scores): \", len(f1_scores))\n",
    "    print(\"avg_val_f1: \", avg_val_f1)\n",
    "    avg_val_em = sum(em_scores) / len(em_scores)\n",
    "#     print(\"len(em_scores): \", len(em_scores))\n",
    "    print(\"avg_val_em: \", avg_val_em)\n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)\n",
    "#     print(\"len(prec_scores): \", len(prec_scores))\n",
    "    print(\"avg_val_prec: \", avg_val_prec)\n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores) \n",
    "#     print(\"len(recall_scores): \", len(recall_scores))\n",
    "    print(\"avg_val_recall: \", avg_val_recall)\n",
    "    \n",
    "    avg_val_sp_sent_f1 = sum(sp_sent_f1_scores) / len(sp_sent_f1_scores)\n",
    "    print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1)\n",
    "    avg_val_sp_sent_em = sum(sp_sent_em_scores) / len(sp_sent_em_scores)\n",
    "    print(\"avg_val_sp_sent_em: \", avg_val_sp_sent_em)\n",
    "    avg_val_sp_sent_prec = sum(sp_sent_prec_scores) / len(sp_sent_prec_scores)\n",
    "    print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec)\n",
    "    avg_val_sp_sent_recall = sum(sp_sent_recall_scores) / len(sp_sent_recall_scores) \n",
    "    print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)\n",
    "        \n",
    "    avg_val_joint_f1 = sum(joint_f1_scores) / len(joint_f1_scores)\n",
    "    print(\"avg_val_joint_f1: \", avg_val_joint_f1)\n",
    "    avg_val_joint_em = sum(joint_em_scores) / len(joint_em_scores)\n",
    "    print(\"avg_val_joint_em: \", avg_val_joint_em)\n",
    "    avg_val_joint_prec = sum(joint_prec_scores) / len(joint_prec_scores)\n",
    "    print(\"avg_val_joint_prec: \", avg_val_joint_prec)\n",
    "    avg_val_joint_recall = sum(joint_recall_scores) / len(joint_recall_scores) \n",
    "    print(\"avg_val_joint_recall: \", avg_val_joint_recall)\n",
    "     \n",
    "    \n",
    "    \n",
    "#     print(\"avg_loss: \", avg_loss)\n",
    "    \n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss, \n",
    "            'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall,\n",
    "            'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,\n",
    "            'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall\n",
    "           }\n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "#     answer_scores =  torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "#     f1_scores =  torch.stack([x['f1'] for x in outputs]).mean()  #\n",
    "#     em_scores =  torch.stack([x['em'] for x in outputs]).mean()  #\n",
    "#     prec_scores =  torch.stack([x['prec'] for x in outputs]).mean()  #\n",
    "#     recall_scores =  torch.stack([x['recall'] for x in outputs]).mean()  #\n",
    "    \n",
    "#     sp_sent_f1_scores =  torch.stack([x['sp_f1'] for x in outputs]).mean() #\n",
    "#     sp_sent_em_scores =  torch.stack([x['sp_em'] for x in outputs]).mean() #\n",
    "#     sp_sent_prec_scores =  torch.stack([x['sp_prec'] for x in outputs]).mean() #\n",
    "#     sp_sent_recall_scores =  torch.stack([x['sp_recall'] for x in outputs]).mean() #\n",
    "     \n",
    "#     joint_f1_scores =  torch.stack([x['joint_f1'] for x in outputs]).mean() #\n",
    "#     joint_em_scores =  torch.stack([x['joint_em'] for x in outputs]).mean() #\n",
    "#     joint_prec_scores =  torch.stack([x['joint_prec'] for x in outputs]).mean() #\n",
    "#     joint_recall_scores = torch.stack([x['joint_recall'] for x in outputs]).mean() #     \n",
    "\n",
    "#     return {'avg_val_loss': avg_loss}\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids = batch\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para)\n",
    "    loss, start_logits, end_logits = output[:3]\n",
    "    answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "    # each batch is one document\n",
    "    answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "    qids = [qids]\n",
    "    assert len(answers) == len(qids)\n",
    "    return {'qids': qids, 'answers': answers}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_end(self, outputs):\n",
    "    qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "    answers = [item for sublist in outputs for item in sublist['answers']]\n",
    "\n",
    "    qa_with_duplicates = defaultdict(list)\n",
    "    for qid, answer in zip(qids, answers):\n",
    "        qa_with_duplicates[qid].append({'answer_score': answer['score'], 'answer_text': answer['text'], })\n",
    "\n",
    "    qid_to_answer_text = {}\n",
    "    for qid, answer_metrics in qa_with_duplicates.items():\n",
    "        top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "        qid_to_answer_text[qid] = top_answer['answer_text']\n",
    "\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(qid_to_answer_text, f)\n",
    "\n",
    "    return {'count': len(qid_to_answer_text)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_load_from_state_dict',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'normalize_answer',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'sp_metrics',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_step',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_step',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_epoch_end',\n",
       " 'validation_step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T_destination', ~T_destination),\n",
       " ('__abstractmethods__', frozenset({'configure_optimizers', 'training_step'})),\n",
       " ('__annotations__',\n",
       "  {'dump_patches': bool,\n",
       "   '_version': int,\n",
       "   'training': bool,\n",
       "   'forward': typing.Callable[..., typing.Any],\n",
       "   '__call__': typing.Callable[..., typing.Any]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_token)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset({'configure_optimizers',\n",
       "                           'training_step'}),\n",
       "                '_abc_registry': <_weakrefset.WeakSet at 0x7f56a924e470>,\n",
       "                '_abc_cache': <_weakrefset.WeakSet at 0x7f56a924e518>,\n",
       "                '_abc_negative_cache': <_weakrefset.WeakSet at 0x7f56a924e588>,\n",
       "                '_abc_negative_cache_version': 230,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'optimizer_step': <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>,\n",
       "                'on_epoch_start': <function __main__.on_epoch_start(self)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>,\n",
       "                'normalize_answer': <function __main__.normalize_answer(self, s)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'sp_metrics': <function __main__.sp_metrics(self, prediction, gold)>,\n",
       "                'validation_epoch_end': <function __main__.validation_epoch_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'test_step': <function __main__.test_step(self, batch, batch_nb)>,\n",
       "                'test_end': <function __main__.test_end(self, outputs)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7f56a92434a8>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'ABC' objects>),\n",
       " ('_abc_cache', <_weakrefset.WeakSet at 0x7f56a924e518>),\n",
       " ('_abc_negative_cache', <_weakrefset.WeakSet at 0x7f56a924e588>),\n",
       " ('_abc_negative_cache_version', 230),\n",
       " ('_abc_registry', <_weakrefset.WeakSet at 0x7f56a924e470>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_token)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self:~T) -> ~T>),\n",
       " ('cuda',\n",
       "  <function torch.nn.modules.module.Module.cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self:~T) -> ~T>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self:~T) -> ~T>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self:~T) -> ~T>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method LightningModule.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method LightningModule.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('normalize_answer', <function __main__.normalize_answer(self, s)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start', <function __main__.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('type',\n",
       "  <function torch.nn.modules.module.Module.type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_token)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self:~T) -> ~T>),\n",
       " ('cuda',\n",
       "  <function torch.nn.modules.module.Module.cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits, sp_para_logits, sp_sent_logits)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self:~T) -> ~T>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self:~T) -> ~T>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type, sp_sent, sp_para)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self:~T) -> ~T>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('normalize_answer', <function __main__.normalize_answer(self, s)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start', <function __main__.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function __main__.optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('sp_metrics', <function __main__.sp_metrics(self, prediction, gold)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end', <function __main__.test_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('type',\n",
       "  <function torch.nn.modules.module.Module.type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/u32/fanluo/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/u32/fanluo/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.tokenization_utils_base:Assigning ['<cls>', '<p>', '<q>', '</q>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <cls> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <p> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <q> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding </q> to the vocabulary\n",
      "INFO:transformers.configuration_utils:loading configuration file longformer-base-4096/config.json\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file longformer-base-4096/pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing Longformer.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of Longformer were initialized from the model checkpoint at longformer-base-4096.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use Longformer for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with config:\n",
      "RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hotpotqa(\n",
       "  (model): Longformer(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50269, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dense_type): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_type): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dense_sp_sent): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_sp_sent): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dense_sp_para): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_sp_para): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "    model.to('cuda')    # this is necessary to use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_size:  16.0\n",
      "num_devices:  1\n",
      ">>>>>>> #train_set_size: 16.0, #steps: 48.0, #epochs: 6, batch_size: 2 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "#     args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus is not \"\" else None\n",
    "    args.gpus = int(args.gpus)\n",
    "    train_set_size = 16 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size)\n",
    "    num_devices =  args.gpus # len(args.gpus) #1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "    args.warmup = int(train_set_size / 10)\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:gpu available: True, used: True\n",
      "INFO:root:VISIBLE GPUS: 0\n",
      "INFO:root:using 16bit precision\n"
     ]
    }
   ],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                         track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                         accumulate_grad_batches=args.batch_size,\n",
    "                         train_percent_check = args.train_percent,\n",
    "#                          val_check_interval=args.val_every,\n",
    "                         val_percent_check=args.val_percent_check,\n",
    "                         test_percent_check=args.val_percent_check,\n",
    "                         logger=logger if not args.disable_checkpointing else False,\n",
    "                         checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                         show_progress_bar=args.no_progress_bar,\n",
    "                         use_amp=not args.fp32, amp_level='O1',\n",
    "                         check_val_every_n_epoch=1\n",
    "                         )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "INFO:root:set slurm handle signals\n",
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: \n",
      "            You're using multiple gpus and multiple nodes without using a DistributedSampler\n",
      "            to assign a subset of your data to each process. To silence this warning, pass a\n",
      "            DistributedSampler to your DataLoader.\n",
      "\n",
      "            ie: this:\n",
      "            dataset = myDataset()\n",
      "            dataloader = Dataloader(dataset)\n",
      "\n",
      "            becomes:\n",
      "            dataset = myDataset()\n",
      "            dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
      "            dataloader = Dataloader(dataset, sampler=dist_sampler)\n",
      "\n",
      "            If you want each process to load the full dataset, ignore this warning.\n",
      "            \n",
      "  warnings.warn(msg)\n",
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:145: UserWarning: \n",
      "                    Your val_dataloader(s) don't use DistributedSampler.\n",
      "\n",
      "                    You're using multiple gpus and multiple nodes without using a\n",
      "                    DistributedSampler to assign a subset of your data to each process.\n",
      "                    To silence this warning, pass a DistributedSampler to your DataLoader.\n",
      "\n",
      "                    ie: this:\n",
      "                    dataset = myDataset()\n",
      "                    dataloader = Dataloader(dataset)\n",
      "\n",
      "                    becomes:\n",
      "                    dataset = myDataset()\n",
      "                    dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
      "                    dataloader = Dataloader(dataset, sampler=dist_sampler)\n",
      "\n",
      "                    If you want each process to load the full dataset, ignore this warning.\n",
      "                    \n",
      "  warnings.warn(msg)\n",
      "INFO:root:\n",
      "                                       Name               Type Params\n",
      "0                                     model         Longformer  148 M\n",
      "1                          model.embeddings  RobertaEmbeddings   41 M\n",
      "2          model.embeddings.word_embeddings          Embedding   38 M\n",
      "3      model.embeddings.position_embeddings          Embedding    3 M\n",
      "4    model.embeddings.token_type_embeddings          Embedding  768  \n",
      "..                                      ...                ...    ...\n",
      "242                             linear_type             Linear    2 K\n",
      "243                           dense_sp_sent             Linear  590 K\n",
      "244                          linear_sp_sent             Linear  769  \n",
      "245                           dense_sp_para             Linear  590 K\n",
      "246                          linear_sp_para             Linear  769  \n",
      "\n",
      "[247 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: small.json\n",
      "['</s>', '<cls>', '</q>', '<q>', '<unk>', '<s>', '<mask>', '<s>', '<p>', '<pad>', '</s>']\n",
      "[2, 50265, 50268, 50267, 3, 0, 50264, 0, 50266, 1, 2]\n",
      "reading file: small_dev.json\n",
      "['</s>', '<cls>', '</q>', '<q>', '<unk>', '<s>', '<mask>', '<s>', '<p>', '<pad>', '</s>']\n",
      "[2, 50265, 50268, 50267, 3, 0, 50264, 0, 50266, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/ipykernel_launcher.py:300: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([6.7578], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.7578, device='cuda:0')\n",
      "loss: tensor([6.6953], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6953, device='cuda:0')\n",
      "loss: tensor([1.1650], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(1.1650, device='cuda:0')\n",
      "loss: tensor([0.6532], device='cuda:0')\n",
      "final loss: tensor(0.6532, device='cuda:0')\n",
      "loss: tensor([0.5972], device='cuda:0')\n",
      "final loss: tensor(0.5972, device='cuda:0')\n",
      "answer_loss: tensor(6.7266, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1711], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1711], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.1172], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1172, device='cuda:0')\n",
      "loss: tensor([4.1719], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1719, device='cuda:0')\n",
      "loss: tensor([1.1592], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(1.1592, device='cuda:0')\n",
      "loss: tensor([0.6703], device='cuda:0')\n",
      "final loss: tensor(0.6703, device='cuda:0')\n",
      "loss: tensor([0.5849], device='cuda:0')\n",
      "final loss: tensor(0.5849, device='cuda:0')\n",
      "answer_loss: tensor(4.1445, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2534], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2534], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([5.6172], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.6172, device='cuda:0')\n",
      "loss: tensor([5.3438], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3438, device='cuda:0')\n",
      "loss: tensor([1.2344], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(1.2344, device='cuda:0')\n",
      "loss: tensor([0.6542], device='cuda:0')\n",
      "final loss: tensor(0.6542, device='cuda:0')\n",
      "loss: tensor([0.5943], device='cuda:0')\n",
      "final loss: tensor(0.5943, device='cuda:0')\n",
      "answer_loss: tensor(5.4805, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.3005], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.3005], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.9766], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.9766, device='cuda:0')\n",
      "loss: tensor([6.5000], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5000, device='cuda:0')\n",
      "loss: tensor([1.2227], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(1.2227, device='cuda:0')\n",
      "loss: tensor([0.6455], device='cuda:0')\n",
      "final loss: tensor(0.6455, device='cuda:0')\n",
      "loss: tensor([0.5780], device='cuda:0')\n",
      "final loss: tensor(0.5780, device='cuda:0')\n",
      "answer_loss: tensor(6.7383, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.1659], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.1659], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([5.8359], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.8359, device='cuda:0')\n",
      "loss: tensor([6.2969], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.2969, device='cuda:0')\n",
      "loss: tensor([1.2246], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(1.2246, device='cuda:0')\n",
      "loss: tensor([0.6324], device='cuda:0')\n",
      "final loss: tensor(0.6324, device='cuda:0')\n",
      "loss: tensor([0.5693], device='cuda:0')\n",
      "final loss: tensor(0.5693, device='cuda:0')\n",
      "answer_loss: tensor(6.0664, device='cuda:0')\n",
      "decode\n",
      "answers_pred: [{'text': 'no', 'score': tensor([0.2369], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.2369], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: no\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  0\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([7.3281], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.9844], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9844, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.2490], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2490, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6624], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6624, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.5901], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.5901, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.1562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2490, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6624, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.5901, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(74.8913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([0.], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([6.4766], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4766, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.4180], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4180, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.2422], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2422, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6608], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.5896], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.5896, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.4473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2422, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.5896, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(73.9891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([0.], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([7.8242], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.8242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.5586], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.5586, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.1348], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.1348, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6605], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6605, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6133], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6133, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.6914, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1348, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6605, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.6133, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(74.2594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([6.6094], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6094, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5117], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5117, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.2510], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2510, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6536], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6536, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6066], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.5605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2510, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6536, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.6066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(74.9826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([0.], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([5.0469], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.0469, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.9961], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9961, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.4502], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.4502, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6705], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6705, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.5616], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.5616, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.0215, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.4502, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6705, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.5616, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(75.5133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.0000e-04], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([7.1172], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.7031], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.7031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.2012], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2012, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6468], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6468, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6194], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6194, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.4102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2012, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6468, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.6194, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(75.3364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.0000e-04], device='cuda:0')\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([5.6797], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.6797, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.4297], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.2607], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2607, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6551], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6551, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6054], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6054, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.5547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.2607, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6551, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.6054, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(74.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.7872e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.6680], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6680, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5000], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.1670], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.1670, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6601], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6046], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6046, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.5840, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.1670, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.6601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.6046, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(73.3576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.7872e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.4883], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.4883, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.3711], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3711, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.0889], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.0889, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3959], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3959, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3302], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3302, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.4297, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.0889, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.3959, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.3302, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(53.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.5745e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([6.6602], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6602, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.7617], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.7617, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.0996], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.0996, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.4100], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.4100, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3999], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3999, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.7109, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(1.0996, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.4100, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.3999, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(56.8995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.5745e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([7.2617], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2617, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.3750], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.5400], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.5400, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3256], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1360], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1360, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.3184, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.5400, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.3256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1360, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(31.4305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.3617e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([7.2070], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2070, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.3281], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.6875], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.6875, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3242], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1500], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.2676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.6875, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.3242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(35.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.3617e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([7.2773], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2773, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.9102], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9102, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1338], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1338, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3281], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0972], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0972, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.0938, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.1338, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.3281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0972, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(21.1937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.1489e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([6.6172], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.8594], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.8594, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1787], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1787, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.3167], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.3167, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0919], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0919, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.7383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.1787, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.3167, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0919, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(21.2395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([9.1489e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([7.0156], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.0156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.2188], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2188, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0820], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0820, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2803], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2803, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0776], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0776, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.1172, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0820, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2803, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0776, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(18.2441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.9362e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.6445], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6445, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5703], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5703, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0684], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0684, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2751], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2751, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1611], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1611, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.6074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0684, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2751, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1611, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(21.5320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.9362e-05], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([6.5977], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5977, device='cuda:0')\n",
      "loss: tensor([6.6992], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6992, device='cuda:0')\n",
      "loss: tensor([0.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0273, device='cuda:0')\n",
      "loss: tensor([0.2504], device='cuda:0')\n",
      "final loss: tensor(0.2504, device='cuda:0')\n",
      "loss: tensor([0.0520], device='cuda:0')\n",
      "final loss: tensor(0.0520, device='cuda:0')\n",
      "answer_loss: tensor(6.6484, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 6, 2010 with 969,000 viewers. <s> The', 'score': tensor([0.7432], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 6, 2010 with 969,000 viewers. <s> The', 'score': tensor([0.7432], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7432], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  6, 2010 with 969,000 viewers. <s> The\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.1719], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1719, device='cuda:0')\n",
      "loss: tensor([4.1953], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1953, device='cuda:0')\n",
      "loss: tensor([0.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0273, device='cuda:0')\n",
      "loss: tensor([0.2513], device='cuda:0')\n",
      "final loss: tensor(0.2513, device='cuda:0')\n",
      "loss: tensor([0.0660], device='cuda:0')\n",
      "final loss: tensor(0.0660, device='cuda:0')\n",
      "answer_loss: tensor(4.1836, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': 'i', 'score': tensor([0.7290], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': 'i', 'score': tensor([0.7290], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7290], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: i\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([5.3750], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3750, device='cuda:0')\n",
      "loss: tensor([5.3477], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3477, device='cuda:0')\n",
      "loss: tensor([0.0312], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0312, device='cuda:0')\n",
      "loss: tensor([0.2504], device='cuda:0')\n",
      "final loss: tensor(0.2504, device='cuda:0')\n",
      "loss: tensor([0.0916], device='cuda:0')\n",
      "final loss: tensor(0.0916, device='cuda:0')\n",
      "answer_loss: tensor(5.3613, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 7, 1964) is an attorney and a Missouri Democratic politician. <s> He represented the 23rd District of Missouri in the', 'score': tensor([0.7544], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 7, 1964) is an attorney and a Missouri Democratic politician. <s> He represented the 23rd District of Missouri in the', 'score': tensor([0.7544], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7544], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  7, 1964) is an attorney and a Missouri Democratic politician. <s> He represented the 23rd District of Missouri in the\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.8594], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.8594, device='cuda:0')\n",
      "loss: tensor([6.7461], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.7461, device='cuda:0')\n",
      "loss: tensor([0.0293], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0293, device='cuda:0')\n",
      "loss: tensor([0.2505], device='cuda:0')\n",
      "final loss: tensor(0.2505, device='cuda:0')\n",
      "loss: tensor([0.0711], device='cuda:0')\n",
      "final loss: tensor(0.0711, device='cuda:0')\n",
      "answer_loss: tensor(6.8027, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Bluff,', 'score': tensor([0.7207], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Bluff,', 'score': tensor([0.7207], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7207], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Bluff,\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([5.5391], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.5391, device='cuda:0')\n",
      "loss: tensor([6.1484], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.1484, device='cuda:0')\n",
      "loss: tensor([0.0312], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0312, device='cuda:0')\n",
      "loss: tensor([0.2501], device='cuda:0')\n",
      "final loss: tensor(0.2501, device='cuda:0')\n",
      "loss: tensor([0.0752], device='cuda:0')\n",
      "final loss: tensor(0.0752, device='cuda:0')\n",
      "answer_loss: tensor(5.8438, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' House of Representatives from District 116. <s> He served until 2010, when he was', 'score': tensor([0.7461], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' House of Representatives from District 116. <s> He served until 2010, when he was', 'score': tensor([0.7461], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7461], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  House of Representatives from District 116. <s> He served until 2010, when he was\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.9922], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.9922, device='cuda:0')\n",
      "loss: tensor([7.4336], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4336, device='cuda:0')\n",
      "loss: tensor([0.0293], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0293, device='cuda:0')\n",
      "loss: tensor([0.2506], device='cuda:0')\n",
      "final loss: tensor(0.2506, device='cuda:0')\n",
      "loss: tensor([0.1003], device='cuda:0')\n",
      "final loss: tensor(0.1003, device='cuda:0')\n",
      "answer_loss: tensor(7.2129, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 9,', 'score': tensor([0.7207], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 9,', 'score': tensor([0.7207], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7207], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  9,\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.4805], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4805, device='cuda:0')\n",
      "loss: tensor([7.4766], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4766, device='cuda:0')\n",
      "loss: tensor([0.0293], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0293, device='cuda:0')\n",
      "loss: tensor([0.2504], device='cuda:0')\n",
      "final loss: tensor(0.2504, device='cuda:0')\n",
      "loss: tensor([0.0549], device='cuda:0')\n",
      "final loss: tensor(0.0549, device='cuda:0')\n",
      "answer_loss: tensor(7.4785, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Masaki Yamada who is living with Manami shortly after they have graduated', 'score': tensor([0.7432], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Masaki Yamada who is living with Manami shortly after they have graduated', 'score': tensor([0.7432], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7432], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Masaki Yamada who is living with Manami shortly after they have graduated\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.0781], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.0781, device='cuda:0')\n",
      "loss: tensor([6.4062], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.4062, device='cuda:0')\n",
      "loss: tensor([0.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0273, device='cuda:0')\n",
      "loss: tensor([0.2501], device='cuda:0')\n",
      "final loss: tensor(0.2501, device='cuda:0')\n",
      "loss: tensor([0.0543], device='cuda:0')\n",
      "final loss: tensor(0.0543, device='cuda:0')\n",
      "answer_loss: tensor(6.2422, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tae-', 'score': tensor([0.7622], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tae-', 'score': tensor([0.7622], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7622], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tae-\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([7.4062], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4062, device='cuda:0')\n",
      "loss: tensor([7.6523], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.6523, device='cuda:0')\n",
      "loss: tensor([0.0254], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0254, device='cuda:0')\n",
      "loss: tensor([0.2518], device='cuda:0')\n",
      "final loss: tensor(0.2518, device='cuda:0')\n",
      "loss: tensor([0.0690], device='cuda:0')\n",
      "final loss: tensor(0.0690, device='cuda:0')\n",
      "answer_loss: tensor(7.5293, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Good Life Cafe to themselves and the hip hop scene. <s> The Cafe was open from 1989 to 1999.<p> Austin Young (born April 12,', 'score': tensor([0.7324], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Good Life Cafe to themselves and the hip hop scene. <s> The Cafe was open from 1989 to 1999.<p> Austin Young (born April 12,', 'score': tensor([0.7324], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7324], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Good Life Cafe to themselves and the hip hop scene. <s> The Cafe was open from 1989 to 1999.<p> Austin Young (born April 12,\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.3047], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.3047, device='cuda:0')\n",
      "loss: tensor([6.4219], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.4219, device='cuda:0')\n",
      "loss: tensor([0.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0273, device='cuda:0')\n",
      "loss: tensor([0.2519], device='cuda:0')\n",
      "final loss: tensor(0.2519, device='cuda:0')\n",
      "loss: tensor([0.0679], device='cuda:0')\n",
      "final loss: tensor(0.0679, device='cuda:0')\n",
      "answer_loss: tensor(6.3633, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': 'Tres días para quererte\" (1945), \"¡Róbame esta', 'score': tensor([0.7354], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': 'Tres días para quererte\" (1945), \"¡Róbame esta', 'score': tensor([0.7354], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7354], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: Tres días para quererte\" (1945), \"¡Róbame esta\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([4.2383], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.2383, device='cuda:0')\n",
      "loss: tensor([0.2505], device='cuda:0')\n",
      "final loss: tensor(0.2505, device='cuda:0')\n",
      "loss: tensor([0.0652], device='cuda:0')\n",
      "final loss: tensor(0.0652, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' F. Kennedy School', 'score': tensor([0.7441], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' F. Kennedy School', 'score': tensor([0.7441], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7441], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  F. Kennedy School\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([4.2695], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.2695, device='cuda:0')\n",
      "loss: tensor([0.2497], device='cuda:0')\n",
      "final loss: tensor(0.2497, device='cuda:0')\n",
      "loss: tensor([0.0935], device='cuda:0')\n",
      "final loss: tensor(0.0935, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane Calder (born June 17,', 'score': tensor([0.7510], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane Calder (born June 17,', 'score': tensor([0.7510], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7510], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane Calder (born June 17,\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([5.2227], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.2227, device='cuda:0')\n",
      "loss: tensor([5.2383], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.2383, device='cuda:0')\n",
      "loss: tensor([0.0293], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0293, device='cuda:0')\n",
      "loss: tensor([0.2509], device='cuda:0')\n",
      "final loss: tensor(0.2509, device='cuda:0')\n",
      "loss: tensor([0.0556], device='cuda:0')\n",
      "final loss: tensor(0.0556, device='cuda:0')\n",
      "answer_loss: tensor(5.2305, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 67th Street and 68th Street, adjacent to the Park Avenue Armory. <s> It was developed in 1924 by Dwight P', 'score': tensor([0.7578], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 67th Street and 68th Street, adjacent to the Park Avenue Armory. <s> It was developed in 1924 by Dwight P', 'score': tensor([0.7578], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7578], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  67th Street and 68th Street, adjacent to the Park Avenue Armory. <s> It was developed in 1924 by Dwight P\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.6/site-packages/pytorch_lightning/callbacks/pt_callbacks.py:314: RuntimeWarning: Can save best model only with avg_val_f1 available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([7.0781], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.0781, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.1172], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0332], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0332, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2553], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2553, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0633], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0633, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.0977, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0332, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2553, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0633, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.7234e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([6.1641], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.1641, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.2891], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.2891, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0352], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0352, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2570], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2570, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0718], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0718, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.2266, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0352, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2570, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0718, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.6568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.7234e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([7.2461], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2461, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.5547], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.5547, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0195], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0195, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2487], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2487, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0739], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0739, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.4004, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0195, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2487, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0739, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.5106e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([6.6719], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6719, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5273], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5273, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0215], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0215, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2461], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2461, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1524], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1524, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.5996, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0215, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2461, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1524, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(19.5707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.5106e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([5.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.9883], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9883, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0156], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2384], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2384, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0495], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.9941, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2384, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(12.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.2979e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([7.3125], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3125, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.4766], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.4766, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0156], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2393], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2393, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0601], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.3945, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2393, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.4977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.2979e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([5.1953], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.1953, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.6562], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.6562, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0098], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2348], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2348, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1397], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1397, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.4258, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2348, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1397, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.3021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.0851e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.4062], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4062, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.3750], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0098], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2349], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2349, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1121], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1121, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.3906, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2349, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1121, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.8876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.0851e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.3438], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3438, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.2148], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2148, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0078], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0078, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2313], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2313, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0437], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.2793, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0078, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2313, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.2495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.8723e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([6.7188], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.7188, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.9531], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9531, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0059], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2309], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2309, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1532], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1532, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.8359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2309, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1532, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(19.2283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.8723e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([6.8945], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.8945, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.2812], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2812, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0059], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2297], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0632], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0632, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.0879, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0632, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.9616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.6596e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([6.8828], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.8828, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.1055], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1055, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2298], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2298, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0865], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0865, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.9941, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2298, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0865, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.9901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.6596e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([6.9883], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9883, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.9688], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9688, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2288], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2288, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0514], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0514, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.9785, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2288, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0514, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.2023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.4468e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([6.4492], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4492, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5078], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5078, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2286], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2286, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0444], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0444, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.4785, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2286, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0444, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.3505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.4468e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([6.4961], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4961, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.0469], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.0469, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2275], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0455], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0455, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.7715, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0455, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.6730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.2340e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.3828], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3828, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.3711], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3711, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2274], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2274, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1362], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1362, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.3770, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2274, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1362, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.8123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.2340e-05], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([6.3594], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.3594, device='cuda:0')\n",
      "loss: tensor([6.5508], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5508, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0377], device='cuda:0')\n",
      "final loss: tensor(0.0377, device='cuda:0')\n",
      "answer_loss: tensor(6.4551, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': \" Tom Verica. <s> The season finale was originally supposed to be the 22nd episode, but because of the show's lead Kerry Washington\", 'score': tensor([0.7310], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': \" Tom Verica. <s> The season finale was originally supposed to be the 22nd episode, but because of the show's lead Kerry Washington\", 'score': tensor([0.7310], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7310], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tom Verica. <s> The season finale was originally supposed to be the 22nd episode, but because of the show's lead Kerry Washington\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.1484], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1484, device='cuda:0')\n",
      "loss: tensor([4.0820], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.0820, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0519], device='cuda:0')\n",
      "final loss: tensor(0.0519, device='cuda:0')\n",
      "answer_loss: tensor(4.1152, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': 'Koz\" Kozmeni', 'score': tensor([0.7363], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': 'Koz\" Kozmeni', 'score': tensor([0.7363], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7363], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text: Koz\" Kozmeni\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([5.0898], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0898, device='cuda:0')\n",
      "loss: tensor([5.1797], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.1797, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2260], device='cuda:0')\n",
      "final loss: tensor(0.2260, device='cuda:0')\n",
      "loss: tensor([0.0779], device='cuda:0')\n",
      "final loss: tensor(0.0779, device='cuda:0')\n",
      "answer_loss: tensor(5.1348, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kamala Devi Harris ( , ; born October 20, 1964) is an American', 'score': tensor([0.7568], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kamala Devi Harris ( , ; born October 20, 1964) is an American', 'score': tensor([0.7568], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7568], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kamala Devi Harris ( , ; born October 20, 1964) is an American\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.5625], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5625, device='cuda:0')\n",
      "loss: tensor([6.6797], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6797, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0575], device='cuda:0')\n",
      "final loss: tensor(0.0575, device='cuda:0')\n",
      "answer_loss: tensor(6.6211, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Hartig', 'score': tensor([0.7402], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Hartig', 'score': tensor([0.7402], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7402], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Hartig\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([5.3750], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3750, device='cuda:0')\n",
      "loss: tensor([5.8711], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.8711, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2260], device='cuda:0')\n",
      "final loss: tensor(0.2260, device='cuda:0')\n",
      "loss: tensor([0.0608], device='cuda:0')\n",
      "final loss: tensor(0.0608, device='cuda:0')\n",
      "answer_loss: tensor(5.6230, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American', 'score': tensor([0.7544], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American', 'score': tensor([0.7544], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7544], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0.3333, device='cuda:0')\n",
      "prec: tensor(0.2000, device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.8906], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.8906, device='cuda:0')\n",
      "loss: tensor([7.1602], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1602, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2260], device='cuda:0')\n",
      "final loss: tensor(0.2260, device='cuda:0')\n",
      "loss: tensor([0.0865], device='cuda:0')\n",
      "final loss: tensor(0.0865, device='cuda:0')\n",
      "answer_loss: tensor(7.0254, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Schemel (born April 24, 1967) is an American', 'score': tensor([0.7363], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Schemel (born April 24, 1967) is an American', 'score': tensor([0.7363], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7363], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Schemel (born April 24, 1967) is an American\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.2891], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.2891, device='cuda:0')\n",
      "loss: tensor([7.4297], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4297, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2260], device='cuda:0')\n",
      "final loss: tensor(0.2260, device='cuda:0')\n",
      "loss: tensor([0.0404], device='cuda:0')\n",
      "final loss: tensor(0.0404, device='cuda:0')\n",
      "answer_loss: tensor(7.3594, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Masaki Yam', 'score': tensor([0.7510], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Masaki Yam', 'score': tensor([0.7510], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7510], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Masaki Yam\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.0273, device='cuda:0')\n",
      "loss: tensor([6.2578], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.2578, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2260], device='cuda:0')\n",
      "final loss: tensor(0.2260, device='cuda:0')\n",
      "loss: tensor([0.0405], device='cuda:0')\n",
      "final loss: tensor(0.0405, device='cuda:0')\n",
      "answer_loss: tensor(6.1426, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Dae-', 'score': tensor([0.7598], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Dae-', 'score': tensor([0.7598], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7598], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Dae-\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([7.2109], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.2109, device='cuda:0')\n",
      "loss: tensor([7.3555], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.3555, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2261], device='cuda:0')\n",
      "final loss: tensor(0.2261, device='cuda:0')\n",
      "loss: tensor([0.0551], device='cuda:0')\n",
      "final loss: tensor(0.0551, device='cuda:0')\n",
      "answer_loss: tensor(7.2832, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Los Angeles', 'score': tensor([0.7383], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Los Angeles', 'score': tensor([0.7383], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7383], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Los Angeles\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.0117], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.0117, device='cuda:0')\n",
      "loss: tensor([6.3125], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.3125, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2261], device='cuda:0')\n",
      "final loss: tensor(0.2261, device='cuda:0')\n",
      "loss: tensor([0.0533], device='cuda:0')\n",
      "final loss: tensor(0.0533, device='cuda:0')\n",
      "answer_loss: tensor(6.1621, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Sillman\\'s Broadway musical revue \"New Faces of 1952\" in which it was sung by June Carroll.<p> Paul David Nassau', 'score': tensor([0.7422], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Sillman\\'s Broadway musical revue \"New Faces of 1952\" in which it was sung by June Carroll.<p> Paul David Nassau', 'score': tensor([0.7422], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7422], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Sillman's Broadway musical revue \"New Faces of 1952\" in which it was sung by June Carroll.<p> Paul David Nassau\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([6.5781], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5781, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0520], device='cuda:0')\n",
      "final loss: tensor(0.0520, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' F. Kennedy School of Government. <s> Dillon was briefly married to Gabe Levin. <s> During the marriage they had a son.<p> Anthony Frederick', 'score': tensor([0.7466], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' F. Kennedy School of Government. <s> Dillon was briefly married to Gabe Levin. <s> During the marriage they had a son.<p> Anthony Frederick', 'score': tensor([0.7466], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7466], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  F. Kennedy School of Government. <s> Dillon was briefly married to Gabe Levin. <s> During the marriage they had a son.<p> Anthony Frederick\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([6.6641], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6641, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0790], device='cuda:0')\n",
      "final loss: tensor(0.0790, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane', 'score': tensor([0.7393], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane', 'score': tensor([0.7393], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7393], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  7, 2015 with a music video that was premiered on Pitchfork Media.<p> Kathryn Jane\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([4.9922], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.9922, device='cuda:0')\n",
      "loss: tensor([5.1445], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.1445, device='cuda:0')\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0.0039, device='cuda:0')\n",
      "loss: tensor([0.2259], device='cuda:0')\n",
      "final loss: tensor(0.2259, device='cuda:0')\n",
      "loss: tensor([0.0419], device='cuda:0')\n",
      "final loss: tensor(0.0419, device='cuda:0')\n",
      "answer_loss: tensor(5.0684, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' P', 'score': tensor([0.7568], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' P', 'score': tensor([0.7568], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.7568], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  P\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  2\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([6.6602], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6602, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.9375], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9375, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2267], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2267, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0469], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0469, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.7988, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2267, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0469, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.7565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.0213e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([6.1797], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.1797, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.1172], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.1172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2267], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2267, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0551], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0551, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.1484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2267, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0551, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([7.0213e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([7.2578], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.5625], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.5625, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2261], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2261, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0626], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0626, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.4102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2261, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0626, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.1418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.8085e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([5.9922], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.9922, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.3125], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3125, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2259], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2259, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1417], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1417, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.1523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2259, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1417, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.8357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.8085e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([4.8594], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8594, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.8867], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8867, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2256], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0418], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0418, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.8730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0418, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.5527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.5957e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([6.9414], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.9414, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.1055], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1055, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2256], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0522], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0522, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.0234, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2256, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0522, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.2251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.5957e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([4.7578], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.7578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.4492], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4492, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2252], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2252, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1350], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1350, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.1035, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2252, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1350, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.4343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.3830e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([5.8555], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.8555, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.4609], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.4609, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2253], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2253, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1069], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1069, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.1582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2253, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1069, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.3830e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.1094], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1094, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.1797], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.1797, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2250], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0399], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0399, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.1445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0399, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.6403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.1702e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([6.2031], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.2031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5430], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5430, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2249], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2249, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1497], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1497, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.3730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2249, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1497, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(18.3567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.1702e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([6.6445], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6445, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.6445], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6445, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2248], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2248, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0602], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0602, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.6445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2248, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0602, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.1500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.9574e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([6.3750], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.8125], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.8125, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2250], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0836], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0836, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.5938, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0836, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.2708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.9574e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([6.2578], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.2578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5156], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5156, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2247], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2247, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0489], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0489, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.3867, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2247, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0489, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.3268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.7447e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([5.8594], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.8594, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.2539], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.2539, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2248], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2248, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0421], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0421, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2248, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0421, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(12.6590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.7447e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([5.7383], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.7383, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.5586], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.5586, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2247], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2247, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0437], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.1484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2247, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(12.8289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.5319e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([6.0234], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.0234, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.3516], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2246], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2246, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1346], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1346, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.1875, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2246, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1346, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.4099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.5319e-05], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([5.8945], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.8945, device='cuda:0')\n",
      "loss: tensor([6.2578], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.2578, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0367], device='cuda:0')\n",
      "final loss: tensor(0.0367, device='cuda:0')\n",
      "answer_loss: tensor(6.0762, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' USA Network in the United States', 'score': tensor([0.8174], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' USA Network in the United States', 'score': tensor([0.8174], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8174], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  USA Network in the United States\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([3.9961], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.9961, device='cuda:0')\n",
      "loss: tensor([4.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4., device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0510], device='cuda:0')\n",
      "final loss: tensor(0.0510, device='cuda:0')\n",
      "answer_loss: tensor(3.9980, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' United States', 'score': tensor([0.8203], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' United States', 'score': tensor([0.8203], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8203], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  United States\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.9062], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.9062, device='cuda:0')\n",
      "loss: tensor([4.9805], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.9805, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0769], device='cuda:0')\n",
      "final loss: tensor(0.0769, device='cuda:0')\n",
      "answer_loss: tensor(4.9434, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.8418], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.8418], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8418], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Samuel Dillon Jackson\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.1641], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.1641, device='cuda:0')\n",
      "loss: tensor([6.6875], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6875, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0565], device='cuda:0')\n",
      "final loss: tensor(0.0565, device='cuda:0')\n",
      "answer_loss: tensor(6.4258, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Gray Drug acquired several Cunningham Drug stores in 1982.<p> Hartig', 'score': tensor([0.8115], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Gray Drug acquired several Cunningham Drug stores in 1982.<p> Hartig', 'score': tensor([0.8115], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8115], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Gray Drug acquired several Cunningham Drug stores in 1982.<p> Hartig\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([4.7891], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.7891, device='cuda:0')\n",
      "loss: tensor([5.4961], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.4961, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0598], device='cuda:0')\n",
      "final loss: tensor(0.0598, device='cuda:0')\n",
      "answer_loss: tensor(5.1426, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American', 'score': tensor([0.8223], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American', 'score': tensor([0.8223], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8223], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kelli Ward (\"née\" Kaznoski; born January 25, 1969) is an American\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0.3333, device='cuda:0')\n",
      "prec: tensor(0.2000, device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.4805], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.4805, device='cuda:0')\n",
      "loss: tensor([6.8438], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.8438, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0855], device='cuda:0')\n",
      "final loss: tensor(0.0855, device='cuda:0')\n",
      "answer_loss: tensor(6.6621, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American', 'score': tensor([0.8125], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American', 'score': tensor([0.8125], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8125], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.9570], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.9570, device='cuda:0')\n",
      "loss: tensor([7.3594], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.3594, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0394], device='cuda:0')\n",
      "final loss: tensor(0.0394, device='cuda:0')\n",
      "answer_loss: tensor(7.1582, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Catching Fire is a 2013 American', 'score': tensor([0.8247], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Catching Fire is a 2013 American', 'score': tensor([0.8247], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8247], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Catching Fire is a 2013 American\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([5.4102], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.4102, device='cuda:0')\n",
      "loss: tensor([5.9961], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.9961, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0394], device='cuda:0')\n",
      "final loss: tensor(0.0394, device='cuda:0')\n",
      "answer_loss: tensor(5.7031, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' South Korea', 'score': tensor([0.8032], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' South Korea', 'score': tensor([0.8032], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8032], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  South Korea\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([6.5039], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5039, device='cuda:0')\n",
      "loss: tensor([7.0859], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.0859, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0540], device='cuda:0')\n",
      "final loss: tensor(0.0540, device='cuda:0')\n",
      "answer_loss: tensor(6.7949, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles', 'score': tensor([0.8286], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles', 'score': tensor([0.8286], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8286], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([5.3594], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3594, device='cuda:0')\n",
      "loss: tensor([5.9922], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.9922, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0522], device='cuda:0')\n",
      "final loss: tensor(0.0522, device='cuda:0')\n",
      "answer_loss: tensor(5.6758, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Carol Lawrence', 'score': tensor([0.8115], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Carol Lawrence', 'score': tensor([0.8115], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8115], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Carol Lawrence\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(1., device='cuda:0')\n",
      "prec: tensor(1., device='cuda:0')\n",
      "recall: tensor(1., device='cuda:0')\n",
      "em: tensor(1., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.1328], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1328, device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0509], device='cuda:0')\n",
      "final loss: tensor(0.0509, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kate Dillon Levin', 'score': tensor([0.8154], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kate Dillon Levin', 'score': tensor([0.8154], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8154], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kate Dillon Levin\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.1797], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1797, device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0779], device='cuda:0')\n",
      "final loss: tensor(0.0779, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.8188], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.8188], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8188], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kathryn Jane Calder\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([4.3594], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.3594, device='cuda:0')\n",
      "loss: tensor([5.0273], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0273, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0409], device='cuda:0')\n",
      "final loss: tensor(0.0409, device='cuda:0')\n",
      "answer_loss: tensor(4.6934, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tinley Park', 'score': tensor([0.8398], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tinley Park', 'score': tensor([0.8398], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8398], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tinley Park\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  3\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([5.9531], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.9531, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.6172], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2246], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2246, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0456], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0456, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.2852, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2246, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0456, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.3191e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([5.8516], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.8516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.8047], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.8047, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0538], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.8281, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.0082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.3191e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([6.7656], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.7656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.3867], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.3867, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0617], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0617, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(7.0762, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0617, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.6481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.1064e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([5.3594], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.3594, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.0195], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.0195, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1408], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.6895, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.2184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([5.1064e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([4.5000], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.5000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.7500], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.7500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0411], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0411, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.6250, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0411, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.8936e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([6.0938], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.0938, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.6250], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0515], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0515, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.3594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0515, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.4194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.8936e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([3.7695], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.7695, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.8242], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1345], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.2969, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.5065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.6809e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([4.8281], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.7656], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.7656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1063], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1063, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.2969, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1063, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.0993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.6809e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.1836], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.1836, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.3906], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.3906, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0395], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0395, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.2871, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0395, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(12.7461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.4681e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([4.9219], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9219, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.1016], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.1016, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1494], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1494, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.5117, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1494, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.4673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.4681e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([5.3203], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.3203, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.9414], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.9414, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0599], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0599, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.6309, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0599, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.2553e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([4.8711], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8711, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.5547], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.5547, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0832], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0832, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.2129, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0832, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.8621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.2553e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([4.9023], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.1797], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.1797, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0487], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0487, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0487, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.9604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.0426e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([4.7578], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.7578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.4688], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4688, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0419], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0419, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.1133, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0419, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.6934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.0426e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([4.1484], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.1484, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.4023], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0436], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0436, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.7754, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0436, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.4417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.8298e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([5.0586], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.0586, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([6.6055], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(6.6055, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2242], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1345], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.8320, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(17.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.8298e-05], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([5.0234], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0234, device='cuda:0')\n",
      "loss: tensor([5.7461], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.7461, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0366], device='cuda:0')\n",
      "final loss: tensor(0.0366, device='cuda:0')\n",
      "answer_loss: tensor(5.3848, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' USA Network in the United States', 'score': tensor([0.8750], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' USA Network in the United States', 'score': tensor([0.8750], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8750], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  USA Network in the United States\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([3.4570], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.4570, device='cuda:0')\n",
      "loss: tensor([3.6016], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.6016, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0509], device='cuda:0')\n",
      "final loss: tensor(0.0509, device='cuda:0')\n",
      "answer_loss: tensor(3.5293, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' US', 'score': tensor([0.8774], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' US', 'score': tensor([0.8774], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8774], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  US\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.0469], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.0469, device='cuda:0')\n",
      "loss: tensor([4.5547], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.5547, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0768], device='cuda:0')\n",
      "final loss: tensor(0.0768, device='cuda:0')\n",
      "answer_loss: tensor(4.3008, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9160], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9160], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9160], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Samuel Dillon Jackson\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([5.0195], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0195, device='cuda:0')\n",
      "loss: tensor([6.6914], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6914, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0565], device='cuda:0')\n",
      "final loss: tensor(0.0565, device='cuda:0')\n",
      "answer_loss: tensor(5.8555, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Warren Bryant', 'score': tensor([0.8770], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Warren Bryant', 'score': tensor([0.8770], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8770], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Warren Bryant\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([4.2969], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.2969, device='cuda:0')\n",
      "loss: tensor([4.7148], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.7148, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0597], device='cuda:0')\n",
      "final loss: tensor(0.0597, device='cuda:0')\n",
      "answer_loss: tensor(4.5059, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Massachusetts House of Representatives since September 2013. <s> He is a Worcester resident and a member of the Democratic Party.<p> Warren T. Furutani', 'score': tensor([0.8794], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Massachusetts House of Representatives since September 2013. <s> He is a Worcester resident and a member of the Democratic Party.<p> Warren T. Furutani', 'score': tensor([0.8794], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8794], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Massachusetts House of Representatives since September 2013. <s> He is a Worcester resident and a member of the Democratic Party.<p> Warren T. Furutani\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([6.9766], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.9766, device='cuda:0')\n",
      "loss: tensor([6.6445], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.6445, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0854], device='cuda:0')\n",
      "final loss: tensor(0.0854, device='cuda:0')\n",
      "answer_loss: tensor(6.8105, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American', 'score': tensor([0.8696], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American', 'score': tensor([0.8696], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8696], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Courtney Michelle Love (born Courtney Michelle Harrison; July 9, 1964) is an American\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.5586], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5586, device='cuda:0')\n",
      "loss: tensor([7.4688], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.4688, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0394], device='cuda:0')\n",
      "final loss: tensor(0.0394, device='cuda:0')\n",
      "answer_loss: tensor(7.0137, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Dōsei', 'score': tensor([0.8623], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Dōsei', 'score': tensor([0.8623], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8623], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Dōsei\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([5.0586], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0586, device='cuda:0')\n",
      "loss: tensor([5.7188], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.7188, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0393], device='cuda:0')\n",
      "final loss: tensor(0.0393, device='cuda:0')\n",
      "answer_loss: tensor(5.3887, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tommy Chang', 'score': tensor([0.8384], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tommy Chang', 'score': tensor([0.8384], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8384], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tommy Chang\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([5.3047], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.3047, device='cuda:0')\n",
      "loss: tensor([6.4453], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.4453, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0540], device='cuda:0')\n",
      "final loss: tensor(0.0540, device='cuda:0')\n",
      "answer_loss: tensor(5.8750, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles', 'score': tensor([0.8877], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles', 'score': tensor([0.8877], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8877], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Austin Young (born April 12, 1966) is an American photographer, film maker and new media artist currently based in Los Angeles\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([4.3945], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.3945, device='cuda:0')\n",
      "loss: tensor([5.2031], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.2031, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0522], device='cuda:0')\n",
      "final loss: tensor(0.0522, device='cuda:0')\n",
      "answer_loss: tensor(4.7988, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Paul David Nassau', 'score': tensor([0.8711], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Paul David Nassau', 'score': tensor([0.8711], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8711], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Paul David Nassau\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.1719], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1719, device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0508], device='cuda:0')\n",
      "final loss: tensor(0.0508, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Kate Dillon Levin', 'score': tensor([0.8828], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kate Dillon Levin', 'score': tensor([0.8828], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8828], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kate Dillon Levin\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.2656], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.2656, device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0779], device='cuda:0')\n",
      "final loss: tensor(0.0779, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.8892], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.8892], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8892], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kathryn Jane Calder\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([3.6055], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.6055, device='cuda:0')\n",
      "loss: tensor([4.7578], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.7578, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0409], device='cuda:0')\n",
      "final loss: tensor(0.0409, device='cuda:0')\n",
      "answer_loss: tensor(4.1816, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tinley Park', 'score': tensor([0.8877], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tinley Park', 'score': tensor([0.8877], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.8877], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tinley Park\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  4\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([4.3477], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.3477, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.7500], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.7500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0456], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0456, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0456, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.8180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.6170e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([4.2031], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.2031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.9297], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0538], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.5664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.7457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.6170e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([4.8242], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([7.2969], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(7.2969, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0618], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0618, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(6.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0618, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.4043e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([3.6758], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.6758, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.5469], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.5469, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1408], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.1113, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.6392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.4043e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([4.2617], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.2617, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.5898], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.5898, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0412], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.4258, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.9716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.1915e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([4.1211], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.1211, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.8125], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8125, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0516], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.4668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.5320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([3.1915e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([3.3516], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.3516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.8047], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.8047, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1346], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1346, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.0781, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1346, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.3714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.9787e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([3.5312], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.5312, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.6562], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.6562, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1065], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.0938, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.9099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.9787e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([4.2500], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.3281], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.3281, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0397], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0397, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.2891, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0397, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.7611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.7660e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([2.6855], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.6855, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.7539], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.7539, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1495], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.7197, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.6821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.7660e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([2.6035], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.6035, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.0234], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.0234, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0601], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.3135, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0601, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.8086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.5532e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([2.7598], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.7598, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.7656], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.7656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0835], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0835, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.2627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0835, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.9233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.5532e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([2.4863], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.4863, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.1387], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.1387, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0488], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0488, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.8125, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0488, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.7426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.3404e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([3.3965], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.3965, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.8203], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.8203, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0421], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0421, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.6084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0421, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.1999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.3404e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([3.0781], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.0781, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.9414], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.9414, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0438], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0438, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0438, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.6905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.1277e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([5.4609], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4609, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.5078], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.5078, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1347], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1347, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(5.4844, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1347, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(16.7052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([2.1277e-05], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([5.0156], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.0156, device='cuda:0')\n",
      "loss: tensor([5.7188], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.7188, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0368], device='cuda:0')\n",
      "final loss: tensor(0.0368, device='cuda:0')\n",
      "answer_loss: tensor(5.3672, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Robert Chase', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Robert Chase', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9375], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Robert Chase\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([2.9727], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(2.9727, device='cuda:0')\n",
      "loss: tensor([3.2266], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.2266, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0510], device='cuda:0')\n",
      "final loss: tensor(0.0510, device='cuda:0')\n",
      "answer_loss: tensor(3.0996, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' US', 'score': tensor([0.9180], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' US', 'score': tensor([0.9180], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9180], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  US\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([3.8262], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.8262, device='cuda:0')\n",
      "loss: tensor([4.3906], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.3906, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0769], device='cuda:0')\n",
      "final loss: tensor(0.0769, device='cuda:0')\n",
      "answer_loss: tensor(4.1084, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9678], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9678], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9678], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Samuel Dillon Jackson\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([3.7188], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.7188, device='cuda:0')\n",
      "loss: tensor([6.7188], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.7188, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0565], device='cuda:0')\n",
      "final loss: tensor(0.0565, device='cuda:0')\n",
      "answer_loss: tensor(5.2188, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Warren Bryant', 'score': tensor([0.9395], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Warren Bryant', 'score': tensor([0.9395], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9395], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Warren Bryant\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([4.5781], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.5781, device='cuda:0')\n",
      "loss: tensor([4.0625], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.0625, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0597], device='cuda:0')\n",
      "final loss: tensor(0.0597, device='cuda:0')\n",
      "answer_loss: tensor(4.3203, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' John Henry Hoeven III', 'score': tensor([0.9478], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' John Henry Hoeven III', 'score': tensor([0.9478], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9478], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  John Henry Hoeven III\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([7.7070], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.7070, device='cuda:0')\n",
      "loss: tensor([7.1875], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1875, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0855], device='cuda:0')\n",
      "final loss: tensor(0.0855, device='cuda:0')\n",
      "answer_loss: tensor(7.4473, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Courtney Michelle Love', 'score': tensor([0.9453], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Courtney Michelle Love', 'score': tensor([0.9453], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9453], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Courtney Michelle Love\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([6.5195], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.5195, device='cuda:0')\n",
      "loss: tensor([8.2422], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(8.2422, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0395], device='cuda:0')\n",
      "final loss: tensor(0.0395, device='cuda:0')\n",
      "answer_loss: tensor(7.3809, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Francis Lawrence', 'score': tensor([0.9258], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Francis Lawrence', 'score': tensor([0.9258], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9258], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Francis Lawrence\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([5.4531], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.4531, device='cuda:0')\n",
      "loss: tensor([6.4023], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.4023, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0394], device='cuda:0')\n",
      "final loss: tensor(0.0394, device='cuda:0')\n",
      "answer_loss: tensor(5.9277, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Chang Mi-hee', 'score': tensor([0.9048], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Chang Mi-hee', 'score': tensor([0.9048], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9048], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Chang Mi-hee\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([4.0469], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.0469, device='cuda:0')\n",
      "loss: tensor([5.6680], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.6680, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0541], device='cuda:0')\n",
      "final loss: tensor(0.0541, device='cuda:0')\n",
      "answer_loss: tensor(4.8574, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Austin Young', 'score': tensor([0.9502], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Austin Young', 'score': tensor([0.9502], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9502], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Austin Young\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([3.2578], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.2578, device='cuda:0')\n",
      "loss: tensor([3.6309], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.6309, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0522], device='cuda:0')\n",
      "final loss: tensor(0.0522, device='cuda:0')\n",
      "answer_loss: tensor(3.4443, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' David Nassau', 'score': tensor([0.9463], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' David Nassau', 'score': tensor([0.9463], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9463], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  David Nassau\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.1211], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1211, device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0509], device='cuda:0')\n",
      "final loss: tensor(0.0509, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Anthony Frederick Levin', 'score': tensor([0.9595], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Anthony Frederick Levin', 'score': tensor([0.9595], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9595], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Anthony Frederick Levin\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.1016], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1016, device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0779], device='cuda:0')\n",
      "final loss: tensor(0.0779, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.9609], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.9609], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9609], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kathryn Jane Calder\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([3.6719], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.6719, device='cuda:0')\n",
      "loss: tensor([4.3789], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.3789, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0409], device='cuda:0')\n",
      "final loss: tensor(0.0409, device='cuda:0')\n",
      "answer_loss: tensor(4.0254, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tinley Park', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tinley Park', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9375], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tinley Park\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "Start epoch  5\n",
      "size of input_ids: torch.Size([1, 1118])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 46, 768])\n",
      "loss: tensor([2.5098], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.5098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.3516], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.3516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0458], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0458, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.9307, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0458, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.7107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.9149e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1079])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 39, 768])\n",
      "loss: tensor([3.2578], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.2578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([4.0547], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(4.0547, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0539], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0539, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.6562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0539, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.8411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.9149e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 2078])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 68, 768])\n",
      "loss: tensor([2.6035], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.6035, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.8672], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.8672, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0620], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0620, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.2354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0620, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.8239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.7021e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 746])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 23, 768])\n",
      "loss: tensor([2.0586], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.0586, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1408], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.5312, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1408, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(14.0590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.7021e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1314])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([2.5859], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.5859, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.9688], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.9688, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0412], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(3.2773, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.8204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.4894e-05], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1872])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 61, 768])\n",
      "loss: tensor([2.5664], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.5664, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.2598], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.2598, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2242], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0517], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0517, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.9131, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0517, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.9847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.4894e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 878])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 32, 768])\n",
      "loss: tensor([1.4531], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.4531, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([3.9102], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(3.9102, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1345], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.6816, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1345, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.8922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.2766e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1494])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([1.2539], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.2539, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.8027], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.8027, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2242], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1068], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1068, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.0283, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1068, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(11.8520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.2766e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1459])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([2.6152], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.6152, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.8320], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.8320, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2242], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0399], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0399, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.7236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0399, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.2049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.0638e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 994])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 29, 768])\n",
      "loss: tensor([1.3887], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.3887, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.6484], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.6484, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1497], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1497, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.0186, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1497, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(13.9914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([1.0638e-05], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1376])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([1.6016], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.6016, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.3340], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.3340, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0606], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0606, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(1.9678, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0606, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(9.4867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.5106e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1316])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 38, 768])\n",
      "loss: tensor([1.9492], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.9492, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.4336], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.4336, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0838], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0838, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.1914, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0838, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(10.9457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([8.5106e-06], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input_ids: torch.Size([1, 1548])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 43, 768])\n",
      "loss: tensor([1.9434], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.9434, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([1.9160], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.9160, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2244], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0494], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0494, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(1.9297, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0494, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(8.8867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.3830e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1597])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 50, 768])\n",
      "loss: tensor([1.8047], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.8047, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.4297], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.4297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0423], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0423, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.1172, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0423, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(8.7223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([6.3830e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1285])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 48, 768])\n",
      "loss: tensor([1.5957], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(1.5957, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([2.5098], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.5098, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2245], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0441], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0441, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(2.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2245, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.0441, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(8.7468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.2553e-06], device='cuda:0')\n",
      "size of input_ids: torch.Size([1, 1345])\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([2.7188], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(2.7188, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([5.4023], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(5.4023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.0039], device='cuda:0', dtype=torch.float16, grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.2243], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "loss: tensor([0.1347], device='cuda:0', grad_fn=<NegBackward>)\n",
      "final loss: tensor(0.1347, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "answer_loss:  tensor(4.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "type_loss:  tensor(0.0039, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_para_loss:  tensor(0.2243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "sp_sent_loss:  tensor(0.1347, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "weighted loss:  tensor(15.3591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "lr:  tensor([4.2553e-06], device='cuda:0')\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 57, 768])\n",
      "loss: tensor([5.4414], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.4414, device='cuda:0')\n",
      "loss: tensor([6.0625], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.0625, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2243], device='cuda:0')\n",
      "final loss: tensor(0.2243, device='cuda:0')\n",
      "loss: tensor([0.0370], device='cuda:0')\n",
      "final loss: tensor(0.0370, device='cuda:0')\n",
      "answer_loss: tensor(5.7520, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' American', 'score': tensor([0.9497], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' American', 'score': tensor([0.9497], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9497], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  American\n",
      "answer_gold_token_ids: tensor([2063], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠFox']\n",
      "answer_gold:  Fox\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([2.9453], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(2.9453, device='cuda:0')\n",
      "loss: tensor([3.3867], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.3867, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0511], device='cuda:0')\n",
      "final loss: tensor(0.0511, device='cuda:0')\n",
      "answer_loss: tensor(3.1660, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' US', 'score': tensor([0.9321], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' US', 'score': tensor([0.9321], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9321], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  US\n",
      "answer_gold_token_ids: tensor([  36, 3789,  322], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ(', '2017', ').']\n",
      "answer_gold:  (2017).\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([4.1406], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1406, device='cuda:0')\n",
      "loss: tensor([4.7109], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.7109, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0770], device='cuda:0')\n",
      "final loss: tensor(0.0770, device='cuda:0')\n",
      "answer_loss: tensor(4.4258, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9766], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Samuel Dillon Jackson', 'score': tensor([0.9766], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9766], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Samuel Dillon Jackson\n",
      "answer_gold_token_ids: tensor([5077,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNevada', '.']\n",
      "answer_gold:  Nevada.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([3.5508], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.5508, device='cuda:0')\n",
      "loss: tensor([6.8945], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.8945, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0566], device='cuda:0')\n",
      "final loss: tensor(0.0566, device='cuda:0')\n",
      "answer_loss: tensor(5.2227, device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Detroit, Michigan', 'score': tensor([0.9365], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Detroit, Michigan', 'score': tensor([0.9365], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9365], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Detroit, Michigan\n",
      "answer_gold_token_ids: tensor([6467,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠHawaii', '.']\n",
      "answer_gold:  Hawaii.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 35, 768])\n",
      "loss: tensor([4.9141], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.9141, device='cuda:0')\n",
      "loss: tensor([3.7891], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.7891, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0598], device='cuda:0')\n",
      "final loss: tensor(0.0598, device='cuda:0')\n",
      "answer_loss: tensor(4.3516, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' John Henry Hoeven III', 'score': tensor([0.9541], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' John Henry Hoeven III', 'score': tensor([0.9541], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9541], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  John Henry Hoeven III\n",
      "answer_gold_token_ids: tensor([  229, 13750,  5986], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠK', 'elli', 'ĠWard']\n",
      "answer_gold:  Kelli Ward\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 37, 768])\n",
      "loss: tensor([8.5469], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(8.5469, device='cuda:0')\n",
      "loss: tensor([8.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(8., device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0857], device='cuda:0')\n",
      "final loss: tensor(0.0857, device='cuda:0')\n",
      "answer_loss: tensor(8.2734, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Courtney Michelle Love', 'score': tensor([0.9629], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Courtney Michelle Love', 'score': tensor([0.9629], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9629], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Courtney Michelle Love\n",
      "answer_gold_token_ids: tensor([   20,  7602,   298, 12363], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠThe', 'ĠWolf', 'h', 'ounds']\n",
      "answer_gold:  The Wolfhounds\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([7.1641], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.1641, device='cuda:0')\n",
      "loss: tensor([8.5469], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(8.5469, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0397], device='cuda:0')\n",
      "final loss: tensor(0.0397, device='cuda:0')\n",
      "answer_loss: tensor(7.8555, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Francis Lawrence', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Francis Lawrence', 'score': tensor([0.9375], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9375], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Francis Lawrence\n",
      "answer_gold_token_ids: tensor([545,  12, 180,  12, 279], device='cuda:0')\n",
      "answer_gold_tokens: ['Ġ16', '-', 'year', '-', 'old']\n",
      "answer_gold:  16-year-old\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 53, 768])\n",
      "loss: tensor([5.5898], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.5898, device='cuda:0')\n",
      "loss: tensor([6.3516], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.3516, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0396], device='cuda:0')\n",
      "final loss: tensor(0.0396, device='cuda:0')\n",
      "answer_loss: tensor(5.9707, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' 2006', 'score': tensor([0.9189], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' 2006', 'score': tensor([0.9189], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9189], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  2006\n",
      "answer_gold_token_ids: tensor([ 623, 1771, 3082], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠWorld', 'ĠWar', 'ĠII']\n",
      "answer_gold:  World War II\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 58, 768])\n",
      "loss: tensor([4.1562], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.1562, device='cuda:0')\n",
      "loss: tensor([5.5352], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(5.5352, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0543], device='cuda:0')\n",
      "final loss: tensor(0.0543, device='cuda:0')\n",
      "answer_loss: tensor(4.8457, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Austin Young', 'score': tensor([0.9619], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Austin Young', 'score': tensor([0.9619], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9619], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Austin Young\n",
      "answer_gold_token_ids: tensor([6540, 7431,    4], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠTodd', 'ĠPhillips', '.']\n",
      "answer_gold:  Todd Phillips.\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 40, 768])\n",
      "loss: tensor([3.0723], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.0723, device='cuda:0')\n",
      "loss: tensor([3.1465], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.1465, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2242], device='cuda:0')\n",
      "final loss: tensor(0.2242, device='cuda:0')\n",
      "loss: tensor([0.0525], device='cuda:0')\n",
      "final loss: tensor(0.0525, device='cuda:0')\n",
      "answer_loss: tensor(3.1094, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' David Nassau', 'score': tensor([0.9561], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' David Nassau', 'score': tensor([0.9561], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9561], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  David Nassau\n",
      "answer_gold_token_ids: tensor([9347, 6226,    6], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠCarol', 'ĠLawrence', ',']\n",
      "answer_gold:  Carol Lawrence,\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 41, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([7.0703], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(7.0703, device='cuda:0')\n",
      "loss: tensor([0.2240], device='cuda:0')\n",
      "final loss: tensor(0.2240, device='cuda:0')\n",
      "loss: tensor([0.0510], device='cuda:0')\n",
      "final loss: tensor(0.0510, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers: [{'text': ' Anthony Frederick Levin', 'score': tensor([0.9717], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Anthony Frederick Levin', 'score': tensor([0.9717], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9717], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Anthony Frederick Levin\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 27, 768])\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([inf], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([6.8672], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(6.8672, device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0781], device='cuda:0')\n",
      "final loss: tensor(0.0781, device='cuda:0')\n",
      "answer_loss: tensor(0., device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.9731], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Kathryn Jane Calder', 'score': tensor([0.9731], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9731], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Kathryn Jane Calder\n",
      "answer_gold: no\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n",
      "validation_step\n",
      "size of sp_para_output: torch.Size([1, 10, 768])\n",
      "size of sp_sent_output: torch.Size([1, 51, 768])\n",
      "loss: tensor([3.4844], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(3.4844, device='cuda:0')\n",
      "loss: tensor([4.3516], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(4.3516, device='cuda:0')\n",
      "loss: tensor([0.], device='cuda:0', dtype=torch.float16)\n",
      "final loss: tensor(0., device='cuda:0')\n",
      "loss: tensor([0.2241], device='cuda:0')\n",
      "final loss: tensor(0.2241, device='cuda:0')\n",
      "loss: tensor([0.0410], device='cuda:0')\n",
      "final loss: tensor(0.0410, device='cuda:0')\n",
      "answer_loss: tensor(3.9180, device='cuda:0')\n",
      "decode\n",
      "answers: [{'text': ' Tinley Park', 'score': tensor([0.9595], device='cuda:0', dtype=torch.float16)}]\n",
      "answers_pred: [{'text': ' Tinley Park', 'score': tensor([0.9595], device='cuda:0', dtype=torch.float16)}]\n",
      "pred answer_score: tensor([0.9595], device='cuda:0', dtype=torch.float16)\n",
      "pred answer_text:  Tinley Park\n",
      "answer_gold_token_ids: tensor([188, 469, 412], device='cuda:0')\n",
      "answer_gold_tokens: ['ĠNew', 'ĠYork', 'ĠCity']\n",
      "answer_gold:  New York City\n",
      "f1: tensor(0., device='cuda:0')\n",
      "prec: tensor(0., device='cuda:0')\n",
      "recall: tensor(0., device='cuda:0')\n",
      "em: tensor(0., device='cuda:0')\n",
      "return\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     if not args.test:\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_dev.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '1')\n",
      "('warmup', 1000)\n",
      "('lr', 0.0001)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 1)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', True)\n",
      "('model_path', '/Users/fan/Downloads/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json --dev_dataset small_dev.json  \\\n",
    "    --gpus 1 --num_workers 1 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer  --model_path /Users/fan/Downloads/longformer-base-4096 --test '\n",
    "# hotpot_dev_distractor_v1.json\n",
    "\n",
    "import argparse \n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    for arg in vars(args):\n",
    "        print((arg, getattr(args, arg)))\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
