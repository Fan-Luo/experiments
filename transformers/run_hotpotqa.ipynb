{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase the cell width \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; } </style>\"))   \n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "# sys.path.insert(0, '/xdisk/msurdeanu/fanluo/miniconda3/envs/hotpotqa/lib/python3.7/site-packages') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm \n",
    "from datetime import datetime \n",
    "import pytz \n",
    "timeZ_Az = pytz.timezone('US/Mountain') \n",
    "#!pip install -U transformers\n",
    "#!pip install torch==1.6.0 torchvision==0.7.0\n",
    "import transformers \n",
    "\n",
    "QUESTION_START = '[question]'\n",
    "QUESTION_END = '[/question]' \n",
    "TITLE_START = '<t>'  # indicating the start of the title of a paragraph (also used for loss over paragraphs)\n",
    "TITLE_END = '</t>'   # indicating the end of the title of a paragraph\n",
    "SENT_MARKER_END = '[/sent]'  # indicating the end of the title of a sentence (used for loss over sentences)\n",
    "PAR = '[/par]'  # used for indicating end of the regular context and beginning of `yes/no/null` answers\n",
    "EXTRA_ANSWERS = \" yes no null\"\n",
    "\n",
    " \n",
    "def create_example_dict(context, answer, id, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answer\": answer,\n",
    "                \"id\": id,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tqdm \n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git \n",
    "# !python -m pip install pytorch-lightning==0.6.0\n",
    "# !python -m pip install jdc  \n",
    "# !wget https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/longformer-base-4096.tar.gz\n",
    "# !tar -xf longformer-base-4096.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )    \n",
    "        \n",
    "        for lst in raw_contexts:\n",
    "            lst[0] = _normalize_text(lst[0])\n",
    "            lst[1] = [_normalize_text(sent) for sent in lst[1]]\n",
    "        \n",
    "        contexts = [lst[0]  + ' '  + ' '.join(lst[1]) for lst in raw_contexts]    \n",
    "        # extra space is fine, which would be ignored latter. most sentences has already have heading space, there are several no heading space; call the _normalize_text() which is same as the one used during evaluation\n",
    "        \n",
    "#         context = \" </s> \".join(contexts)\n",
    "#         print(context)\n",
    "        \n",
    "#         exit(0)\n",
    "\n",
    "        \n",
    "        answer = _normalize_text(example[\"answer\"]) \n",
    "#         print(\"answer: \", answer)\n",
    "        if(len(answer) > 0):   # answer can be '' after normalize\n",
    "            new_dict[\"data\"].append(\n",
    "                create_para_dict(\n",
    "                    create_example_dict(\n",
    "                        context=contexts,\n",
    "                        answer=answer,\n",
    "                        id = example[\"_id\"],\n",
    "                        question=_normalize_text(example[\"question\"]),\n",
    "                        is_sup_fact = is_sup_fact,\n",
    "                        is_supporting_para = is_supporting_para \n",
    "                    )\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def _normalize_text(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": [\n",
      "        \"dr seuss how grinch stole christmas dr seuss how grinch stole christmas is video game based on dr seuss book with same name but mostly based on film game was released on november 8 2007\",\n",
      "        \"lorax film lorax also known as dr seuss lorax is 2012 american 3d computeranimated musical fantasy\\u2013comedy film produced by illumination entertainment and based on dr seusss childrens book of same name film was released by universal pictures on march 2 2012 on 108th birthday of dr seuss second film adaptation of book following 1972 animated television special film builds on book by expanding story of ted previously unnamed boy who visits onceler cast includes danny devito as lorax ed helms as onceler and zac efron as ted new characters introduced in film are audrey voiced by taylor swift aloysius ohare rob riggle mrs wiggins teds mother jenny slate and grammy norma betty white\",\n",
      "        \"horton hears who tv special horton hears who is 1970 television special based on dr seuss book of same name horton hears who  it was produced and directed by chuck jones \\u2013 who previously produced seuss special how grinch stole christmas \\u2013 for mgm television and first broadcast march 19 1970 on cbs special contains songs with lyrics by seuss and music by eugene poddany who previously wrote songs for seuss book cat in hat song book\",\n",
      "        \"dr seuss memorial dr seuss national memorial sculpture garden is sculpture garden in springfield massachusetts that honors theodor seuss geisel better known to world as dr seuss located at quadrangle dr seuss national memorial sculpture garden honors author and illustrator who was born in springfield in 1904 monument was designed by lark grey dimondcates authors stepdaughter and created by sculptor and artist ron henson\",\n",
      "        \"dr seuss bibliography theodor seuss geisel better known as dr seuss published over 60 childrens books over course of his long career though most were published under his wellknown pseudonym dr seuss he also authored over dozen books as theo lesieg and one as rosetta stone as one of most popular childrens authors of all time geisels books have topped many bestseller lists sold over 222 million copies and been translated into more than 15 languages in 2000 when publishers weekly compiled their list of bestselling childrens books of all time 16 of top 100 hardcover books were written by geisel including green eggs and ham at number 4 cat in hat at number 9 and one fish two fish red fish blue fish at number 13 and dr seusss abc in years following his death in 1991 several additional books based on his sketches and notes were published including hooray for diffendoofer day and daisyhead mayzie although they were all published under name dr seuss only my many colored days originally written in 1973 was entirely by geisel\",\n",
      "        \"how grinch stole christmas 2018 film dr seuss how grinch stole christmas promoted theatrically as dr seuss grinch is upcoming american 3d computeranimated christmas musical comedy film produced by illumination entertainment it is based on 1957 dr seuss story of same name film will be released on november 9 2018 by universal pictures\",\n",
      "        \"do you know what im going to do next saturday do you know what im going to do next saturday is 1963 childrens book published by beginner books and written by helen palmer geisel first wife of theodor seuss geisel dr seuss unlike most of beginner books do you know what im going to do next saturday did not follow format of text with inline drawings being illustrated with blackandwhite photographs by lynn fayman featuring boy named rawli davis it is sometimes misattributed to dr seuss himself books cover features photograph of young boy sitting at breakfast table with huge pile of pancakes\",\n",
      "        \"wubbulous world of dr seuss wubbulous world of dr seuss is liveactionpuppet television series based on characters created by dr seuss produced by jim henson company it aired from october 13 1996 to december 28 1998 on nickelodeon it is notable for its use of live puppets with digitally animated backgrounds and in its first season for refashioning characters and themes from original dr seuss books into new stories that often retained much of flavor of dr seuss own works\",\n",
      "        \"cat in hat film dr seuss cat in hat is 2003 american family comedy film directed by bo welch it is based on 1957 dr seuss book of same name film stars mike myers in title role of cat in hat and dakota fanning as sally sallys brother who is unnamed in book and 1971 tv special conrad is portrayed by spencer breslin film is second featurelength dr seuss adaptation after 2000 holiday film how grinch stole christmas\",\n",
      "        \"kyle balda kyle balda is american animator and film director best known for codirecting animated films lorax 2012 with chris renaud and minions 2015 with pierre coffin he has also worked as animator on several films including jumanji toy story 2 and despicable me he has worked for pixar for years and now he is working for illumination entertainment\"\n",
      "      ],\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answer\": \"lorax\",\n",
      "          \"id\": \"5ab990925542996be2020553\",\n",
      "          \"question\": \"what film did kyle balda work on that was based on dr seuss book\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/Data')\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[76200:76280]' > small.json\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[37:50]' > small_dev.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/hotpot_train_v1.1.json | ../../helper/jq-linux64 -c '.[31:50]' > sample.json\n",
    "# !cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/reduced_questions/hotpot_reduced_context_04-08-2021-01:12:53/hotpot_dev_reduced_context_coref_fuzzy.json | ../../helper/jq-linux64 -c '.[6666:7000]' > small_dev.json\n",
    "!cat /xdisk/msurdeanu/fanluo/hotpotQA/Data/reduced_questions/hotpot_reduced_context_04-08-2021-01:12:53/hotpot_dev_reduced_context_coref_fuzzy.json | ../../helper/jq-linux64 -c '.[1515:1525]' > small_dev3.json\n",
    "        \n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "\n",
    "# !pip uninstall longformer -y\n",
    "# !python -m pip uninstall longformer -y\n",
    "# !pip install git+https://github.com/allenai/longformer.git \n",
    "# !python -m pip uninstall pytorch-lightning -y\n",
    "# !pip uninstall pytorch-lightning -y\n",
    "# !python -m pip install git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning\n",
    "#!pip install torch==1.6.0 torchvision==0.7.0\n",
    " \n",
    "\n",
    "\n",
    "####requirements.txt:torch>=1.2.0, transformers>=3.0.2, tensorboardX, pytorch-lightning==0.6.0, test-tube==0.7.5\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "#!python -m pip install jdc \n",
    "# !pip install test-tube \n",
    "#!python -m pip install ipywidgets \n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "import jdc\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(pl.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "#         if(\"reduced_context\" not in self.file_path):\n",
    "        with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "            print(f'reading file: {self.file_path}')\n",
    "            self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "                \n",
    "#         else:\n",
    "#             with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "#                 print(f'reading file: {self.file_path}')\n",
    "#                 self.data_json = json.load(f)['data']            \n",
    "#                 print(self.data_json[0])\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "\n",
    "#         print(tokenizer.all_special_tokens) \n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =                  {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        if(len(tensors_list) != 1):\n",
    "            print(\"tensors_list: \", tensors_list)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     106,
     122,
     147,
     162
    ]
   },
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset):\n",
    "            # char offset to word offset\n",
    "            if(answer_start >= len(char_to_word_offset)):\n",
    "                print(\"answer_start: \", answer_start)\n",
    "                print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "            # char offset to word offset\n",
    "            start_word_position = char_to_word_offset[answer_start]\n",
    "            end_word_position = char_to_word_offset[answer_end-1] \n",
    "\n",
    "#             print(\"start_word_position: \", start_word_position)\n",
    "#             print(\"end_word_position: \", end_word_position)\n",
    "            # sub_tokens postion reletive to context\n",
    "            tok_start_position_in_doc = orig_to_tok_index[start_word_position]  \n",
    "            not_end_of_doc = int(end_word_position + 1 < len(orig_to_tok_index))\n",
    "            tok_end_position_in_doc = orig_to_tok_index[end_word_position + not_end_of_doc] - not_end_of_doc\n",
    "            \n",
    "            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                return (-1, -1) # this answer is outside the current slice                     \n",
    "            \n",
    "            # sub_tokens postion reletive to begining of all the tokens, including query sub tokens  \n",
    "            start_position = tok_start_position_in_doc + doc_offset  \n",
    "            end_position = tok_end_position_in_doc + doc_offset\n",
    "            \n",
    "            return (start_position, end_position)\n",
    "        \n",
    "#         print(\"idx: \", idx)\n",
    "#         print(\"len(example): \", \"len(example)\")\n",
    "        if(len(example[\"paragraphs\"])==0):\n",
    "            print(\"idx: \", idx, \"'s len(example[‘paragraphs’])==0\")\n",
    "\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            # print(\"for paragraph in example['paragraphs']: \") \n",
    "            context = self.tokenizer.sep_token + ' ' + (' ' + self.tokenizer.sep_token + ' ').join(paragraph[\"context\"] )   \n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in context:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            \n",
    "#             print(\"len(char_to_word_offset): \", len(char_to_word_offset))\n",
    "#             print(\"char_to_word_offset: \", char_to_word_offset)\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "                # print(\"question text: \", question_text)  \n",
    "                # sp_sent = qa[\"is_sup_fact\"]\n",
    "                # sp_para = qa[\"is_supporting_para\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None \n",
    "\n",
    "#                     print(\"len(sp_sent):\", len(sp_sent))\n",
    "#                     print(\"sp_sent\", sp_sent) \n",
    "#                     print(\"doc_tokens\", doc_tokens)\n",
    " \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                \n",
    "                   \n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                 \n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                \n",
    "                # each original token in the context is tokenized to multiple sub_tokens\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "                \n",
    "                # all sub tokens, truncate up to limit\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len-7] \n",
    "\n",
    "                # The -7 accounts for CLS, QUESTION_START, QUESTION_END， [/par]， yes， no， </s>   \n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 7\n",
    "                if(max_tokens_per_doc_slice <= 0):\n",
    "                    print(\"(max_tokens_per_doc_slice <= 0)\")\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -negtive value for the next loop to execute once\n",
    "                \n",
    "                # inputs to the model\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                # sp_sent_list =  [1 if ss else 0 for ss in sp_sent]\n",
    "                # sp_para_list = [1 if sp else 0 for sp in sp_para]\n",
    "                \n",
    "                if(len(all_doc_tokens) == 0):\n",
    "                    print(\"idx: \", idx, \" len(all_doc_tokens) == 0\")\n",
    "#               \n",
    "                \n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                \n",
    "                    # print(\"slice_start in range\") \n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [self.tokenizer.cls_token] + [QUESTION_START] + query_tokens + [QUESTION_END] + doc_slice_tokens + [PAR] + self.tokenizer.tokenize(\"yes\") + self.tokenizer.tokenize(\"no\") + [self.tokenizer.eos_token]   \n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] * (len(doc_slice_tokens) + 4) \n",
    "#                     if(len(segment_ids) != len(tokens)):\n",
    "#                         print(\"len(segment_ids): \", len(segment_ids))\n",
    "#                         print(\"len(tokens): \", len(tokens))\n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)   \n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start  # where context starts\n",
    "                    \n",
    "                    # ===== answer positions tensors  ============\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    " \n",
    "                    answer = qa[\"answer\"] \n",
    "                    # print(\"idx: \", idx, \" qa['id']: \", qa['id'], \" answer: \", answer)\n",
    "                    if answer == '':\n",
    "                        q_type = -1\n",
    "                        start_positions.append(-1)   \n",
    "                        end_positions.append(-1)           \n",
    "                    \n",
    "                    elif answer == 'yes':\n",
    "                        q_type = 1\n",
    "                        start_positions.append(len(tokens)-3)   \n",
    "                        end_positions.append(len(tokens)-3) \n",
    "                    elif answer == 'no':\n",
    "                        q_type = 2\n",
    "                        start_positions.append(len(tokens)-2)   \n",
    "                        end_positions.append(len(tokens)-2)  \n",
    "                    else:\n",
    "                        # keep all the occurences of answer in the context \n",
    "#                         for m in re.finditer(\"\\s?\".join(answer.split()), context):   # \"\\s?\".join(answer.split()) in order to match even with extra space in answer or context\n",
    "                        for m in re.finditer(_normalize_text(answer), context, re.IGNORECASE):\n",
    "                            answer_start, answer_end = m.span() \n",
    "                            start_position, end_position = map_answer_positions(char_to_word_offset, orig_to_tok_index, answer_start, answer_end, slice_start, slice_end, doc_offset)\n",
    "                            if(start_position != -1):\n",
    "                                start_positions.append(start_position)   \n",
    "                                end_positions.append(end_position)\n",
    "                            \n",
    "                        if(len(start_positions) > 0): \n",
    "                            q_type = 0\n",
    "                        else: # answer not found in context\n",
    "                            q_type = -1\n",
    "                            start_positions.append(-1)   \n",
    "                            end_positions.append(-1) \n",
    "\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        \n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "                    \n",
    "                                         \n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "                        \n",
    "                        print(\"self.doc_stride >= 0\")\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len  \n",
    "                        \n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                    \n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list), torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                    #   torch.tensor([sp_sent_list]),  torch.tensor([sp_para_list]),\n",
    "                                     qa['id'], answer))     \n",
    "        return tensors_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 2  # qid and answer  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7f211d608828>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_,  forward, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    " \n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [TITLE_START, TITLE_END, SENT_MARKER_END, QUESTION_START , QUESTION_END, PAR]})\n",
    "#         print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "         \n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 3)   #  question type (yes/no/span/null) classification \n",
    "\n",
    "#         self.fnn_sp_sent = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "        \n",
    "#         self.fnn_sp_para = torch.nn.Sequential(\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size), \n",
    "#           torch.nn.GELU(),\n",
    "#           torch.nn.Linear(self.model.config.hidden_size, 1),      # score for 'yes', while 0 for 'no'\n",
    "#         )\n",
    "         \n",
    "        \n",
    "        self.train_dataloader_object = self.val_dataloader_object = self.test_dataloader_object = None\n",
    "        \n",
    " \n",
    "    def load_model(self):\n",
    "        \n",
    "        config = LongformerConfig.from_pretrained(self.args.model_path) \n",
    "        # choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n",
    "        # 'n2': for regular n2 attantion\n",
    "        # 'tvm': a custom CUDA kernel implementation of our sliding window attention\n",
    "        # 'sliding_chunks': a PyTorch implementation of our sliding window attention\n",
    "        config.attention_mode = 'sliding_chunks'\n",
    "        model = Longformer.from_pretrained(self.args.model_path, config=config)\n",
    "\n",
    "        print(\"self.args.model_path: \", self.args.model_path)\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "#%%add_to hotpotqa    # does not seems to work for the @pl.data_loader decorator, missing which causes error \"validation_step() takes 3 positional arguments but 4 were given\"    \n",
    "###################################################### dataloaders ########################################################### \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,   # set shuffle=False, otherwise it will sample a different subset of data every epoch with train_percent_check\n",
    "                        num_workers=self.args.num_workers,  \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "        self.train_dataloader_object = dl  \n",
    "        return self.train_dataloader_object\n",
    "    \n",
    " \n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples \n",
    "\n",
    "        \n",
    "        \n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataloader_object is not None:\n",
    "            return self.test_dataloader_object\n",
    "        dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "#         dist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, \n",
    "                        collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "        self.test_dataloader_object = dl\n",
    "        return self.test_dataloader_object\n",
    "\n",
    "#%%add_to hotpotqa  \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type):  #, sp_sent, sp_para):\n",
    " \n",
    " \n",
    "        if 'longformer' in self.args.model_path:\n",
    "            \n",
    "            if(input_ids.size(0) != 1):\n",
    "                print(\"input_ids.size(0) != 1\")\n",
    "            assert(input_ids.size(0)==1)\n",
    "            # Each batch is one document, and each row of the batch is a chunck of the document.    ????\n",
    "            # Make sure all rows have the same question length.\n",
    "            \n",
    "#             print(\"start_positions: \", start_positions)\n",
    "#             print(\"end_positions: \", end_positions)\n",
    "            # local attention everywhere\n",
    "            attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "            \n",
    "            # global attention for the cls and all question tokens\n",
    "            # print(\"question_end_index\")\n",
    "            question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "    #         if(question_end_index.size(0) == 1):\n",
    "    #             attention_mask[:,:question_end_index.item()] = 2  \n",
    "    #         else:\n",
    "            attention_mask[:,:question_end_index[0].item()+1] = 2  # from <cls> until </q>\n",
    "    #             print(\"more than 1 <q> in: \", self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) )\n",
    "            \n",
    "            # global attention for the sentence and paragraph special tokens  \n",
    "            # print(\"sent_indexes\")\n",
    "            sent_indexes = self._get_special_index(input_ids, [SENT_MARKER_END])\n",
    "            attention_mask[:, sent_indexes] = 2\n",
    "            \n",
    "            # print(\"para_indexes\")\n",
    "            para_indexes = self._get_special_index(input_ids, [TITLE_START])\n",
    "            attention_mask[:, para_indexes] = 2       \n",
    "             \n",
    "    \n",
    "            # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "            input_ids, attention_mask = pad_to_window_size(\n",
    "                input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "    \n",
    "            sequence_output = self.model(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask)[0]\n",
    "    #         print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "            print(\"sequence_output: \" + str(sequence_output))\n",
    "    \n",
    "            # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "            # before computing loss and decoding.\n",
    "            padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "            if padding_len > 0:\n",
    "                sequence_output = sequence_output[:, :-padding_len]\n",
    "    #         print(\"size of sequence_output after removing padding: \" + str(sequence_output.size()))\n",
    "        else:\n",
    "            sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]      \n",
    "        \n",
    "        ###################################### layers on top of sequence_output ##################################\n",
    "        \n",
    "\n",
    "        ### 1. answer start and end positions classification ###   \n",
    "        logits = self.qa_outputs(sequence_output) \n",
    "        start_logits, end_logits = logits.split(1, dim=-1) \n",
    "        start_logits = start_logits.squeeze(-1) \n",
    "        end_logits = end_logits.squeeze(-1)\n",
    " \n",
    "        ### 2. type classification, similar as class LongformerClassificationHead(nn.Module) https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification.forward ### \n",
    "        type_logits = self.linear_type(sequence_output[:,0]) \n",
    "        \n",
    "        # ### 3. supporting paragraph classification ###  \n",
    "        # sp_para_output = sequence_output[:,para_indexes,:]  \n",
    "        # sp_para_output_t = self.fnn_sp_para(sp_para_output) \n",
    "\n",
    "        #  # linear_sp_sent generates a single score for each sentence, instead of 2 scores for yes and no.   \n",
    "        # # Argument the score with additional score=0. The same way did in the HOTPOTqa paper\n",
    "        # sp_para_output_aux = torch.zeros(sp_para_output_t.shape, dtype=torch.float, device=sp_para_output_t.device) \n",
    "        # predict_support_para = torch.cat([sp_para_output_aux, sp_para_output_t], dim=-1).contiguous() \n",
    " \n",
    "        # ### 4. supporting fact classification ###     \n",
    "        # # the first sentence in a paragraph is leading by <p>, other sentences are leading by <s>\n",
    " \n",
    "        # sp_sent_output = sequence_output[:,sent_indexes,:]  \n",
    "        # sp_sent_output_t = self.fnn_sp_sent(sp_sent_output)     \n",
    "        # sp_sent_output_aux = torch.zeros(sp_sent_output_t.shape, dtype=torch.float, device=sp_sent_output_t.device) \n",
    "        # predict_support_sent = torch.cat([sp_sent_output_aux, sp_sent_output_t], dim=-1).contiguous() \n",
    "        \n",
    "        answer_loss, type_loss = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits)\n",
    "        # answer_loss, type_loss, sp_para_loss, sp_sent_loss  = self.loss_computation(start_positions, end_positions, start_logits, end_logits, q_type, type_logits, sp_para, predict_support_para, sp_sent, predict_support_sent)\n",
    "         \n",
    " \n",
    "        return answer_loss, type_loss, start_logits, end_logits, type_logits\n",
    "    \n",
    "    def loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits):  #, sp_para, predict_support_para, sp_sent, predict_support_sent):\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    " \n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "  \n",
    "\n",
    "            else: \n",
    "                start_positions = start_positions[:, 0:1]   # only use the top1 start_position considering only one appearance of the answer string\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = crossentropy(start_logits, start_positions[:, 0])\n",
    "                end_loss = crossentropy(end_logits, end_positions[:, 0])\n",
    "                \n",
    " \n",
    "            crossentropy = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            type_loss = crossentropy(type_logits, q_type)  \n",
    "            \n",
    "            # crossentropy_average = torch.nn.CrossEntropyLoss(reduction = 'mean', ignore_index=-1)      \n",
    "            # sp_para_loss = crossentropy_average(predict_support_para.view(-1, 2), sp_para.view(-1))\n",
    "            # sp_sent_loss = crossentropy_average(predict_support_sent.view(-1, 2), sp_sent.view(-1))      \n",
    " \n",
    "            answer_loss = (start_loss + end_loss) / 2 \n",
    "        return answer_loss, type_loss#, sp_para_loss, sp_sent_loss  \n",
    "\n",
    "\n",
    "#     %%add_to hotpotqa    \n",
    "    def _get_special_index(self, input_ids, special_tokens):\n",
    "        \n",
    "        if(input_ids.size(0)!=1):\n",
    "            print(\"input_ids.size(0): \", input_ids.size(0))\n",
    "            print(\"input_ids: \", input_ids)\n",
    "        \n",
    "        assert(input_ids.size(0)==1) \n",
    "        mask = input_ids != input_ids # initilaize \n",
    "        for special_token in special_tokens:\n",
    "            mask = torch.logical_or(mask, input_ids.eq(self.tokenizer.convert_tokens_to_ids(special_token))) \n",
    " \n",
    "        token_indices = torch.nonzero(mask, as_tuple=False)    \n",
    "         \n",
    " \n",
    "        return token_indices[:,1]    \n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0) \n",
    "        \n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "        # target are indexes of tokens, padded with ignore_index=-1\n",
    "        # logits are scores (one for each label) for each token\n",
    " \n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "        masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "    \n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    " \n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    " \n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    " \n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    " \n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm) \n",
    "        \n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "        # when `target` is all `ignore_index`, loss is 0 \n",
    "        loss = loss[~torch.isinf(loss)].sum()\n",
    "#         loss = torch.tanh(loss)\n",
    "#         print(\"final loss: \" + str(loss)) \n",
    "        return loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# input_ids = torch.tensor([[-1, 5, -1, 2]])\n",
    "# input_ids.size(0)\n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(-1))[:,1]\n",
    "# # token_indices\n",
    "# # token_indices.item()\n",
    "# # indices =  torch.LongTensor([[2],[0,2]])\n",
    "\n",
    "# # torch.gather(input_ids, 1, token_indices.unsqueeze(0))\n",
    "# # p_index = token_indices.view(input_ids.size(0), -1)[:,1::2]   \n",
    "# # attention_mask = torch.ones(input_ids.shape, dtype=torch.long) \n",
    "# # attention_mask[:,token_indices] = 2\n",
    "# # attention_mask\n",
    "# p_index = torch.tensor([1, 3, 4])\n",
    "# s_index = torch.tensor([1,3,6])\n",
    "# torch.sort(torch.cat((s_index, p_index)))[0]\n",
    "# attention_mask.view(-1)[ p_index.view(-1), :].view(attention_mask.size(0), -1)\n",
    "# # for pi in p_index[0]:\n",
    "# #     attention_mask[:, pi] = 2\n",
    "# # attention_mask\n",
    "# # s_index = torch.tensor([[1,3]])\n",
    "# # torch.sort(torch.cat((p_index, s_index), -1), -1)\n",
    "\n",
    "# sequence_output  = torch.tensor([[[-1, 5, -1, 2],\n",
    "#                                  [-2, 27, 2, 9],\n",
    "#                                  [3, 6, 1, 65],\n",
    "#                                  [52, 36, 13, 2],\n",
    "#                                  [73, 26, 1, 7]\n",
    "#                                 ]])\n",
    "\n",
    "# sp_para_output_t   = torch.tensor([[[-1],\n",
    "#                                  [-2 ],\n",
    "#                                  [3],\n",
    "#                                  [52],\n",
    "#                                  [73]\n",
    "#                                 ]])\n",
    "# torch.zeros(sp_para_output_t.shape, dtype=torch.float) \n",
    "\n",
    "# print(\"size of sequence_output: \" + str(sequence_output.size()))\n",
    "# # print(\"size of p_index.unsqueeze(0).unsqueeze(-1): \" + str(p_index.unsqueeze(0).size()))\n",
    "# sequence_output[:,p_index,:]\n",
    "# b = torch.tensor([0, 1, 2, 3])\n",
    "# p_index.unsqueeze(-1) * b\n",
    "\n",
    "# input_ids = torch.tensor([[0.2, 0.0, 0.6, 0.6], [0.2, 0.6, 0.0, 0.0]]) \n",
    "# # input_ids.tolist()\n",
    "# p_index =  torch.nonzero(input_ids == torch.tensor(0.2))\n",
    "# print(p_index)\n",
    "# s_index =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# print(s_index)\n",
    "\n",
    "# sp_sent = torch.tensor([[0, 1, 1, 0]])\n",
    "# torch.nonzero(sp_sent, as_tuple=True)[1]\n",
    "# cat_index = torch.tensor([])\n",
    "# cat_index = torch.cat((cat_index, ids[0][1]))\n",
    "# print(ids)\n",
    "# print(cat_index)\n",
    "# p_index[p_index[:,0] == 0]\n",
    "\n",
    "# cat_index[cat_index[:,0].argsort()]\n",
    "\n",
    "# sorted(torch.cat((p_index, s_index)), key = lambda x: x[0])\n",
    "# torch.sort(torch.cat((p_index, s_index)), 0)[0]\n",
    "# for cor in token_indices:\n",
    "#     attention_mask[cor[0].item()][cor[1].item()] = 2\n",
    "# attention_mask \n",
    "# input_ids = torch.tensor([[-1, 5, -6, 2]])\n",
    "# print(input_ids.size())\n",
    "# input_ids.topk(k=2, dim=-1).indices\n",
    "\n",
    "# predict_type = torch.tensor([[-0.0925, -0.0999, -0.1671]])\n",
    "# p_type = torch.argmax(predict_type, dim=1).item()\n",
    "# p_type_score = torch.max(predict_type, dim=1)[0].item()\n",
    "# print(\"predict_type: \", predict_type)\n",
    "# print(\"p_type: \", p_type)\n",
    "# print(\"p_type_score: \", p_type_score)\n",
    "    \n",
    "# a = torch.tensor([[0.9213,  1.0887, -0.8858, -1.7683]])\n",
    "# a.view(-1).size() \n",
    "# print(torch.sigmoid(a))\n",
    "# a = torch.tensor([ 9.213,  1.0887, -0.8858, 7683])\n",
    "# print(torch.sigmoid(a))\n",
    "\n",
    "# a = torch.tensor([[[1],[2],[4],[-1],[-1]]])\n",
    "# a= a.squeeze(-1)\n",
    "# a.size() \n",
    "# a[:, torch.where(a!=-1)[1]]\n",
    "# m = torch.nn.Sigmoid()\n",
    "# print(\"m: \", m)\n",
    "# loss = torch.nn.BCELoss()\n",
    "# # input = torch.randn(3, requires_grad=True)\n",
    "# # print(\"input: \", input)\n",
    "# # target = torch.empty(3).random_(2)\n",
    "# # print(\"target: \", target)\n",
    "# # output = loss(m(input), target)\n",
    "# # print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([1.0293, -0.1585,  1.1408], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# print(\"Sigmoid(input): \", m(input))\n",
    "# target = torch.tensor([0., 1., 0.])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# input = torch.tensor([[1.0293, -0.1585,  1.1408]], requires_grad=True)\n",
    "# print(\"input: \", input)\n",
    "# target = torch.tensor([[0., 1., 0.]])\n",
    "# print(\"target: \", target)\n",
    "# output = loss(m(input), target)\n",
    "# print(\"output: \", output)\n",
    "\n",
    "# 1.1761 * 3\n",
    "# soft_input = torch.nn.Softmax(dim=-1)\n",
    "# log_soft_input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss() \n",
    "# loss(log_soft_input, target)\n",
    "# input = torch.log(soft_input(input))\n",
    "# loss=torch.nn.NLLLoss()\n",
    "# loss(input,target)\n",
    "\n",
    "# loss =torch.nn.CrossEntropyLoss()\n",
    "# loss(input,target) \n",
    "\n",
    "# sp_sent_logits =torch.tensor([[[0.0988],\n",
    "#          [0.0319],\n",
    "#          [0.0314]]])\n",
    "# sp_sent_logits.squeeze()\n",
    "\n",
    "# input_ids = torch.tensor([[0.6, 0.0, 0.6, 0.0]]) \n",
    "# token_indices =  torch.nonzero(input_ids == torch.tensor(0.6))\n",
    "# token_indices[:,1][0].item()\n",
    "\n",
    "# def or_softmax_cross_entropy_loss_one_doc(logits, target, ignore_index=-1, dim=-1):\n",
    "#     \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "#     assert logits.ndim == 2\n",
    "#     assert target.ndim == 2\n",
    "#     assert logits.size(0) == target.size(0) \n",
    "\n",
    "#     # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target, considing only one correct target \n",
    "#     # here, the numerator is the sum of a few potential targets, where some of them is the correct answer, considing more correct targets\n",
    "\n",
    "#     # target are indexes of tokens, padded with ignore_index=-1\n",
    "#     # logits are scores (one for each label) for each token\n",
    "# #         print(\"or_softmax_cross_entropy_loss_one_doc\" ) \n",
    "# #         print(\"size of logits: \" + str(logits.size()))                    # torch.Size([1, 746]), 746 is number of all tokens \n",
    "# #         print(\"size of target: \" + str(target.size()))                    # torch.Size([1, 64]),  -1 padded\n",
    "#     print(\"target: \" + str(target)) \n",
    "\n",
    "#     # compute a target mask\n",
    "#     target_mask = target == ignore_index\n",
    "#     # replaces ignore_index with 0, so `gather` will select logit at index 0 for the masked targets\n",
    "#     masked_target = target * (1 - target_mask.long())                 # replace all -1 in target with 0， tensor([[447,   0,   0,   0, ...]])\n",
    "#     print(\"masked_target: \" + str(masked_target))     \n",
    "#     # gather logits\n",
    "#     gathered_logits = logits.gather(dim=dim, index=masked_target)     # tensor([[0.4382, 0.2340, 0.2340, 0.2340 ... ]]), padding logits are all replaced by logits[0] \n",
    "# #         print(\"size of gathered_logits: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "#     print(\"gathered_logits: \" + str(gathered_logits)) \n",
    "#     # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "#     gathered_logits[target_mask] = float('-inf')                      # padding logits are all replaced by -inf\n",
    "#     print(\"gathered_logits after -inf: \" + str(gathered_logits))      # tensor([[0.4382,   -inf,   -inf,   -inf,   -inf,...]])\n",
    "\n",
    "#     # each batch is one example\n",
    "#     gathered_logits = gathered_logits.view(1, -1)\n",
    "#     logits = logits.view(1, -1)\n",
    "# #         print(\"size of gathered_logits after view: \" + str(gathered_logits.size()))  # torch.Size([1, 64])\n",
    "# #         print(\"size of logits after view: \" + str(logits.size()))                    # torch.Size([1, 746])　　\n",
    "\n",
    "#     # numerator = log(sum(exp(gathered logits)))\n",
    "#     log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_score: \" + str(log_score)) \n",
    "#     # denominator = log(sum(exp(logits)))\n",
    "#     log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "#     print(\"log_norm: \" + str(log_norm)) \n",
    "\n",
    "#     # compute the loss\n",
    "#     loss = -(log_score - log_norm)\n",
    "#     print(\"loss: \" + str(loss))\n",
    "\n",
    "\n",
    "#     # some of the examples might have a loss of `inf` when `target` is all `ignore_index`: when computing start_loss and end_loss for question with the gold answer of yes/no \n",
    "#     # replace -inf with 0\n",
    "#     loss = loss[~torch.isinf(loss)].sum()\n",
    "#     print(\"final loss: \" + str(loss)) \n",
    "#     return loss \n",
    "\n",
    "# # input = torch.tensor([[ 0,  0.0780],\n",
    "# #         [0, 0.9253 ],\n",
    "# #         [0, 0.0987]])\n",
    "# # target = torch.tensor([0,1,0])\n",
    "# # target.size(0) < 1\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312],\n",
    "# #         [-0.3499, -1.9253, -1.5725],\n",
    "# #         [-0.6578, -0.0987,  1.1570]])\n",
    "# # target=torch.tensor([0,1,2])\n",
    "# # predict_support_para.view(-1, 2), sp_para.view(-1)\n",
    "# # input = torch.tensor([[ 1.1879,  1.0780,  0.5312]])\n",
    "# # target=torch.tensor([0])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([2])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# # target=torch.tensor([-1])\n",
    "# # or_softmax_cross_entropy_loss_one_doc(input, target.unsqueeze(-1))\n",
    "# a = torch.tensor([6.4062])    \n",
    "# b = torch.tensor([2.23])\n",
    "# torch.cat((a,b))\n",
    " \n",
    "# for a in list_tensor\n",
    "# from functools import reduce\n",
    "# reduce(lambda x,y: torch.cat((x,y)), list_tensor[:-1])\n",
    "\n",
    "# torch.tanh(a)\n",
    "# # if(torch.isinf(a)):\n",
    "# #     print(\"is inf\")\n",
    "# 5 * 1e-2\n",
    "\n",
    "\n",
    "# import torch\n",
    "# special_tokens = [1,2]\n",
    "# input_ids = torch.tensor([[ 1, 0, 2, 1, 0, 2]])\n",
    "\n",
    "# mask = input_ids != input_ids # initilaize \n",
    "# for special_token in special_tokens:\n",
    "#     mask = torch.logical_or(mask, input_ids.eq(special_token)) \n",
    "#     print(\"mask: \", mask)\n",
    "# torch.nonzero(mask)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug: check loaded dataset by DataLoader\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# num_new_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"</q>\"]})\n",
    "# # # # print(tokenizer.all_special_tokens)    \n",
    "# # # # print(tokenizer.all_special_ids)     \n",
    "# # # # tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "# # # # tokenizer.sep_token\n",
    "# print(tokenizer.tokenize(\"yes\"))\n",
    "# print(tokenizer.tokenize(\"no\"))\n",
    "# print(tokenizer.tokenize(\"null\"))\n",
    "# # # all_doc_tokens = []\n",
    "# # # orig_to_tok_index = []\n",
    "# # # tok_to_orig_index = []\n",
    "# # # for (i, token) in enumerate([\"<s>\", \"da\", \"tell\", \"<p>\", \"say\"]):\n",
    "# # #     orig_to_tok_index.append(len(all_doc_tokens))\n",
    "# # #     sub_tokens = tokenizer.tokenize(f'. {token}')[1:] if i > 0 else tokenizer.tokenize(token)\n",
    "# # #     for sub_token in sub_tokens:\n",
    "# # #         tok_to_orig_index.append(i)\n",
    "# # #         all_doc_tokens.append(sub_token)\n",
    "# # # all_doc_tokens\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=tokenizer,\n",
    "#                           max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "#                           doc_stride= args.doc_stride,\n",
    "#                           max_num_answers= args.max_num_answers,\n",
    "#                           max_question_len= args.max_question_len,\n",
    "#                           ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "# print(len(dataset))\n",
    "\n",
    "# # # dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "# # #                     num_workers=args.num_workers, sampler=None,\n",
    "# # #                     collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "# example = dataset[3]  \n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qids] = example\n",
    " \n",
    "\n",
    "# print(input_ids[0][:20].tolist())\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends)\n",
    "# print(q_type)\n",
    "# print(sp_sent) \n",
    "# print(sp_para) \n",
    "# print(qids)\n",
    "# print(tokenizer.convert_ids_to_tokens(input_ids[0][667:669+1].tolist()))\n",
    "# 0.0033 * 90447 \n",
    "# 28*4\n",
    "# torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "    return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch \n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch \n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch \n",
    "    # print(\"size of input_ids: \" + str(input_ids.size())) \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    answer_loss, type_loss = output[:2]\n",
    "    # answer_loss, type_loss = output[:4]\n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss  = output[:4]\n",
    "    # print(\"answer_loss: \", answer_loss)\n",
    "    # print(\"type_loss: \", type_loss)\n",
    "    # print(\"sp_para_loss: \", sp_para_loss)\n",
    "    # print(\"sp_sent_loss: \", sp_sent_loss)\n",
    "\n",
    "#     loss  = answer_loss +  type_loss + sp_para_loss + sp_sent_loss\n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "#     print(\"weighted loss: \", loss)\n",
    "#     print(\"self.trainer.optimizers[0].param_groups[0]['lr']: \", self.trainer.optimizers[0].param_groups[0]['lr'])\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']  # loss.new_zeros(1) is tensor([0.]), converting 'lr' to tensor' by adding it.  \n",
    "\n",
    "    tensorboard_logs = {'loss': loss, 'train_answer_loss': answer_loss, 'train_type_loss': type_loss, \n",
    "                        # 'train_sp_para_loss': sp_para_loss, 'train_sp_sent_loss': sp_sent_loss, \n",
    "                        'lr': lr #,\n",
    "                        # 'mem': torch.tensor(torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3).type_as(loss) \n",
    "    }\n",
    "    return tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%add_to hotpotqa\n",
    "    # # the function is called for each batch after every epoch is completed\n",
    "    # def training_end(self, output): \n",
    "    #     # print(\"training_end at epoch: \", self.current_epoch)\n",
    "    # #     print(\"len(outputs): \",len(outputs))\n",
    "    # #     print(\"output: \",output)\n",
    "    \n",
    "    #     # one batch only has one example\n",
    "    #     avg_loss = output['loss']    \n",
    "    #     avg_answer_loss = output['train_answer_loss']  \n",
    "    #     avg_type_loss = output['train_type_loss']    \n",
    "    #     avg_sp_para_loss = output['train_sp_para_loss']   \n",
    "    #     avg_sp_sent_loss = output['train_sp_sent_loss'] \n",
    "    #     avg_lr = output['lr']      \n",
    "         \n",
    "     \n",
    "    #     if self.trainer.use_ddp:\n",
    "    #         torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_answer_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_type_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_para_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_sp_sent_loss /= self.trainer.world_size \n",
    "    #         torch.distributed.all_reduce(avg_lr, op=torch.distributed.ReduceOp.SUM)\n",
    "    #         avg_lr /= self.trainer.world_size \n",
    "            \n",
    "     \n",
    "    #     tensorboard_logs = { #'avg_train_loss': avg_loss, \n",
    "    #             'avg_train_answer_loss': avg_answer_loss, 'avg_train_type_loss': avg_type_loss, 'avg_train_sp_para_loss': avg_sp_para_loss, 'avg_train_sp_sent_loss': avg_sp_sent_loss, 'lr': avg_lr\n",
    "    #           }\n",
    "    \n",
    "    #     return {'loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    print(\"validation_step\")\n",
    "    print(\"batch_nb: \", batch_nb)\n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch\n",
    "    print(\"qid: \", qid)\n",
    "    print(\"q_type: \", q_type)\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    answer_loss, type_loss, start_logits, end_logits, type_logits = output \n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    if(q_type.item() != -1 ):\n",
    "    # answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "        answers_pred  = self.decode(input_ids, start_logits, end_logits, type_logits)\n",
    "    else:\n",
    "        answers_pred  = [{'text': '', 'score': -1000000, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': 1}]\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "#         print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "#         print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = _normalize_text(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "#         print(\"f1: \" + str(f1))\n",
    "#         print(\"prec: \" + str(prec))\n",
    "#         print(\"recall: \" + str(recall))\n",
    "#         print(\"em: \" + str(em))  \n",
    "\n",
    "#         if(len(sp_sent_pred) > 0):\n",
    "#             sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = self.sp_metrics(sp_sent_pred, torch.where(sp_sent.squeeze())[0].tolist())\n",
    "#             sp_sent_em = torch.tensor(sp_sent_em).type_as(loss)\n",
    "#             sp_sent_precision = torch.tensor(sp_sent_precision).type_as(loss)\n",
    "#             sp_sent_recall = torch.tensor(sp_sent_recall).type_as(loss)\n",
    "#             sp_sent_f1 = torch.tensor(sp_sent_f1).type_as(loss)\n",
    "\n",
    "#   #         print(\"sp_sent_em: \" + str(sp_sent_em))\n",
    "#   #         print(\"sp_sent_precision: \" + str(sp_sent_precision))\n",
    "#   #         print(\"sp_sent_recall: \" + str(sp_sent_recall))    \n",
    "#   #         print(\"sp_sent_f1: \" + str(sp_sent_f1))    \n",
    "\n",
    "#             joint_prec = prec * sp_sent_precision\n",
    "#             joint_recall = recall * sp_sent_recall\n",
    "#             if joint_prec + joint_recall > 0:\n",
    "#                 joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "#             else:\n",
    "#                 joint_f1 = torch.tensor(0.0).type_as(loss)\n",
    "#             joint_em = em * sp_sent_em \n",
    "\n",
    "#         else:\n",
    "#             sp_sent_em, sp_sent_precision, sp_sent_recall, sp_sent_f1 = torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "#             joint_em, joint_f1, joint_prec, joint_recall =  torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss), torch.tensor(0.0).type_as(loss)\n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "            # 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss,\n",
    "               'answer_score': pre_answer_score, 'f1': f1, 'prec':prec, 'recall':recall, 'em': em #,\n",
    "            #   'sp_em': sp_sent_em, 'sp_f1': sp_sent_f1, 'sp_prec': sp_sent_precision, 'sp_recall': sp_sent_recall,\n",
    "            #   'joint_em': joint_em, 'joint_f1': joint_f1, 'joint_prec': joint_prec, 'joint_recall': joint_recall\n",
    "\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def decode(self, input_ids, start_logits, end_logits, type_logits): #, sp_para_logits, sp_sent_logits):\n",
    "    # print(\"decode\")\n",
    "\n",
    "    question_end_index = self._get_special_index(input_ids, [QUESTION_END])\n",
    "#     print(\"question_end_index: \", question_end_index)\n",
    "\n",
    "    # one example per batch\n",
    "    start_logits = start_logits.squeeze()\n",
    "    end_logits = end_logits.squeeze()\n",
    "    print(\"start_logits: \", start_logits)\n",
    "    print(\"end_logits: \", end_logits)\n",
    "    start_logits_indices = start_logits.topk(k=min(self.args.n_best_size, start_logits.size(0)), dim=-1).indices \n",
    "    end_logits_indices = end_logits.topk(k=min(self.args.n_best_size, end_logits.size(0)), dim=-1).indices \n",
    "    if(len(start_logits_indices.size()) > 1):\n",
    "        print(\"len(start_logits_indices.size()): \", len(start_logits_indices.size()))\n",
    "        assert(\"len(start_logits_indices.size()) > 1\")\n",
    "    p_type = torch.argmax(type_logits, dim=1).item()\n",
    "    p_type_score = torch.max(type_logits, dim=1)[0] \n",
    "#     print(\"type_logits: \", type_logits)\n",
    "#         print(\"p_type: \", p_type)\n",
    "#         print(\"p_type_score: \", p_type_score)\n",
    "\n",
    "    answers = []\n",
    "    if p_type == 0:\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices: \n",
    "            for end_logit_index in end_logits_indices: \n",
    "                if start_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index.item():\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[start_logit_index],  # single logit score for start position at start_logit_index\n",
    "                                          'end_logit': end_logits[end_logit_index]})    \n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True) \n",
    "#             print(\"sorted_answers: \" + str(sorted_answers))\n",
    "\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[0, answer['start']: answer['end'] + 1]\n",
    "\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "\n",
    "            # remove [/sent], <t> and </t>\n",
    "\n",
    "            for special_token in [SENT_MARKER_END, TITLE_START, TITLE_END, self.tokenizer.sep_token]:\n",
    "                try:\n",
    "                    if(answer_tokens[0] == special_token):\n",
    "                        answer['start_logit'] = -2000000\n",
    "                    elif(answer_tokens[-1] == special_token):\n",
    "                        answer['end_logit'] = -2000000\n",
    "\n",
    "                    answer_tokens.remove(special_token)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens) \n",
    "            score = (answer['start_logit'] + answer['end_logit'] + p_type_score) / 3\n",
    "            answers.append({'text': text, 'score': score, 'start_logit': answer['start_logit'], 'end_logit': answer['end_logit'], 'p_type_score': p_type_score})\n",
    "\n",
    "    elif p_type == 1: \n",
    "        answers.append({'text': 'yes', 'score': p_type_score, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score})\n",
    "    elif p_type == 2:\n",
    "        answers.append({'text': 'no', 'score': p_type_score, 'start_logit': -1000000, 'end_logit': -1000000, 'p_type_score': p_type_score}) \n",
    "    else:\n",
    "        assert False \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "\n",
    "\n",
    "def f1_score(self, prediction, ground_truth):\n",
    "    normalized_prediction = _normalize_text(prediction)\n",
    "    normalized_ground_truth = _normalize_text(ground_truth)\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(self, prediction, ground_truth):\n",
    "    return int(_normalize_text(prediction) == _normalize_text(ground_truth))\n",
    "\n",
    "\n",
    "# def sp_metrics(self, prediction, gold): \n",
    "#     tp, fp, fn = 0, 0, 0\n",
    "#     for e in prediction:\n",
    "#         if e in gold:\n",
    "#             tp += 1\n",
    "#         else:\n",
    "#             fp += 1 \n",
    "#     for e in gold:\n",
    "#         if e not in prediction:\n",
    "#             fn += 1 \n",
    "#     prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "#     recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "#     f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "#     em = 1.0 if fp + fn == 0 else 0.0 \n",
    "#     return em, prec, recall, f1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If a validation_step is not defined, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_epoch_end(self, outputs):\n",
    "    print(\"validation_epoch_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs] \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "    # sp_sent_f1_scores = [x['sp_f1'] for x in outputs]   \n",
    "    # sp_sent_em_scores = [x['sp_em'] for x in outputs]   \n",
    "    # sp_sent_prec_scores = [x['sp_prec'] for x in outputs]   \n",
    "    # sp_sent_recall_scores = [x['sp_recall'] for x in outputs]   \n",
    "    # joint_f1_scores = [x['joint_f1'] for x in outputs]  \n",
    "    # joint_em_scores = [x['joint_em'] for x in outputs]  \n",
    "    # joint_prec_scores = [x['joint_prec'] for x in outputs]  \n",
    "    # joint_recall_scores = [x['joint_recall'] for x in outputs]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_para_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_sent_loss /= self.trainer.world_size \n",
    "\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        # sp_sent_f1_scores = self.sync_list_across_gpus(sp_sent_f1_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_em_scores = self.sync_list_across_gpus(sp_sent_em_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_prec_scores = self.sync_list_across_gpus(sp_sent_prec_scores, avg_loss.device, torch.float)\n",
    "        # sp_sent_recall_scores = self.sync_list_across_gpus(sp_sent_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "        # joint_f1_scores = self.sync_list_across_gpus(joint_f1_scores, avg_loss.device, torch.float)\n",
    "        # joint_em_scores = self.sync_list_across_gpus(joint_em_scores, avg_loss.device, torch.float)\n",
    "        # joint_prec_scores = self.sync_list_across_gpus(joint_prec_scores, avg_loss.device, torch.float)\n",
    "        # joint_recall_scores = self.sync_list_across_gpus(joint_recall_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_val_em = sum(em_scores) / len(em_scores)\n",
    "    avg_val_prec = sum(prec_scores) / len(prec_scores)\n",
    "    avg_val_recall = sum(recall_scores) / len(recall_scores)\n",
    "    # avg_val_sp_sent_f1 = torch.tensor(sum(sp_sent_f1_scores) / len(sp_sent_f1_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_sp_sent_em = torch.tensor(sum(sp_sent_em_scores) / len(sp_sent_em_scores) ).type_as(avg_loss)    \n",
    "    # avg_val_sp_sent_prec = torch.tensor(sum(sp_sent_prec_scores) / len(sp_sent_prec_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_sp_sent_recall = torch.tensor(sum(sp_sent_recall_scores) / len(sp_sent_recall_scores) ).type_as(avg_loss)    \n",
    "    # avg_val_joint_f1 = torch.tensor(sum(joint_f1_scores) / len(joint_f1_scores) ).type_as(avg_loss)  \n",
    "    # avg_val_joint_em = torch.tensor(sum(joint_em_scores) / len(joint_em_scores) ).type_as(avg_loss)  \n",
    "    # avg_val_joint_prec = torch.tensor(sum(joint_prec_scores) / len(joint_prec_scores) ).type_as(avg_loss)   \n",
    "    # avg_val_joint_recall = torch.tensor(sum(joint_recall_scores) / len(joint_recall_scores) ).type_as(avg_loss) \n",
    "\n",
    "    print(\"avg_loss: \", avg_loss, end = '\\t')   \n",
    "    print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t')   \n",
    "    # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss)   \n",
    "    print(\"avg_val_f1: \", avg_val_f1, end = '\\t')   \n",
    "    print(\"avg_val_em: \", avg_val_em, end = '\\t')   \n",
    "    print(\"avg_val_prec: \", avg_val_prec, end = '\\t')   \n",
    "    print(\"avg_val_recall: \", avg_val_recall)   \n",
    "    # print(\"avg_val_sp_sent_f1: \", avg_val_sp_sent_f1, end = '\\t')   \n",
    "    # print(\"avg_val_sp_sent_em: \" , avg_val_sp_sent_em, end = '\\t')  \n",
    "    # print(\"avg_val_sp_sent_prec: \", avg_val_sp_sent_prec, end = '\\t')   \n",
    "    # print(\"avg_val_sp_sent_recall: \", avg_val_sp_sent_recall)   \n",
    "    # print(\"avg_val_joint_f1: \" , avg_val_joint_f1, end = '\\t')  \n",
    "    # print(\"avg_val_joint_em: \", avg_val_joint_em, end = '\\t')   \n",
    "    # print(\"avg_val_joint_prec: \", avg_val_joint_prec, end = '\\t')   \n",
    "    # print(\"avg_val_joint_recall: \", avg_val_joint_recall)   \n",
    "\n",
    "\n",
    "    logs = {'avg_val_loss': avg_loss, 'avg_val_answer_loss': avg_answer_loss, 'avg_val_type_loss': avg_type_loss, \n",
    "        # 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss,   \n",
    "        'avg_val_f1': avg_val_f1 , 'avg_val_em': avg_val_em,  'avg_val_prec': avg_val_prec, 'avg_val_recall': avg_val_recall #,    \n",
    "        # 'avg_val_sp_sent_f1': avg_val_sp_sent_f1, 'avg_val_sp_sent_em': avg_val_sp_sent_em,  'avg_val_sp_sent_prec': avg_val_sp_sent_prec, 'avg_val_sp_sent_recall': avg_val_sp_sent_recall,    \n",
    "        # 'avg_val_joint_f1': avg_val_joint_f1, 'avg_val_joint_em': avg_val_joint_em,  'avg_val_joint_prec': avg_val_joint_prec, 'avg_val_joint_recall': avg_val_joint_recall \n",
    "    }   \n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    # input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, sp_sent, sp_para, qid, answer = batch\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type, qid, answer = batch\n",
    "\n",
    "    print(\"test_step of qid: \", qid, end=\"\\t\") \n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, q_type) #, sp_sent, sp_para)\n",
    "    # answer_loss, type_loss, sp_para_loss, sp_sent_loss, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output = output \n",
    "    answer_loss, type_loss, start_logits, end_logits, type_logits = output \n",
    "    loss = answer_loss + 5*type_loss #+ 10*sp_para_loss + 10*sp_sent_loss\n",
    "\n",
    "    # answers_pred, sp_sent_pred, sp_para_pred = self.decode(input_ids, start_logits, end_logits, type_logits, sp_para_output, sp_sent_output)\n",
    "    answers_pred = self.decode(input_ids, start_logits, end_logits, type_logits)\n",
    "\n",
    "    if(len(answers_pred) != 1):\n",
    "        print(\"len(answers_pred) != 1\")\n",
    "        assert(len(answers_pred) == 1)\n",
    "\n",
    "    pre_answer_score = answers_pred[0]['score']  # (start_logit + end_logit + p_type_score) / 3\n",
    "    start_logit = answers_pred[0]['start_logit']\n",
    "    end_logit = answers_pred[0]['end_logit']\n",
    "    type_score = answers_pred[0]['p_type_score']\n",
    "    pre_answer = _normalize_text(answers_pred[0]['text'])\n",
    "    # print(\"pred answer_score: \" + str(pre_answer_score))\n",
    "    # print(\"pred answer_text: \" + str(pre_answer)) \n",
    "\n",
    "    gold_answer = _normalize_text(answer)\n",
    "    f1, prec, recall = self.f1_score(pre_answer, gold_answer)\n",
    "    em = self.exact_match_score(pre_answer, gold_answer) \n",
    "    f1 = torch.tensor(f1).type_as(loss)\n",
    "    prec = torch.tensor(prec).type_as(loss)\n",
    "    recall = torch.tensor(recall).type_as(loss)\n",
    "    em = torch.tensor(em).type_as(loss)\n",
    "\n",
    "    print(\"pre_answer:\\t\", pre_answer, \"\\tgold_answer:\\t\", gold_answer) #, \"\\tstart_logits:\\t\", start_logits.cpu(), \"\\tend_logits:\\t\", end_logits.cpu(), \"\\ttype_logits:\\t\", type_logits.cpu())\n",
    "\n",
    "    self.logger.log_metrics({'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "                                'answer_score': pre_answer_score, 'start_logit': start_logit, 'end_logit': end_logit,  \n",
    "                                'type_score': type_score,\n",
    "                                'f1': f1, 'prec':prec, 'recall':recall, 'em': em \n",
    "                            }) \n",
    "\n",
    "\n",
    "    return { 'vloss': loss, 'answer_loss': answer_loss, 'type_loss': type_loss, \n",
    "             'answer_score': pre_answer_score, 'start_logit': start_logit, 'end_logit': end_logit, 'type_score': type_score,\n",
    "             'f1': f1, 'prec':prec, 'recall':recall, 'em': em\n",
    "            # 'sp_para_loss': sp_para_loss, 'sp_sent_loss': sp_sent_loss, \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_epoch_end(self, outputs):\n",
    "    print(\"test_epoch_end\")\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()  \n",
    "    avg_answer_loss = torch.stack([x['answer_loss'] for x in outputs]).mean()  \n",
    "    avg_type_loss = torch.stack([x['type_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_para_loss = torch.stack([x['sp_para_loss'] for x in outputs]).mean()  \n",
    "    # avg_sp_sent_loss = torch.stack([x['sp_sent_loss'] for x in outputs]).mean()  \n",
    "\n",
    "    answer_scores = [x['answer_score'] for x in outputs]  # [item for sublist in outputs for item in sublist['answer_score']] #torch.stack([x['answer_score'] for x in outputs]).mean() # \n",
    "    f1_scores = [x['f1'] for x in outputs]  \n",
    "    em_scores = [x['em'] for x in outputs]  \n",
    "    prec_scores =  [x['prec'] for x in outputs] \n",
    "    recall_scores = [x['recall'] for x in outputs]  \n",
    "\n",
    "    print(f'before sync --> sizes:  {len(answer_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_answer_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_answer_loss /= self.trainer.world_size \n",
    "        torch.distributed.all_reduce(avg_type_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_type_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_para_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_para_loss /= self.trainer.world_size \n",
    "        # torch.distributed.all_reduce(avg_sp_sent_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        # avg_sp_sent_loss /= self.trainer.world_size \n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.float)\n",
    "        prec_scores = self.sync_list_across_gpus(prec_scores, avg_loss.device, torch.float)\n",
    "        recall_scores = self.sync_list_across_gpus(recall_scores, avg_loss.device, torch.float)\n",
    "#         int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "\n",
    "\n",
    "    print(f'after sync --> sizes: {len(answer_scores)}')\n",
    "    # print(\"answer_scores: \", answer_scores)\n",
    "    avg_test_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_test_em = sum(em_scores) / len(em_scores)\n",
    "    avg_test_prec = sum(prec_scores) / len(prec_scores)\n",
    "    avg_test_recall = sum(recall_scores) / len(recall_scores)     \n",
    "    # print(\"avg_loss: \", avg_loss, end = '\\t') \n",
    "    # print(\"avg_answer_loss: \", avg_answer_loss, end = '\\t') \n",
    "    # print(\"avg_type_loss: \", avg_type_loss, end = '\\t') \n",
    "    # print(\"avg_sp_para_loss: \", avg_sp_para_loss, end = '\\t') \n",
    "    # print(\"avg_sp_sent_loss: \", avg_sp_sent_loss, end = '\\t')  \n",
    "\n",
    "    logs = {'avg_test_loss': avg_loss, 'avg_test_answer_loss': avg_answer_loss, 'avg_test_type_loss': avg_type_loss, \n",
    "            'avg_test_f1': avg_test_f1 , 'avg_test_em': avg_test_em,  'avg_test_prec': avg_test_prec, 'avg_test_recall': avg_test_recall #,    \n",
    "            # 'avg_val_sp_para_loss': avg_sp_para_loss, 'avg_val_sp_sent_loss': avg_sp_sent_loss\n",
    "           }\n",
    "\n",
    "    return {'avg_test_loss': avg_loss, 'log': logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='jupyter-hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1000, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"How often within one training epoch to check the validation set.\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "    parser.add_argument('--train_percent', type=float, default=1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHECKPOINT_HYPER_PARAMS_KEY',\n",
       " 'CHECKPOINT_HYPER_PARAMS_NAME',\n",
       " 'CHECKPOINT_HYPER_PARAMS_TYPE',\n",
       " 'T_destination',\n",
       " '_LightningModule__get_hparams_assignment_variable',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_apply',\n",
       " '_auto_collect_arguments',\n",
       " '_call_impl',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_get_special_index',\n",
       " '_init_slurm_connection',\n",
       " '_load_from_state_dict',\n",
       " '_load_model_state',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_hparams',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_model_specific_args',\n",
       " 'add_module',\n",
       " 'amp_scale_loss',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decode',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'exact_match_score',\n",
       " 'example_input_array',\n",
       " 'extra_repr',\n",
       " 'f1_score',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'get_progress_bar_dict',\n",
       " 'get_tqdm_dict',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'hparams',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'loss_computation',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_fit_end',\n",
       " 'on_fit_start',\n",
       " 'on_gpu',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'optimizer_zero_grad',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'prepare_data',\n",
       " 'print',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'save_hyperparameters',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'sync_list_across_gpus',\n",
       " 'tbptt_split_batch',\n",
       " 'teardown',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_epoch_end',\n",
       " 'test_step',\n",
       " 'test_step_end',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_epoch_end',\n",
       " 'training_step',\n",
       " 'training_step_end',\n",
       " 'transfer_batch_to_device',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_epoch_end',\n",
       " 'validation_step',\n",
       " 'validation_step_end',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHECKPOINT_HYPER_PARAMS_KEY', 'hyper_parameters'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_NAME', 'hparams_name'),\n",
       " ('CHECKPOINT_HYPER_PARAMS_TYPE', 'hparams_type'),\n",
       " ('T_destination', ~T_destination),\n",
       " ('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__abstractmethods__', frozenset()),\n",
       " ('__annotations__',\n",
       "  {'_device': Ellipsis, '_dtype': typing.Union[str, torch.dtype]}),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'train_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'val_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'test_dataloader': <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>,\n",
       "                'loss_computation': <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>,\n",
       "                '_get_special_index': <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset(),\n",
       "                '_abc_registry': <_weakrefset.WeakSet at 0x7f20fd98b438>,\n",
       "                '_abc_cache': <_weakrefset.WeakSet at 0x7f20fd98b470>,\n",
       "                '_abc_negative_cache': <_weakrefset.WeakSet at 0x7f20fd98b4e0>,\n",
       "                '_abc_negative_cache_version': 72,\n",
       "                'configure_ddp': <function __main__.configure_ddp(self, model, device_ids)>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>,\n",
       "                'validation_step': <function __main__.validation_step(self, batch, batch_nb)>,\n",
       "                'decode': <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>,\n",
       "                'f1_score': <function __main__.f1_score(self, prediction, ground_truth)>,\n",
       "                'exact_match_score': <function __main__.exact_match_score(self, prediction, ground_truth)>,\n",
       "                'validation_epoch_end': <function __main__.validation_epoch_end(self, outputs)>,\n",
       "                'sync_list_across_gpus': <function __main__.sync_list_across_gpus(self, l, device, dtype)>,\n",
       "                'test_step': <function __main__.test_step(self, batch, batch_nb)>,\n",
       "                'test_epoch_end': <function __main__.test_epoch_end(self, outputs)>,\n",
       "                'add_model_specific_args': <staticmethod at 0x7f20fd94be10>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'ABC' objects>),\n",
       " ('_abc_cache', <_weakrefset.WeakSet at 0x7f20fd98b470>),\n",
       " ('_abc_negative_cache', <_weakrefset.WeakSet at 0x7f20fd98b4e0>),\n",
       " ('_abc_negative_cache_version', 72),\n",
       " ('_abc_registry', <_weakrefset.WeakSet at 0x7f20fd98b438>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_auto_collect_arguments',\n",
       "  <bound method LightningModule._auto_collect_arguments of <class '__main__.hotpotqa'>>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_load_model_state',\n",
       "  <bound method ModelIO._load_model_state of <class '__main__.hotpotqa'>>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('device', <property at 0x7f20ffc06048>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('dtype', <property at 0x7f20ffc01f48>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('example_input_array', <property at 0x7f20ffcea818>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('hparams', <property at 0x7f20ffc0cd68>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method ModelIO.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method ModelIO.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_gpu', <property at 0x7f20ffcea8b8>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int, second_order_closure:Union[Callable, NoneType]=None, on_tpu:bool=False, using_native_amp:bool=False, using_lbfgs:bool=False) -> None>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end', <function __main__.test_epoch_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_LightningModule__get_hparams_assignment_variable',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.__get_hparams_assignment_variable(self)>),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_call_impl',\n",
       "  <function torch.nn.modules.module.Module._call_impl(self, *input, **kwargs)>),\n",
       " ('_forward_unimplemented',\n",
       "  <function torch.nn.modules.module.Module._forward_unimplemented(self, *input:Any) -> None>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_get_special_index',\n",
       "  <function __main__.hotpotqa._get_special_index(self, input_ids, special_tokens)>),\n",
       " ('_init_slurm_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._init_slurm_connection(self) -> None>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_set_hparams',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule._set_hparams(self, hp:Union[dict, argparse.Namespace, str]) -> None>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_model_specific_args',\n",
       "  <function __main__.add_model_specific_args(parser, root_dir)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name:str, module:'Module') -> None>),\n",
       " ('amp_scale_loss',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.amp_scale_loss(self, unscaled_loss, optimizer, optimizer_idx)>),\n",
       " ('apply',\n",
       "  <function torch.nn.modules.module.Module.apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, trainer, loss:torch.Tensor, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int) -> None>),\n",
       " ('bfloat16',\n",
       "  <function torch.nn.modules.module.Module.bfloat16(self:~T) -> ~T>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]>),\n",
       " ('children',\n",
       "  <function torch.nn.modules.module.Module.children(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp:object, model:'LightningModule', optimizers:List[torch.optim.optimizer.Optimizer], amp_level:str) -> Tuple[_ForwardRef('LightningModule'), List[torch.optim.optimizer.Optimizer]]>),\n",
       " ('configure_ddp', <function __main__.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu(self) -> torch.nn.modules.module.Module>),\n",
       " ('cuda',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda(self, device:Union[int, NoneType]=None) -> torch.nn.modules.module.Module>),\n",
       " ('decode',\n",
       "  <function __main__.decode(self, input_ids, start_logits, end_logits, type_logits)>),\n",
       " ('double',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double(self) -> torch.nn.modules.module.Module>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self:~T) -> ~T>),\n",
       " ('exact_match_score',\n",
       "  <function __main__.exact_match_score(self, prediction, ground_truth)>),\n",
       " ('extra_repr',\n",
       "  <function torch.nn.modules.module.Module.extra_repr(self) -> str>),\n",
       " ('f1_score', <function __main__.f1_score(self, prediction, ground_truth)>),\n",
       " ('float',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float(self) -> torch.nn.modules.module.Module>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_type)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self) -> None>),\n",
       " ('get_progress_bar_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('get_tqdm_dict',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.get_tqdm_dict(self) -> Dict[str, Union[str, int]]>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type:Union[float, int, str]) -> Dict[str, float]>),\n",
       " ('half',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half(self) -> torch.nn.modules.module.Module>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, global_rank:int, world_size:int, is_slurm_managing_tasks:bool=True) -> None>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict:Dict[str, torch.Tensor], strict:bool=True)>),\n",
       " ('loss_computation',\n",
       "  <function __main__.hotpotqa.loss_computation(self, start_positions, end_positions, start_logits, end_logits, q_type, type_logits)>),\n",
       " ('modules',\n",
       "  <function torch.nn.modules.module.Module.modules(self) -> Iterator[_ForwardRef('Module')]>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self) -> None>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self) -> None>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch:Any) -> None>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer:torch.optim.optimizer.Optimizer) -> None>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self) -> None>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self) -> None>),\n",
       " ('on_fit_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_end(self)>),\n",
       " ('on_fit_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_fit_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self) -> None>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self) -> None>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint:Dict[str, Any]) -> None>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self) -> None>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self) -> None>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int, second_order_closure:Union[Callable, NoneType]=None, on_tpu:bool=False, using_native_amp:bool=False, using_lbfgs:bool=False) -> None>),\n",
       " ('optimizer_zero_grad',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad(self, epoch:int, batch_idx:int, optimizer:torch.optim.optimizer.Optimizer, optimizer_idx:int)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]>),\n",
       " ('prepare_data',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.prepare_data(self) -> None>),\n",
       " ('print',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.print(self, *args, **kwargs) -> None>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name:str, tensor:torch.Tensor, persistent:bool=True) -> None>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name:str, param:torch.nn.parameter.Parameter) -> None>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self:~T, requires_grad:bool=True) -> ~T>),\n",
       " ('save_hyperparameters',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.save_hyperparameters(self, *args, frame=None) -> None>),\n",
       " ('setup',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.setup(self, stage:str)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self:~T) -> ~T>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode:str='top') -> pytorch_lightning.core.memory.ModelSummary>),\n",
       " ('sync_list_across_gpus',\n",
       "  <function __main__.sync_list_across_gpus(self, l, device, dtype)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch:torch.Tensor, split_size:int) -> list>),\n",
       " ('teardown',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.teardown(self, stage:str)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_epoch_end', <function __main__.test_epoch_end(self, outputs)>),\n",
       " ('test_step', <function __main__.test_step(self, batch, batch_nb)>),\n",
       " ('test_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tng_dataloader(self)>),\n",
       " ('to',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to(self, *args, **kwargs) -> torch.nn.modules.module.Module>),\n",
       " ('train',\n",
       "  <function torch.nn.modules.module.Module.train(self:~T, mode:bool=True) -> ~T>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_epoch_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_epoch_end(self, outputs:Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]) -> Dict[str, Dict[str, torch.Tensor]]>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('training_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_step_end(self, *args, **kwargs) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]>),\n",
       " ('transfer_batch_to_device',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device(self, batch:Any, device:torch.device) -> Any>),\n",
       " ('type',\n",
       "  <function pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type(self, dst_type:Union[str, torch.dtype]) -> torch.nn.modules.module.Module>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self) -> None>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>.inner_fx(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_epoch_end',\n",
       "  <function __main__.validation_epoch_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function __main__.validation_step(self, batch, batch_nb)>),\n",
       " ('validation_step_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step_end(self, *args, **kwargs) -> Dict[str, torch.Tensor]>),\n",
       " ('zero_grad',\n",
       "  <function torch.nn.modules.module.Module.zero_grad(self) -> None>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module __main__:\n",
      "\n",
      "configure_optimizers(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "# print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    if not args.test:     # if it needs to train, remove exsiting folder\n",
    "        import shutil\n",
    "        save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "        if os.path.exists(save_folder):\n",
    "            shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder \n",
    "        \n",
    "    import shutil\n",
    "    save_folder = os.path.join(args.save_dir, args.save_prefix)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder, ignore_errors=True)  #delete non-empty folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.args.model_path:  /xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096\n",
      "Loaded model with config:\n",
      "LongformerConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"sliding_chunks\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)\n",
    "#     model.to('cuda')    # this is necessary to use gpu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_size:  9.0\n",
      "num_devices:  1\n",
      ">>>>>>> #train_set_size: 90447.0, #steps: 271341.0,  #warmup steps: 1000, #epochs: 6, batch_size: 2 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "    train_set_size = 9 * args.train_percent # 90447 * args.train_percent   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    print(\"train_set_size: \", train_set_size) \n",
    "\n",
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus!='' else None\n",
    "    num_devices = 1 or len(args.gpus)\n",
    "    print(\"num_devices: \", num_devices)\n",
    "\n",
    "    train_set_size = 90447 * args.train_percent    # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "\n",
    "    print(f'>>>>>>> #train_set_size: {train_set_size}, #steps: {args.steps},  #warmup steps: {args.warmup}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Multi-processing is handled by Slurm.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp', # if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                             track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                             accumulate_grad_batches=args.batch_size,\n",
    "                             train_percent_check = args.train_percent,\n",
    "        #                          val_check_interval=args.val_every,\n",
    "                             val_percent_check=args.val_percent_check,\n",
    "                             test_percent_check=args.val_percent_check,\n",
    "                             logger=logger if not args.disable_checkpointing else False,\n",
    "                             checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                             show_progress_bar=args.no_progress_bar,\n",
    "                             use_amp=not args.fp32, \n",
    "                             amp_level='O2',\n",
    "#                              check_val_every_n_epoch=1\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=ddp\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | Longformer | 148 M \n",
      "1 | qa_outputs  | Linear     | 1 K   \n",
      "2 | linear_type | Linear     | 2 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: small_dev3.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.1338,  0.0667,  0.0160,  ..., -0.3417, -0.0044,  0.0286],\n",
      "         [ 0.2016, -0.1327,  0.1563,  ..., -0.4514,  0.1033,  0.1491],\n",
      "         [-0.0061,  0.0996,  0.0902,  ..., -0.7683,  0.0708,  0.1613],\n",
      "         ...,\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744],\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744],\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2712, 0.4490, 0.2316,  ..., 0.4741, 0.4094, 0.4285], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1185, -0.1704,  0.0685,  ..., -0.1342, -0.1160,  0.0457],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0958,  0.0528,  0.0185,  ..., -0.2272, -0.0123,  0.0446],\n",
      "         [ 0.1191, -0.2136,  0.1076,  ...,  0.0071, -0.0292,  0.1146],\n",
      "         [-0.0850, -0.2070,  0.0285,  ..., -0.2293,  0.0659,  0.0250],\n",
      "         ...,\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744],\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744],\n",
      "         [-0.0236,  0.0741, -0.0146,  ..., -0.0990, -0.0409, -0.0744]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.3013,  0.7031,  0.4495,  0.6016,  0.4695,  0.4214,  0.2198,  0.4556,\n",
      "         0.2727,  0.4626,  0.5205,  0.2568,  0.5005,  0.3955,  0.5288,  0.4629,\n",
      "         0.3821,  0.6821,  0.5806,  0.2703,  0.3962,  0.1620,  0.3770,  0.1415,\n",
      "         0.3950,  0.5303,  0.3333,  0.3579,  0.0839,  0.0611,  0.3582,  0.4932,\n",
      "         0.0717,  0.2180,  0.3069,  0.2245,  0.3521,  0.2328,  0.3767,  0.4702,\n",
      "         0.3757,  0.7812,  0.7026,  0.5488,  0.5186,  0.4246,  0.2961,  0.3567,\n",
      "         0.4839,  0.5366,  0.3916,  0.0953,  0.3066,  0.1948,  0.0665,  0.3640,\n",
      "         0.2406,  0.4927,  0.2935,  0.3989,  0.3237,  0.3440,  0.4238,  0.8823,\n",
      "         0.6396,  0.6055,  0.4062,  0.4343,  0.4497,  0.2593,  0.2634,  0.2539,\n",
      "         0.3857,  0.4109,  0.2788,  0.4761,  0.4897,  0.6274,  0.3086,  0.3726,\n",
      "         0.4160,  0.2847,  0.5454,  0.5454,  0.6055,  0.5176,  0.6558,  0.4043,\n",
      "         0.6045,  0.7026,  0.4702,  0.3606,  0.8062,  0.4849,  0.5054,  0.2112,\n",
      "         0.4165,  0.3235,  0.6890,  0.4834,  0.5137,  0.5093,  0.5713,  0.4138,\n",
      "         0.6201,  0.8765,  0.7720,  0.4719,  0.5322,  0.5762,  0.6221,  0.6260,\n",
      "         0.1646,  0.1721,  0.2837,  0.4971,  0.2949,  0.4121,  0.4456,  0.4888,\n",
      "         0.1562,  0.4966,  0.5283,  0.4072,  0.5347,  0.2247,  0.4539,  0.4834,\n",
      "         0.5547,  0.5601,  0.4119,  0.6440,  0.5498,  0.4243,  0.4814,  0.4570,\n",
      "         0.5361,  0.6001,  0.4387,  0.3940,  0.4602,  0.5874,  0.6797,  0.3599,\n",
      "         0.5552,  0.7144,  0.6030,  0.5215,  0.5425,  0.5972,  0.4407,  0.6099,\n",
      "         0.3933,  0.2416,  0.4375,  0.2479,  0.6924,  0.6333,  0.5889,  0.6465,\n",
      "         0.2357,  0.4353,  0.5039,  0.3572,  0.5049,  0.3604,  0.4058,  0.6294,\n",
      "         0.5410,  0.5352,  0.5293,  0.6294,  0.4912,  0.3218,  0.4941,  0.4163,\n",
      "         0.6318,  0.3542,  0.6953,  0.6890,  0.5396,  0.7603,  0.5869,  0.6108,\n",
      "         0.3628,  0.4626,  0.4539,  0.2340,  0.4282,  0.4863,  0.5581,  0.2225,\n",
      "         0.4670,  0.4543,  0.8550,  0.4023,  0.4573,  0.7080,  0.5435,  0.2394,\n",
      "         0.1611,  0.4434,  0.5874,  0.4551,  0.4810,  0.4543,  0.2024,  0.4790,\n",
      "         0.7627,  0.3962,  0.7510,  0.3037,  0.5771,  0.4294,  0.5332,  0.4055,\n",
      "         0.8672,  0.6304,  0.9170,  0.7119,  0.3962,  0.5254,  0.8535,  0.4050,\n",
      "         0.6816,  0.5024,  0.2136,  0.5586,  0.7349,  0.5161,  0.2991,  0.3948,\n",
      "         0.5547,  0.4390,  0.3403,  0.3616,  0.5776,  0.2607,  0.4570,  0.4478,\n",
      "         0.2764,  0.4888,  0.2335,  0.4990,  0.4062,  0.3740,  0.4824,  0.2491,\n",
      "         0.5273,  0.2460,  0.4424,  0.2598,  0.2603,  0.7769,  0.7466,  0.5210,\n",
      "         0.2147,  0.5425,  0.8262,  0.3286,  0.6729,  0.7944,  0.4705,  0.3628,\n",
      "         0.2048,  0.6323,  0.5273,  0.4795,  0.5488,  0.4785,  0.3904,  0.5552,\n",
      "         0.2639,  0.4680,  0.4714,  0.5220,  0.4226,  0.4084,  0.4780,  0.3586,\n",
      "         0.4917,  0.2399,  0.3965,  0.3892,  0.4260,  0.4897,  0.4512,  0.4487,\n",
      "         0.3911,  0.3188, -0.0057,  0.5752,  0.3557,  0.6045,  0.0811,  0.2905,\n",
      "         0.4766,  0.2913,  0.3220,  0.4214,  0.3149,  0.3179,  0.2791,  0.4487,\n",
      "         0.6211,  0.2761,  0.2788,  0.2617,  0.0073,  0.5298,  0.1649,  0.3020,\n",
      "         0.3096,  0.3528,  0.3298,  0.2585,  0.3772,  0.2954,  0.3577,  0.2467,\n",
      "         0.4595,  0.5806,  0.6626,  0.5312,  0.7012,  0.3174,  0.5615,  0.1616,\n",
      "         0.5454,  0.3811,  0.7686,  0.7451,  0.6064,  0.4255,  0.2517,  0.3105,\n",
      "         0.4517,  0.3960,  0.2776,  0.3420,  0.1373,  0.2445,  0.2915,  0.3916,\n",
      "         0.3384,  0.3894,  0.3169,  0.2571,  0.4082,  0.5283,  0.4033,  0.3267,\n",
      "         0.2585,  0.3691,  0.6523,  0.4622,  0.6724,  0.1820,  0.6479,  0.6123,\n",
      "         0.0779,  0.5098,  0.7715,  0.3694,  0.7129,  0.2969,  0.7163,  0.5278,\n",
      "         0.7749,  0.2487,  0.4829,  0.1978,  0.3618,  0.5630,  0.5728,  0.4258,\n",
      "         0.7915,  0.3674,  0.3892,  0.7812,  0.1680,  0.4858,  0.2377,  0.6909,\n",
      "         0.2130,  0.6963,  0.2515,  0.2214,  0.2496,  0.4272,  0.3674,  0.2834,\n",
      "         0.4553,  0.2347,  0.0966,  0.7090,  0.3687,  0.3833,  0.7720,  0.5054,\n",
      "         0.4011,  0.5234,  0.5225,  0.1783,  0.6753,  0.6772,  0.4133,  0.7134,\n",
      "         0.2883,  0.5737,  0.4160,  0.1881,  0.5098,  0.3132,  0.3479,  0.5908,\n",
      "         0.3411,  0.6514,  0.0039,  0.3452,  0.4106,  0.3882,  0.3860,  0.2683,\n",
      "         0.3611,  0.2856,  0.4236,  0.2854,  0.2881,  0.4858,  0.2590,  0.7070,\n",
      "         0.2318,  0.3794,  0.3745,  0.2472,  0.4023,  0.3286,  0.2491,  0.3918,\n",
      "         0.6875,  0.3369,  0.4688,  0.1364,  0.4883,  0.3208,  0.4529,  0.4482,\n",
      "         0.4446,  0.5200,  0.4565,  0.5391,  0.1301,  0.3386,  0.0652,  0.1908,\n",
      "         0.3826,  0.0745,  0.4814,  0.3884,  0.6069,  0.4878,  0.3342,  0.1813,\n",
      "         0.2827,  0.2517,  0.4824,  0.5991,  0.6626,  0.6060,  0.4590,  0.3433,\n",
      "         0.3464,  0.6733,  0.3103,  0.4011,  0.7280,  0.4563,  0.3579,  0.5479,\n",
      "         0.2281,  0.3835,  0.5640,  0.3162,  0.2983,  0.2024,  0.3831,  0.3667,\n",
      "         0.3457,  0.1564,  0.3711,  0.2233,  0.1864,  0.3894,  0.1578,  0.4663,\n",
      "         0.1929,  0.4233,  0.3806,  0.5273,  0.2335, -0.0372,  0.2382,  0.3557,\n",
      "         0.3716,  0.4678,  0.2981,  0.4058,  0.2421,  0.2795,  0.4409,  0.3848,\n",
      "         0.2693,  0.6538,  0.6074,  0.5684,  0.4609,  0.4580,  0.4490,  0.4065,\n",
      "         0.5659,  0.5640,  0.5576,  0.2856,  0.2094,  0.3667,  0.3123,  0.4155,\n",
      "         0.2537,  0.3369,  0.2515,  0.5303,  0.3188,  0.4282,  0.3813,  0.4182,\n",
      "         0.2496,  0.3604,  0.2625,  0.1741,  0.1876,  0.3416,  0.3303,  0.3772,\n",
      "         0.5063,  0.4546,  0.4790,  0.6885,  0.2847,  0.3313,  0.7002,  0.4351,\n",
      "         0.5474,  0.3467,  0.4214,  0.2115,  0.4351,  0.3286,  0.4316,  0.3022,\n",
      "         0.2330,  0.3008,  0.5312,  0.4883,  0.3364,  0.4299,  0.3872,  0.3542,\n",
      "         0.3411,  0.1573,  0.3938,  0.4653,  0.4351,  0.3101,  0.4285,  0.3428,\n",
      "         0.2632,  0.3611,  0.4170,  0.5752,  0.2825,  0.3892,  0.4326,  0.4036,\n",
      "         0.2991,  0.4158,  0.3076,  0.3972,  0.3501,  0.2369,  0.3389,  0.2489,\n",
      "         0.5371,  0.4753,  0.4158,  0.3940,  0.4700,  0.2944,  0.4346,  0.6924,\n",
      "         0.5425,  0.6821,  0.4282,  0.4460,  0.4138,  0.4387,  0.2969,  0.3645,\n",
      "         0.4673,  0.3491,  0.2172,  0.2209,  0.4048,  0.5381,  0.2386,  0.4001,\n",
      "         0.4294,  0.3313,  0.1725,  0.3967,  0.6245,  0.5151,  0.6724,  0.4204,\n",
      "         0.4702,  0.4121,  0.3655,  0.3181,  0.3245,  0.3101,  0.6602,  0.5410,\n",
      "         0.4397,  0.2590], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 9.5337e-02, -3.1226e-01, -1.6907e-01, -1.2854e-01, -7.1777e-01,\n",
      "        -3.5010e-01, -2.5439e-01, -1.7761e-01, -1.7029e-01, -8.3618e-02,\n",
      "        -2.2144e-01, -1.3098e-01,  7.2632e-02, -2.9663e-01, -5.2429e-02,\n",
      "         1.3281e-01, -3.3386e-02,  1.4307e-01, -1.9983e-01,  1.4478e-01,\n",
      "        -4.5105e-02, -5.2429e-02,  1.2866e-01, -2.4487e-01, -5.4810e-02,\n",
      "        -6.1157e-02, -1.2842e-01, -1.8311e-01,  3.2056e-01,  3.9722e-01,\n",
      "         2.2552e-02,  3.2690e-01,  1.8140e-01, -1.4917e-01,  7.2556e-03,\n",
      "         6.9336e-02,  1.3074e-01,  9.3811e-02,  4.6570e-02,  1.7578e-01,\n",
      "         1.8176e-01,  1.5405e-01,  1.1986e-02, -1.9058e-02,  6.9702e-02,\n",
      "        -1.4148e-01, -7.7393e-02, -4.0207e-03, -2.6718e-02, -5.6213e-02,\n",
      "        -2.5488e-01,  7.5626e-04,  4.3457e-02,  2.2705e-01, -1.2408e-01,\n",
      "         6.4331e-02,  1.3794e-01,  6.1676e-02,  1.9507e-01,  1.6718e-03,\n",
      "        -5.2429e-02, -4.4312e-02,  9.8755e-02,  3.6865e-02,  3.5950e-02,\n",
      "        -7.5562e-02,  2.3621e-01,  3.3207e-03, -5.8472e-02, -2.4023e-01,\n",
      "         1.4392e-01, -2.2339e-01, -7.1838e-02, -4.4800e-02, -9.3079e-02,\n",
      "         7.4890e-02,  2.5928e-01,  3.1592e-01, -1.5942e-01, -1.2756e-01,\n",
      "        -5.9814e-02, -1.2878e-01, -3.3130e-01, -1.1627e-01,  1.8140e-01,\n",
      "        -3.4546e-02,  1.8530e-01, -2.4121e-01,  2.7124e-01, -2.0218e-02,\n",
      "        -1.3013e-01, -2.1866e-02, -3.9697e-01, -3.4766e-01, -1.0077e-01,\n",
      "        -1.9592e-01, -8.8501e-02,  4.1901e-02, -2.7786e-02, -2.1863e-01,\n",
      "        -6.8481e-02, -2.7002e-01,  2.7313e-02, -1.6064e-01, -1.3013e-01,\n",
      "        -5.7800e-02, -2.6294e-01, -7.0251e-02, -3.4912e-02,  1.5015e-01,\n",
      "        -4.0955e-02, -3.6743e-01, -4.3750e-01, -2.9068e-02, -2.2241e-01,\n",
      "        -2.9861e-02, -1.2421e-01, -9.5764e-02, -2.2559e-01, -3.1934e-01,\n",
      "         5.9967e-02, -1.6382e-01, -2.0325e-01, -4.6265e-01, -1.5369e-01,\n",
      "        -3.5718e-01, -1.3342e-03,  3.2135e-02, -2.6749e-02, -2.0764e-01,\n",
      "        -1.9373e-01, -3.8452e-01, -1.0399e-02, -2.2925e-01, -2.1814e-01,\n",
      "        -1.9885e-01, -1.0826e-02, -2.0581e-01, -9.2163e-02, -9.5276e-02,\n",
      "        -2.3178e-02, -2.2751e-02,  1.7944e-01,  1.0551e-02, -1.1664e-01,\n",
      "        -4.6997e-02, -3.3020e-02,  1.2311e-01,  1.8494e-02, -5.5725e-02,\n",
      "        -1.3110e-01, -1.1223e-02, -9.1614e-02, -1.4368e-01, -1.4305e-02,\n",
      "        -5.4260e-02, -2.7939e-02, -3.0176e-01, -3.3765e-01, -3.2983e-01,\n",
      "        -2.0679e-01, -1.1407e-01, -1.9824e-01, -2.0081e-01, -2.0020e-01,\n",
      "        -4.3579e-02, -3.2007e-01, -3.1396e-01, -2.8296e-01, -4.3945e-02,\n",
      "        -2.5122e-01,  2.6443e-02, -1.7529e-01,  1.4417e-01, -1.3904e-01,\n",
      "         4.2511e-02,  4.3259e-03,  7.9895e-02,  7.0618e-02, -7.3776e-03,\n",
      "        -8.9783e-02,  7.4158e-02,  3.5801e-03, -4.6753e-02, -3.0835e-01,\n",
      "         1.7624e-02, -1.8994e-01, -1.6089e-01,  3.7720e-02, -4.8096e-01,\n",
      "        -1.4575e-01, -1.4331e-01,  2.3132e-02, -3.8916e-01,  2.3132e-01,\n",
      "        -3.5889e-02, -1.3281e-01, -3.9648e-01, -2.6636e-01, -4.3213e-02,\n",
      "         4.1333e-01,  3.0918e-03,  3.7327e-03, -2.3178e-02,  8.9359e-04,\n",
      "        -3.9673e-01, -4.3530e-01, -5.2392e-05,  9.1125e-02, -1.0883e-01,\n",
      "        -1.5540e-01, -1.7529e-01, -2.3340e-01, -4.5898e-01, -1.0211e-01,\n",
      "        -1.5526e-02, -6.0852e-02,  5.3497e-02, -7.6721e-02, -8.3691e-01,\n",
      "        -1.0712e-01, -1.4343e-01, -4.3518e-02, -1.7383e-01, -2.1228e-01,\n",
      "        -2.0471e-01, -2.6440e-01, -1.8494e-01, -3.3154e-01, -3.5669e-01,\n",
      "        -1.3477e-01, -8.1299e-02,  3.7689e-02, -2.5610e-01, -1.9458e-01,\n",
      "        -1.1163e-01, -3.9160e-01,  5.9296e-02, -2.0911e-01, -2.4429e-02,\n",
      "         4.7180e-02, -6.8298e-02, -6.7978e-03, -5.2063e-02, -2.9099e-02,\n",
      "        -2.2852e-01, -9.8572e-02, -1.2817e-01, -6.0043e-03, -3.2031e-01,\n",
      "        -5.4248e-01,  1.3513e-01, -1.1676e-01, -1.2482e-01, -1.9580e-01,\n",
      "        -1.7432e-01, -5.0586e-01, -9.6252e-02, -3.0167e-02, -9.6008e-02,\n",
      "         2.2095e-02, -1.4807e-01, -1.9165e-01, -1.1859e-01, -1.7126e-01,\n",
      "        -7.6111e-02, -4.1809e-02, -1.9519e-01, -2.1072e-02, -6.1829e-02,\n",
      "         1.9104e-02, -1.1285e-01, -9.1797e-02,  6.4636e-02, -3.0225e-01,\n",
      "        -1.0443e-01,  1.7059e-02, -3.8513e-02, -1.5640e-02, -5.8899e-02,\n",
      "        -2.4854e-01, -1.3184e-01,  3.1891e-02, -1.7227e-02,  8.3740e-02,\n",
      "        -2.7515e-01, -1.7334e-01,  3.4428e-03,  2.0218e-02, -1.4978e-01,\n",
      "        -1.9312e-01, -1.6739e-02,  4.9057e-03, -7.1472e-02, -2.2119e-01,\n",
      "        -6.6040e-02,  2.5040e-02, -8.7463e-02, -1.4868e-03, -5.7495e-02,\n",
      "        -2.1582e-01,  1.2891e-01, -9.2773e-02,  2.9358e-02, -3.7842e-01,\n",
      "        -4.0207e-03, -8.5510e-02,  1.7929e-02,  1.9547e-02,  4.7974e-02,\n",
      "        -4.1931e-02, -5.6091e-02,  6.5186e-02,  1.6413e-03, -4.2017e-01,\n",
      "         1.3293e-01, -5.4321e-02, -3.7964e-02, -1.5793e-02, -2.0959e-01,\n",
      "        -2.4707e-01, -6.8237e-02,  1.4050e-01,  1.2549e-01,  4.0039e-01,\n",
      "        -3.9941e-01, -9.0637e-02,  1.4160e-01,  2.4368e-02, -1.3756e-02,\n",
      "         1.9678e-01,  4.5746e-02,  7.7820e-02, -6.6650e-02, -2.0532e-01,\n",
      "         8.1711e-03, -1.0028e-01,  1.0094e-02,  3.2166e-02,  5.4901e-02,\n",
      "        -2.6245e-01,  1.9312e-03, -1.1406e-02,  2.0703e-01,  1.2550e-02,\n",
      "         7.6294e-02,  8.3008e-02, -1.1957e-01, -6.5613e-02,  4.3610e-02,\n",
      "        -3.0713e-01, -3.9795e-01,  1.3269e-01, -1.5698e-01, -2.5562e-01,\n",
      "        -4.8523e-02, -2.6099e-01, -2.7466e-01, -1.2817e-01, -1.6772e-01,\n",
      "        -2.2827e-01, -1.2213e-01, -7.1899e-02, -1.2671e-01, -3.9551e-01,\n",
      "        -1.7627e-01, -2.8906e-01, -1.9394e-02, -2.8589e-01, -1.0992e-01,\n",
      "        -1.3416e-01,  2.6947e-02, -3.9490e-02, -1.1786e-01, -7.4524e-02,\n",
      "        -2.1252e-01, -2.5220e-01,  5.0201e-02,  1.6541e-01, -1.0338e-02,\n",
      "        -4.1077e-02, -3.9856e-02,  8.7097e-02,  7.5378e-02, -7.5500e-02,\n",
      "        -1.7319e-02,  9.7733e-03,  5.1270e-02, -2.5488e-01,  8.8577e-03,\n",
      "        -2.1106e-01, -1.9214e-01, -2.0911e-01,  1.0071e-01,  1.5320e-01,\n",
      "        -4.0588e-02, -3.7500e-01,  6.2317e-02, -4.2383e-01, -5.6824e-02,\n",
      "         1.1469e-01,  3.0746e-02, -1.6931e-01, -3.4546e-02, -2.0032e-01,\n",
      "        -1.1957e-01, -3.3875e-02, -6.1035e-02, -1.6052e-01, -1.2512e-01,\n",
      "         3.0243e-02,  1.4722e-01, -1.2482e-01, -6.9504e-03,  1.5930e-02,\n",
      "        -1.2152e-01, -1.7957e-01,  2.0126e-02,  2.0508e-01,  8.1177e-02,\n",
      "         1.1469e-01, -1.2079e-01, -1.0284e-01, -2.7319e-01, -4.3884e-02,\n",
      "        -1.9312e-01,  1.2183e-01, -2.3413e-01, -4.9011e-02,  1.2891e-01,\n",
      "        -1.5454e-01,  4.8120e-01, -4.9988e-02, -6.7978e-03, -1.3184e-01,\n",
      "        -2.6465e-01, -3.8357e-03, -1.1224e-01,  4.0710e-02,  1.2659e-01,\n",
      "         9.2712e-02, -5.2307e-02, -5.8716e-02, -4.9988e-02,  1.5283e-01,\n",
      "        -2.4243e-01,  3.2257e-02,  2.3230e-01,  1.7517e-01,  4.4775e-01,\n",
      "        -2.8320e-01, -2.3584e-01, -2.3462e-01,  5.4901e-02,  2.4194e-01,\n",
      "         5.8167e-02,  1.1206e-01, -4.4373e-02,  5.2734e-02,  1.1078e-01,\n",
      "         1.4563e-01,  1.5430e-01, -2.8976e-02, -1.5393e-01, -3.1519e-01,\n",
      "         1.2671e-01, -7.9895e-02,  1.2354e-01,  2.1057e-02,  1.7676e-01,\n",
      "        -5.6836e-01, -2.4805e-01,  1.8677e-02,  1.2213e-01, -2.4643e-02,\n",
      "         9.3689e-02, -1.4673e-01, -1.2121e-03, -9.8694e-02,  5.5389e-02,\n",
      "         1.9928e-02,  3.1555e-02,  1.6980e-01, -1.1737e-01, -6.6284e-02,\n",
      "        -1.0521e-02, -3.9185e-02, -1.1163e-01,  8.4839e-02,  9.8816e-02,\n",
      "         2.3340e-01,  1.1499e-01, -1.0834e-01, -5.4749e-02, -8.2092e-02,\n",
      "        -8.7585e-02, -5.9570e-02,  1.2720e-01, -2.2876e-01, -2.2314e-01,\n",
      "        -3.6157e-01, -6.1768e-02, -2.4585e-01, -1.8445e-01, -8.3008e-02,\n",
      "         3.0566e-01, -1.3672e-01,  9.1858e-02, -2.0264e-01, -3.0566e-01,\n",
      "        -9.4238e-02, -2.9150e-01,  1.0852e-01, -3.6108e-01, -2.7515e-01,\n",
      "        -2.4536e-01, -7.0947e-01, -1.7542e-01, -1.5259e-01, -2.2018e-02,\n",
      "        -2.6929e-01, -2.3340e-01, -3.3008e-01,  3.5919e-02, -5.1147e-02,\n",
      "        -8.3130e-02, -6.2790e-03,  1.3257e-01, -1.3440e-01, -1.8469e-01,\n",
      "        -2.7271e-01, -8.9050e-02, -1.3611e-01, -3.0289e-02, -8.6243e-02,\n",
      "        -4.4785e-03,  1.3599e-01,  1.4839e-02, -1.1650e-02,  1.2549e-01,\n",
      "         4.2053e-02,  2.5955e-02,  7.6294e-02,  6.8359e-02,  6.8420e-02,\n",
      "         2.0544e-01, -1.8225e-03,  2.7023e-02,  1.7578e-01, -1.1517e-01,\n",
      "         2.2831e-03,  8.4106e-02, -4.1321e-02,  1.4490e-01,  7.1472e-02,\n",
      "        -2.7145e-02, -1.8298e-01,  1.1833e-02,  2.0276e-01, -1.0443e-01,\n",
      "         1.6235e-02, -7.2754e-02,  7.4890e-02, -1.3489e-01, -3.2007e-01,\n",
      "        -5.2734e-02, -2.8223e-01, -3.0472e-02,  1.8018e-01,  3.3203e-02,\n",
      "         1.9196e-02,  1.5942e-01, -1.5649e-01,  5.0964e-02,  1.5930e-01,\n",
      "         5.1300e-02,  7.5256e-02, -9.4543e-02, -3.7781e-02, -3.0737e-01,\n",
      "        -2.1313e-01,  7.6660e-02, -1.0857e-02,  9.4482e-02, -5.7190e-02,\n",
      "        -3.5583e-02, -2.7084e-02, -1.1835e-01,  9.6375e-02, -2.0981e-02,\n",
      "         1.0170e-02, -3.2886e-01, -1.8982e-01, -1.8652e-01, -6.0486e-02,\n",
      "        -2.4292e-01, -8.0994e-02, -2.4512e-01, -3.4595e-01,  4.0924e-02,\n",
      "         1.0785e-01,  1.7859e-01, -8.7830e-02, -1.8127e-01, -1.0284e-01,\n",
      "        -2.2070e-01,  5.9021e-02, -1.3953e-01,  5.0995e-02,  1.6443e-01,\n",
      "         5.7434e-02, -1.7566e-01, -1.8750e-01,  3.3970e-03, -1.1859e-01,\n",
      "        -1.2054e-01, -1.3831e-01, -7.9712e-02, -2.9736e-01, -3.1689e-01,\n",
      "         1.4258e-01,  1.0114e-01,  1.8201e-01, -2.2803e-01, -2.2192e-01,\n",
      "        -1.0968e-01, -7.0496e-02, -2.5610e-01, -6.3416e-02, -2.5391e-01,\n",
      "        -4.5557e-01, -1.3806e-01, -1.8982e-01,  1.2244e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  2, 2, 2\n",
      "after sync --> sizes: 2, 2, 2\n",
      "avg_loss:  tensor(14.1604, device='cuda:0')\tavg_answer_loss:  tensor(6.7197, device='cuda:0')\tavg_type_loss:  tensor(1.4881, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n",
      "reading file: small.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bd1196c8774f53901c164520cdd189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0174,  0.0914, -0.0345,  ..., -0.5148, -0.0147,  0.0217],\n",
      "         [ 0.0346,  0.0550, -0.0189,  ..., -0.4221,  0.0510,  0.0646],\n",
      "         [-0.1056,  0.0133, -0.0286,  ..., -0.6591, -0.0356, -0.0507],\n",
      "         ...,\n",
      "         [-0.0167,  0.0845,  0.0060,  ..., -0.0998, -0.0184, -0.0681],\n",
      "         [-0.0277,  0.0671, -0.0200,  ..., -0.0961, -0.0520, -0.0593],\n",
      "         [-0.0315,  0.1438,  0.0743,  ..., -0.1464, -0.0833, -0.0465]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0190, -0.0692,  0.1879,  ..., -0.1308, -0.1271,  0.1017],\n",
      "         [-0.0847,  0.1646,  0.0380,  ..., -0.5251,  0.0175,  0.1606],\n",
      "         [ 0.0430,  0.1183,  0.0704,  ..., -0.4415, -0.0452,  0.1463],\n",
      "         ...,\n",
      "         [-0.1000,  0.1169, -0.0225,  ..., -0.0885, -0.0727, -0.1586],\n",
      "         [-0.0656,  0.1944, -0.0150,  ..., -0.0738, -0.0479, -0.1995],\n",
      "         [-0.0248,  0.0442, -0.0150,  ..., -0.1023, -0.0381,  0.0141]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0777, -0.1160,  0.0401,  ..., -0.3026,  0.0912,  0.0238],\n",
      "         [ 0.0946, -0.0747,  0.0079,  ..., -0.3773,  0.0829,  0.1608],\n",
      "         [ 0.1840,  0.0047,  0.0625,  ..., -1.0105, -0.0134,  0.1837],\n",
      "         ...,\n",
      "         [-0.0140,  0.0649, -0.0090,  ..., -0.1012, -0.0295, -0.0398],\n",
      "         [-0.0094,  0.0599, -0.0182,  ..., -0.0921, -0.0230, -0.0507],\n",
      "         [-0.0543,  0.1497,  0.0227,  ..., -0.0798, -0.1351, -0.1529]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0660,  0.0840,  0.0511,  ..., -0.2681, -0.0367,  0.0257],\n",
      "         [ 0.0442, -0.0139,  0.1108,  ..., -0.3550, -0.0020,  0.0304],\n",
      "         [ 0.0307, -0.1111,  0.0318,  ..., -0.5102, -0.0319, -0.0999],\n",
      "         ...,\n",
      "         [-0.0304,  0.0506, -0.0110,  ..., -0.1014, -0.0435,  0.0250],\n",
      "         [-0.0163,  0.0703, -0.0159,  ..., -0.1052, -0.0269, -0.0669],\n",
      "         [ 0.0020,  0.1759, -0.1532,  ..., -0.0103, -0.1326,  0.0432]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0099,  0.1581,  0.0331,  ..., -0.2286, -0.0152,  0.0525],\n",
      "         [ 0.0566,  0.1444,  0.0835,  ..., -0.2505,  0.0764,  0.1460],\n",
      "         [ 0.1798,  0.1468,  0.4696,  ..., -0.7988, -0.0789,  0.1757],\n",
      "         ...,\n",
      "         [-0.0220,  0.0982, -0.0143,  ..., -0.1074, -0.0411, -0.0658],\n",
      "         [-0.0188,  0.3967, -0.0867,  ..., -0.4502,  0.0298,  0.0758],\n",
      "         [-0.0221,  0.0719, -0.0295,  ..., -0.1038, -0.0481, -0.0559]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-1.3971e-01, -5.8002e-02,  1.5637e-01,  ..., -8.6983e-01,\n",
      "          -1.5502e-03,  1.1400e-01],\n",
      "         [ 1.5787e-01,  4.9602e-02, -2.4088e-02,  ..., -4.1750e-01,\n",
      "           1.0096e-01,  8.1633e-02],\n",
      "         [ 4.5647e-02,  1.1018e-01,  2.7709e-02,  ..., -2.2446e+00,\n",
      "           2.2303e-01,  4.5598e-02],\n",
      "         ...,\n",
      "         [ 8.7755e-02,  9.1760e-02, -5.5760e-02,  ...,  1.1562e-02,\n",
      "           3.3128e-01, -6.6169e-02],\n",
      "         [-6.9424e-02,  1.1150e-01, -9.3488e-03,  ..., -9.2927e-02,\n",
      "          -4.3738e-02, -1.5240e-01],\n",
      "         [-2.3679e-02,  6.6532e-02, -2.4406e-02,  ..., -1.0525e-01,\n",
      "          -1.3099e-02, -5.1465e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0476,  0.0028,  0.0592,  ..., -0.2588,  0.0312,  0.0955],\n",
      "         [ 0.1740, -0.0648,  0.0515,  ..., -0.2136,  0.2589,  0.2956],\n",
      "         [ 0.0640, -0.1086, -0.0297,  ..., -0.2011, -0.0410,  0.0637],\n",
      "         ...,\n",
      "         [-0.0202,  0.1087,  0.0399,  ..., -0.0939, -0.0969, -0.1606],\n",
      "         [-0.0182,  0.1254, -0.0231,  ..., -0.1117, -0.0571, -0.1312],\n",
      "         [-0.0148,  0.0719, -0.0076,  ..., -0.0974, -0.0414,  0.0162]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0248,  0.0532,  0.0414,  ..., -0.5547,  0.1199,  0.0065],\n",
      "         [-0.1448,  0.1546, -0.2786,  ..., -0.1352,  0.1988,  0.6346],\n",
      "         [ 0.0163,  0.1206, -0.0017,  ..., -0.2853,  0.0369,  0.0854],\n",
      "         ...,\n",
      "         [-0.0158,  0.0677, -0.0172,  ..., -0.1004, -0.0431, -0.0571],\n",
      "         [-0.0276,  0.0788, -0.0165,  ..., -0.1153, -0.0398, -0.0686],\n",
      "         [-0.0159,  0.0712, -0.0127,  ..., -0.1114, -0.0346, -0.0704]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0014,  0.1655,  0.0248,  ..., -0.1036, -0.0387,  0.0834],\n",
      "         [ 0.1461,  0.2072,  0.1040,  ..., -0.4098,  0.1213,  0.1498],\n",
      "         [ 0.1213,  0.1889,  0.1254,  ..., -0.4241, -0.0755,  0.0084],\n",
      "         ...,\n",
      "         [-0.0164,  0.0741, -0.0226,  ..., -0.0940, -0.0362, -0.0768],\n",
      "         [-0.0227,  0.0700, -0.0117,  ..., -0.1068, -0.0173,  0.0239],\n",
      "         [ 0.1370,  0.1639,  0.1439,  ..., -0.0170, -0.0741, -0.2636]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0875, -0.0466, -0.0334,  ..., -0.3624, -0.1074, -0.1189],\n",
      "         [ 0.2560,  0.0544,  0.1192,  ..., -0.3315,  0.1224,  0.2356],\n",
      "         [ 0.1234,  0.0670,  0.0064,  ..., -0.4730, -0.1380,  0.1720],\n",
      "         ...,\n",
      "         [-0.0793,  0.1203,  0.0082,  ..., -0.1049, -0.0355, -0.1669],\n",
      "         [-0.0249,  0.0621,  0.0060,  ..., -0.1019, -0.0178, -0.0639],\n",
      "         [-0.0267,  0.0712, -0.0298,  ..., -0.1073, -0.0490,  0.0122]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1076, -0.0640, -0.0321,  ..., -0.2364, -0.1191,  0.0351],\n",
      "         [ 0.0754, -0.0694, -0.0828,  ..., -0.7157, -0.1934,  0.2834],\n",
      "         [ 0.2513, -0.0751, -0.2590,  ..., -1.1555,  0.1025,  0.3534],\n",
      "         ...,\n",
      "         [-0.0229,  0.0638, -0.0153,  ..., -0.0996, -0.0390,  0.0192],\n",
      "         [-0.0206,  0.0706, -0.0119,  ..., -0.1182, -0.0180, -0.0646],\n",
      "         [-0.0258,  0.0715,  0.0084,  ..., -0.0960, -0.0418,  0.0188]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-2.1570e-02,  6.6465e-02, -5.4865e-03,  ..., -3.1403e-01,\n",
      "           7.3704e-03,  8.6812e-02],\n",
      "         [ 2.1852e-01, -2.4558e-02,  1.3306e-01,  ..., -4.9086e-01,\n",
      "          -3.0577e-04,  1.2980e-01],\n",
      "         [ 2.4900e-01, -1.0335e-01,  2.3958e-02,  ..., -1.2176e+00,\n",
      "           2.8122e-03,  1.3184e-01],\n",
      "         ...,\n",
      "         [-1.7439e-02,  7.1045e-02,  2.9609e-03,  ..., -1.1743e-01,\n",
      "          -3.5532e-02, -7.6674e-02],\n",
      "         [-2.7615e-02,  7.0186e-02, -1.7707e-02,  ..., -9.6587e-02,\n",
      "          -4.2557e-02, -6.9333e-02],\n",
      "         [-4.5475e-02,  3.5304e-01,  1.2410e-01,  ...,  3.3326e-02,\n",
      "           3.3503e-01, -4.1432e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0974,  0.1981, -0.0250,  ..., -0.1491,  0.0663,  0.0624],\n",
      "         [ 0.0147,  0.1362,  0.0701,  ..., -0.1691, -0.0964,  0.0875],\n",
      "         [-0.0421,  0.1943,  0.0948,  ..., -0.2349, -0.0023,  0.0824],\n",
      "         ...,\n",
      "         [ 0.0021,  0.0703, -0.0124,  ..., -0.1000, -0.0358,  0.0195],\n",
      "         [-0.0186,  0.0671, -0.0084,  ..., -0.0930, -0.0371, -0.0705],\n",
      "         [-0.0914,  0.2548, -0.3630,  ..., -0.3352, -0.0789, -0.1380]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-6.6422e-02,  2.6970e-01,  3.9855e-02,  ..., -1.9627e-01,\n",
      "           7.9697e-03, -8.6922e-02],\n",
      "         [-5.0360e-02, -2.2137e-02,  3.0775e-01,  ...,  3.3438e-01,\n",
      "           4.5479e-02,  6.3077e-02],\n",
      "         [-5.7621e-02, -3.2859e-04,  4.2735e-02,  ..., -2.1111e-01,\n",
      "          -9.2858e-02, -1.6754e-01],\n",
      "         ...,\n",
      "         [-2.0553e-02,  7.6824e-02, -3.5095e-02,  ..., -1.4737e-01,\n",
      "          -1.1832e-02, -8.2273e-02],\n",
      "         [-2.2863e-02,  1.6682e-01, -1.5790e-01,  ..., -2.3066e-01,\n",
      "          -1.8841e-02,  2.4980e-02],\n",
      "         [-7.7622e-03,  7.3335e-02, -1.9401e-02,  ..., -1.1206e-01,\n",
      "          -4.2609e-02, -7.2986e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0965,  0.0399,  0.0194,  ..., -0.2701, -0.0177,  0.0252],\n",
      "         [ 0.0267,  0.0255,  0.0581,  ..., -0.3400, -0.0447,  0.0690],\n",
      "         [-0.0359,  0.4556,  0.0250,  ..., -0.4744,  0.0844,  0.0492],\n",
      "         ...,\n",
      "         [-0.0134,  0.0479,  0.0128,  ..., -0.0750, -0.0500, -0.0879],\n",
      "         [-0.0783,  0.1273, -0.0341,  ..., -0.1075, -0.0614,  0.0263],\n",
      "         [-0.0148,  0.0670, -0.0150,  ..., -0.0931, -0.0333, -0.0580]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0157,  0.0473,  0.0101,  ..., -0.1684, -0.0311,  0.0401],\n",
      "         [ 0.0606,  0.1890,  0.0539,  ..., -0.1941,  0.1739,  0.2492],\n",
      "         [-0.0672, -0.0378, -0.0134,  ..., -0.3837, -0.0285,  0.0709],\n",
      "         ...,\n",
      "         [-0.0570,  0.1694,  0.0492,  ..., -0.0725, -0.0727, -0.2172],\n",
      "         [-0.0170,  0.0630, -0.0242,  ..., -0.1086, -0.0345,  0.0338],\n",
      "         [-0.0179,  0.0727, -0.0063,  ..., -0.0939, -0.0366, -0.0729]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0413,  0.1470, -0.0389,  ..., -0.1454, -0.0400,  0.0023],\n",
      "         [ 0.0243,  0.2951,  0.0011,  ..., -0.4795, -0.0340, -0.0342],\n",
      "         [ 0.1052,  0.2047, -0.1017,  ..., -0.3722,  0.0531,  0.0431],\n",
      "         ...,\n",
      "         [ 0.0048,  0.0679, -0.0294,  ..., -0.1174, -0.0391, -0.0033],\n",
      "         [-0.0804,  0.1317, -0.0318,  ..., -0.0890, -0.0456, -0.1674],\n",
      "         [ 0.0600,  0.1042, -0.1461,  ...,  0.0370,  0.1210, -0.1046]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.1620,  0.0443,  0.0053,  ..., -0.3244,  0.0029,  0.0419],\n",
      "         [ 0.0164,  0.2000, -0.0606,  ..., -0.3308,  0.0732,  0.1576],\n",
      "         [-0.1859,  0.0274, -0.0375,  ..., -0.3138, -0.1438,  0.0456],\n",
      "         ...,\n",
      "         [ 0.0683,  0.0507, -0.2163,  ...,  0.1047,  0.0480, -0.1540],\n",
      "         [-0.0154,  0.0720, -0.0081,  ..., -0.0889, -0.0326, -0.0580],\n",
      "         [-0.0173,  0.0662, -0.0144,  ..., -0.1118, -0.0422, -0.0700]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0489,  0.1651,  0.0845,  ..., -0.2767,  0.0013,  0.0902],\n",
      "         [-0.0495,  0.0009,  0.0102,  ..., -0.2487,  0.0553,  0.0607],\n",
      "         [-0.0221, -0.1089, -0.0413,  ..., -0.3653, -0.1276, -0.0088],\n",
      "         ...,\n",
      "         [-0.0296,  0.0729, -0.0123,  ..., -0.0955, -0.0451, -0.0673],\n",
      "         [-0.0288,  0.0492, -0.0267,  ..., -0.1227, -0.0524, -0.0803],\n",
      "         [-0.0169,  0.0608, -0.0168,  ..., -0.0962, -0.0500, -0.0571]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0884,  0.1092, -0.1862,  ..., -0.4096, -0.0344,  0.0721],\n",
      "         [ 0.1092,  0.2209,  0.1262,  ..., -0.2226, -0.1013,  0.1922],\n",
      "         [ 0.0073,  0.2127, -0.0214,  ..., -0.5097,  0.0175, -0.0700],\n",
      "         ...,\n",
      "         [-0.0178,  0.0586, -0.0232,  ..., -0.1102, -0.0072, -0.0760],\n",
      "         [-0.0271,  0.0681, -0.0141,  ..., -0.0937, -0.0411, -0.0655],\n",
      "         [ 0.0124,  0.0568, -0.0288,  ..., -0.0983, -0.0325, -0.0443]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0276, -0.0016,  0.1470,  ..., -0.3062,  0.1510,  0.0451],\n",
      "         [ 0.1202, -0.0887, -0.0010,  ..., -0.2832,  0.1560,  0.0875],\n",
      "         [ 0.0601,  0.0120,  0.0232,  ..., -0.2214,  0.0217,  0.0253],\n",
      "         ...,\n",
      "         [-0.0266,  0.0686, -0.0173,  ..., -0.0971, -0.0378, -0.0455],\n",
      "         [-0.0166,  0.0879, -0.0109,  ..., -0.1181, -0.0428, -0.0832],\n",
      "         [-0.0204,  0.0747, -0.0011,  ..., -0.0956, -0.0380, -0.0336]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0044,  0.1595,  0.1109,  ..., -0.2626, -0.0389,  0.0222],\n",
      "         [ 0.1139,  0.0536,  0.1017,  ..., -0.1503, -0.0025,  0.2243],\n",
      "         [-0.0997,  0.1577,  0.0528,  ..., -0.1456, -0.0807,  0.4548],\n",
      "         ...,\n",
      "         [-0.0755, -0.3468,  0.3130,  ..., -0.0161, -0.0576, -0.0140],\n",
      "         [ 0.0249,  0.5466, -0.1030,  ..., -0.8833,  0.0708, -0.0012],\n",
      "         [-0.0289,  0.0581, -0.0174,  ..., -0.1029, -0.0435, -0.0676]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0274,  0.0739,  0.0510,  ..., -0.2763, -0.0670,  0.1695],\n",
      "         [ 0.1464, -0.0038,  0.0287,  ..., -0.4168, -0.0156,  0.2198],\n",
      "         [ 0.0658, -0.0233,  0.2230,  ..., -0.9813,  0.1081,  0.4268],\n",
      "         ...,\n",
      "         [ 0.1552, -0.1553, -0.1963,  ..., -0.1226,  0.2282, -0.4507],\n",
      "         [-0.2290,  0.4101, -0.0939,  ..., -0.3710, -0.0237,  0.0279],\n",
      "         [ 0.1778,  0.0142, -0.2870,  ...,  0.0404,  0.0089, -0.5456]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-8.0253e-02,  3.9547e-02,  7.2975e-03,  ..., -2.8760e-01,\n",
      "          -2.0762e-02,  2.2783e-02],\n",
      "         [ 7.1481e-02,  1.5929e-01,  6.6091e-02,  ..., -3.2339e-01,\n",
      "          -2.5540e-02,  1.0723e-01],\n",
      "         [ 6.0325e-02,  1.8119e-02,  3.3749e-02,  ..., -5.1480e-01,\n",
      "           1.0177e-04, -5.2829e-02],\n",
      "         ...,\n",
      "         [ 4.7141e-02,  2.5672e-01,  6.4249e-02,  ..., -8.8375e-02,\n",
      "          -2.9096e-02, -3.3485e-01],\n",
      "         [-2.3214e-02,  6.4351e-02, -7.9932e-03,  ..., -9.8383e-02,\n",
      "          -4.3733e-02, -5.6109e-02],\n",
      "         [-3.2080e-02,  6.8874e-02, -1.9728e-03,  ..., -1.0857e-01,\n",
      "          -3.5676e-02,  2.8939e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.2357,  0.0816, -0.0421,  ..., -0.1846,  0.0682,  0.1251],\n",
      "         [-0.0656,  0.0224, -0.0658,  ..., -0.0680,  0.0182,  0.2138],\n",
      "         [-0.1147, -0.0338, -0.0620,  ..., -0.1613,  0.0689,  0.0416],\n",
      "         ...,\n",
      "         [ 0.1117,  0.2962, -0.2061,  ...,  0.1601,  0.2562, -0.2088],\n",
      "         [-0.0437,  0.2943,  0.0252,  ..., -0.1201, -0.0791, -0.3599],\n",
      "         [ 0.0274,  0.1830,  0.0981,  ..., -0.1236, -0.0280, -0.2552]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0514,  0.1180, -0.0905,  ..., -0.2814, -0.0453,  0.1514],\n",
      "         [ 0.0711,  0.0775,  0.1070,  ..., -0.1849, -0.1014,  0.1445],\n",
      "         [-0.0587, -0.0675,  0.0787,  ..., -0.4294,  0.0409,  0.0230],\n",
      "         ...,\n",
      "         [ 0.0759,  0.3890,  0.0298,  ..., -0.0654, -0.0293, -0.2329],\n",
      "         [-0.0313,  0.0595, -0.0125,  ..., -0.1077, -0.0169, -0.0591],\n",
      "         [-0.1850,  0.3646,  0.2009,  ..., -0.4941, -0.0700,  0.1024]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0363,  0.0925, -0.0185,  ..., -0.2471, -0.0376,  0.0880],\n",
      "         [ 0.0757,  0.0328,  0.0084,  ..., -0.1474,  0.0825,  0.1440],\n",
      "         [ 0.0921,  0.0518,  0.0390,  ..., -0.3475,  0.0639,  0.0189],\n",
      "         ...,\n",
      "         [-0.0210,  0.0412, -0.0215,  ..., -0.1043, -0.0430,  0.0152],\n",
      "         [-0.0736,  0.1476, -0.0454,  ..., -0.1046, -0.0727, -0.1641],\n",
      "         [-0.0279,  0.0692, -0.0214,  ..., -0.1072, -0.0410, -0.0715]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0639,  0.1965, -0.0016,  ..., -0.5041,  0.2188,  0.4569],\n",
      "         [ 0.0347, -0.1303,  0.1195,  ..., -0.2679,  0.0486,  0.0329],\n",
      "         [ 0.0912, -0.2733,  0.0571,  ..., -1.2373,  0.1095, -0.3127],\n",
      "         ...,\n",
      "         [ 0.0240,  0.1126, -0.1251,  ..., -0.2203,  0.0187, -0.2747],\n",
      "         [ 0.0981, -0.0226, -0.1700,  ..., -0.1312,  0.1265, -0.1441],\n",
      "         [-0.0090,  0.0942,  0.0047,  ..., -0.0975, -0.0145, -0.0595]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0409,  0.1754,  0.1228,  ..., -0.1044,  0.0370,  0.0876],\n",
      "         [ 0.1751,  0.3247, -0.0804,  ...,  0.7901,  0.1116,  0.2018],\n",
      "         [ 0.1352,  0.0283,  0.1620,  ..., -0.1685,  0.1125,  0.1561],\n",
      "         ...,\n",
      "         [-0.0706,  0.1975, -0.0476,  ..., -0.1143, -0.0386,  0.1970],\n",
      "         [-0.0553,  0.1174, -0.0244,  ..., -0.0829, -0.0397, -0.1786],\n",
      "         [-0.0126,  0.0683, -0.0085,  ..., -0.1091, -0.0389,  0.0175]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0896,  0.0301, -0.0114,  ..., -0.2388, -0.0560,  0.0046],\n",
      "         [-0.0667,  0.1439,  0.0722,  ..., -0.1603,  0.0153,  0.0595],\n",
      "         [-0.0563,  0.0703, -0.0331,  ..., -0.3824, -0.0806, -0.0228],\n",
      "         ...,\n",
      "         [-0.0321,  0.0423, -0.0231,  ..., -0.1059, -0.0450, -0.0736],\n",
      "         [-0.0118,  0.0917,  0.0033,  ..., -0.0985, -0.0346, -0.0692],\n",
      "         [-0.0261,  0.0543, -0.0205,  ..., -0.0991, -0.0463,  0.0050]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0342,  0.0426,  0.1489,  ..., -0.3178, -0.0210, -0.0282],\n",
      "         [-0.1532,  0.1435,  0.0973,  ..., -0.0353,  0.1455,  0.1464],\n",
      "         [-0.0408,  0.0695,  0.0933,  ..., -0.3261, -0.0174, -0.1185],\n",
      "         ...,\n",
      "         [ 0.0106,  0.0674, -0.0209,  ..., -0.0902, -0.0375, -0.0659],\n",
      "         [-0.0177,  0.0707, -0.0233,  ..., -0.1122, -0.0304, -0.0739],\n",
      "         [ 0.0129,  0.0735, -0.0065,  ..., -0.0917, -0.0428, -0.0593]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0043,  0.1033, -0.0573,  ..., -0.3128, -0.0581,  0.2144],\n",
      "         [-0.0056, -0.1335,  0.0626,  ..., -0.2679, -0.0324,  0.1284],\n",
      "         [ 0.0770, -0.0856, -0.0301,  ..., -0.4008, -0.0388,  0.1456],\n",
      "         ...,\n",
      "         [-0.0140,  0.0417, -0.0024,  ..., -0.1302, -0.0446, -0.0810],\n",
      "         [-0.0282,  0.0715, -0.0149,  ..., -0.0978, -0.0177, -0.0655],\n",
      "         [-0.0234,  0.0560, -0.0110,  ..., -0.1095, -0.0360, -0.0599]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0095, -0.1340,  0.1129,  ..., -0.5423, -0.0271, -0.0113],\n",
      "         [ 0.1292, -0.1219,  0.2394,  ..., -0.6872, -0.0930,  0.2188],\n",
      "         [-0.1371, -0.1032,  0.2101,  ..., -0.2258,  0.1981,  0.3427],\n",
      "         ...,\n",
      "         [-0.0230,  0.0730, -0.0259,  ..., -0.1006, -0.0472, -0.0563],\n",
      "         [-0.0270,  0.0689, -0.0186,  ..., -0.1008, -0.0336, -0.0686],\n",
      "         [-0.0921,  0.1288, -0.0164,  ..., -0.0908, -0.0551, -0.1763]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0226,  0.1289,  0.0704,  ..., -0.2093, -0.0648,  0.1527],\n",
      "         [ 0.1572,  0.1128,  0.1546,  ..., -0.1890,  0.1259,  0.5896],\n",
      "         [ 0.0285,  0.1170, -0.0273,  ..., -0.2910, -0.1366,  0.1089],\n",
      "         ...,\n",
      "         [-0.0037,  0.1035, -0.0247,  ..., -0.1351, -0.0155, -0.1405],\n",
      "         [-0.0075,  0.1107, -0.0029,  ..., -0.0696, -0.0887, -0.2282],\n",
      "         [-0.0263,  0.0721,  0.0091,  ..., -0.1156, -0.0453, -0.0664]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0832,  0.0115,  0.0160,  ..., -0.2325, -0.0327,  0.0280],\n",
      "         [ 0.0271, -0.0413, -0.0275,  ..., -0.0821,  0.1052,  0.0434],\n",
      "         [ 0.0412, -0.0584, -0.0491,  ..., -0.5312, -0.0098, -0.0423],\n",
      "         ...,\n",
      "         [ 0.0864,  0.1654, -0.0565,  ..., -0.2036, -0.0295,  0.0601],\n",
      "         [-0.0252,  0.0539, -0.0127,  ..., -0.0984, -0.0405, -0.0693],\n",
      "         [-0.0145,  0.3019,  0.0610,  ..., -0.0462, -0.1795, -0.2972]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0195,  0.1423,  0.0309,  ..., -0.2473, -0.0083,  0.0078],\n",
      "         [ 0.0008,  0.1850, -0.0059,  ..., -0.1108, -0.0149,  0.1796],\n",
      "         [-0.1100,  0.2549,  0.0345,  ..., -0.0566, -0.0113,  0.0497],\n",
      "         ...,\n",
      "         [-0.0242,  0.1527, -0.0007,  ..., -0.0976, -0.0628, -0.2279],\n",
      "         [-0.0685,  0.0429,  0.0019,  ..., -0.0612,  0.0271, -0.1867],\n",
      "         [-0.0109,  0.0603, -0.0122,  ..., -0.1005, -0.0218, -0.0496]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0200,  0.0112,  0.0175,  ..., -0.2976, -0.0471,  0.0610],\n",
      "         [-0.0825,  0.1929,  0.0661,  ..., -0.0873,  0.0569,  0.1038],\n",
      "         [ 0.0858,  0.1197,  0.0038,  ..., -0.3891,  0.1266,  0.1095],\n",
      "         ...,\n",
      "         [-0.0191,  0.0760, -0.0193,  ..., -0.0962, -0.0499, -0.0741],\n",
      "         [ 0.0123,  0.0391, -0.0148,  ..., -0.1014, -0.0390, -0.0657],\n",
      "         [-0.0144,  0.0692,  0.0089,  ..., -0.1018, -0.0405, -0.0602]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0967,  0.0467, -0.0581,  ..., -0.5012,  0.0344, -0.0980],\n",
      "         [ 0.0345,  0.0270, -0.0770,  ..., -0.4093, -0.0032, -0.0067],\n",
      "         [-0.0071, -0.0941,  0.0377,  ..., -0.5139, -0.0464, -0.2626],\n",
      "         ...,\n",
      "         [ 0.0170,  0.0714, -0.0214,  ..., -0.1057, -0.0335, -0.0667],\n",
      "         [-0.0312,  0.0827, -0.0238,  ..., -0.1121, -0.0459,  0.0145],\n",
      "         [ 0.0015,  0.0665, -0.0260,  ..., -0.0906, -0.0504, -0.0710]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1206, -0.3823,  0.0749,  ..., -0.3339,  0.0759,  0.4929],\n",
      "         [-0.0363,  0.0579,  0.1844,  ..., -0.5394,  0.0328,  0.1228],\n",
      "         [ 0.0797,  0.2358,  0.0346,  ..., -0.5688,  0.0745,  0.0796],\n",
      "         ...,\n",
      "         [-0.0224,  0.1010, -0.0157,  ..., -0.1006, -0.0396, -0.0745],\n",
      "         [-0.0299,  0.0702, -0.0140,  ..., -0.0926, -0.0421, -0.0709],\n",
      "         [-0.0218,  0.0453, -0.0222,  ..., -0.0917, -0.0386, -0.0448]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0293,  0.0401, -0.0231,  ..., -0.3144,  0.0234,  0.0325],\n",
      "         [ 0.0117, -0.1416,  0.0908,  ..., -0.2892,  0.1193,  0.0794],\n",
      "         [-0.0535,  0.0431,  0.0327,  ..., -0.3356, -0.0319,  0.0253],\n",
      "         ...,\n",
      "         [-0.0374,  0.0831, -0.0232,  ..., -0.1676, -0.0583, -0.1198],\n",
      "         [-0.0202,  0.0716, -0.0040,  ..., -0.0923, -0.0373, -0.0709],\n",
      "         [ 0.0776,  0.2274, -0.0584,  ..., -0.1732, -0.0083, -0.3580]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0923,  0.0562, -0.0046,  ..., -0.3551, -0.0213,  0.0604],\n",
      "         [-0.0051, -0.1040,  0.3522,  ..., -1.1216,  0.1308,  0.4145],\n",
      "         [-0.0117,  0.1454, -0.1569,  ..., -0.5095, -0.0595,  0.0271],\n",
      "         ...,\n",
      "         [-0.0478,  0.1319, -0.0089,  ..., -0.0808, -0.0276, -0.1640],\n",
      "         [-0.1298,  0.2110, -0.1961,  ..., -0.0868, -0.0330, -0.0096],\n",
      "         [ 0.0635,  0.1967,  0.0444,  ..., -0.1599,  0.0170, -0.2607]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0714,  0.1903,  0.1048,  ..., -0.2506,  0.0459, -0.1731],\n",
      "         [-0.2147,  0.0286,  0.0493,  ..., -0.0104,  0.2519,  0.2677],\n",
      "         [ 0.1688,  0.0528,  0.0909,  ..., -0.5255, -0.0461,  0.0307],\n",
      "         ...,\n",
      "         [-0.0110,  0.0721, -0.0244,  ..., -0.1093, -0.0451, -0.0657],\n",
      "         [-0.0278,  0.0715, -0.0138,  ..., -0.0990, -0.0390, -0.0683],\n",
      "         [-0.0221,  0.0645, -0.0010,  ..., -0.0894, -0.0402, -0.0723]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0811,  0.0607,  0.1082,  ..., -0.4869,  0.0417,  0.0870],\n",
      "         [ 0.1500, -0.1185,  0.0432,  ..., -0.2827,  0.0023,  0.0912],\n",
      "         [ 0.0074, -0.0723,  0.0230,  ..., -0.7303, -0.0103,  0.0447],\n",
      "         ...,\n",
      "         [ 0.0130,  0.1437,  0.0468,  ..., -0.0839, -0.1076, -0.2064],\n",
      "         [-0.0152,  0.0691, -0.0149,  ..., -0.1012, -0.0279, -0.0690],\n",
      "         [ 0.0132,  0.0986,  0.0018,  ..., -0.1012, -0.0333, -0.0640]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0141,  0.1475,  0.0995,  ..., -0.5244,  0.0131,  0.1009],\n",
      "         [ 0.2050, -0.1252,  0.0156,  ..., -0.5261,  0.0598,  0.0324],\n",
      "         [ 0.0384,  0.0190,  0.0085,  ..., -0.4303,  0.0794,  0.0965],\n",
      "         ...,\n",
      "         [ 0.0371,  0.2898,  0.0554,  ..., -0.0914, -0.1416, -0.0717],\n",
      "         [-0.0338,  0.0755, -0.0087,  ..., -0.0950, -0.0486, -0.0786],\n",
      "         [ 0.0127,  0.0525, -0.0231,  ..., -0.1110, -0.0233, -0.0750]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0439,  0.0418,  0.0211,  ..., -0.1614, -0.0052,  0.0171],\n",
      "         [ 0.2903, -0.2426,  0.1118,  ..., -0.0784,  0.0848,  0.1695],\n",
      "         [ 0.1723, -0.1105,  0.1271,  ..., -0.3740,  0.0244, -0.0019],\n",
      "         ...,\n",
      "         [ 0.0086,  0.1040, -0.0316,  ..., -0.0860, -0.0102, -0.0773],\n",
      "         [-0.0125,  0.0682, -0.0095,  ..., -0.0894, -0.0352, -0.0702],\n",
      "         [ 0.0624,  0.0902,  0.0808,  ..., -0.2129,  0.1903, -0.6912]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0812,  0.1250,  0.1110,  ..., -0.1431, -0.1100,  0.0733],\n",
      "         [ 0.1408,  0.1238, -0.0217,  ..., -0.0637,  0.0400,  0.1010],\n",
      "         [ 0.0082,  0.0500,  0.0090,  ..., -0.1598, -0.1454, -0.0856],\n",
      "         ...,\n",
      "         [ 0.0233,  0.0316, -0.1021,  ...,  0.0117,  0.0275, -0.0131],\n",
      "         [-0.0228,  0.0720,  0.0011,  ..., -0.0989, -0.0425, -0.0630],\n",
      "         [ 0.0494,  0.0369, -0.0280,  ...,  0.2271,  0.1027, -0.2338]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0990,  0.0171,  0.0161,  ..., -0.2412, -0.0352, -0.0153],\n",
      "         [-0.0198,  0.1809,  0.4349,  ..., -0.7342,  0.2375, -0.0316],\n",
      "         [-0.0277,  0.0435,  0.0612,  ..., -0.3107,  0.0979,  0.0198],\n",
      "         ...,\n",
      "         [ 0.0109,  0.0542, -0.0066,  ..., -0.1018, -0.0361, -0.0644],\n",
      "         [-0.0163,  0.0631, -0.0111,  ..., -0.0967, -0.0409, -0.0659],\n",
      "         [-0.1105,  0.1235, -0.0091,  ..., -0.0798, -0.0445, -0.1695]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1078,  0.0718,  0.0180,  ..., -0.1826, -0.0230,  0.0327],\n",
      "         [-0.1073,  0.1814,  0.4214,  ...,  0.0067, -0.1633,  0.1053],\n",
      "         [-0.1872,  0.3321, -0.0308,  ..., -0.5656,  0.0061, -0.1486],\n",
      "         ...,\n",
      "         [-0.0301,  0.0609, -0.0103,  ..., -0.0974, -0.0153, -0.0709],\n",
      "         [-0.0215,  0.0989, -0.0095,  ..., -0.1009, -0.0336, -0.0718],\n",
      "         [-0.0132,  0.0695, -0.0122,  ..., -0.0973, -0.0258, -0.0692]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0548,  0.0324,  0.0140,  ..., -0.1692, -0.0431,  0.0137],\n",
      "         [ 0.3219,  0.0127,  0.0954,  ..., -0.1541, -0.0552,  0.0345],\n",
      "         [ 0.0806,  0.1072,  0.0793,  ..., -0.2815, -0.0021,  0.3043],\n",
      "         ...,\n",
      "         [-0.0257,  0.0613, -0.0114,  ..., -0.0928, -0.0323, -0.0693],\n",
      "         [-0.0643,  0.1185, -0.0159,  ..., -0.0889, -0.0518, -0.1655],\n",
      "         [-0.0365,  0.0919, -0.0249,  ..., -0.1174, -0.0485, -0.1745]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0664,  0.0607,  0.0041,  ..., -0.2252, -0.0466,  0.0158],\n",
      "         [ 0.3137,  0.0205,  0.0265,  ..., -0.1324,  0.0768,  0.1869],\n",
      "         [-0.0094,  0.0014,  0.0434,  ..., -0.3725,  0.0161, -0.0341],\n",
      "         ...,\n",
      "         [-0.0208,  0.0671, -0.0111,  ..., -0.0957, -0.0373, -0.0555],\n",
      "         [-0.0767,  0.1191, -0.0041,  ..., -0.0791, -0.0277,  0.0319],\n",
      "         [-0.0773,  0.1217,  0.0214,  ..., -0.0730, -0.0482, -0.1837]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-7.3230e-03,  9.8019e-02,  5.9214e-02,  ..., -3.8230e-01,\n",
      "           2.4126e-03,  2.6270e-02],\n",
      "         [ 1.5082e-01,  8.6067e-02,  1.6000e-01,  ..., -9.0226e-01,\n",
      "           5.3014e-02,  3.1217e-01],\n",
      "         [ 2.6597e-01,  7.5573e-02,  9.3693e-02,  ..., -2.1033e+00,\n",
      "           4.3333e-02,  3.0919e-01],\n",
      "         ...,\n",
      "         [-1.5435e-02,  6.9987e-02, -1.4432e-02,  ..., -9.3684e-02,\n",
      "          -3.8420e-02, -7.5125e-02],\n",
      "         [-1.9458e-02,  6.8650e-02, -2.2144e-02,  ..., -8.8276e-02,\n",
      "          -1.8861e-02, -6.9373e-02],\n",
      "         [-3.5848e-02,  1.5357e-01,  5.7412e-03,  ..., -5.4852e-02,\n",
      "          -1.2347e-03, -2.0971e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1361, -0.0434, -0.0802,  ..., -0.2089, -0.1085, -0.0041],\n",
      "         [ 0.0391, -0.1529, -0.0735,  ..., -0.3168, -0.1387,  0.1143],\n",
      "         [ 0.0832, -0.0921,  0.0083,  ..., -0.3791, -0.0688,  0.0231],\n",
      "         ...,\n",
      "         [-0.0216,  0.0627, -0.0264,  ..., -0.1190, -0.0434, -0.0657],\n",
      "         [-0.0252,  0.0688, -0.0019,  ..., -0.0904, -0.0421, -0.0552],\n",
      "         [-0.0119,  0.1608,  0.0467,  ..., -0.0658, -0.1145, -0.0929]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0978,  0.0424,  0.0253,  ..., -0.2220, -0.0318,  0.0173],\n",
      "         [ 0.2568,  0.2681, -0.0020,  ..., -0.1638, -0.0827, -0.0087],\n",
      "         [ 0.0302,  0.1647, -0.0462,  ..., -0.5283, -0.0197, -0.0563],\n",
      "         ...,\n",
      "         [ 0.0892,  0.2696,  0.0454,  ..., -0.6236,  0.2506, -0.0623],\n",
      "         [-0.0242,  0.0738, -0.0209,  ..., -0.1010, -0.0137, -0.0636],\n",
      "         [-0.0140,  0.0794, -0.0191,  ..., -0.0958, -0.0494, -0.0830]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0664,  0.0435,  0.1155,  ..., -0.2337,  0.0967, -0.0621],\n",
      "         [ 0.0217, -0.0989,  0.1515,  ..., -0.5546, -0.1234, -0.3184],\n",
      "         [-0.1171,  0.0008,  0.0736,  ..., -0.2349,  0.0295, -0.2562],\n",
      "         ...,\n",
      "         [ 0.0096,  0.0967,  0.0029,  ..., -0.1108, -0.0425, -0.0768],\n",
      "         [-0.0054,  0.0479, -0.0094,  ..., -0.1022, -0.0410, -0.0705],\n",
      "         [ 0.0152,  0.1026, -0.0212,  ..., -0.1417, -0.0725, -0.0733]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0309,  0.0564,  0.0690,  ..., -0.1927,  0.0038,  0.0379],\n",
      "         [ 0.0956,  0.0239, -0.0096,  ..., -0.0629, -0.0737,  0.0893],\n",
      "         [-0.0327, -0.0631,  0.0846,  ..., -0.4579,  0.1281, -0.0144],\n",
      "         ...,\n",
      "         [-0.0224,  0.0631, -0.0120,  ..., -0.0971, -0.0179, -0.0669],\n",
      "         [-0.0128,  0.1079,  0.0078,  ..., -0.0586, -0.0321, -0.1316],\n",
      "         [-0.0058,  0.1735,  0.0325,  ..., -0.0838, -0.0547, -0.2437]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-7.3420e-02,  6.3760e-02,  3.6842e-02,  ..., -2.4435e-01,\n",
      "          -3.1678e-02,  1.1564e-02],\n",
      "         [ 5.0941e-02,  5.5580e-05,  1.1478e-01,  ..., -2.2794e-01,\n",
      "           5.5756e-02,  1.0171e-01],\n",
      "         [-7.9919e-02,  4.9400e-02,  1.0568e-01,  ..., -2.4431e-01,\n",
      "           1.3321e-02,  8.9496e-03],\n",
      "         ...,\n",
      "         [ 2.1402e-01,  2.1026e-01, -2.6741e-02,  ..., -3.3977e-02,\n",
      "           2.2484e-01, -1.6319e-01],\n",
      "         [-2.0140e-02,  7.0864e-02, -2.1489e-02,  ..., -9.8092e-02,\n",
      "          -3.2008e-02, -7.3333e-02],\n",
      "         [-8.5912e-02,  1.1247e-01, -1.8819e-02,  ..., -1.0162e-01,\n",
      "          -7.3279e-02,  2.1122e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0076,  0.0651,  0.0100,  ..., -0.1380, -0.0408, -0.0038],\n",
      "         [ 0.0908,  0.0015,  0.0716,  ..., -0.0717, -0.0923,  0.0326],\n",
      "         [-0.0012, -0.0623,  0.0423,  ..., -0.2504, -0.0063,  0.0865],\n",
      "         ...,\n",
      "         [ 0.0150,  0.0423, -0.0085,  ..., -0.1171, -0.0425, -0.0585],\n",
      "         [-0.0241,  0.0660, -0.0153,  ..., -0.0958, -0.0328, -0.0533],\n",
      "         [ 0.0121,  0.0544, -0.0150,  ..., -0.0989, -0.0189, -0.0528]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0827,  0.0309, -0.0118,  ..., -0.1488, -0.0205,  0.0097],\n",
      "         [ 0.2484,  0.0213,  0.1588,  ..., -0.0460, -0.0323,  0.3218],\n",
      "         [ 0.0590,  0.1324,  0.0717,  ..., -0.0726,  0.0423,  0.1139],\n",
      "         ...,\n",
      "         [ 0.1311,  0.0511, -0.0758,  ..., -0.2284, -0.0123, -0.2315],\n",
      "         [-0.3199,  0.5136, -0.0968,  ..., -0.1676, -0.0700, -0.3886],\n",
      "         [-0.0256,  0.0722, -0.0124,  ..., -0.0917, -0.0449, -0.0771]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0357,  0.0044,  0.0700,  ..., -0.1523, -0.0401,  0.0743],\n",
      "         [ 0.0809, -0.0325,  0.0325,  ..., -0.0585, -0.0245,  0.1555],\n",
      "         [ 0.0435, -0.0801,  0.0776,  ..., -0.1700, -0.0874,  0.1088],\n",
      "         ...,\n",
      "         [-0.0820,  0.0534, -0.0387,  ..., -0.1000, -0.0677, -0.1892],\n",
      "         [-0.0159,  0.0636, -0.0029,  ..., -0.0954, -0.0358, -0.0598],\n",
      "         [-0.0242,  0.0685, -0.0101,  ..., -0.1069, -0.0145, -0.0725]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0648,  0.0751,  0.0297,  ..., -0.2393,  0.0051,  0.0074],\n",
      "         [ 0.1953,  0.0137, -0.0320,  ..., -0.1449, -0.0428,  0.2155],\n",
      "         [ 0.1413,  0.0148,  0.0511,  ..., -0.3443,  0.1110,  0.0291],\n",
      "         ...,\n",
      "         [-0.0128,  0.0691, -0.0192,  ..., -0.1094, -0.0337, -0.0699],\n",
      "         [-0.0295,  0.0615, -0.0125,  ..., -0.1029, -0.0314, -0.0709],\n",
      "         [ 0.1599,  0.1044, -0.1236,  ..., -0.0858, -0.1572, -0.2514]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.3599,  0.1409, -0.1716,  ..., -0.9273,  0.0114,  0.3271],\n",
      "         [ 0.2871, -0.1015,  0.0447,  ..., -0.0995,  0.1731,  0.4465],\n",
      "         [ 0.0051,  0.1330,  0.1093,  ..., -0.2449, -0.0834,  0.1151],\n",
      "         ...,\n",
      "         [-0.0193,  0.1903, -0.0585,  ..., -0.1029,  0.1928, -0.2339],\n",
      "         [-0.0257,  0.0734, -0.0191,  ..., -0.0909, -0.0547, -0.0820],\n",
      "         [ 0.0032,  0.1939, -0.0115,  ..., -0.0592, -0.0481, -0.2232]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0857,  0.0528,  0.0027,  ..., -0.0967,  0.0054,  0.0122],\n",
      "         [-0.0408,  0.1376, -0.0048,  ..., -0.0846, -0.0031,  0.0561],\n",
      "         [-0.0798,  0.1469,  0.0122,  ..., -0.2805, -0.0350, -0.0075],\n",
      "         ...,\n",
      "         [-0.0171,  0.0605, -0.0107,  ..., -0.0903, -0.0265, -0.0550],\n",
      "         [-0.0130,  0.0698, -0.0132,  ..., -0.0907, -0.0422, -0.0694],\n",
      "         [-0.0273,  0.0716, -0.0071,  ..., -0.0729, -0.0369,  0.0351]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0729,  0.0464, -0.0011,  ..., -0.1405, -0.0268,  0.0152],\n",
      "         [ 0.0757,  0.0432,  0.0595,  ...,  0.1558, -0.0141,  0.0715],\n",
      "         [-0.1621,  0.0624,  0.0066,  ...,  0.2197,  0.0949, -0.2146],\n",
      "         ...,\n",
      "         [ 0.2639,  0.1966,  0.0379,  ..., -0.3758,  0.0866, -0.0033],\n",
      "         [-0.0151,  0.0623, -0.0232,  ..., -0.0907, -0.0537, -0.0614],\n",
      "         [ 0.1458, -0.0552, -0.3924,  ...,  0.3330,  0.0392, -0.1861]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1918,  0.1157,  0.0049,  ..., -0.3407, -0.0149,  0.0146],\n",
      "         [ 0.0609,  0.0740,  0.0016,  ..., -0.3811,  0.0184,  0.0458],\n",
      "         [ 0.0136,  0.0356, -0.0360,  ..., -0.6454,  0.0482,  0.0217],\n",
      "         ...,\n",
      "         [-0.1366,  0.1341, -0.4336,  ..., -0.1834,  0.2909, -0.1221],\n",
      "         [ 0.0243,  0.0697, -0.0184,  ..., -0.0895, -0.0344,  0.0162],\n",
      "         [-0.0247,  0.0615, -0.0159,  ..., -0.1015, -0.0134, -0.0737]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1446,  0.2205,  0.0328,  ..., -0.2913,  0.0458,  0.1582],\n",
      "         [-0.1678,  0.2049, -0.1407,  ..., -0.4519,  0.0061,  0.0796],\n",
      "         [ 0.0157,  0.1643,  0.0092,  ..., -0.3796, -0.0206,  0.1388],\n",
      "         ...,\n",
      "         [ 0.2586,  0.1238, -0.2083,  ...,  0.2404,  0.0180, -0.2549],\n",
      "         [ 0.0809,  0.7798,  0.0194,  ..., -0.1252,  0.1787, -0.4294],\n",
      "         [-0.0505,  0.0789, -0.0138,  ..., -0.0733, -0.0172, -0.2251]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 5.5658e-04,  2.6515e-02,  1.7679e-04,  ..., -2.6088e-01,\n",
      "           3.6726e-02,  3.0306e-01],\n",
      "         [ 6.1409e-02, -1.9845e-01,  6.2956e-03,  ..., -5.7293e-01,\n",
      "           5.5017e-02,  2.1736e-01],\n",
      "         [-9.6440e-03,  3.8969e-02,  5.5108e-02,  ..., -4.9226e-01,\n",
      "           4.1308e-02,  5.9225e-02],\n",
      "         ...,\n",
      "         [-2.3959e-02,  6.4956e-02, -1.4376e-02,  ..., -1.0373e-01,\n",
      "          -3.9832e-02, -7.6933e-02],\n",
      "         [-4.7912e-03,  5.3596e-02, -1.5724e-02,  ..., -1.0276e-01,\n",
      "          -1.7575e-02, -5.0068e-02],\n",
      "         [-3.9472e-02,  6.8461e-02, -2.2244e-02,  ..., -9.2105e-02,\n",
      "          -1.0261e-02, -7.3789e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1920,  0.1187,  0.0572,  ..., -0.2400,  0.0188,  0.1127],\n",
      "         [ 0.1006,  0.0895,  0.0083,  ...,  0.3227, -0.1515, -0.0072],\n",
      "         [-0.0619, -0.3068,  0.0139,  ..., -0.6799,  0.0942, -0.0029],\n",
      "         ...,\n",
      "         [-0.0221,  0.0624, -0.0237,  ..., -0.0933, -0.0184, -0.0570],\n",
      "         [-0.0221,  0.0632, -0.0097,  ..., -0.0909, -0.0386, -0.0629],\n",
      "         [ 0.0243,  0.1423,  0.0624,  ..., -0.0895, -0.1065, -0.2307]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0559, -0.0139,  0.0582,  ..., -0.2554,  0.1623,  0.1323],\n",
      "         [ 0.1829, -0.0281,  0.1052,  ..., -0.3715,  0.0155, -0.1519],\n",
      "         [-0.0903,  0.0080, -0.0051,  ..., -0.2519, -0.0088,  0.0436],\n",
      "         ...,\n",
      "         [-0.1354,  0.4597, -0.1103,  ..., -0.5535,  0.2556, -0.2635],\n",
      "         [-0.0500,  0.2014, -0.1616,  ..., -0.2376, -0.0205, -0.4600],\n",
      "         [-0.0278,  0.0764, -0.0257,  ..., -0.0871, -0.0475, -0.0782]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0703,  0.0268,  0.0308,  ..., -0.2090, -0.0011,  0.0493],\n",
      "         [-0.1062,  0.0244,  0.1473,  ..., -0.2745, -0.0788,  0.0250],\n",
      "         [ 0.0303,  0.0879, -0.0156,  ..., -0.2877, -0.0035, -0.0653],\n",
      "         ...,\n",
      "         [-0.0246,  0.0632, -0.0218,  ..., -0.0968, -0.0443, -0.0609],\n",
      "         [-0.0291,  0.0670, -0.0114,  ..., -0.0904, -0.0345,  0.0158],\n",
      "         [-0.0663,  0.1648, -0.0365,  ..., -0.0444, -0.0577, -0.1579]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0340,  0.0550, -0.0054,  ..., -0.2960, -0.0008,  0.1242],\n",
      "         [ 0.1374,  0.0033,  0.1679,  ..., -0.1474, -0.0658,  0.1873],\n",
      "         [ 0.0678,  0.1222,  0.0624,  ..., -0.2652, -0.0054,  0.0421],\n",
      "         ...,\n",
      "         [ 0.0040,  0.0484,  0.0370,  ..., -0.0936, -0.0585, -0.1805],\n",
      "         [-0.0091,  0.0636, -0.0207,  ..., -0.1036, -0.0343, -0.0700],\n",
      "         [-0.0153,  0.0693, -0.0145,  ..., -0.0961, -0.0394, -0.0704]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0626,  0.2132,  0.0165,  ..., -0.2415,  0.1056,  0.2244],\n",
      "         [-0.0270,  0.2152, -0.0427,  ..., -0.0977,  0.0686,  0.0184],\n",
      "         [-0.0056,  0.0302,  0.0019,  ..., -0.6301, -0.0490,  0.0264],\n",
      "         ...,\n",
      "         [-0.0137,  0.0653, -0.0116,  ..., -0.0988, -0.0412, -0.0701],\n",
      "         [-0.0214,  0.0670, -0.0211,  ..., -0.0930, -0.0391, -0.0720],\n",
      "         [-0.0634,  0.1648,  0.1059,  ..., -0.6017, -0.1187,  0.0134]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0123,  0.1628,  0.0294,  ..., -0.0616,  0.0612,  0.0020],\n",
      "         [ 0.0115,  0.0886,  0.0501,  ...,  0.1289,  0.0223,  0.3280],\n",
      "         [ 0.0178,  0.0957,  0.0616,  ...,  0.0198,  0.0527,  0.0766],\n",
      "         ...,\n",
      "         [-0.0240,  0.0386,  0.0167,  ..., -0.0210, -0.0282, -0.1462],\n",
      "         [-0.0142,  0.0695, -0.0159,  ..., -0.0993, -0.0321, -0.0719],\n",
      "         [ 0.0104,  0.0892, -0.0238,  ..., -0.1050, -0.0372,  0.0151]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0369,  0.1271,  0.0446,  ..., -0.2730,  0.0645,  0.1110],\n",
      "         [ 0.1314,  0.0214,  0.1222,  ..., -0.2198,  0.0557,  0.1474],\n",
      "         [ 0.0178,  0.0801,  0.0215,  ..., -0.5068, -0.0598,  0.1567],\n",
      "         ...,\n",
      "         [-0.0401,  0.2115,  0.0384,  ..., -0.0961, -0.0955, -0.1425],\n",
      "         [-0.0211,  0.0652, -0.0154,  ..., -0.0869, -0.0437, -0.0657],\n",
      "         [-0.0223,  0.0942,  0.0099,  ..., -0.0971, -0.0374, -0.0628]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0405,  0.1288,  0.0506,  ..., -0.1994, -0.0241,  0.0203],\n",
      "         [ 0.2063,  0.0617,  0.0505,  ...,  0.3384,  0.0451,  0.0348],\n",
      "         [ 0.0097, -0.0293,  0.0306,  ..., -0.2125,  0.0827,  0.0158],\n",
      "         ...,\n",
      "         [ 0.1720,  0.3165, -0.0878,  ..., -0.1154, -0.0560, -0.0465],\n",
      "         [-0.0281,  0.0385, -0.0140,  ..., -0.0964, -0.0265, -0.0672],\n",
      "         [-0.0030,  0.0670,  0.0096,  ..., -0.1362, -0.0052, -0.1481]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0343,  0.0465,  0.0687,  ..., -0.2835,  0.0043,  0.0460],\n",
      "         [ 0.1480,  0.0648,  0.0897,  ..., -0.0555,  0.1266,  0.1879],\n",
      "         [ 0.1562,  0.0092,  0.0589,  ..., -0.7618, -0.0622,  0.1795],\n",
      "         ...,\n",
      "         [ 0.0160,  0.0634, -0.0008,  ..., -0.0987, -0.0348,  0.0195],\n",
      "         [-0.0256,  0.0931, -0.0090,  ..., -0.1121, -0.0377, -0.0775],\n",
      "         [ 0.1036,  0.0731, -0.0432,  ..., -0.1053, -0.0956, -0.2376]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0081,  0.0695, -0.0157,  ..., -0.1302,  0.0035,  0.0191],\n",
      "         [-0.0255,  0.4176,  0.0582,  ...,  0.1699,  0.1151,  0.1038],\n",
      "         [-0.0440,  0.0592,  0.0589,  ..., -0.8436, -0.0302,  0.1124],\n",
      "         ...,\n",
      "         [-0.0216,  0.0589, -0.0143,  ..., -0.1018, -0.0296, -0.0670],\n",
      "         [-0.0313,  0.1024,  0.0521,  ..., -0.0608, -0.1440, -0.1740],\n",
      "         [-0.0168,  0.0682,  0.0100,  ..., -0.0880, -0.0284, -0.0698]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0312,  0.0733,  0.0301,  ..., -0.2658, -0.0215, -0.0455],\n",
      "         [ 0.2324, -0.0658,  0.1951,  ..., -0.0802,  0.1316,  0.2064],\n",
      "         [ 0.0510, -0.0075,  0.0508,  ..., -0.3056, -0.0467,  0.1799],\n",
      "         ...,\n",
      "         [-0.0161,  0.0630, -0.0121,  ..., -0.0994, -0.0135, -0.0799],\n",
      "         [ 0.0525,  0.3465, -0.0359,  ..., -0.1489,  0.1894, -0.1851],\n",
      "         [-0.0262,  0.0682, -0.0110,  ..., -0.0806, -0.0300, -0.0665]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-8.2622e-02,  5.7863e-02,  2.8840e-02,  ..., -2.1777e-01,\n",
      "           1.0852e-03,  6.2655e-02],\n",
      "         [-1.7713e-02,  1.4849e-01, -2.5201e-02,  ..., -1.1160e-01,\n",
      "           1.0604e-02,  1.0411e-01],\n",
      "         [ 3.6056e-02,  8.6601e-02, -1.0551e-02,  ..., -8.0112e-01,\n",
      "          -8.0931e-02,  7.8945e-02],\n",
      "         ...,\n",
      "         [-1.8466e-02,  7.1322e-02, -2.4763e-02,  ..., -9.3844e-02,\n",
      "          -3.8798e-02, -4.5195e-02],\n",
      "         [ 3.8355e-02,  5.7617e-01,  1.1055e-01,  ..., -1.6972e-01,\n",
      "           1.9704e-01, -2.3179e-01],\n",
      "         [-2.5311e-02,  8.7503e-02, -4.4714e-04,  ..., -1.1515e-01,\n",
      "          -3.7193e-02, -7.2641e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0351,  0.0629, -0.0223,  ..., -0.1743, -0.0477,  0.0765],\n",
      "         [ 0.1575,  0.0125, -0.0691,  ..., -0.1078,  0.0015,  0.0651],\n",
      "         [ 0.2393,  0.2618, -0.1416,  ..., -1.2191,  0.1044, -0.0376],\n",
      "         ...,\n",
      "         [ 0.1176,  0.1688, -0.0613,  ..., -0.0306, -0.0263, -0.1892],\n",
      "         [-0.0632,  0.2603, -0.1430,  ..., -0.0730,  0.0275, -0.1534],\n",
      "         [-0.0175,  0.0628, -0.0180,  ..., -0.1122, -0.0387, -0.0676]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0414,  0.0852,  0.0497,  ..., -0.2947,  0.0461,  0.0373],\n",
      "         [ 0.2932, -0.1220,  0.1596,  ..., -0.2039,  0.1716,  0.1360],\n",
      "         [ 0.0138,  0.1081,  0.0411,  ..., -0.6164,  0.1164,  0.1407],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.3147, 0.5942, 0.2944,  ..., 0.4907, 0.4028, 0.4690], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1390, -0.1967,  0.0688,  ..., -0.1492, -0.1160,  0.0366],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0433, -0.0101,  0.0167,  ..., -0.0825,  0.0281,  0.0618],\n",
      "         [ 0.2116, -0.2398,  0.1490,  ...,  0.1340, -0.0180,  0.1131],\n",
      "         [ 0.0085, -0.1915,  0.0161,  ..., -0.0932,  0.1275,  0.0598],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.3538,  0.8286,  0.4443,  0.6182,  0.4834,  0.4397,  0.2330,  0.4565,\n",
      "         0.3091,  0.5073,  0.5532,  0.2698,  0.5376,  0.4089,  0.5474,  0.4790,\n",
      "         0.4031,  0.6685,  0.6470,  0.2771,  0.4385,  0.1838,  0.3892,  0.1414,\n",
      "         0.3904,  0.5747,  0.3601,  0.3887,  0.1082,  0.0679,  0.3486,  0.5239,\n",
      "         0.0770,  0.2218,  0.3259,  0.2360,  0.3738,  0.2445,  0.3835,  0.4709,\n",
      "         0.3853,  0.7886,  0.6997,  0.5620,  0.5132,  0.4424,  0.2771,  0.3706,\n",
      "         0.4932,  0.5381,  0.4077,  0.1197,  0.3191,  0.1987,  0.0962,  0.3833,\n",
      "         0.2522,  0.5229,  0.2915,  0.3931,  0.3040,  0.3650,  0.4258,  0.8794,\n",
      "         0.6167,  0.6260,  0.4194,  0.4395,  0.4614,  0.2976,  0.2698,  0.3052,\n",
      "         0.3950,  0.4197,  0.2944,  0.4917,  0.4888,  0.6211,  0.3381,  0.3997,\n",
      "         0.4280,  0.3062,  0.5947,  0.5718,  0.6245,  0.5347,  0.6621,  0.3972,\n",
      "         0.6040,  0.7100,  0.4614,  0.3464,  0.7954,  0.4785,  0.5176,  0.2029,\n",
      "         0.4314,  0.3289,  0.6895,  0.5171,  0.5161,  0.5024,  0.5986,  0.4119,\n",
      "         0.6265,  0.8682,  0.7617,  0.4604,  0.5435,  0.5586,  0.6191,  0.6406,\n",
      "         0.1594,  0.1703,  0.2617,  0.4956,  0.3003,  0.4551,  0.4524,  0.5122,\n",
      "         0.1599,  0.4893,  0.5239,  0.3938,  0.5439,  0.2311,  0.4602,  0.4648,\n",
      "         0.5796,  0.5742,  0.4060,  0.6782,  0.5610,  0.4207,  0.4744,  0.4534,\n",
      "         0.5249,  0.6025,  0.4153,  0.4043,  0.5171,  0.5840,  0.6753,  0.3337,\n",
      "         0.5361,  0.6992,  0.5854,  0.5034,  0.5420,  0.6343,  0.4492,  0.6040,\n",
      "         0.3860,  0.2396,  0.4275,  0.2450,  0.7178,  0.6426,  0.5596,  0.6489,\n",
      "         0.2225,  0.4946,  0.5107,  0.3672,  0.5205,  0.3689,  0.3987,  0.6494,\n",
      "         0.5747,  0.5303,  0.5088,  0.6538,  0.4824,  0.3169,  0.4966,  0.4111,\n",
      "         0.6260,  0.3330,  0.7061,  0.6768,  0.5239,  0.7822,  0.5845,  0.5771,\n",
      "         0.4045,  0.5718,  0.4941,  0.2505,  0.4739,  0.5054,  0.6128,  0.2489,\n",
      "         0.5005,  0.4734,  0.8672,  0.4102,  0.4597,  0.6943,  0.5449,  0.2418,\n",
      "         0.1494,  0.4236,  0.6006,  0.4600,  0.5225,  0.4709,  0.2386,  0.5078,\n",
      "         0.7944,  0.4011,  0.7559,  0.2981,  0.5654,  0.4575,  0.5493,  0.4224,\n",
      "         0.8486,  0.6255,  0.9092,  0.6997,  0.3918,  0.5454,  0.8833,  0.4026,\n",
      "         0.6943,  0.5034,  0.2161,  0.5371,  0.7031,  0.5254,  0.2935,  0.3994,\n",
      "         0.5562,  0.4285,  0.3462,  0.3550,  0.5928,  0.2576,  0.4485,  0.4578,\n",
      "         0.2905,  0.5127,  0.2494,  0.4883,  0.3938,  0.3682,  0.5122,  0.2673,\n",
      "         0.5483,  0.2559,  0.4780,  0.2656,  0.2993,  0.7871,  0.7573,  0.5630,\n",
      "         0.2222,  0.5659,  0.8369,  0.3164,  0.6812,  0.7603,  0.4844,  0.3855,\n",
      "         0.2163,  0.6621,  0.5312,  0.5078,  0.5669,  0.4690,  0.3792,  0.5952,\n",
      "         0.2913,  0.4858,  0.4795,  0.5454,  0.4331,  0.4016,  0.4795,  0.3694,\n",
      "         0.5195,  0.2445,  0.4182,  0.3818,  0.4285,  0.4995,  0.4465,  0.4341,\n",
      "         0.3894,  0.2969,  0.0015,  0.5850,  0.3511,  0.5972,  0.0821,  0.2898,\n",
      "         0.4736,  0.3020,  0.3005,  0.4336,  0.3159,  0.3008,  0.2749,  0.4663,\n",
      "         0.6182,  0.2695,  0.2771,  0.2632,  0.0122,  0.5327,  0.1439,  0.2969,\n",
      "         0.3152,  0.3447,  0.3638,  0.2642,  0.4036,  0.3132,  0.3752,  0.2524,\n",
      "         0.5176,  0.6025,  0.7012,  0.5488,  0.7012,  0.3035,  0.5566,  0.1482,\n",
      "         0.5522,  0.3584,  0.7637,  0.7358,  0.6509,  0.4304,  0.2671,  0.3186,\n",
      "         0.4402,  0.4136,  0.2937,  0.3706,  0.1376,  0.2756,  0.3020,  0.4009,\n",
      "         0.3452,  0.3787,  0.3176,  0.2639,  0.4390,  0.5605,  0.4214,  0.3574,\n",
      "         0.2649,  0.3972,  0.6582,  0.4927,  0.6797,  0.1796,  0.6665,  0.5957,\n",
      "         0.0751,  0.4956,  0.7842,  0.3386,  0.6821,  0.2727,  0.7065,  0.5322,\n",
      "         0.7734,  0.2583,  0.4736,  0.1934,  0.3796,  0.5981,  0.5498,  0.4243,\n",
      "         0.7881,  0.3589,  0.3828,  0.7837,  0.1697,  0.4785,  0.2236,  0.6836,\n",
      "         0.2103,  0.6743,  0.2367,  0.2263,  0.2263,  0.4475,  0.3755,  0.2729,\n",
      "         0.4724,  0.2294,  0.0822,  0.7168,  0.3459,  0.3826,  0.7593,  0.4839,\n",
      "         0.3816,  0.5229,  0.5137,  0.1700,  0.6763,  0.6787,  0.4036,  0.7148,\n",
      "         0.2686,  0.5801,  0.4260,  0.1871,  0.4946,  0.2949,  0.3687,  0.5835,\n",
      "         0.3259,  0.6553,  0.0065,  0.3506,  0.4121,  0.3691,  0.3850,  0.2893,\n",
      "         0.4080,  0.3335,  0.4656,  0.3069,  0.3245,  0.5190,  0.2896,  0.7305,\n",
      "         0.2396,  0.3843,  0.3801,  0.2781,  0.4270,  0.3152,  0.2499,  0.4324,\n",
      "         0.7188,  0.3372,  0.4700,  0.1417,  0.5078,  0.3682,  0.4873,  0.4878,\n",
      "         0.4612,  0.5273,  0.4651,  0.5562,  0.1187,  0.3491,  0.0553,  0.2104,\n",
      "         0.3862,  0.0759,  0.4907,  0.3730,  0.6006,  0.5259,  0.3452,  0.1913,\n",
      "         0.3066,  0.2573,  0.5283,  0.5962,  0.7002,  0.6118,  0.4575,  0.3394,\n",
      "         0.3708,  0.6978,  0.3018,  0.4128,  0.7251,  0.4456,  0.3462,  0.5337,\n",
      "         0.2069,  0.3892,  0.5674,  0.3325,  0.3289,  0.2092,  0.3892,  0.3752,\n",
      "         0.3552,  0.1814,  0.3848,  0.2157,  0.2036,  0.3948,  0.1692,  0.5073,\n",
      "         0.2109,  0.4490,  0.3899,  0.5444,  0.2435, -0.0480,  0.2404,  0.3479,\n",
      "         0.3728,  0.4565,  0.2847,  0.3916,  0.2491,  0.2859,  0.4714,  0.4233,\n",
      "         0.2812,  0.6836,  0.6245,  0.5703,  0.4717,  0.4639,  0.4583,  0.3965,\n",
      "         0.5444,  0.5806,  0.5801,  0.2607,  0.2062,  0.4014,  0.3220,  0.4646,\n",
      "         0.2683,  0.3386,  0.2527,  0.5151,  0.2966,  0.4617,  0.3845,  0.4458,\n",
      "         0.2448,  0.3557,  0.2551,  0.1609,  0.1716,  0.3552,  0.3110,  0.3877,\n",
      "         0.4976,  0.4343,  0.4863,  0.6763,  0.2837,  0.3372,  0.7056,  0.4392,\n",
      "         0.5645,  0.3430,  0.4221,  0.2065,  0.4211,  0.3350,  0.4834,  0.3230,\n",
      "         0.2321,  0.3098,  0.5430,  0.4893,  0.3323,  0.4297,  0.3882,  0.3489,\n",
      "         0.3342,  0.1395,  0.3931,  0.4612,  0.4338,  0.3027,  0.4460,  0.3638,\n",
      "         0.2671,  0.3635,  0.4182,  0.5942,  0.3081,  0.4116,  0.4385,  0.4226,\n",
      "         0.2849,  0.4260,  0.3130,  0.3655,  0.3826,  0.2507,  0.3555,  0.2360,\n",
      "         0.5254,  0.4731,  0.4272,  0.3792,  0.4980,  0.3230,  0.4224,  0.7363,\n",
      "         0.5874,  0.6611,  0.4192,  0.4492,  0.4211,  0.4402,  0.3049,  0.3633,\n",
      "         0.4705,  0.3567,  0.2180,  0.2303,  0.4390,  0.5757,  0.2499,  0.3872,\n",
      "         0.4421,  0.3286,  0.1941,  0.3928,  0.6616,  0.5483,  0.6616,  0.4128,\n",
      "         0.5015,  0.4023,  0.3792,  0.3279,  0.3613,  0.3374,  0.7251,  0.5732,\n",
      "         0.4507,  0.2642], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 6.5186e-02, -3.8281e-01, -1.7773e-01, -1.0028e-01, -6.7432e-01,\n",
      "        -3.0713e-01, -2.3474e-01, -1.4783e-01, -1.9934e-01, -9.6863e-02,\n",
      "        -2.2815e-01, -1.0822e-01,  9.3262e-02, -2.6489e-01, -5.4688e-02,\n",
      "         1.1072e-01, -2.6154e-02,  1.7615e-01, -1.8396e-01,  1.5271e-01,\n",
      "        -4.0710e-02, -3.7598e-02,  1.4893e-01, -2.2180e-01, -4.9988e-02,\n",
      "        -5.1941e-02, -9.9060e-02, -1.9043e-01,  3.2666e-01,  4.0112e-01,\n",
      "         2.5192e-02,  3.1030e-01,  1.9397e-01, -1.6614e-01,  1.2527e-02,\n",
      "         6.7139e-02,  1.3330e-01,  9.3445e-02,  5.2185e-02,  1.8103e-01,\n",
      "         1.9470e-01,  1.7847e-01,  3.0624e-02, -5.0697e-03,  8.9478e-02,\n",
      "        -1.1505e-01, -7.6660e-02,  2.4231e-02,  3.9787e-03, -3.2654e-02,\n",
      "        -2.2620e-01, -8.6365e-03,  3.7567e-02,  2.3975e-01, -1.4697e-01,\n",
      "         8.3435e-02,  1.3989e-01,  7.6904e-02,  2.0129e-01, -1.7414e-03,\n",
      "        -6.2012e-02, -2.9541e-02,  1.0669e-01,  5.6488e-02,  5.3619e-02,\n",
      "        -6.8054e-02,  2.5317e-01,  3.4973e-02, -2.8168e-02, -2.5439e-01,\n",
      "         1.5161e-01, -1.9946e-01, -3.6133e-02, -4.1847e-03, -7.3181e-02,\n",
      "         7.7087e-02,  2.7515e-01,  3.2178e-01, -1.4185e-01, -1.0138e-01,\n",
      "        -2.9633e-02, -1.0138e-01, -3.1299e-01, -1.2067e-01,  1.4941e-01,\n",
      "        -1.1322e-02,  2.2302e-01, -2.5708e-01,  2.7002e-01,  2.2869e-03,\n",
      "        -1.0895e-01,  8.0633e-04, -3.7256e-01, -3.3496e-01, -7.5012e-02,\n",
      "        -1.7346e-01, -6.8237e-02,  7.0496e-02, -2.1606e-02, -1.7859e-01,\n",
      "        -4.4800e-02, -2.3450e-01,  6.2744e-02, -1.3916e-01, -1.2878e-01,\n",
      "        -4.4128e-02, -2.5366e-01, -6.9153e-02, -1.6205e-02,  1.5234e-01,\n",
      "        -2.1637e-02, -3.3423e-01, -4.3604e-01, -1.8005e-02, -2.1143e-01,\n",
      "        -1.9592e-02, -1.1029e-01, -6.8237e-02, -1.9446e-01, -2.9932e-01,\n",
      "         7.1899e-02, -1.4941e-01, -1.9495e-01, -4.6484e-01, -1.2585e-01,\n",
      "        -3.4326e-01,  2.7405e-02,  5.4657e-02,  8.2855e-03, -1.8774e-01,\n",
      "        -1.9250e-01, -3.9526e-01, -2.4933e-02, -2.2815e-01, -2.0325e-01,\n",
      "        -2.0911e-01, -1.8494e-02, -1.8506e-01, -6.9580e-02, -1.0187e-01,\n",
      "        -1.3611e-02, -5.9242e-03,  1.6064e-01,  1.5976e-02, -1.2573e-01,\n",
      "        -4.1626e-02, -9.3079e-03,  1.3660e-01,  5.0323e-02, -3.9490e-02,\n",
      "        -8.8623e-02,  1.5152e-02, -6.5125e-02, -1.2445e-01,  3.0041e-03,\n",
      "        -4.5532e-02, -5.2528e-03, -2.6416e-01, -3.1934e-01, -2.9736e-01,\n",
      "        -1.9446e-01, -9.9060e-02, -1.6943e-01, -1.6638e-01, -1.8250e-01,\n",
      "        -1.5259e-02, -3.3130e-01, -2.9932e-01, -2.3328e-01, -2.6367e-02,\n",
      "        -2.0703e-01,  6.2866e-02, -1.5271e-01,  1.5588e-01, -1.2286e-01,\n",
      "         5.8228e-02,  1.5076e-02,  7.0312e-02,  9.1187e-02,  1.5717e-02,\n",
      "        -1.0492e-01,  8.3374e-02,  3.3600e-02, -4.3068e-03, -3.2739e-01,\n",
      "         9.6436e-03, -1.7590e-01, -1.3940e-01,  6.2866e-02, -4.4946e-01,\n",
      "        -1.3318e-01, -1.2323e-01,  4.2877e-02, -3.6646e-01,  2.2583e-01,\n",
      "        -2.8351e-02, -1.0199e-01, -3.4937e-01, -2.4695e-01, -3.3508e-02,\n",
      "         4.1602e-01,  1.2665e-02,  6.5269e-03, -4.7035e-03,  1.6602e-02,\n",
      "        -3.6963e-01, -4.2407e-01,  1.4893e-02,  8.8623e-02, -1.0309e-01,\n",
      "        -1.2805e-01, -1.7114e-01, -1.9287e-01, -4.8193e-01, -1.0431e-01,\n",
      "         7.3357e-03, -5.1758e-02,  8.8623e-02, -5.1392e-02, -8.0859e-01,\n",
      "        -6.5918e-02, -1.3611e-01, -3.6987e-02, -1.6016e-01, -1.8127e-01,\n",
      "        -1.7554e-01, -2.5610e-01, -1.4648e-01, -2.8491e-01, -3.2715e-01,\n",
      "        -1.1505e-01, -6.8359e-02,  5.2765e-02, -2.3083e-01, -1.7163e-01,\n",
      "        -9.2712e-02, -3.9624e-01,  7.7820e-02, -1.8530e-01, -1.2329e-02,\n",
      "         4.6417e-02, -7.1716e-02, -4.0321e-03, -3.3875e-02, -2.4780e-02,\n",
      "        -2.0337e-01, -9.5642e-02, -1.1835e-01,  1.4172e-03, -2.9858e-01,\n",
      "        -5.6396e-01,  1.4307e-01, -6.8481e-02, -1.0382e-01, -1.5112e-01,\n",
      "        -1.5063e-01, -4.6851e-01, -7.7087e-02, -2.2369e-02, -8.2581e-02,\n",
      "         4.8676e-02, -1.0358e-01, -1.8567e-01, -8.0505e-02, -1.4746e-01,\n",
      "        -5.4199e-02, -3.3508e-02, -1.8518e-01, -1.5991e-02, -4.8584e-02,\n",
      "         1.4053e-02, -1.1212e-01, -7.2815e-02,  8.8440e-02, -2.7930e-01,\n",
      "        -8.5938e-02,  1.2344e-02, -2.7588e-02, -6.7177e-03, -6.2866e-02,\n",
      "        -2.3279e-01, -1.2201e-01,  4.2938e-02, -1.8158e-02,  9.7046e-02,\n",
      "        -2.7856e-01, -1.6492e-01,  4.7722e-03,  2.9419e-02, -1.2561e-01,\n",
      "        -1.9604e-01, -2.1393e-02,  1.6785e-02, -4.9255e-02, -2.0630e-01,\n",
      "        -4.9988e-02,  4.6814e-02, -7.9773e-02,  2.4188e-04, -5.4504e-02,\n",
      "        -2.0361e-01,  1.5125e-01, -8.0688e-02,  2.7252e-02, -3.5156e-01,\n",
      "         1.8600e-02, -9.7473e-02,  2.2797e-02,  3.3386e-02,  3.8483e-02,\n",
      "        -2.1088e-02, -6.8665e-02,  9.2468e-02,  1.0864e-02, -4.6143e-01,\n",
      "         1.4197e-01, -2.7771e-02, -1.3397e-02,  2.3285e-02, -1.6980e-01,\n",
      "        -2.5464e-01, -7.8430e-02,  1.0352e-01,  1.2280e-01,  4.1650e-01,\n",
      "        -4.2944e-01, -9.7839e-02,  8.4351e-02,  1.8372e-02, -1.0590e-02,\n",
      "         2.0837e-01,  3.6224e-02,  8.3801e-02, -5.3528e-02, -1.8628e-01,\n",
      "         2.9739e-02, -9.2590e-02,  1.9058e-02,  3.2501e-02,  8.1482e-02,\n",
      "        -2.2595e-01,  3.4008e-03,  1.4450e-02,  2.1082e-01,  3.6072e-02,\n",
      "         8.5693e-02,  8.7097e-02, -1.1151e-01, -5.3162e-02,  6.0730e-02,\n",
      "        -2.8955e-01, -4.1528e-01,  1.4258e-01, -1.5051e-01, -2.5024e-01,\n",
      "        -2.7710e-02, -2.5610e-01, -2.7686e-01, -1.2128e-01, -1.4026e-01,\n",
      "        -2.3230e-01, -1.0406e-01, -4.6753e-02, -1.2347e-01, -3.5132e-01,\n",
      "        -1.6931e-01, -2.7808e-01, -1.9897e-02, -2.8223e-01, -1.2323e-01,\n",
      "        -1.2396e-01, -1.0090e-03, -5.4993e-02, -1.0992e-01, -7.4646e-02,\n",
      "        -2.1338e-01, -2.6562e-01,  4.9072e-02,  1.7004e-01,  3.3245e-03,\n",
      "        -4.4556e-02, -1.9257e-02,  9.0698e-02,  8.8318e-02, -6.7993e-02,\n",
      "        -2.6875e-03,  7.5340e-03,  6.4758e-02, -2.8760e-01,  1.9852e-02,\n",
      "        -2.1021e-01, -2.0129e-01, -2.1399e-01,  1.2573e-01,  1.6882e-01,\n",
      "        -2.6581e-02, -3.7305e-01,  7.4585e-02, -4.1309e-01, -5.1025e-02,\n",
      "         1.2634e-01,  4.2511e-02, -1.6125e-01, -3.8635e-02, -1.9458e-01,\n",
      "        -1.1737e-01, -1.5839e-02, -5.8350e-02, -1.6125e-01, -1.1798e-01,\n",
      "         3.9001e-02,  1.5674e-01, -1.1469e-01, -1.6266e-02,  2.8702e-02,\n",
      "        -1.0968e-01, -1.9055e-01,  2.6123e-02,  2.1631e-01,  8.9905e-02,\n",
      "         1.2756e-01, -1.0626e-01, -9.4238e-02, -2.8589e-01, -4.1504e-02,\n",
      "        -1.8738e-01,  1.4734e-01, -2.1497e-01, -2.8809e-02,  1.3733e-01,\n",
      "        -1.4075e-01,  4.6045e-01, -3.0487e-02,  2.8671e-02, -1.0626e-01,\n",
      "        -2.6953e-01, -9.5215e-03, -1.1115e-01,  3.5034e-02,  1.4661e-01,\n",
      "         9.1187e-02, -4.8706e-02, -4.7607e-02, -6.3293e-02,  1.7163e-01,\n",
      "        -2.5000e-01,  1.1200e-02,  2.0056e-01,  1.7712e-01,  4.6460e-01,\n",
      "        -2.9346e-01, -2.1753e-01, -2.7051e-01,  3.3264e-02,  2.0874e-01,\n",
      "         6.6101e-02,  1.3269e-01, -2.9083e-02,  4.7882e-02,  1.0901e-01,\n",
      "         1.4587e-01,  1.5869e-01, -1.9012e-02, -1.3306e-01, -3.2910e-01,\n",
      "         1.3562e-01, -9.5398e-02,  1.5002e-01,  6.3438e-03,  2.1130e-01,\n",
      "        -5.8691e-01, -2.1790e-01,  2.1057e-02,  1.1682e-01, -1.7487e-02,\n",
      "         1.0254e-01, -1.3293e-01,  1.8402e-02, -1.0211e-01,  8.5510e-02,\n",
      "         2.6989e-03,  4.2419e-02,  1.8396e-01, -1.0577e-01, -7.5562e-02,\n",
      "         8.6737e-04, -6.9008e-03, -1.0114e-01,  1.0449e-01,  9.5337e-02,\n",
      "         2.4915e-01,  1.1639e-01, -1.0602e-01, -4.4739e-02, -7.7576e-02,\n",
      "        -8.7769e-02, -5.6335e-02,  1.4709e-01, -2.1252e-01, -2.2864e-01,\n",
      "        -3.7817e-01, -4.2358e-02, -2.5024e-01, -1.5906e-01, -7.2754e-02,\n",
      "         3.1763e-01, -1.1505e-01,  1.0858e-01, -2.0142e-01, -3.0396e-01,\n",
      "        -7.6355e-02, -2.8760e-01,  1.1115e-01, -3.8330e-01, -2.5439e-01,\n",
      "        -2.1362e-01, -6.8311e-01, -1.5430e-01, -1.3647e-01, -6.8092e-03,\n",
      "        -2.2424e-01, -2.2009e-01, -3.2007e-01,  5.9113e-02, -4.2542e-02,\n",
      "        -6.5674e-02,  4.3427e-02,  1.6882e-01, -1.2042e-01, -1.6809e-01,\n",
      "        -2.5854e-01, -4.8401e-02, -1.2988e-01, -7.6942e-03, -4.3518e-02,\n",
      "         1.3336e-02,  1.5881e-01,  4.0985e-02,  2.6684e-03,  1.4343e-01,\n",
      "         4.4891e-02,  4.5288e-02,  8.7341e-02,  8.5999e-02,  8.8745e-02,\n",
      "         2.2620e-01,  1.7548e-02,  5.6274e-02,  1.7480e-01, -1.0870e-01,\n",
      "         3.1250e-02,  8.0566e-02, -3.8391e-02,  1.6113e-01,  8.2275e-02,\n",
      "        -1.8433e-02, -1.9751e-01, -1.7426e-02,  2.3657e-01, -9.2651e-02,\n",
      "         4.3976e-02, -6.0425e-02,  9.1736e-02, -1.2610e-01, -3.2861e-01,\n",
      "        -4.7668e-02, -2.8857e-01, -3.5217e-02,  1.7969e-01,  3.2990e-02,\n",
      "         2.3026e-02,  1.6711e-01, -1.4819e-01,  4.2236e-02,  1.4062e-01,\n",
      "         6.4148e-02,  6.8848e-02, -9.4788e-02, -3.7109e-02, -3.1616e-01,\n",
      "        -2.2253e-01,  7.4707e-02,  2.4445e-02,  1.3562e-01, -2.2827e-02,\n",
      "        -5.9242e-03, -2.8706e-03, -8.2214e-02,  1.4246e-01, -1.2817e-02,\n",
      "         3.5126e-02, -2.9028e-01, -1.4771e-01, -1.4038e-01, -2.2339e-02,\n",
      "        -2.0178e-01, -4.3152e-02, -1.9849e-01, -3.4326e-01,  4.5280e-03,\n",
      "         1.2335e-01,  1.9287e-01, -5.5481e-02, -1.3684e-01, -8.3557e-02,\n",
      "        -2.0422e-01,  9.3933e-02, -1.1798e-01,  7.6050e-02,  1.9287e-01,\n",
      "         7.9590e-02, -1.7444e-01, -1.7407e-01,  1.0544e-02, -6.8542e-02,\n",
      "        -9.0942e-02, -9.1980e-02, -3.5767e-02, -2.9883e-01, -3.3228e-01,\n",
      "         1.2695e-01,  1.2036e-01,  2.1313e-01, -2.3376e-01, -1.9812e-01,\n",
      "        -7.8735e-02, -2.7527e-02, -2.0691e-01, -4.3823e-02, -2.3669e-01,\n",
      "        -4.6411e-01, -1.1371e-01, -1.5967e-01,  1.3171e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0791,  0.0878,  0.0786,  ..., -0.3869,  0.0665,  0.0449],\n",
      "         [ 0.0178, -0.0123,  0.0865,  ..., -0.0886,  0.1315,  0.0034],\n",
      "         [-0.0323,  0.0226,  0.0536,  ..., -0.4902,  0.1464, -0.1046],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.2744,  0.5264,  0.3086,  0.3992,  0.1299,  0.1387,  0.5146,  0.5552,\n",
      "         0.2751,  0.2637,  0.2491,  0.2303,  0.4971,  0.3538,  0.3667,  0.4741,\n",
      "         0.1152,  0.5918,  0.3394,  0.4082,  0.3811,  0.5361,  0.3823,  0.3811,\n",
      "         0.3923,  0.2854,  0.2549,  0.4199,  0.3096,  0.2903,  0.4590,  0.2715,\n",
      "         0.4097,  0.3618,  0.3420,  0.4128,  0.3430,  0.3816,  0.3389,  0.6855,\n",
      "         0.1602,  0.8442,  0.2277,  0.5210,  0.4429,  0.2087,  0.4189,  0.0874,\n",
      "         0.3523,  0.1949,  0.3628,  0.2372,  0.3889,  0.2296,  0.4543,  0.3406,\n",
      "         0.1835,  0.4304,  0.4543,  0.0615,  0.0623,  0.2839,  0.5845, -0.0047,\n",
      "         0.3855,  0.4487,  0.4888,  0.4065,  0.2457,  0.0425,  0.1835,  0.4258,\n",
      "         0.3711,  0.5854,  0.3770,  0.4487,  0.4167,  0.6411,  0.3989,  0.5786,\n",
      "         0.5552,  0.3289,  0.4250,  0.4795,  0.5591,  0.3569,  0.6836,  0.3267,\n",
      "         0.3813,  0.6987,  0.4719,  0.4463,  0.4849,  0.3726,  0.2216,  0.5928,\n",
      "         0.4729,  0.4902,  0.4390,  0.2830,  0.5557,  0.7715,  0.5146,  0.4714,\n",
      "         0.3662,  0.5767,  0.5435,  0.5889,  0.5444,  0.7314,  0.4446,  0.3125,\n",
      "         0.3230,  0.5654,  0.4587,  0.5493,  0.4961,  0.5591,  0.3337,  0.4106,\n",
      "         0.4077,  0.4304,  0.4263,  0.5986,  0.4321,  0.4294,  0.4927,  0.3735,\n",
      "         0.6387,  0.4231,  0.4573,  0.5220,  0.5615,  0.5610,  0.3013,  0.4478,\n",
      "         0.3474,  0.3313,  0.2920,  0.5317,  0.3516,  0.5552,  0.2861,  0.5356,\n",
      "         0.6035,  0.6699,  0.4468,  0.5176,  0.4031,  0.5151,  0.3262,  0.4609,\n",
      "         0.4048,  0.1962,  0.3860,  0.3606,  0.3638,  0.3604,  0.3459,  0.4333,\n",
      "         0.5464,  0.3687,  0.3999,  0.6616,  0.4819,  0.3015,  0.4487,  0.2771,\n",
      "         0.2732,  0.5171,  0.7070,  0.4436,  0.3892,  0.4456,  0.3838,  0.3003,\n",
      "         0.4031,  0.6177,  0.1946,  0.7080,  0.7607,  0.5503,  0.3547,  0.3606,\n",
      "         0.4419,  0.5029,  0.4473,  0.2211,  0.4534,  0.7100,  0.5317,  0.3779,\n",
      "         0.3674,  0.4749,  0.2749,  0.4966,  0.6870,  0.5430,  0.4036,  0.5972,\n",
      "         0.5186,  0.4185,  0.4805,  0.4863,  0.1810,  0.2966,  0.1707,  0.3154,\n",
      "         0.4014,  0.4539,  0.8589,  0.4775,  0.6460,  0.2910,  0.3691,  0.3535,\n",
      "         0.2262,  0.4258,  0.1329,  0.3311,  0.3535,  0.6445,  0.6396,  0.5840,\n",
      "         0.1437,  0.5752,  0.2281,  0.1893,  0.6274,  0.4604,  0.3545,  0.5942,\n",
      "         0.4705,  0.3975,  0.5308,  0.5591,  0.3076,  0.7954,  0.3789,  0.6426,\n",
      "         0.6958,  0.7007,  0.7744,  0.4688,  0.5688,  0.5293,  0.7236,  0.4062,\n",
      "         0.3884,  0.5469,  0.2502,  0.4563,  0.6548,  0.3105,  0.3696,  0.1896,\n",
      "         0.6636,  0.7471,  0.6084,  0.4155,  0.6440,  0.6235,  0.4380,  0.4739,\n",
      "         0.3625,  0.1366,  0.2703,  0.4670,  0.3989,  0.5273,  0.5415,  0.3918,\n",
      "         0.8369,  0.5864,  0.4722,  0.2820,  0.4038,  0.2708,  0.4749,  0.3926,\n",
      "         0.5625,  0.3718,  0.2272,  0.2676,  0.2389,  0.3625,  0.3271,  0.2452,\n",
      "         0.4304,  0.4663,  0.5830,  0.3169,  0.3896,  0.2583,  0.6006,  0.5576,\n",
      "         0.6309,  0.2673,  0.5820,  0.6426,  0.3479,  0.4053,  0.5796,  0.4470,\n",
      "         0.6133,  0.5405,  0.4570,  0.4150,  0.6040,  0.4141,  0.2457,  0.5620,\n",
      "         0.3330,  0.3931,  0.4446,  1.0010,  0.5000,  0.5537,  0.5093, -0.1230,\n",
      "         0.5986,  0.2742,  0.0926,  0.2971,  0.3665,  0.1593,  0.3513,  0.2610,\n",
      "         0.8726,  0.4172,  0.5161,  0.4395,  0.3127,  0.2423,  0.2898,  0.6318,\n",
      "         0.5215,  0.4688,  0.5332,  0.6465,  0.5107,  0.4443,  0.7622,  0.5845,\n",
      "         0.6963,  0.3896,  0.3059,  0.5649,  0.3967,  0.5298,  0.3035,  0.1320,\n",
      "         0.6045,  0.1962,  0.4746,  0.4392,  0.4805,  0.5591,  0.5264,  0.3467,\n",
      "         0.3455,  0.2360,  0.3667,  0.5581,  0.4587,  0.7422,  0.4907,  0.5825,\n",
      "         0.5210,  0.4951,  0.4971,  0.3599,  0.6533,  0.3721,  0.3845, -0.0256,\n",
      "         0.2256,  0.0606,  0.2827,  0.3767,  0.7280,  0.2394,  0.4265,  0.4294,\n",
      "         0.5239,  0.3972,  0.6274,  0.7744,  0.1830,  0.1783,  0.5840,  0.5630,\n",
      "         0.2607,  0.5552,  0.2798,  0.4246,  0.3687,  0.4019,  0.0948,  0.4102,\n",
      "         0.6772,  0.3979,  0.4456,  0.4370,  0.6089,  0.7217,  0.1236,  0.2861,\n",
      "         0.6934,  0.1411,  0.5361,  0.2128,  0.3516,  0.5205,  0.7100,  0.2445,\n",
      "         0.1348,  0.3882,  0.2006,  0.2209,  0.0341,  0.3142,  0.3958,  0.2323,\n",
      "         0.2467,  0.5464,  0.1373,  0.3987,  0.4990,  0.5229,  0.2576,  0.3840,\n",
      "         0.3931,  0.4187,  0.2034,  0.2219,  0.6069,  0.1460,  0.5356,  0.3176,\n",
      "         0.4651,  0.2098,  0.5518,  0.1785,  0.3098,  0.3567,  0.4697,  0.0213,\n",
      "         0.1260,  0.3816,  0.6367,  0.4082,  0.2180,  0.3877,  0.0591,  0.3201,\n",
      "         0.7520,  0.3418,  0.6235,  0.4363,  0.3833,  0.1151,  0.4136,  0.4561,\n",
      "         0.1407,  0.3564,  0.3030,  0.3418,  0.4573,  0.4028,  0.5059,  0.2294,\n",
      "         0.3455,  0.2751,  0.2620,  0.3809,  0.4390,  0.2864,  0.3987,  0.3689,\n",
      "         0.2583, -0.2013,  0.4658,  0.2798,  0.6699,  0.3464,  0.3533,  0.3618,\n",
      "         0.3225,  0.5698,  0.3823,  0.2177,  0.6094,  0.2866,  0.6255,  0.1575,\n",
      "         0.2974,  0.1393,  0.2272,  0.3708,  0.1771,  0.1541,  0.3350,  0.6440,\n",
      "         0.6021,  0.6089,  0.3931,  0.4932,  0.5625,  0.2150, -0.0372,  0.1571,\n",
      "         0.2600,  0.2651,  0.4480,  0.3826,  0.2178,  0.4888,  0.5215,  0.4683,\n",
      "         0.4084,  0.4194,  0.4365,  0.4236,  0.3877,  0.1849,  0.4751,  0.2546,\n",
      "         0.3328,  0.3796,  0.3010,  0.0564,  0.4695,  0.5942,  0.5571,  0.3474,\n",
      "         0.4702,  0.4744,  0.4214,  0.2681,  0.1801,  0.2168,  0.3518,  0.4463,\n",
      "         0.0807,  0.5073,  0.4453,  0.3552,  0.4724,  0.3691,  0.2720,  0.3730,\n",
      "         0.2321,  0.0029,  0.5264,  0.3640,  0.5576,  0.4016,  0.4006,  0.1368,\n",
      "         0.2372,  0.3293,  0.3491,  0.5308,  0.3186,  0.7407,  0.3723,  0.3999,\n",
      "         0.0191,  0.5171,  0.1531,  0.2214,  0.0168,  0.5815,  0.3789,  0.3025,\n",
      "         0.4268,  0.0982,  0.3765,  0.2722,  0.5068,  0.5713,  0.3716,  0.3821,\n",
      "         0.4741,  0.4790,  0.2869,  0.4241,  0.4170,  0.2546,  0.3862,  0.1857,\n",
      "         0.0367, -0.2020,  0.2015,  0.1572,  0.3357,  0.2573,  0.4771,  0.4260,\n",
      "         0.3433,  0.5039,  0.4917,  0.3398,  0.4197,  0.4502,  0.4172,  0.4407,\n",
      "         0.4390,  0.4756,  0.3147,  0.3013,  0.2939,  0.5981,  0.4102,  0.3591,\n",
      "         0.2219], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([-0.0327, -0.2279,  0.0064, -0.2189, -0.1589, -0.2030, -0.2676, -0.3672,\n",
      "        -0.5039, -0.4436, -0.3616,  0.0712, -0.0844, -0.2058,  0.0016, -0.2559,\n",
      "        -0.1221, -0.2178, -0.0353,  0.0582,  0.0356, -0.0053,  0.0261, -0.0592,\n",
      "         0.2408, -0.2522, -0.0081, -0.2693, -0.0441,  0.1818,  0.0430, -0.1512,\n",
      "         0.0267,  0.1318,  0.1611,  0.3052,  0.1340,  0.0840,  0.0971,  0.0037,\n",
      "        -0.3313, -0.3101, -0.0700, -0.0347, -0.0342,  0.0217, -0.1967, -0.0326,\n",
      "         0.0042, -0.0273, -0.0291, -0.0837, -0.0733, -0.1576,  0.1215,  0.0124,\n",
      "        -0.0896,  0.0726, -0.0350, -0.2556,  0.0075, -0.0730, -0.2296, -0.1615,\n",
      "         0.0372,  0.1907,  0.1388,  0.0065,  0.0324,  0.1058, -0.0442,  0.2859,\n",
      "         0.0790, -0.0089, -0.0187,  0.0672,  0.0153,  0.1194,  0.0562, -0.2465,\n",
      "        -0.4070, -0.0910, -0.0055, -0.0693, -0.1699, -0.1287, -0.1157,  0.1609,\n",
      "        -0.1121, -0.0164,  0.1245, -0.0693,  0.1703,  0.3123, -0.1065, -0.4346,\n",
      "        -0.0092, -0.0317,  0.1022, -0.0039,  0.1118, -0.0685, -0.1508, -0.0456,\n",
      "        -0.2379,  0.0207,  0.1231, -0.0518, -0.3992, -0.3210,  0.0695, -0.1732,\n",
      "        -0.2340, -0.1617, -0.0616, -0.1353,  0.0590, -0.2830, -0.1478,  0.0019,\n",
      "        -0.0707,  0.0360, -0.0789, -0.0472, -0.2668, -0.0465, -0.1952, -0.1672,\n",
      "        -0.1323, -0.1462, -0.0623, -0.2788, -0.0289, -0.2952, -0.3950, -0.1567,\n",
      "        -0.3464, -0.2705, -0.2793, -0.1807,  0.2656, -0.1198, -0.1838, -0.1036,\n",
      "        -0.2457, -0.0074, -0.0808,  0.0285, -0.2815, -0.1582, -0.1431,  0.1729,\n",
      "        -0.0121, -0.1570,  0.0497,  0.1527, -0.0052,  0.0625, -0.0821, -0.0172,\n",
      "        -0.1232,  0.1173,  0.0940, -0.0602, -0.1136, -0.2812,  0.2343,  0.2128,\n",
      "        -0.0736, -0.2632, -0.2355, -0.0053, -0.6050, -0.5977, -0.5039,  0.0167,\n",
      "        -0.0312, -0.0267, -0.0750, -0.0209, -0.0462,  0.0986, -0.0481,  0.0916,\n",
      "        -0.5005, -0.1562, -0.4224, -0.1088, -0.3357, -0.1310, -0.5347, -0.4277,\n",
      "         0.0756, -0.1267, -0.3745,  0.1626,  0.0228, -0.2947, -0.2323, -0.0621,\n",
      "        -0.2566, -0.0955, -0.0430, -0.1753, -0.7798, -0.5898, -0.3098, -0.3940,\n",
      "        -0.1707,  0.0863, -0.3667, -0.1396,  0.0917, -0.0846, -0.1201, -0.5977,\n",
      "         0.1365, -0.3560, -0.5537, -0.0063, -0.0998,  0.1251, -0.1108, -0.3374,\n",
      "        -0.5938, -0.4167,  0.1349,  0.1865,  0.0054, -0.1272, -0.0050, -0.1199,\n",
      "        -0.1074,  0.0197, -0.3220, -0.4888, -0.2798, -0.4849, -0.3157, -0.2081,\n",
      "        -0.1949, -0.1595,  0.2224, -0.2284, -0.2688,  0.0079, -0.2489, -0.4382,\n",
      "        -0.5693, -0.1442, -0.0490, -0.2634,  0.0197, -0.2180, -0.0656, -0.2069,\n",
      "        -0.6338, -0.1722, -0.2389, -0.1980, -0.1584, -0.0977, -0.3223, -0.0513,\n",
      "        -0.1764, -0.2805, -0.0654, -0.2152, -0.1201, -0.0021, -0.3611, -0.2710,\n",
      "        -0.1804, -0.2328, -0.2494,  0.0224, -0.2350, -0.2520,  0.0495, -0.0778,\n",
      "        -0.0989, -0.4629,  0.1309, -0.3398,  0.2051, -0.3560, -0.2484,  0.2512,\n",
      "        -0.2886, -0.4307, -0.2494, -0.0255,  0.1670,  0.0132,  0.2172,  0.0382,\n",
      "        -0.0911, -0.1791,  0.0750, -0.0659, -0.1166,  0.1213, -0.0939, -0.1165,\n",
      "         0.1063,  0.0986,  0.0069, -0.1638, -0.2440, -0.0034, -0.2043,  0.1299,\n",
      "         0.2651, -0.1285, -0.2135,  0.0017, -0.1464, -0.0691, -0.2362, -0.0890,\n",
      "        -0.5488, -0.6035, -0.0829, -0.3335, -0.1517, -0.0848,  0.1276, -0.3826,\n",
      "         0.0082, -0.1515, -0.0783, -0.5317, -0.5039, -0.0641, -0.1920,  0.0104,\n",
      "         0.0536,  0.0635,  0.1015, -0.0333,  0.0688, -0.0605, -0.1577,  0.1301,\n",
      "        -0.1198, -0.1395, -0.1536, -0.1462, -0.5962, -0.3755, -0.2729, -0.1165,\n",
      "        -0.0934, -0.1583, -0.2639, -0.1181, -0.1475, -0.1454, -0.0860, -0.1165,\n",
      "         0.0115,  0.0516, -0.2487, -0.2781, -0.2343, -0.1381, -0.0291, -0.1183,\n",
      "         0.0712, -0.0257, -0.1191,  0.2336, -0.1247, -0.4741, -0.2026, -0.0766,\n",
      "        -0.0802, -0.0112, -0.1552, -0.1140, -0.4155, -0.1180, -0.0438, -0.0927,\n",
      "        -0.0189, -0.0456, -0.0180, -0.3994, -0.5669, -0.3516, -0.1873, -0.0529,\n",
      "        -0.5522, -0.3120, -0.1678, -0.1389, -0.1935, -0.1576, -0.0991,  0.2671,\n",
      "         0.0261, -0.4072, -0.1033,  0.1361, -0.1071, -0.0156, -0.2734,  0.1892,\n",
      "        -0.3101, -0.0746,  0.0435,  0.0526,  0.1172,  0.1885,  0.0648,  0.2354,\n",
      "         0.0595,  0.0423, -0.0575,  0.2109,  0.0608,  0.0056,  0.0493, -0.0136,\n",
      "         0.1284, -0.2350, -0.1027,  0.2346,  0.2708,  0.2263,  0.0146,  0.1497,\n",
      "         0.0912,  0.0625, -0.1663,  0.1639, -0.2791, -0.0762,  0.1215,  0.0472,\n",
      "         0.1514,  0.0158,  0.1243, -0.0928,  0.1126,  0.0045, -0.0982, -0.3792,\n",
      "        -0.2377, -0.1114,  0.0633, -0.1035, -0.1637,  0.0323,  0.0774, -0.0511,\n",
      "        -0.0643, -0.0525, -0.0009, -0.2925, -0.1242, -0.0270,  0.0259,  0.0488,\n",
      "        -0.1216,  0.0410, -0.0839,  0.1436, -0.0193, -0.3809, -0.1707, -0.0177,\n",
      "        -0.1226, -0.0851,  0.1921, -0.0414,  0.0246,  0.0876,  0.0350, -0.0577,\n",
      "         0.0576, -0.2903, -0.0738,  0.0158, -0.0655, -0.2507, -0.0658, -0.2810,\n",
      "        -0.3442,  0.0391, -0.1126, -0.2966,  0.0226, -0.0209, -0.0196, -0.2267,\n",
      "         0.1705,  0.1342, -0.3120, -0.0555,  0.0133,  0.2249, -0.0555, -0.1115,\n",
      "        -0.0516, -0.0783, -0.4438, -0.0958, -0.0517, -0.3635, -0.0952, -0.2561,\n",
      "         0.0192, -0.2230, -0.2396, -0.4731,  0.1202, -0.1232, -0.2690, -0.2827,\n",
      "        -0.0404, -0.2048, -0.2500, -0.6704, -0.2242, -0.2717, -0.0629,  0.0072,\n",
      "        -0.1055, -0.0724, -0.0721, -0.1120, -0.0345, -0.0283,  0.1831,  0.0254,\n",
      "        -0.0943, -0.3196, -0.4907,  0.1659, -0.0100, -0.0944, -0.1752, -0.0341,\n",
      "        -0.1543,  0.0190, -0.2479,  0.0952, -0.1770, -0.1769, -0.3079, -0.3359,\n",
      "         0.0711, -0.0898,  0.0128, -0.2507, -0.2020, -0.0282,  0.1438,  0.0480,\n",
      "         0.1001, -0.0050, -0.0032, -0.1664,  0.0496, -0.1759, -0.0084,  0.0158,\n",
      "        -0.1075,  0.0230, -0.2286,  0.1412, -0.0432,  0.0378, -0.2090, -0.4121,\n",
      "        -0.2522, -0.2164,  0.0635,  0.0573,  0.0762,  0.0255, -0.2051,  0.0907,\n",
      "        -0.1873, -0.0614, -0.3308,  0.0221, -0.1259, -0.0628, -0.2369, -0.4736,\n",
      "        -0.1005, -0.2487, -0.2903, -0.2524,  0.0547,  0.0387,  0.0461,  0.0252,\n",
      "        -0.2382, -0.4004, -0.1267, -0.4639, -0.0825, -0.2010, -0.3713, -0.2742,\n",
      "         0.1423, -0.0331, -0.2622,  0.0675, -0.1582, -0.4558, -0.2272, -0.2069,\n",
      "         0.1199], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.1530,  0.3829,  0.0597,  ..., -0.1053,  0.2577, -0.1018],\n",
      "         [ 0.0008,  0.1262,  0.0737,  ...,  0.3680,  0.0147,  0.1895],\n",
      "         [ 0.0351,  0.0267,  0.0814,  ..., -0.3837,  0.0744,  0.3225],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 2.3450e-01,  6.2988e-01,  2.9907e-01,  5.5762e-01,  3.4302e-01,\n",
      "         3.6548e-01,  4.2920e-01,  3.6768e-01,  3.5718e-01,  5.3662e-01,\n",
      "         3.6499e-01,  4.5410e-01,  5.0049e-01,  5.3223e-01,  4.7852e-01,\n",
      "         2.2131e-01,  4.9170e-01,  4.5605e-01,  4.2896e-01,  2.9248e-01,\n",
      "         3.8965e-01,  6.1523e-01,  4.2627e-01,  4.3701e-01,  2.6758e-01,\n",
      "         3.0981e-01,  4.8340e-01,  4.2139e-01,  3.6963e-01,  1.8079e-01,\n",
      "         4.0942e-01,  1.7834e-01,  2.0203e-01,  3.7183e-01,  5.7343e-02,\n",
      "         3.0762e-01,  1.6602e-01,  6.8481e-02,  6.6589e-02,  5.6982e-01,\n",
      "         5.0879e-01,  4.7241e-01,  2.6099e-01,  1.5344e-01,  3.0835e-01,\n",
      "         3.3521e-01,  1.6724e-01, -4.9011e-02, -3.0975e-02,  1.7773e-01,\n",
      "         3.0127e-01,  2.7222e-01,  6.4355e-01,  2.7075e-01,  4.4287e-01,\n",
      "         3.3545e-01,  4.0942e-01,  5.4169e-02,  4.1333e-01,  2.1631e-01,\n",
      "         5.4395e-01,  3.5889e-01,  1.9556e-01,  1.7969e-01,  2.0593e-01,\n",
      "         2.8955e-01,  6.5723e-01,  5.6543e-01,  4.2651e-01,  4.1577e-01,\n",
      "         4.1333e-01,  3.7061e-01,  6.7383e-01,  4.3921e-01,  2.5854e-01,\n",
      "         1.1238e-02,  2.6611e-01,  3.0908e-01,  4.3872e-01,  3.7354e-01,\n",
      "         3.7769e-01,  3.3325e-01,  3.3325e-01,  2.1545e-01,  4.5068e-01,\n",
      "         2.6343e-01,  6.4551e-01,  1.5466e-01,  1.4441e-01,  2.0752e-01,\n",
      "         5.8740e-01,  8.3838e-01,  2.2815e-01,  3.6719e-01,  2.7490e-01,\n",
      "         6.1133e-01,  3.2324e-01,  6.0400e-01,  3.7402e-01,  5.0146e-01,\n",
      "         3.4302e-01,  3.8623e-01,  1.6968e-01,  3.5596e-01,  5.4541e-01,\n",
      "         7.2998e-01,  3.3618e-01,  5.3125e-01,  4.3286e-01,  1.4331e-01,\n",
      "         4.1260e-01,  4.8438e-01,  3.9868e-01,  5.0049e-01,  2.4744e-01,\n",
      "         4.8340e-01,  5.3760e-01,  4.8145e-01,  2.2021e-01,  1.0504e-01,\n",
      "         3.7567e-02,  2.4280e-01,  3.5254e-01,  3.0737e-01,  6.1572e-01,\n",
      "         3.2568e-01,  2.1350e-01,  4.3481e-01,  3.3765e-01,  6.9458e-02,\n",
      "         3.9551e-01,  4.2725e-01,  5.0781e-01,  6.4014e-01,  5.4492e-01,\n",
      "         4.5972e-01,  6.8994e-01,  3.9941e-01,  8.5938e-02,  3.9795e-01,\n",
      "         2.5195e-01,  5.5029e-01,  2.6123e-01,  4.1748e-01,  6.7334e-01,\n",
      "         4.4604e-01,  2.5781e-01,  2.9370e-01,  2.8369e-01,  1.4746e-01,\n",
      "         4.1943e-01,  4.3579e-01,  2.3328e-01,  1.1859e-01,  6.0577e-02,\n",
      "         2.6758e-01,  3.6401e-01,  3.5132e-01,  7.6611e-01,  7.2803e-01,\n",
      "         4.7998e-01,  3.2104e-01,  6.2207e-01,  5.8447e-01,  3.9966e-01,\n",
      "         2.9126e-01,  3.0444e-01,  4.6460e-01,  5.8838e-01,  1.4294e-01,\n",
      "         5.1904e-01,  4.0308e-01,  7.9785e-01,  5.1904e-01,  5.8398e-01,\n",
      "         3.3228e-01,  8.2520e-02,  3.4839e-01,  6.2158e-01,  2.8809e-01,\n",
      "         2.2839e-01,  2.6855e-01,  2.9810e-01,  5.3076e-01,  2.2913e-01,\n",
      "         3.8696e-01,  4.2627e-01,  3.0371e-01,  3.9868e-01,  2.0679e-01,\n",
      "         3.5205e-01,  3.7085e-01,  8.4277e-01,  2.7588e-01,  6.5039e-01,\n",
      "         6.0010e-01,  5.5469e-01,  5.1416e-01,  6.2939e-01,  5.0488e-01,\n",
      "         1.3794e-01,  4.4434e-01,  2.6685e-01,  4.0283e-01,  3.4863e-01,\n",
      "         4.5679e-01,  3.3423e-01,  8.2910e-01,  4.7754e-01,  6.2891e-01,\n",
      "         4.7314e-01,  4.8926e-01,  2.9785e-01,  2.8320e-01,  5.8350e-01,\n",
      "         2.3035e-01,  5.9131e-01,  6.8164e-01,  4.4385e-01,  3.4814e-01,\n",
      "         2.7002e-01,  3.4351e-01,  4.1284e-01,  3.8989e-01,  3.3862e-01,\n",
      "         1.8616e-01,  3.3472e-01,  1.9275e-01,  4.4263e-01,  4.9316e-01,\n",
      "         4.9658e-01,  3.3032e-01,  2.1643e-01,  6.2500e-01,  5.7666e-01,\n",
      "         5.4150e-01,  3.8208e-01,  1.9629e-01,  6.3330e-01,  5.4736e-01,\n",
      "         5.2734e-01,  3.4570e-01,  1.4001e-01,  3.4717e-01,  3.1982e-01,\n",
      "         8.8882e-03,  2.6562e-01,  5.9473e-01,  5.6787e-01,  5.2197e-01,\n",
      "         2.4573e-01,  2.8564e-01,  2.6880e-01,  2.0471e-01,  2.4329e-01,\n",
      "         5.9180e-01,  4.3701e-01,  3.0640e-01,  3.3179e-01,  2.6245e-01,\n",
      "         2.3767e-01,  6.9824e-01,  6.6260e-01,  4.3140e-01,  4.4873e-01,\n",
      "         4.7900e-01,  3.8696e-01,  5.0293e-01,  5.6445e-01,  3.3057e-01,\n",
      "         4.5703e-01,  3.4912e-01,  3.2568e-01,  2.0459e-01,  1.7896e-01,\n",
      "         2.1289e-01,  2.4377e-01,  6.8066e-01,  3.8623e-01,  5.3320e-01,\n",
      "         3.2324e-01,  3.2495e-01,  2.2107e-01,  2.9053e-01,  3.6304e-01,\n",
      "         4.9170e-01,  2.4890e-01,  2.8271e-01,  4.2188e-01,  3.6060e-01,\n",
      "         6.4746e-01,  3.6060e-01,  7.1631e-01,  4.5483e-01,  3.2910e-01,\n",
      "         2.7686e-01,  2.5781e-01,  2.3914e-01,  3.1470e-01,  5.9033e-01,\n",
      "         4.2896e-01,  3.5718e-01,  6.0059e-01,  8.0859e-01,  2.4573e-01,\n",
      "         7.1143e-01,  2.3645e-01,  3.1006e-01,  2.1643e-01,  1.4929e-01,\n",
      "         1.0492e-01,  1.0370e-01,  1.4221e-01,  2.3425e-01,  2.2266e-01,\n",
      "         1.8555e-01,  1.4832e-01,  1.2622e-01, -1.6394e-01,  6.4600e-01,\n",
      "         5.3857e-01,  2.8540e-01,  2.6953e-01,  2.4646e-01,  3.3203e-01,\n",
      "         4.4043e-01,  4.7485e-01,  5.1074e-01,  4.5312e-01,  1.9287e-01,\n",
      "         6.3135e-01,  2.5537e-01,  5.1904e-01,  2.9907e-01,  2.5171e-01,\n",
      "         3.7012e-01,  4.3359e-01,  4.1699e-01,  5.5078e-01,  4.5563e-02,\n",
      "         5.0244e-01,  8.9661e-02,  1.3611e-01,  1.2549e-01,  3.0249e-01,\n",
      "         2.0190e-01,  1.7334e-01,  1.4697e-01,  1.8701e-01,  2.2351e-01,\n",
      "         2.3474e-01,  2.8882e-01,  2.1448e-01,  5.1953e-01,  4.8340e-01,\n",
      "         4.5361e-01,  3.3350e-01,  5.0830e-01,  5.6543e-01,  4.7314e-01,\n",
      "         4.3164e-01,  3.1104e-01,  4.4849e-01,  2.5928e-01,  1.8604e-01,\n",
      "         2.6929e-01,  6.8176e-02,  5.9131e-01,  5.2832e-01,  4.2969e-01,\n",
      "         2.4304e-01,  4.1235e-01,  1.5393e-01,  3.6841e-01,  1.5503e-01,\n",
      "         7.8271e-01,  2.6636e-01,  5.3857e-01,  2.0081e-01,  2.9419e-01,\n",
      "         4.2236e-01,  3.2153e-01,  4.3579e-01,  4.2407e-01,  3.9014e-01,\n",
      "         4.5996e-01,  2.9468e-01,  1.9812e-01,  1.7188e-01,  2.7417e-01,\n",
      "         6.6699e-01,  1.9202e-01,  4.8877e-01,  2.9614e-01,  6.4697e-01,\n",
      "         3.1567e-01,  6.4746e-01,  6.3184e-01,  3.7183e-01,  3.6792e-01,\n",
      "         4.2969e-01,  1.7981e-01,  7.0410e-01,  1.9116e-01,  4.9219e-01,\n",
      "         1.6248e-01,  2.4353e-01,  5.3467e-01,  1.0199e-01,  3.0103e-01,\n",
      "         2.2229e-01,  3.2227e-01,  2.3450e-01,  2.0142e-01,  1.9434e-01,\n",
      "         2.5293e-01,  6.5869e-01,  2.0532e-01,  3.8232e-01,  8.8013e-02,\n",
      "         2.9126e-01,  2.9468e-01,  1.5967e-01,  3.0298e-01,  1.8823e-01,\n",
      "        -7.2083e-02,  6.7368e-03,  4.2267e-02,  4.9713e-02,  1.0297e-01,\n",
      "         1.3354e-01,  6.6528e-02,  2.4399e-02, -2.4866e-01,  5.7959e-01,\n",
      "         5.0244e-01,  2.6562e-01,  1.9995e-01,  1.8396e-01,  2.1704e-01,\n",
      "         2.5244e-01,  3.0347e-01,  3.6890e-01,  5.6836e-01,  3.6548e-01,\n",
      "         3.8867e-01,  4.2407e-01,  3.1421e-01,  2.8076e-01,  3.2520e-01,\n",
      "         5.7178e-01,  3.3447e-01,  3.4619e-01,  4.1602e-01,  4.3579e-01,\n",
      "         4.1333e-01,  2.6099e-01,  2.7515e-01,  3.0811e-01,  5.4736e-01,\n",
      "         3.6304e-01,  4.0381e-01,  1.0529e-01,  4.0356e-01,  8.7036e-02,\n",
      "         4.7363e-01,  3.0151e-01,  5.2490e-01,  3.9966e-01,  6.1377e-01,\n",
      "         3.9868e-01,  2.7100e-01,  7.2070e-01,  2.3132e-01,  2.5415e-01,\n",
      "         2.9834e-01,  3.2642e-01,  5.8740e-01,  5.2051e-01,  3.6157e-01,\n",
      "         4.6191e-01,  4.1382e-01,  2.4670e-01,  2.7319e-01,  1.3586e-01,\n",
      "         7.3486e-02,  5.1416e-01,  4.5166e-01,  2.6782e-01,  4.6826e-01,\n",
      "         1.0553e-01,  3.1567e-01,  2.3230e-01,  1.7749e-01,  3.2104e-01,\n",
      "         1.5503e-01,  2.8516e-01,  4.7241e-01,  4.3506e-01,  2.5537e-01,\n",
      "         3.1006e-01,  2.6562e-01,  3.3618e-01,  5.6592e-01,  4.6826e-01,\n",
      "         4.6875e-01,  5.5859e-01,  1.8539e-02,  5.3027e-01,  2.4796e-02,\n",
      "         5.4443e-01,  5.7568e-01,  2.6025e-01,  3.7061e-01,  4.1357e-01,\n",
      "         2.2192e-01,  3.4473e-01,  4.6924e-01,  5.1123e-01,  6.0889e-01,\n",
      "         4.4775e-01,  5.2539e-01,  1.3074e-01,  5.6494e-01,  4.9591e-02,\n",
      "         3.8379e-01,  4.2700e-01,  4.6753e-01,  3.2104e-01,  4.4800e-01,\n",
      "         1.7566e-01,  2.2107e-01,  2.9028e-01,  4.5483e-01,  3.2593e-01,\n",
      "         4.6362e-01,  1.0028e-01,  6.2256e-01,  1.8689e-01,  5.0098e-01,\n",
      "         3.0151e-01,  2.5049e-01,  3.8794e-01,  5.4980e-01,  6.9824e-02,\n",
      "         8.3252e-02,  3.6230e-01,  3.4839e-01,  3.9746e-01,  6.2744e-01,\n",
      "         1.0724e-01,  4.7070e-01,  2.8149e-01,  6.4453e-01,  2.7295e-01,\n",
      "         4.8584e-01,  2.2217e-01,  3.7549e-01,  5.8252e-01,  1.3403e-01,\n",
      "         4.4385e-01,  3.0200e-01,  2.0630e-01,  2.4817e-01,  1.8433e-01,\n",
      "         3.0762e-01,  4.5215e-01,  4.5337e-01,  4.1870e-01,  2.5171e-01,\n",
      "         2.6978e-01,  8.0627e-02,  5.2295e-01,  4.5752e-01,  4.5337e-01,\n",
      "         2.8271e-01,  1.6980e-01, -2.8062e-04,  4.5166e-01,  4.3652e-01,\n",
      "         4.0112e-01,  1.8164e-01,  4.1919e-01,  2.3889e-01,  2.2021e-01,\n",
      "         1.9897e-01,  9.2896e-02,  1.2634e-01,  3.7183e-01,  3.0640e-01,\n",
      "         2.3499e-01,  4.2212e-01,  2.6416e-01,  2.8516e-01,  4.8242e-01,\n",
      "         1.2512e-01,  4.4360e-01,  2.1582e-01,  1.9556e-01,  8.3838e-01,\n",
      "         4.1895e-01,  1.9714e-01,  4.7095e-01,  2.5781e-01,  2.7759e-01,\n",
      "         4.7217e-01,  6.1572e-01,  5.0439e-01,  3.5620e-01,  3.1445e-01,\n",
      "         2.4670e-01,  7.0862e-02,  4.7192e-01,  7.3926e-01,  4.6924e-01,\n",
      "         5.8105e-01,  4.1333e-01,  3.7842e-01,  2.3914e-01,  3.3398e-01,\n",
      "         5.2441e-01,  2.7856e-01,  2.1301e-01,  2.5366e-01,  3.6816e-01,\n",
      "         3.4302e-01,  4.2651e-01,  7.6074e-01,  1.3770e-01,  5.2295e-01,\n",
      "         4.6704e-01,  2.1912e-01,  5.2148e-01,  2.9639e-01,  5.9717e-01,\n",
      "         3.9697e-01,  1.5149e-01,  2.5220e-01,  6.1475e-01,  3.3789e-01,\n",
      "         8.2861e-01,  4.1748e-01,  5.1465e-01,  4.0747e-01,  6.8457e-01,\n",
      "         2.7686e-01,  3.0933e-01,  4.5239e-01,  4.3701e-01,  4.5752e-01,\n",
      "         3.9966e-01,  3.6792e-01,  5.1318e-01,  5.2197e-01,  4.2041e-01,\n",
      "         4.5874e-01,  7.6318e-01,  3.5400e-01,  5.2686e-01,  2.2375e-01,\n",
      "         2.9614e-01,  5.2930e-01,  5.6787e-01,  4.4214e-01,  4.9854e-01,\n",
      "         4.0112e-01,  4.0674e-01,  2.1960e-01,  6.2549e-01,  5.2881e-01,\n",
      "         5.0146e-01,  3.7646e-01,  3.8916e-01,  2.8223e-01,  3.6890e-01,\n",
      "         5.2197e-01,  5.0586e-01,  4.4873e-01,  3.1299e-01,  5.1514e-01,\n",
      "         5.3906e-01,  5.1367e-01,  4.9658e-01,  3.7354e-01,  4.9268e-01,\n",
      "         2.9663e-01,  2.6904e-01,  2.3645e-01,  1.6187e-01,  5.9912e-01,\n",
      "         5.7324e-01,  4.6338e-01,  3.0884e-01,  5.3662e-01,  2.3474e-01,\n",
      "         5.8105e-01,  7.2949e-01,  2.6733e-01,  4.4653e-01,  3.1836e-01,\n",
      "         5.1074e-01,  2.2363e-01,  4.4751e-01,  2.2693e-01,  5.6641e-01,\n",
      "         3.1274e-01,  3.7646e-01,  3.2642e-01,  4.2700e-01,  4.3433e-01,\n",
      "         2.4353e-01,  1.5771e-01,  4.8926e-01,  4.7583e-01,  5.0879e-01,\n",
      "         2.5244e-01,  2.0752e-01,  2.2229e-01,  2.4841e-01,  5.9229e-01,\n",
      "         4.8926e-01,  4.2627e-01,  3.8330e-01,  4.1431e-01,  3.9014e-01,\n",
      "         2.8589e-01,  1.8347e-01,  5.5127e-01,  4.4360e-01,  6.8408e-01,\n",
      "         5.5859e-01,  3.1592e-01,  4.2383e-01,  3.1323e-01,  7.0996e-01,\n",
      "         3.6353e-01,  5.9521e-01,  5.1465e-01,  4.4775e-01,  6.7529e-01,\n",
      "         5.9277e-01,  3.4814e-01,  4.1724e-01,  4.4238e-01,  5.1758e-01,\n",
      "         7.2021e-01,  2.5098e-01,  3.2690e-01,  2.9614e-01,  4.7852e-01,\n",
      "         7.2217e-01,  6.4404e-01,  4.9463e-01,  4.7705e-01,  5.4248e-01,\n",
      "         4.9658e-01,  2.6831e-01,  6.4795e-01,  3.6523e-01,  4.4312e-01,\n",
      "         4.9219e-01,  4.3579e-01,  3.9380e-01,  5.4688e-01,  8.3008e-01,\n",
      "         3.2471e-01,  2.2510e-01,  4.6729e-01,  5.2441e-01,  4.2529e-01,\n",
      "         5.1416e-01,  4.8242e-01,  4.1431e-01,  5.9033e-01,  4.5996e-01,\n",
      "         3.5474e-01,  2.0068e-01,  3.9209e-01,  4.7192e-01,  4.7607e-01,\n",
      "         1.7419e-01,  6.1084e-01,  6.6895e-01,  4.6533e-01,  4.6655e-01,\n",
      "         5.1172e-01,  2.5806e-01,  4.8779e-01,  5.5273e-01,  4.4458e-01,\n",
      "         6.8018e-01,  3.3472e-01,  4.5898e-01,  3.7524e-01,  2.1533e-01,\n",
      "         2.7393e-01,  4.0698e-01,  1.5369e-01,  5.1709e-01,  4.0503e-01,\n",
      "         2.2034e-01,  5.0684e-01,  3.0615e-01,  5.4297e-01,  9.0625e-01,\n",
      "         1.0602e-01,  2.0044e-01,  2.9077e-01,  4.1553e-01,  1.4014e-01,\n",
      "         3.3398e-01,  3.5840e-01,  3.7866e-01,  4.0503e-01,  2.1729e-01,\n",
      "         2.8638e-01,  3.7671e-01,  3.8916e-01,  1.1774e-01,  1.9495e-01,\n",
      "         3.8574e-01,  5.1562e-01,  1.8091e-01,  4.1626e-01,  1.7627e-01,\n",
      "         3.9038e-01,  4.1528e-01,  1.7566e-01,  5.8496e-01,  4.0942e-01,\n",
      "         2.4451e-01,  4.1675e-01,  1.3550e-01,  2.1423e-01,  2.0886e-01,\n",
      "         4.1455e-01,  4.3530e-01,  5.5420e-01,  4.6411e-01,  5.1953e-01,\n",
      "         3.5962e-01,  5.3271e-01,  1.4368e-01,  2.2583e-01,  4.8242e-01,\n",
      "         5.6738e-01,  5.6201e-01,  6.4697e-01,  5.6836e-01,  5.8936e-01,\n",
      "         7.2363e-01,  2.6904e-01,  6.3623e-01,  2.3303e-01,  2.2607e-01,\n",
      "         4.7754e-01,  4.6289e-01,  4.4019e-01,  3.0249e-01,  5.6885e-01,\n",
      "         4.8584e-01,  4.6948e-01,  4.3750e-01,  2.5122e-01,  4.6094e-01,\n",
      "         1.9153e-01,  1.9409e-01,  3.3887e-01,  9.0820e-02,  4.7656e-01,\n",
      "         5.1660e-01,  4.8682e-01,  1.9006e-01,  2.2302e-01,  5.4980e-01,\n",
      "         4.4897e-01,  3.5693e-01,  3.1006e-01,  2.1265e-01,  1.3196e-01,\n",
      "         1.0468e-01,  1.0443e-01,  5.4248e-01,  3.3325e-01,  2.2400e-01,\n",
      "         2.8809e-01,  1.1774e-01,  1.4868e-01,  5.5127e-01,  6.1084e-01,\n",
      "         5.9717e-01,  3.9062e-01,  4.5386e-01,  2.4060e-01,  4.1821e-01,\n",
      "         1.9763e-01,  4.0796e-01,  2.1741e-01,  1.4001e-01,  4.5319e-02,\n",
      "         1.2469e-01,  5.6689e-01,  5.8057e-01,  4.5190e-01,  3.7598e-01,\n",
      "         5.1514e-01,  2.8906e-01,  4.9219e-01,  4.4409e-01,  5.0146e-01,\n",
      "         1.3708e-01,  5.1465e-01,  4.6533e-01,  2.8833e-01,  3.5303e-01,\n",
      "         4.0723e-01,  1.6565e-01,  3.0298e-01,  3.0371e-01,  5.3516e-01,\n",
      "         4.5142e-01,  1.7676e-01,  4.5459e-01,  3.8281e-01,  2.3889e-01,\n",
      "         4.7656e-01,  3.4741e-01,  3.3374e-01,  2.2766e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_logits:  tensor([ 9.5764e-02, -3.3813e-01, -6.0608e-02, -5.1697e-02, -2.9028e-01,\n",
      "        -3.5419e-03, -1.0992e-01,  8.7097e-02,  3.0502e-02, -9.9548e-02,\n",
      "        -1.8750e-01, -3.8184e-01, -3.0542e-01, -1.9751e-01, -2.4109e-01,\n",
      "         1.1987e-01, -3.3643e-01, -1.3379e-01,  2.7756e-02,  9.7351e-02,\n",
      "         3.3752e-02, -2.7612e-01, -1.7761e-01,  1.8585e-02,  1.0510e-01,\n",
      "         1.2817e-02, -2.6416e-01, -1.0870e-01,  2.2144e-01,  1.5344e-01,\n",
      "         3.8849e-02,  1.2009e-02, -1.6876e-02, -8.2636e-04, -4.5898e-02,\n",
      "         2.1533e-01,  1.2006e-01, -4.5166e-02, -3.8916e-01, -2.9370e-01,\n",
      "        -1.1481e-01, -4.6021e-02,  1.9989e-02, -2.4841e-01,  2.3560e-02,\n",
      "         8.8013e-02,  8.9966e-02, -4.3213e-02,  6.6101e-02,  1.6589e-01,\n",
      "        -1.5625e-01, -4.3640e-02, -1.6284e-01, -1.8225e-01, -6.2195e-02,\n",
      "         1.9373e-01, -7.1350e-02, -1.3660e-01,  2.1155e-01, -3.5522e-02,\n",
      "        -8.2214e-02, -2.4207e-01, -1.7029e-01, -1.8420e-01, -1.5320e-02,\n",
      "        -1.0797e-01, -2.5854e-01, -1.2585e-01,  9.0027e-02, -6.3820e-03,\n",
      "        -3.9734e-02,  3.2379e-02, -1.5442e-01,  2.7649e-02,  7.4585e-02,\n",
      "        -1.5710e-01, -3.7720e-02, -1.2213e-01,  6.7406e-03,  8.9264e-03,\n",
      "        -6.6772e-02, -6.6467e-02, -1.7615e-01, -1.4087e-01,  2.0349e-01,\n",
      "        -6.6101e-02, -8.3679e-02, -2.7979e-01, -2.6147e-01,  1.9775e-01,\n",
      "        -5.1636e-02, -4.4092e-01, -2.5220e-01, -1.1200e-01, -1.9189e-01,\n",
      "        -1.8774e-01, -2.1130e-01, -2.5513e-01, -1.5564e-01,  1.8677e-01,\n",
      "        -1.6406e-01, -5.0476e-02,  7.4158e-02, -2.7295e-01,  6.3232e-02,\n",
      "        -5.4443e-02, -5.0049e-02, -9.0576e-02, -7.7209e-02, -1.6754e-02,\n",
      "        -5.2185e-02, -1.2659e-01, -2.0728e-01, -2.1729e-02, -1.2445e-01,\n",
      "        -1.3293e-01, -8.4961e-02,  5.4230e-02,  6.7566e-02, -7.6416e-02,\n",
      "         4.0802e-02,  1.0052e-01, -1.9824e-01, -1.2311e-01, -8.3313e-02,\n",
      "        -2.8760e-01, -1.5662e-01, -1.3000e-01, -1.0907e-01, -2.4414e-02,\n",
      "        -9.6741e-02, -2.7002e-01, -2.6440e-01, -8.9294e-02,  1.0730e-01,\n",
      "        -8.6304e-02,  1.0223e-01, -8.5388e-02,  9.3811e-02, -8.2153e-02,\n",
      "         4.7729e-02,  1.1823e-01, -1.2018e-01,  2.0325e-02, -1.4673e-01,\n",
      "        -1.0040e-01, -2.8003e-01, -1.1078e-01, -5.0391e-01, -2.6270e-01,\n",
      "        -9.2285e-02,  2.1801e-03,  2.6962e-02, -1.1639e-01,  2.5391e-02,\n",
      "         1.0040e-01, -1.9470e-01, -1.0760e-01, -2.6978e-01, -8.6914e-02,\n",
      "        -1.6479e-01, -2.5391e-01, -2.8638e-01, -9.5276e-02, -2.2766e-01,\n",
      "        -1.0345e-02,  1.4111e-01,  6.6772e-02,  1.8970e-01, -1.0553e-01,\n",
      "         1.4832e-01, -3.8452e-02, -1.4575e-01, -3.4204e-01, -1.1420e-01,\n",
      "         1.6617e-02, -2.0642e-01, -9.3201e-02, -4.3237e-01,  1.2988e-01,\n",
      "         2.6855e-02,  2.9938e-02, -1.6980e-01,  8.3557e-02,  1.5686e-01,\n",
      "         2.3937e-03,  8.9111e-03, -2.7124e-01, -7.5256e-02, -3.9331e-01,\n",
      "        -4.7302e-02, -1.0895e-01, -7.4463e-02, -2.4255e-01,  6.2218e-03,\n",
      "        -1.7615e-01, -2.9834e-01, -2.1271e-02, -1.3953e-01, -3.5693e-01,\n",
      "        -7.8613e-02, -5.8289e-02,  4.0466e-02,  1.7273e-02,  8.1299e-02,\n",
      "         1.0931e-01, -1.3611e-01, -4.1284e-01, -2.2424e-01, -9.9304e-02,\n",
      "        -1.0333e-01, -3.0566e-01, -3.2397e-01, -3.0396e-01,  4.1008e-03,\n",
      "        -1.0358e-01, -1.9202e-01, -2.6660e-01, -3.8037e-01, -3.9429e-01,\n",
      "        -1.4490e-01, -7.2144e-02, -7.6660e-02, -1.6815e-02, -7.7515e-02,\n",
      "        -1.3965e-01, -2.7222e-01, -2.9492e-01, -1.2634e-01, -1.4758e-01,\n",
      "        -3.5693e-01, -4.5093e-01,  1.1389e-01, -5.1416e-01, -2.4207e-01,\n",
      "        -8.6914e-02,  1.0651e-02, -1.4673e-01, -3.7451e-01, -2.3572e-01,\n",
      "        -2.8778e-02,  4.4830e-02, -1.2299e-01,  1.0754e-01,  3.1464e-02,\n",
      "        -8.1360e-02, -6.4551e-01, -4.1797e-01, -2.8442e-01, -8.5022e-02,\n",
      "        -3.5583e-02, -3.0859e-01,  3.6774e-02,  8.0414e-03,  1.2439e-01,\n",
      "        -1.9434e-01, -2.8125e-01, -1.3574e-01, -1.7529e-01, -5.1880e-02,\n",
      "        -5.3613e-01, -2.8613e-01, -2.0312e-01,  6.5369e-02, -7.1594e-02,\n",
      "        -4.6692e-02, -2.8076e-01,  1.1493e-01,  1.7044e-02, -6.9702e-02,\n",
      "         6.0577e-02,  8.2397e-02, -1.1395e-01, -3.0103e-01, -2.8320e-01,\n",
      "        -2.8809e-02, -4.8340e-01, -1.9531e-01, -1.3416e-01,  2.0300e-01,\n",
      "        -1.1346e-01, -2.0905e-02,  6.2675e-03, -2.0959e-01, -2.7417e-01,\n",
      "         1.1206e-01,  1.3391e-01, -5.8899e-02, -9.6741e-02, -1.4758e-01,\n",
      "        -1.2213e-01, -1.5900e-02, -1.0480e-01, -4.9011e-02,  4.9103e-02,\n",
      "        -2.7832e-01,  6.1768e-02, -8.0933e-02, -1.5332e-01, -4.1138e-02,\n",
      "         1.3953e-01, -3.4912e-02,  5.7159e-02, -4.0627e-03, -4.6509e-02,\n",
      "        -1.0455e-01, -5.8222e-04,  1.6675e-01, -6.7377e-04, -7.9590e-02,\n",
      "        -4.3274e-02, -7.8491e-02, -8.0078e-02, -3.2258e-04,  7.1478e-04,\n",
      "        -9.8450e-02, -3.0981e-01, -1.9519e-01, -5.1709e-01, -2.6392e-01,\n",
      "        -1.0687e-01,  4.9896e-02, -1.4880e-01, -5.1367e-01, -1.4526e-01,\n",
      "        -1.0590e-01, -1.9373e-01, -8.3862e-02, -2.0325e-01, -5.1465e-01,\n",
      "        -2.1387e-01, -3.4644e-01, -2.4158e-01, -1.8347e-01, -1.0260e-01,\n",
      "        -9.3750e-02,  1.1560e-01,  5.7159e-02, -4.2542e-02,  1.1658e-01,\n",
      "         1.2561e-01, -3.5645e-02,  1.6251e-02, -1.4160e-01,  2.0905e-02,\n",
      "        -6.8665e-02, -1.4319e-01, -4.6204e-02,  7.5684e-02,  1.0967e-03,\n",
      "         2.2369e-02, -4.1064e-01,  1.0944e-01, -4.8633e-01, -2.4133e-01,\n",
      "        -5.1758e-02,  7.1838e-02, -1.1932e-01, -2.5439e-01, -1.9604e-01,\n",
      "         2.0813e-02,  1.4050e-01, -9.0942e-02,  1.8176e-01,  9.9915e-02,\n",
      "        -2.6831e-01, -6.5869e-01, -2.8979e-01, -1.4771e-01,  3.4454e-02,\n",
      "         5.6641e-02, -3.3228e-01, -1.2421e-02, -2.8101e-01,  2.2411e-03,\n",
      "        -4.0649e-02,  7.6561e-03,  4.5654e-02,  1.6449e-02, -6.2744e-02,\n",
      "        -5.7617e-02, -2.4548e-01,  5.3467e-02, -1.2756e-02,  5.3009e-02,\n",
      "        -2.1753e-01, -2.4353e-01,  7.4707e-02,  5.2887e-02,  1.4978e-01,\n",
      "        -1.1591e-01, -1.6016e-01,  4.5685e-02, -2.9297e-01,  4.8485e-03,\n",
      "        -8.5022e-02, -2.4207e-01, -1.7639e-01,  1.2433e-01,  7.2571e-02,\n",
      "        -2.9011e-03, -1.5320e-02,  4.6112e-02,  8.5815e-02, -5.4016e-02,\n",
      "         8.4473e-02, -9.3018e-02,  1.0254e-01, -8.0750e-02,  1.7603e-01,\n",
      "         3.7781e-02, -7.0984e-02, -1.3904e-01,  1.0413e-01,  1.4282e-01,\n",
      "         2.1838e-01, -3.4363e-02, -1.6113e-01,  3.4485e-02, -4.2664e-02,\n",
      "        -4.9121e-01, -9.7473e-02, -1.5717e-02,  1.4160e-01, -1.1505e-02,\n",
      "        -8.1177e-03, -4.7913e-02, -3.9551e-02,  9.0637e-02,  1.2274e-01,\n",
      "         4.0245e-03, -2.1790e-01, -1.1285e-01, -4.6411e-01, -2.6294e-01,\n",
      "        -5.6458e-02,  7.2998e-02, -6.4453e-02, -4.6851e-01,  1.1053e-01,\n",
      "        -1.0590e-02,  2.2607e-01, -1.2695e-02,  3.0731e-02,  2.0605e-01,\n",
      "         4.1595e-02, -1.4832e-01,  3.0762e-02,  2.1826e-01, -9.7656e-03,\n",
      "         5.1788e-02,  2.0032e-01,  1.0358e-01, -1.1224e-01, -3.0737e-01,\n",
      "        -1.3843e-01,  9.2407e-02,  2.2522e-01, -1.6174e-02,  6.1249e-02,\n",
      "         8.5083e-02, -1.9189e-01,  9.3079e-02,  1.5466e-01, -3.9209e-01,\n",
      "        -1.2238e-01,  6.8321e-03, -1.2054e-01, -2.4426e-01, -3.4155e-01,\n",
      "         1.3403e-01,  2.8580e-02, -6.4087e-02, -1.9684e-02,  3.0273e-02,\n",
      "         1.2891e-01, -6.7078e-02, -2.6794e-02,  1.9788e-01, -5.6496e-03,\n",
      "        -5.8937e-03,  2.3209e-02, -7.7209e-02,  2.4567e-02,  1.0968e-01,\n",
      "         8.3679e-02,  1.7624e-02,  8.3008e-02, -1.0236e-01,  4.3915e-02,\n",
      "         8.9600e-02,  1.1725e-01,  4.9866e-02, -1.4441e-01,  2.2913e-01,\n",
      "         1.8066e-01,  3.4637e-02,  6.9397e-02, -6.1279e-02, -1.6150e-01,\n",
      "         1.3000e-02,  1.7407e-01, -4.2847e-02,  7.7454e-02,  2.3279e-01,\n",
      "        -9.9915e-02, -3.0933e-01, -2.9541e-01, -3.1921e-02, -1.8298e-01,\n",
      "        -3.6719e-01, -3.2080e-01,  1.0114e-01, -1.1841e-02, -1.0376e-02,\n",
      "         8.1787e-03, -6.3721e-02, -4.1199e-02, -1.0529e-01,  8.7402e-02,\n",
      "         5.2094e-02, -2.3132e-01, -2.1985e-01,  4.0466e-02, -1.2091e-01,\n",
      "        -3.2056e-01, -2.3792e-01,  8.7830e-02,  1.4453e-01,  8.3191e-02,\n",
      "        -5.2490e-01,  1.1151e-01, -8.4839e-02,  1.4075e-01, -6.0364e-02,\n",
      "         1.4709e-01, -4.4629e-01, -1.3049e-01, -3.2520e-01, -2.0801e-01,\n",
      "        -1.5430e-01, -8.8623e-02, -2.0355e-02, -2.5940e-02,  1.0413e-01,\n",
      "        -1.7468e-01, -3.1452e-03, -1.6028e-01, -9.5032e-02,  2.3026e-02,\n",
      "         1.4954e-02, -9.9304e-02, -4.9622e-02, -1.4368e-01, -8.5999e-02,\n",
      "         1.1212e-01, -3.1885e-01,  9.5398e-02, -1.7859e-01, -2.8589e-01,\n",
      "        -2.3596e-01, -1.3147e-01, -9.9182e-02, -3.5571e-01, -4.8145e-01,\n",
      "        -1.1420e-01, -5.1025e-01, -2.0166e-01, -2.8412e-02,  9.6985e-02,\n",
      "        -1.9580e-01, -1.4648e-01, -3.3960e-01, -2.2864e-01,  1.5697e-03,\n",
      "         8.4167e-02, -1.9080e-01, -1.0504e-01, -3.1396e-01, -2.3206e-01,\n",
      "         2.0740e-01,  3.6469e-02, -5.2002e-02, -8.9294e-02, -3.1036e-02,\n",
      "        -1.6931e-01, -1.1530e-01, -5.9033e-01, -8.1482e-02,  1.9678e-01,\n",
      "         2.0767e-02, -1.0895e-01, -3.1311e-02, -2.7124e-01, -7.3608e-02,\n",
      "        -8.5688e-04, -8.9722e-02, -7.6843e-02, -2.4524e-01,  1.7981e-01,\n",
      "        -2.5726e-02,  3.7323e-02,  1.4514e-01,  1.5588e-01,  1.0626e-01,\n",
      "        -6.1646e-02, -4.5898e-02, -2.0581e-01, -1.6345e-01, -3.9478e-01,\n",
      "        -6.3538e-02, -1.8384e-01, -3.8501e-01, -1.7590e-01, -8.6243e-02,\n",
      "         4.2664e-02, -1.0065e-01, -1.3489e-01, -1.0919e-01, -2.7637e-01,\n",
      "        -2.5562e-01, -1.4575e-01, -3.4619e-01,  4.0039e-02, -2.4866e-01,\n",
      "        -2.7856e-01, -2.0325e-01, -7.0429e-04, -2.1375e-01,  2.3804e-02,\n",
      "         7.2327e-02, -9.1187e-02, -1.0626e-01, -1.1786e-01, -8.6365e-02,\n",
      "        -7.5256e-02, -6.4514e-02, -4.1211e-01, -1.8896e-01, -1.7566e-01,\n",
      "        -3.9111e-01, -3.5156e-02, -3.1812e-01, -2.1399e-01, -1.5503e-01,\n",
      "        -8.5876e-02,  2.5421e-02, -5.0171e-02,  3.4668e-02, -9.9182e-02,\n",
      "        -6.5369e-02, -7.9041e-02, -4.7646e-03, -3.4912e-01, -2.0312e-01,\n",
      "         1.2207e-01, -1.2915e-01,  2.4826e-02,  9.2041e-02, -8.8623e-02,\n",
      "         8.5510e-02, -6.2805e-02,  9.1400e-03, -1.3721e-01, -4.4922e-02,\n",
      "         2.3468e-02,  1.6064e-01, -2.6782e-01, -3.0933e-01, -1.7395e-01,\n",
      "         3.5095e-02,  3.0701e-02, -8.4961e-02, -5.5078e-01, -1.5137e-01,\n",
      "        -5.6885e-01, -2.6782e-01, -5.5115e-02,  7.8735e-03, -3.5547e-01,\n",
      "        -3.0762e-01, -2.3792e-01,  1.9665e-03,  5.4871e-02, -3.5400e-01,\n",
      "         1.3538e-01, -3.4546e-02, -4.0698e-01, -8.2178e-01, -3.3569e-01,\n",
      "        -2.1313e-01, -4.1077e-02, -4.5349e-02, -5.4297e-01, -2.3474e-01,\n",
      "        -2.9712e-01, -1.3708e-01, -6.0059e-02,  5.2910e-03, -9.1980e-02,\n",
      "        -5.8105e-02,  1.0574e-02,  5.6824e-02, -1.9263e-01, -8.8989e-02,\n",
      "        -9.5032e-02, -9.8694e-02, -2.0056e-01,  2.6886e-02, -1.4136e-01,\n",
      "        -1.1542e-01,  2.2751e-02, -1.1963e-02, -1.4923e-02, -2.5415e-01,\n",
      "        -2.7710e-01,  1.1993e-01,  1.2131e-02,  9.5032e-02,  7.6866e-03,\n",
      "        -2.1094e-01, -2.1875e-01, -4.3213e-02,  8.3069e-02, -7.6111e-02,\n",
      "        -2.2253e-01, -4.0112e-01, -1.0638e-01, -9.5154e-02, -2.5342e-01,\n",
      "        -3.2495e-01, -3.3691e-01, -4.4360e-01, -5.9717e-01, -1.6663e-01,\n",
      "        -3.0713e-01, -4.0601e-01, -9.6252e-02, -2.3108e-01, -2.0789e-01,\n",
      "        -2.7979e-01, -3.3716e-01, -2.8491e-01, -1.7273e-01, -1.5857e-01,\n",
      "         1.7288e-02, -3.3569e-01, -1.5015e-02, -2.2229e-01, -4.6582e-01,\n",
      "        -2.7759e-01, -1.9580e-01,  3.2349e-02, -3.5461e-02, -1.4014e-01,\n",
      "        -4.6875e-01, -2.0410e-01, -1.1664e-01, -2.9492e-01, -1.6516e-01,\n",
      "         4.1077e-02,  1.4679e-02, -1.1078e-01,  1.0889e-01, -1.6638e-01,\n",
      "        -4.6118e-01,  1.0809e-01, -2.5708e-01, -1.5869e-01, -2.0386e-01,\n",
      "        -3.6279e-01, -1.5747e-01, -2.8149e-01, -3.6890e-01, -2.4304e-01,\n",
      "         1.1572e-01, -2.9297e-02, -6.4941e-02,  1.1032e-02, -2.2815e-01,\n",
      "        -7.5293e-01, -3.3398e-01, -2.4524e-01, -4.2188e-01, -2.8882e-01,\n",
      "        -1.7688e-01, -9.2407e-02, -1.7529e-01, -7.2998e-02, -3.3105e-01,\n",
      "        -2.9590e-01,  2.9327e-02, -9.0210e-02, -4.3335e-02, -2.0178e-01,\n",
      "        -2.8076e-01, -4.3726e-01, -3.5522e-01, -7.6050e-02, -2.3474e-01,\n",
      "        -3.3667e-01, -2.3962e-01, -3.3154e-01, -1.7578e-01, -2.5635e-01,\n",
      "         4.6997e-02, -1.2695e-01, -5.6702e-02, -1.3232e-01, -1.7883e-01,\n",
      "        -2.6123e-01, -1.3257e-01, -2.1399e-01, -1.3818e-01, -2.7881e-01,\n",
      "        -1.7542e-01, -1.6211e-01, -2.7466e-01,  1.0040e-01, -2.0142e-01,\n",
      "         8.6365e-02, -1.4746e-01,  1.3586e-01,  2.1021e-01,  1.5344e-01,\n",
      "         3.7518e-03, -1.5149e-01, -1.0321e-01, -3.2673e-03, -1.8274e-01,\n",
      "        -3.6108e-01, -2.5928e-01, -4.4873e-01, -3.7549e-01, -5.5908e-02,\n",
      "        -3.9185e-02, -2.0105e-01, -3.5718e-01, -1.6711e-01, -2.9492e-01,\n",
      "        -6.8665e-02, -1.1566e-01, -2.2375e-01,  8.6304e-02, -3.2776e-02,\n",
      "        -2.4738e-03, -2.2424e-01, -1.1200e-02,  2.3331e-02, -6.8665e-02,\n",
      "        -2.1826e-01, -1.0663e-01, -1.4294e-01, -3.6499e-01,  1.1285e-01,\n",
      "        -3.9062e-01, -2.7808e-01,  1.0117e-02,  8.5205e-02, -2.9370e-01,\n",
      "        -2.0081e-01, -2.3010e-01,  6.8970e-02,  1.5027e-01, -2.9468e-01,\n",
      "         2.4695e-01,  4.0771e-02, -2.7344e-01, -7.0801e-01, -2.5513e-01,\n",
      "        -2.0227e-01, -6.4125e-03,  4.9225e-02, -3.2593e-01,  2.7145e-02,\n",
      "         7.0915e-03,  3.8147e-02, -1.2622e-01, -1.6528e-01,  1.6797e-01,\n",
      "        -1.7151e-02,  1.6357e-01, -1.7297e-01, -1.9568e-01, -1.2335e-01,\n",
      "        -2.7802e-02,  5.4962e-02, -4.8486e-01, -2.9688e-01, -2.4475e-01,\n",
      "        -3.2063e-03,  7.7026e-02, -1.1945e-01, -3.2104e-01,  5.0446e-02,\n",
      "         3.7518e-03, -1.7358e-01, -2.2522e-01, -3.0322e-01,  8.6731e-02,\n",
      "        -4.6777e-01, -2.1265e-01, -1.6748e-01,  1.2598e-01,  7.2937e-02,\n",
      "        -3.5034e-02, -2.5806e-01, -1.0162e-02,  7.5928e-02,  1.4661e-01,\n",
      "        -2.4744e-01,  1.4954e-01, -6.6711e-02,  9.8694e-02,  8.2092e-02,\n",
      "         4.9255e-02, -3.1342e-02, -2.9236e-02,  2.4133e-01,  1.5430e-01,\n",
      "        -2.2961e-01, -2.0776e-01,  9.0454e-02,  5.3955e-02,  1.4099e-01,\n",
      "        -1.0870e-01, -2.8870e-02, -5.5237e-02,  1.0303e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0488,  0.0478,  0.0370,  ..., -0.2421, -0.0288,  0.2487],\n",
      "         [ 0.0485,  0.0183,  0.0960,  ..., -0.1039, -0.0550,  0.0843],\n",
      "         [ 0.0056, -0.0733,  0.0301,  ..., -0.1319, -0.0465,  0.1158],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.2649,  0.3196,  0.3027,  0.4058,  0.3071,  0.2795,  0.1566,  0.6665,\n",
      "         0.1401,  0.2644,  0.2793,  0.4067,  0.4392,  0.0711,  0.4470,  0.1539,\n",
      "         0.3933,  0.2308,  0.2527,  0.5029, -0.0339,  0.3298, -0.0993,  0.2217,\n",
      "         0.3525,  0.1934,  0.5254,  0.4265,  0.2974,  0.4214,  0.2864,  0.4287,\n",
      "         0.2308,  0.2866,  0.2360,  0.2462,  0.6133,  0.3452,  0.2374,  0.2238,\n",
      "         0.6055,  0.0565,  0.4263,  0.1218,  0.5640, -0.0140,  0.1642,  0.4951,\n",
      "         0.5635,  0.2112,  0.4314,  0.6343,  0.3162,  0.4712,  0.3215,  0.2791,\n",
      "         0.1543,  0.2147,  0.4263, -0.0044,  0.5015,  0.3601,  0.3074,  0.3899,\n",
      "         0.2496,  0.4521,  0.1693,  0.3464,  0.5576,  0.1153,  0.5376,  0.4143,\n",
      "         0.0280,  0.6470,  0.1698,  0.2842,  0.1626,  0.6284,  0.1851,  0.7114,\n",
      "         0.1969,  0.2886,  0.1075,  0.6338,  0.2246,  0.2917,  0.2874,  0.4636,\n",
      "         0.1099,  0.4314,  0.3328,  0.3704,  0.8276,  0.0991,  0.4065,  0.3511,\n",
      "         0.6724,  0.5625,  0.1863,  0.3843,  0.5640,  0.4937,  0.3457,  0.4839,\n",
      "         0.5967,  0.4434,  0.0799,  0.3135,  0.3821,  0.5508,  0.1227,  0.4097,\n",
      "         0.3022,  0.3494,  0.6260,  0.4365,  0.5679,  0.3293,  0.4976,  0.4426,\n",
      "         0.6313,  0.5054,  0.5083,  0.3792,  0.5371,  0.6025,  0.5107,  0.5107,\n",
      "         0.5430,  0.2534,  0.5752,  0.3362,  0.3953,  0.4785,  0.5078,  0.3916,\n",
      "         0.3530,  0.6572,  0.2073,  0.5674,  0.0638,  0.4854,  0.7583,  0.3896,\n",
      "         0.4221,  0.3464,  0.3726,  0.2661,  0.4180,  0.5669,  0.3547,  0.2367,\n",
      "         0.3977,  0.0375,  0.4580,  0.6455,  0.6255,  0.4785,  0.3936,  0.5537,\n",
      "         0.1544,  0.2433,  0.3167, -0.0605,  0.3953,  0.2294,  0.3608,  0.4971,\n",
      "         0.1545,  0.5044,  0.6289,  0.5337,  0.4209,  0.7363,  0.2930,  0.0435,\n",
      "         0.4912, -0.0040,  0.2893,  0.2294,  0.3892,  0.4294,  0.3533,  0.5952,\n",
      "         0.2382, -0.1356,  0.4795,  0.2612,  0.4385,  0.2942,  0.2316,  0.3916,\n",
      "         0.4629,  0.1453,  0.1407,  0.4226,  0.2642,  0.5410,  0.0193,  0.2255,\n",
      "         0.2922,  0.3105,  0.4736,  0.3213,  0.2169,  0.3367,  0.1899,  0.5400,\n",
      "         0.6753,  0.4382,  0.4912,  0.3328,  0.4082,  0.5020,  0.7817,  0.6079,\n",
      "         0.5107,  0.4546,  0.5767,  0.7939,  0.7905,  0.8198,  0.7661,  0.8994,\n",
      "         0.5405,  0.4302,  0.3918,  0.5371,  0.7871,  0.7100,  0.8066,  0.7690,\n",
      "         0.4502,  0.4272,  0.7700,  0.5557,  0.8472,  0.3049,  0.6426,  0.4192,\n",
      "         0.9429,  0.5620,  0.4419,  0.4009,  0.9023,  0.1924,  0.3848,  0.4109,\n",
      "         0.6411,  0.0069,  0.6240,  0.1190,  0.4309,  0.2659,  0.2460,  0.6313,\n",
      "        -0.0213,  0.3411,  0.1098,  0.2465,  0.4995,  0.1842,  0.2205,  0.8193,\n",
      "         0.3821,  0.4717,  0.3086,  0.3782,  0.6274,  0.4099,  0.6284,  0.2627,\n",
      "         0.4280,  0.2081,  0.3074,  0.2717,  0.3044,  0.3694,  0.3179,  0.4014,\n",
      "         0.3857,  0.7485,  0.4314,  0.4412,  0.5205,  0.4819,  0.1696,  0.4268,\n",
      "         0.3552,  0.4744,  0.8667,  0.4001,  0.4749,  0.4065,  0.3123,  0.5942,\n",
      "         0.2512,  0.6597,  0.5757,  0.2744,  0.5151,  0.3330,  0.2886,  0.0701,\n",
      "         0.4492,  0.4509,  0.3364,  0.4165,  0.2949,  0.3115,  0.3523,  0.5571,\n",
      "         0.3645,  0.4617,  0.4429,  0.4399,  0.2416,  0.6572,  0.3899,  0.1508,\n",
      "         0.0677,  0.6592,  0.3364,  0.6348,  0.4258,  0.4102,  0.5742,  0.5601,\n",
      "         0.5947,  0.3811,  0.5757,  0.3936,  0.5972,  0.4648,  0.3538,  0.2382,\n",
      "         0.3174,  0.5381,  0.5220,  0.2252,  0.3433,  0.4248,  0.3416,  0.3711,\n",
      "         0.5986,  0.5894,  0.7046,  0.4336,  0.5425,  0.4333,  0.4849,  0.5073,\n",
      "         0.4131,  0.1995,  0.3245,  0.3730,  0.5322,  0.0497,  0.3931,  0.2708,\n",
      "         0.2700, -0.1101,  0.3223,  0.0870,  0.3386,  0.3594,  0.6260,  0.5322,\n",
      "         0.6338,  0.7510,  0.4233,  0.3403,  0.5273,  0.5117,  0.4353,  0.6128,\n",
      "         0.4670,  0.2861,  0.5796,  0.3057,  0.6763,  0.4839,  0.5063,  0.3284,\n",
      "         0.2396,  0.4937,  0.4734,  1.0869,  0.6685,  0.5049,  1.2061,  0.3262,\n",
      "         0.3301,  0.2632,  0.4009,  0.7114,  0.6880,  0.4495,  0.5425,  0.0408,\n",
      "         0.5107,  0.3396,  0.2369,  0.2046,  0.2343,  0.4761,  0.3398,  0.5562,\n",
      "         0.3127,  0.7559,  0.4907,  0.3826,  0.3181,  0.5264,  0.7104,  0.6870,\n",
      "         0.7349,  0.6675,  0.3015,  0.2408,  0.3391,  0.1613,  0.1699,  0.4631,\n",
      "         0.1953,  0.1538,  0.6616,  0.6074,  0.2866,  0.3582,  0.2747,  0.4985,\n",
      "         0.2098,  0.5537,  0.3098,  0.7593,  0.1407,  0.6025,  0.2244,  0.0754,\n",
      "         0.5088,  0.6128,  0.7456,  0.5068,  0.3196,  0.3218,  0.5312,  0.6216,\n",
      "         0.5991,  0.7041,  0.6313,  0.2800,  0.2443,  0.4065,  0.2462,  0.0937,\n",
      "         0.4250,  0.2771,  0.4956,  0.4368,  0.3960,  0.4312,  0.1393,  0.4592,\n",
      "         0.3149,  0.3147,  0.3359,  0.3718,  0.1181,  0.3108,  0.2061,  0.3113,\n",
      "         0.3118,  0.5635,  0.3796,  0.4045,  0.3984,  0.1323,  0.1306,  0.6001,\n",
      "         0.4375,  0.3843,  0.3313,  0.4675,  0.1791,  0.6836,  0.3074,  0.5327,\n",
      "         0.4126,  0.2642,  0.3582,  0.4509,  0.3972,  0.6890,  0.5078,  0.3262,\n",
      "         0.6719,  0.6646,  0.1403,  0.6104,  0.3987,  0.6567,  0.3831,  0.4590,\n",
      "         0.6108,  0.2170,  0.3987,  0.6191,  0.4795,  0.2372,  0.5005,  0.3533,\n",
      "         0.2460,  0.3149,  0.4160,  0.1219,  0.1960,  0.1632,  0.2211,  0.1722,\n",
      "         0.1070,  0.3550,  0.2849,  0.1409,  0.3049, -0.0333,  0.3145,  0.1803,\n",
      "         0.1783,  0.3394,  0.3589,  0.2062,  0.1648,  0.1158,  0.1555,  0.2418,\n",
      "         0.2350,  0.3857,  0.3347,  0.3369,  0.7031,  0.1996,  0.3367,  0.3320,\n",
      "         0.1881,  0.4333,  0.4663,  0.3274,  0.7217,  0.3794,  0.4741,  0.3120,\n",
      "         0.2654,  0.2581,  0.3132,  0.3398,  0.1910,  0.4368,  0.2874,  0.0876,\n",
      "         0.1658,  0.1243,  0.0803,  0.2225,  0.3882,  0.0140,  0.3352,  0.4927,\n",
      "         0.5498,  0.4927,  0.7788,  0.6631,  0.5840,  0.2917,  0.3679,  0.3762,\n",
      "         0.6787,  0.8945,  0.1736,  0.2656,  0.3340,  0.8545,  0.5483,  0.4888,\n",
      "         0.4211,  0.5884,  0.7627,  0.6704,  0.7305,  0.7051,  0.4272,  0.0367,\n",
      "         0.5713,  0.3691,  0.2891,  0.8994,  0.3479,  0.3394,  0.2050,  0.2629,\n",
      "         0.4858,  0.2512,  0.5264,  0.5835,  0.3428,  0.4434,  0.4961,  0.6997,\n",
      "         0.5752,  0.5361,  0.2842,  0.7910,  0.5977,  0.5352,  0.4084,  0.5386,\n",
      "         0.6108,  0.7837,  0.6367,  0.3020,  0.7207,  0.5054,  0.2869,  0.7471,\n",
      "         0.5479,  0.4937,  0.2443], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 3.7567e-02, -1.8860e-02, -2.2156e-02,  3.6102e-02,  9.2773e-02,\n",
      "        -6.9031e-02, -1.5979e-01, -1.3660e-01, -3.4546e-02, -3.9355e-01,\n",
      "         1.7471e-02, -8.8196e-02, -7.9529e-02, -1.4307e-01, -2.7679e-02,\n",
      "        -3.2300e-01, -4.0210e-01,  2.0703e-01,  6.2523e-03, -3.6865e-02,\n",
      "        -8.3740e-02,  4.0283e-01, -2.4744e-01, -2.4524e-01, -1.0712e-02,\n",
      "        -6.8848e-02, -1.5717e-02,  1.2512e-01, -1.4795e-01, -9.0210e-02,\n",
      "         1.6113e-01, -1.8921e-01,  1.5015e-01, -3.4619e-01,  2.1570e-01,\n",
      "        -1.0492e-01, -1.4795e-01, -3.1250e-01,  2.0862e-01, -9.5520e-02,\n",
      "        -1.2225e-01, -2.1802e-01,  2.1069e-01,  7.9834e-02, -7.0923e-02,\n",
      "        -3.4302e-02,  7.9407e-02,  1.2433e-01,  1.5649e-01, -2.9877e-02,\n",
      "         3.1586e-02,  1.1786e-01, -1.3074e-01, -2.8168e-02,  5.2856e-02,\n",
      "         5.5908e-02,  2.3621e-01,  1.2598e-01, -2.1826e-01, -1.9092e-01,\n",
      "        -3.5327e-01, -3.3813e-01,  1.2103e-01, -1.4221e-01, -3.8501e-01,\n",
      "        -2.2717e-01, -1.6370e-01, -1.9299e-01, -1.7566e-01, -1.3696e-01,\n",
      "        -7.2754e-02, -8.1055e-02, -1.5186e-01, -4.4214e-01, -4.5581e-01,\n",
      "         6.1554e-02, -3.8208e-02, -5.0842e-02, -2.9492e-01, -4.7412e-01,\n",
      "        -3.2715e-02,  1.0773e-02, -1.0956e-01, -2.8369e-01, -2.1716e-01,\n",
      "        -2.2229e-01, -1.8018e-01, -3.0859e-01, -2.1021e-01, -5.1465e-01,\n",
      "         7.7881e-02, -1.3525e-01, -4.4165e-01, -1.9934e-01,  3.9978e-02,\n",
      "        -1.2939e-01, -1.6565e-01, -1.6589e-01, -2.5848e-02, -9.6741e-03,\n",
      "        -1.3025e-01,  4.4678e-02, -1.7981e-01, -1.7395e-01, -1.5466e-01,\n",
      "         7.0915e-03,  4.6753e-02, -1.4954e-01, -9.0210e-02, -9.4421e-02,\n",
      "        -1.3892e-01, -5.2832e-01,  1.4270e-01, -8.1055e-02, -1.1212e-01,\n",
      "        -4.3311e-01, -4.7241e-02, -1.0468e-01,  1.2915e-01,  8.0200e-02,\n",
      "        -7.8491e-02,  9.3262e-02,  1.5576e-01, -1.6650e-01,  5.5023e-02,\n",
      "        -7.7637e-02, -1.4191e-02, -4.3945e-02, -3.8696e-02, -2.2498e-01,\n",
      "        -7.2632e-02,  4.9072e-02,  1.8445e-01, -4.5288e-02, -3.0322e-01,\n",
      "         1.3452e-01, -1.3953e-01, -1.7871e-01, -3.2739e-01, -8.9722e-02,\n",
      "        -8.5693e-02, -1.3447e-03, -7.9651e-02, -2.9907e-01, -8.5205e-02,\n",
      "        -6.0425e-02, -7.4341e-02, -1.4221e-01, -1.6199e-01, -2.0288e-01,\n",
      "        -3.8550e-01,  1.4600e-01, -8.7036e-02, -3.4729e-02,  1.3062e-01,\n",
      "         7.9346e-02, -2.1985e-01, -2.4890e-01,  5.3406e-02, -2.8198e-02,\n",
      "        -3.7134e-01,  8.3618e-02, -8.8196e-02, -1.4624e-01,  3.6865e-02,\n",
      "        -4.6936e-02,  5.6114e-03,  7.3004e-04, -1.1383e-01,  1.6003e-01,\n",
      "         1.3708e-01, -2.3743e-01, -2.0190e-01, -2.8418e-01, -2.3645e-01,\n",
      "        -4.2798e-01, -1.4209e-01, -4.1577e-01, -1.0724e-01, -2.3035e-01,\n",
      "        -1.5271e-01, -3.0640e-02,  5.1910e-02, -2.3315e-02, -2.8198e-01,\n",
      "        -1.6785e-01, -1.9177e-01, -2.4084e-01, -8.7952e-02, -7.4585e-02,\n",
      "        -1.8335e-01, -1.3684e-01,  3.8403e-01, -1.7798e-01, -3.3130e-01,\n",
      "        -3.6316e-02, -4.1870e-01, -2.5220e-01, -4.2822e-01, -1.2286e-01,\n",
      "        -1.3208e-01, -1.1298e-01, -2.8351e-02, -4.9927e-02, -2.5464e-01,\n",
      "         1.8753e-02, -1.1505e-02,  2.4918e-02,  5.8197e-02, -2.2986e-01,\n",
      "         4.0253e-02, -1.4539e-01, -4.2188e-01,  2.4582e-02, -1.2891e-01,\n",
      "         2.7734e-01, -1.2250e-01,  1.7908e-01,  1.2500e-01,  2.5928e-01,\n",
      "        -8.2764e-02, -2.3669e-01,  5.8098e-03, -1.6309e-01,  2.4475e-01,\n",
      "        -1.5503e-01,  1.8555e-01,  1.2390e-01,  3.0396e-01, -1.0504e-01,\n",
      "        -1.9995e-01,  2.8305e-02, -5.6006e-01, -9.1125e-02, -1.7786e-01,\n",
      "        -1.5289e-02, -1.4722e-01,  1.8372e-02, -4.2542e-02,  9.1248e-02,\n",
      "        -1.8433e-01, -1.1298e-01, -7.0496e-02, -2.6001e-01, -2.9370e-01,\n",
      "        -6.4648e-01, -7.0801e-02, -2.2693e-01, -3.4424e-01, -2.9492e-01,\n",
      "        -5.0684e-01, -4.8682e-01, -4.5850e-01,  1.5027e-01,  2.1439e-02,\n",
      "        -1.5125e-01, -2.8540e-01,  3.7622e-01, -3.1006e-01, -2.6782e-01,\n",
      "        -6.3782e-02,  1.4324e-03, -4.8096e-02, -1.6333e-01, -3.8306e-01,\n",
      "        -2.2552e-02, -7.3120e-02, -5.9753e-02, -1.0248e-01, -4.2297e-02,\n",
      "        -1.5649e-01, -1.0492e-01,  4.2908e-02, -4.2152e-03, -1.7749e-01,\n",
      "        -1.9165e-01,  4.1466e-03, -3.4363e-02,  5.4138e-02, -1.9971e-01,\n",
      "        -2.3792e-01, -1.9629e-01,  9.7656e-03, -5.8289e-02, -1.7029e-01,\n",
      "        -2.3926e-02, -5.4016e-02, -7.7858e-03, -1.3275e-02, -2.1027e-02,\n",
      "        -3.2104e-02,  4.3671e-02, -5.4688e-02,  1.6833e-01, -1.3696e-01,\n",
      "        -9.7839e-02, -1.3867e-01, -1.6804e-03,  1.7868e-02, -1.0138e-01,\n",
      "         8.8562e-02,  8.8806e-02,  7.4707e-02, -1.0211e-01,  1.7139e-01,\n",
      "        -1.0358e-01, -2.7808e-01, -4.4434e-02,  5.3619e-02,  3.2959e-02,\n",
      "         1.1212e-01, -1.9165e-02, -3.4668e-02, -2.5122e-01,  1.9054e-03,\n",
      "        -2.2400e-01,  1.0132e-01,  1.5234e-01,  4.8511e-01, -1.5039e-01,\n",
      "        -1.7712e-01, -1.2585e-01, -7.3975e-02, -1.7871e-01,  4.4464e-02,\n",
      "        -1.5717e-02,  4.5837e-02,  1.7224e-01, -1.2054e-01,  3.4180e-02,\n",
      "         2.0776e-01,  3.3661e-02,  1.1475e-01,  1.1224e-01, -3.7720e-01,\n",
      "         1.4575e-01, -2.2620e-01, -6.8542e-02,  1.8677e-01, -3.8391e-02,\n",
      "         1.0773e-01, -8.4656e-02, -2.1826e-01, -1.9165e-02,  4.5197e-02,\n",
      "        -1.1530e-01,  4.5441e-02,  1.4807e-01,  1.2225e-01, -6.3293e-02,\n",
      "        -2.3529e-02, -1.5747e-01,  3.1812e-01,  3.1891e-02,  1.0272e-01,\n",
      "        -7.7881e-02,  4.2633e-02, -9.3872e-02, -1.5625e-02,  1.1517e-01,\n",
      "         4.6448e-02, -1.5222e-01,  4.6191e-01, -1.0394e-01, -1.7725e-01,\n",
      "        -1.6113e-01,  9.2102e-02, -1.0797e-01,  1.3928e-01,  1.1829e-01,\n",
      "        -6.1951e-02,  7.4883e-03,  1.4954e-01,  1.3684e-01, -1.0846e-01,\n",
      "        -6.2439e-02,  2.4811e-02,  1.0513e-02,  6.6032e-03, -1.2494e-01,\n",
      "         3.3478e-02, -7.4341e-02,  1.1795e-02, -3.6719e-01,  1.4551e-01,\n",
      "        -5.2368e-02, -9.7107e-02, -2.1851e-01, -1.0321e-01, -3.3569e-02,\n",
      "        -1.9641e-01, -4.3018e-01, -6.8909e-02, -5.9570e-02, -7.8125e-02,\n",
      "         2.4188e-04,  9.8419e-03,  1.4490e-01,  7.1289e-02,  4.2419e-02,\n",
      "         6.5491e-02,  6.6956e-02, -1.7249e-01, -2.6001e-01,  3.2690e-01,\n",
      "         1.1438e-01, -8.1421e-02,  7.2441e-03, -8.6243e-02, -3.2776e-02,\n",
      "         4.2383e-01,  2.0123e-03,  2.9248e-01,  2.3633e-01,  4.3408e-01,\n",
      "         9.3445e-02, -9.8816e-02,  2.2791e-01, -3.4497e-01,  1.4661e-01,\n",
      "        -7.7271e-02, -1.0071e-02, -2.1204e-01,  4.7272e-02,  9.3628e-02,\n",
      "        -1.4197e-01,  4.2139e-01,  1.6846e-01,  2.9434e-02, -3.2471e-02,\n",
      "         5.1392e-02,  1.5434e-02, -9.9304e-02, -1.9287e-02,  1.4893e-01,\n",
      "         1.0211e-01,  1.7471e-02,  5.5725e-02,  2.6636e-01,  3.4863e-01,\n",
      "         1.6479e-01,  3.1372e-02,  3.7170e-02,  4.4043e-01, -2.0508e-02,\n",
      "         2.8979e-01,  2.8809e-01,  5.0537e-01,  1.5625e-01, -7.7087e-02,\n",
      "         2.4280e-01, -3.0591e-01,  1.4600e-01, -1.8311e-02, -5.6152e-02,\n",
      "        -4.7852e-02,  1.5515e-01,  7.2998e-02,  2.1924e-01,  2.4634e-01,\n",
      "         3.3203e-02, -8.8196e-02, -2.7649e-02, -8.0261e-02, -7.7881e-02,\n",
      "         1.2842e-01, -1.5015e-01,  1.7090e-01, -1.9263e-01,  7.3120e-02,\n",
      "        -4.5166e-02, -1.3092e-02,  7.1068e-03,  1.3306e-01,  2.0276e-01,\n",
      "         1.1090e-01, -6.7017e-02, -2.6581e-02,  1.7688e-01,  1.8921e-02,\n",
      "         1.9751e-01,  1.0315e-02,  3.4466e-03, -4.0588e-02, -4.0649e-02,\n",
      "         2.6953e-01, -1.3757e-01,  3.1219e-02,  8.7097e-02, -1.3635e-01,\n",
      "        -2.3621e-02,  4.0649e-02,  2.3633e-01,  8.4900e-02,  1.6370e-01,\n",
      "         5.8472e-02,  2.8418e-01, -7.7553e-03, -2.7649e-02,  2.4890e-01,\n",
      "        -4.3457e-02,  1.5881e-01, -4.3457e-02, -1.0223e-01, -8.3374e-02,\n",
      "         7.0740e-02,  1.7798e-01,  3.7781e-02, -1.8652e-01, -4.2908e-02,\n",
      "         5.5046e-03, -2.2449e-01,  9.5520e-02, -1.6382e-01, -3.3813e-02,\n",
      "        -1.2891e-01, -7.5317e-02, -2.5854e-01, -1.5045e-02,  8.8623e-02,\n",
      "        -1.6248e-01,  4.0314e-02,  2.7100e-01,  1.2152e-01,  4.7705e-01,\n",
      "        -7.9041e-02, -3.9490e-02, -1.0431e-01, -1.2115e-01, -9.3811e-02,\n",
      "        -6.6345e-02, -1.8372e-01, -4.7119e-02, -3.8037e-01, -9.9915e-02,\n",
      "        -4.3774e-01, -4.0381e-01, -1.0345e-02,  1.4374e-02,  2.3816e-01,\n",
      "         7.8735e-02, -1.2695e-01, -5.9265e-02, -7.3669e-02, -2.1692e-01,\n",
      "         1.5427e-02,  1.4160e-01,  2.2171e-02, -4.5898e-02,  9.2590e-02,\n",
      "         2.2888e-01,  3.1909e-01,  9.0210e-02,  1.9690e-01,  6.7261e-02,\n",
      "        -3.0365e-02,  2.5543e-02,  2.0215e-01,  2.2595e-01,  1.2741e-02,\n",
      "         2.2546e-01, -7.9584e-04,  6.8420e-02,  6.3721e-02,  3.8940e-01,\n",
      "        -1.3354e-01, -3.6475e-01, -6.5430e-02, -3.0823e-02,  7.2754e-02,\n",
      "        -9.4666e-02, -6.5857e-02, -2.5238e-02, -5.4639e-01, -2.4475e-01,\n",
      "        -8.8013e-02,  6.2408e-02, -3.1677e-02, -3.1445e-01, -5.2856e-02,\n",
      "        -2.0386e-01, -1.2360e-01,  3.0762e-01, -4.2419e-02,  2.6587e-01,\n",
      "         1.7139e-01,  3.6060e-01,  6.6490e-03, -8.9478e-02,  1.2964e-01,\n",
      "        -2.1240e-01, -2.3163e-02, -6.4514e-02,  3.1872e-03, -2.9370e-01,\n",
      "        -2.7271e-01, -5.6641e-01,  1.1487e-01,  1.9363e-02,  1.3074e-01,\n",
      "         6.5186e-02, -3.5303e-01, -2.5952e-01,  1.5450e-02, -1.0455e-01,\n",
      "        -7.0679e-02,  2.2095e-01,  2.9614e-01,  2.5806e-01,  1.8274e-01,\n",
      "         1.1639e-01, -1.6882e-01,  2.9785e-01, -1.1285e-01,  2.6245e-01,\n",
      "         1.5283e-01,  3.3539e-02, -2.8259e-02,  9.7595e-02, -5.7495e-02,\n",
      "        -3.1006e-02, -1.2103e-01, -6.7078e-02, -2.1387e-01, -1.0297e-01,\n",
      "        -7.1289e-02,  1.3904e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0066,  0.1406, -0.0847,  ..., -0.2711,  0.2464,  0.1078],\n",
      "         [ 0.1150, -0.0782, -0.0599,  ..., -0.0352, -0.1040,  0.0604],\n",
      "         [ 0.1261,  0.0143,  0.0695,  ..., -0.1907, -0.0285, -0.1228],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2632, 0.5278, 0.2542,  ..., 0.4602, 0.3628, 0.2382], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.2590, -0.0740,  0.1274,  ..., -0.0948, -0.1102,  0.1549],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0130, -0.0194, -0.0148,  ..., -0.3226,  0.0310, -0.0151],\n",
      "         [ 0.1873, -0.1196,  0.1227,  ..., -0.1602, -0.0037,  0.0615],\n",
      "         [ 0.0937,  0.0563,  0.1356,  ..., -0.1148,  0.0494,  0.1148],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.3462,  0.4453,  0.4001,  0.3674,  0.3579,  0.4839,  0.4512,  0.4290,\n",
      "         0.4658,  0.5156,  0.4519,  0.6870,  0.3616,  0.4272,  0.5991,  0.3645,\n",
      "         0.6411,  0.6357,  0.5562,  0.2556,  0.5151,  0.3538,  0.5718,  0.6138,\n",
      "         0.3994,  0.5229,  0.1166,  0.1675,  0.1014,  0.6548,  0.6440,  0.5972,\n",
      "         0.2047,  0.4270,  0.2876,  0.5020,  0.0668,  0.2433,  0.3845,  0.5581,\n",
      "         0.5635,  0.4888,  0.7207,  0.6196,  0.3289,  0.0688,  0.3027, -0.0279,\n",
      "         0.1998,  0.2389,  0.3010,  0.3730,  0.4346,  0.4883,  0.4553,  0.4353,\n",
      "         0.6465,  0.4055,  0.0553,  0.1996,  0.6108,  0.3960,  0.5894,  0.3511,\n",
      "         0.1411,  0.4819,  0.1447,  0.4736,  0.4556,  0.3081,  0.3452,  0.1013,\n",
      "         0.1730,  0.0117,  0.5107,  0.0489,  0.2145,  0.3633,  0.4263,  0.4146,\n",
      "         0.1055,  0.5601,  0.2065,  0.3711,  0.3923,  0.1475,  0.5708,  0.5649,\n",
      "         0.5737,  0.1871,  0.6030,  0.6616,  0.3479,  0.5703,  0.1493,  0.2930,\n",
      "         0.4124,  0.5986,  0.6855,  0.3616,  0.3848,  0.3027,  0.7358,  0.2146,\n",
      "         0.5869,  0.1615,  0.6157,  0.4780,  0.2102,  0.5288,  0.5283,  0.3533,\n",
      "         0.4233,  0.5137,  0.5298,  0.5508,  0.6958,  0.4690,  0.6729,  0.3486,\n",
      "         0.5615,  0.3774,  0.5562,  0.5210,  0.0922,  0.3523,  0.4106,  0.1299,\n",
      "         0.3215,  0.4568,  0.1959,  0.3501,  0.2435,  0.3594,  0.5786,  0.1003,\n",
      "         0.2991,  0.4158,  0.5879,  0.2869,  0.2469,  0.4021,  0.0940,  0.2546,\n",
      "         0.5669,  0.4470,  0.1892,  0.2722,  0.6226,  0.3311,  0.5952,  0.1848,\n",
      "         0.2971,  0.4272,  0.4333,  0.3616,  0.4907,  0.7153,  0.9541,  0.6836,\n",
      "         0.6978,  0.5894,  0.4634,  0.2421,  0.5161,  0.5225,  0.5977,  0.6245,\n",
      "         0.3918,  0.6597,  0.6665,  0.5044,  0.3640,  0.2905,  0.4766,  0.4873,\n",
      "         0.1261,  0.4570,  0.5430,  0.5479,  0.5054,  0.6724,  0.4878,  0.7295,\n",
      "         0.4194,  0.6787,  0.3948,  0.5791,  0.5352,  0.0914,  0.2815,  0.5171,\n",
      "         0.1688,  0.5430,  0.5176,  0.2103,  0.4473,  0.1674,  0.2935,  0.1617,\n",
      "         0.3079,  0.5732,  0.0980,  0.2749,  0.3853,  0.4863,  0.2986,  0.4072,\n",
      "         0.7363,  0.5889,  0.7661,  0.6348,  0.2517,  0.3091,  0.1694,  0.5220,\n",
      "         0.4294,  0.2141,  0.5952,  0.2979,  0.7314,  0.2014,  0.3125,  0.4751,\n",
      "         0.7017,  0.6948,  0.3823,  0.2418,  0.3228,  0.4971,  0.5581,  0.1520,\n",
      "         0.2896,  0.4214,  0.4561,  0.4136,  0.4304,  0.3210,  0.7720,  0.6499,\n",
      "         0.2944,  0.5659,  0.2155,  0.8062,  0.2184,  0.3643,  0.5327,  0.7432,\n",
      "         0.6240,  0.4019,  0.3032,  0.5044,  0.4849,  0.3738,  0.1906,  0.6714,\n",
      "         0.4058,  0.3528,  0.4941,  0.6816,  0.2927,  0.5107,  0.3286,  0.4292,\n",
      "         0.5049,  0.5923,  0.7563,  0.3347,  0.6255,  0.4824,  0.7744,  0.7715,\n",
      "         0.5903,  0.3472,  0.5488,  0.4573,  0.3347,  0.6323,  0.4775,  0.4053,\n",
      "         0.3206,  0.5796,  0.5151,  0.6719,  0.5527,  0.4204,  0.8315,  0.5864,\n",
      "         0.4011,  0.3164,  0.4326,  0.6104,  0.6611,  0.4277,  0.6006,  0.5137,\n",
      "         0.5459,  0.2241,  0.5239,  0.1063,  0.4849,  0.3230,  0.3521,  0.6733,\n",
      "         0.5562,  0.4995,  0.5195,  0.8101,  0.5483,  0.6670,  0.9287,  0.2534,\n",
      "         0.3955,  0.7056,  0.2898,  0.4365,  0.6431,  0.2269,  0.1721,  0.4951,\n",
      "         0.3157,  0.4319,  0.4771,  0.5244,  0.3931,  0.6392,  0.3037,  0.4902,\n",
      "         0.4019,  0.3870,  0.0492,  0.3713,  0.5527,  0.5771,  0.2637,  0.2367,\n",
      "         0.7979,  0.3320,  0.5015,  0.3677,  0.5972,  0.5200,  0.3667,  0.4553,\n",
      "         0.7158,  0.2460,  0.5244,  0.4609,  0.5767,  0.7363,  0.6558,  0.3613,\n",
      "         0.4397,  0.5986,  0.5884,  0.6353,  0.4675,  0.3210,  0.6836,  0.4697,\n",
      "         0.6489,  0.5645,  0.3357,  0.3054,  0.6450,  0.6348,  0.4180,  0.2915,\n",
      "         0.6914,  0.8110,  0.6855,  0.5903,  0.3171,  0.2340,  0.4612,  0.7681,\n",
      "         0.4412,  0.5293,  0.4536,  0.3420,  0.4373,  0.4602,  0.5288,  0.2593,\n",
      "         0.2416,  0.9502,  0.2450,  0.4502,  0.1964,  0.4280,  0.4302,  0.3152,\n",
      "         0.1975,  0.3696,  0.4387,  0.4480,  0.3975,  0.2571,  0.2886, -0.0753,\n",
      "         0.2993,  0.3738,  0.2430,  0.1407,  0.5815,  0.5225,  0.4985,  0.5791,\n",
      "         0.2003,  0.1765,  0.5918,  0.5776,  0.3289,  0.2367,  0.4065,  0.6553,\n",
      "         0.4443,  0.2659,  0.3594,  0.3501,  0.4714,  0.2323,  0.3643,  0.1940,\n",
      "         0.1700,  0.3875,  0.5020,  0.2396,  0.3489,  0.2966,  0.3604,  0.3899,\n",
      "         0.3801,  0.5552,  0.3782,  0.4238,  0.4248,  0.2418,  0.4971,  0.1913,\n",
      "         0.1962,  0.4966,  0.4502,  0.4453,  0.6045,  0.1979,  0.5039,  0.4829,\n",
      "         0.2756,  0.3538,  0.4873,  0.4536,  0.3125,  0.3398,  0.2389,  0.0797,\n",
      "         0.2734,  0.7612,  0.3762,  0.5054,  0.5928,  0.4187,  0.0270,  0.3052,\n",
      "         0.3362,  0.2739,  0.2347,  0.7246,  0.2465,  0.5132,  0.3342,  0.7036,\n",
      "         0.4758,  0.4753,  0.6616,  0.2695,  0.3821,  0.4133,  0.4771,  0.1910,\n",
      "         0.1733,  0.5117,  0.4824,  0.1616,  0.3823,  0.0684,  0.0998,  0.4302,\n",
      "         0.3442,  0.4255,  0.4961,  0.2386,  0.2345,  0.3623,  0.3687,  0.4517,\n",
      "         0.3547,  0.5161,  0.4702,  0.4048,  0.4722,  0.3735,  0.5757,  0.2937,\n",
      "         0.4666,  0.6265,  0.6211,  0.5684,  0.1342,  0.2191,  0.5142,  0.2893,\n",
      "         0.2983,  0.3438,  0.5073,  0.3506,  0.2874,  0.3362,  0.3179,  0.2150,\n",
      "         0.4348,  0.3174,  0.3523,  0.4451,  0.6377,  0.4700,  0.2893,  0.3484,\n",
      "         0.2686,  0.4097,  0.3669,  0.4219,  0.5098,  0.4004,  0.5894,  0.3801,\n",
      "         0.5088,  0.6030,  0.2184,  0.4680,  0.3245,  0.3711,  0.3110,  0.6450,\n",
      "         0.6992,  0.1732,  0.3765,  0.2910,  0.6357,  0.2920,  0.3311,  0.5942,\n",
      "         0.4590,  0.5762,  0.3542,  0.4854,  0.2810,  0.4646,  0.1484,  0.4492,\n",
      "         0.1904, -0.0209,  0.4089,  0.5537, -0.0412,  0.5044,  0.4275,  0.6021,\n",
      "         0.0805,  0.5425,  0.2832,  0.3877,  0.3542,  0.3564,  0.2734,  0.2406,\n",
      "         0.4514,  0.4441,  0.2915,  0.4409,  0.1410,  0.3914,  0.0740,  0.4485,\n",
      "         0.3369,  0.6470,  0.3811,  0.6299,  0.4468,  0.2333,  0.5273,  0.1124,\n",
      "         0.3516,  0.4558,  0.4570,  0.0593,  0.3459,  0.1952,  0.5659,  0.5864,\n",
      "         0.2849,  0.6919,  0.3171,  0.2325,  0.4299,  0.1897,  0.3979,  0.4526,\n",
      "         0.3806,  0.4580,  0.4868,  0.2228,  0.4026,  0.5190,  0.4307,  0.4927,\n",
      "         0.6821,  0.5024,  0.2252,  0.4021,  0.3875,  0.5039,  0.4719,  0.3838,\n",
      "         0.7046,  0.5898,  0.4385,  0.4460,  0.2399,  0.3042,  0.3655,  0.4370,\n",
      "         0.3445,  0.7402,  0.6528,  0.3857,  0.6943,  0.3428,  0.5415,  0.2966,\n",
      "         0.5098,  0.5884,  0.5791,  0.5063,  0.2661,  0.4084,  0.6626,  0.5977,\n",
      "         0.5835,  0.2842,  0.5327,  0.2971,  0.3928,  0.2708,  0.6138,  0.2900,\n",
      "         0.3433,  0.4314,  0.6177,  0.7197,  0.4524,  0.6987,  0.1714,  0.6997,\n",
      "         0.6831,  0.3518,  0.5054,  0.6665,  0.4692,  0.3645,  0.3042,  0.5981,\n",
      "         0.1674,  0.5923,  0.6440,  0.4189,  0.5625,  0.5122,  0.4729,  0.5942,\n",
      "         0.6509,  0.2202,  0.2969,  0.4705,  0.3389,  0.5972,  0.6987,  0.6343,\n",
      "         0.5264,  0.7358,  0.5957,  0.5435,  0.5034,  0.4146,  0.3357,  0.4465,\n",
      "         0.5601,  0.4343,  0.4197,  0.5176,  0.7217,  0.7241,  0.3940,  0.3250,\n",
      "         0.5200,  0.2729,  0.4624,  0.5791,  0.4324,  0.4114,  0.4800],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.2085, -0.0254,  0.1334,  0.1013,  0.1493,  0.0332, -0.2177,  0.0358,\n",
      "        -0.0255,  0.2101,  0.0535,  0.0395, -0.1315, -0.1259,  0.0732, -0.0444,\n",
      "        -0.0514, -0.1497, -0.1350,  0.1560, -0.0127,  0.0174, -0.0258, -0.0154,\n",
      "        -0.0378, -0.0408, -0.3384,  0.3113, -0.2079,  0.2671, -0.0644, -0.0348,\n",
      "        -0.3457,  0.0707,  0.0105,  0.0828, -0.0213,  0.1219,  0.1205, -0.0917,\n",
      "         0.2471,  0.0808,  0.2869, -0.0544, -0.2578, -0.2074, -0.0071,  0.2563,\n",
      "         0.1293, -0.3948, -0.4751, -0.1169,  0.0208, -0.0638,  0.1974,  0.0108,\n",
      "        -0.0892,  0.5000, -0.0717,  0.0659,  0.3884, -0.1562,  0.1272,  0.0050,\n",
      "        -0.1194,  0.3413,  0.1250,  0.0221, -0.0364, -0.4661,  0.2671,  0.0839,\n",
      "        -0.0446,  0.1688,  0.0029, -0.0840,  0.0765,  0.0651, -0.0704,  0.2067,\n",
      "        -0.1582, -0.1785, -0.3115,  0.0313,  0.1860,  0.0836, -0.1309, -0.1353,\n",
      "         0.1899,  0.0718, -0.1565, -0.1963, -0.5649, -0.0988, -0.1621,  0.0136,\n",
      "         0.0176, -0.2612,  0.0605, -0.3643, -0.1302, -0.3181, -0.1405, -0.1262,\n",
      "         0.2058,  0.0535, -0.1285, -0.0641, -0.2413,  0.0943, -0.0268, -0.2629,\n",
      "         0.0142, -0.0430,  0.2084,  0.0569,  0.1758, -0.1469,  0.0614, -0.0306,\n",
      "         0.0127, -0.0054,  0.3643, -0.1389,  0.0213, -0.2008,  0.3735, -0.1631,\n",
      "        -0.0828, -0.4158, -0.3203, -0.0974,  0.1869, -0.0541, -0.1534, -0.1691,\n",
      "        -0.0038,  0.0266, -0.2250, -0.3718,  0.1544,  0.2206, -0.1871, -0.1720,\n",
      "        -0.1240,  0.2183, -0.2321, -0.2030, -0.1254, -0.6147, -0.0743, -0.1820,\n",
      "         0.0202, -0.0167,  0.0458,  0.0113, -0.0534, -0.1298, -0.0182, -0.1477,\n",
      "         0.0832, -0.0697,  0.2494, -0.1763, -0.1636,  0.2054, -0.0110,  0.0475,\n",
      "         0.1024,  0.1008, -0.1544, -0.2423, -0.1321, -0.4456, -0.1479,  0.1732,\n",
      "        -0.1220,  0.0287, -0.0607,  0.2231,  0.0562,  0.2224, -0.1394,  0.1259,\n",
      "        -0.0464, -0.0262, -0.0183,  0.3455, -0.1119,  0.0199, -0.1971,  0.2676,\n",
      "         0.0437, -0.0654, -0.5708, -0.4050,  0.0905, -0.0177, -0.1212,  0.0894,\n",
      "         0.0158, -0.0948, -0.1661,  0.0186,  0.0009, -0.1997, -0.3940,  0.0219,\n",
      "        -0.1785, -0.0762, -0.2096, -0.1320, -0.6152,  0.0611, -0.5288,  0.2238,\n",
      "        -0.1624, -0.0668, -0.0800, -0.3264,  0.1327, -0.0410,  0.1422,  0.1279,\n",
      "        -0.3149, -0.1390, -0.5664,  0.0059,  0.1160, -0.0086, -0.0656, -0.1371,\n",
      "         0.0675,  0.0168,  0.0225, -0.0784,  0.0475, -0.1465, -0.1807,  0.0535,\n",
      "        -0.0258, -0.0998, -0.0740,  0.1204,  0.0113,  0.2510,  0.1948, -0.1825,\n",
      "         0.1392, -0.2500, -0.3267, -0.0444, -0.1417, -0.0674,  0.0470, -0.1348,\n",
      "         0.1093, -0.1256, -0.1454, -0.0643, -0.3940, -0.1196,  0.0104, -0.3345,\n",
      "         0.0112,  0.0009, -0.1085, -0.3579, -0.0464, -0.0721, -0.1025, -0.3757,\n",
      "        -0.0056, -0.3137,  0.2020,  0.0701, -0.2996, -0.1971, -0.0677, -0.2272,\n",
      "        -0.1564, -0.0258, -0.1860,  0.0978, -0.0263, -0.0308, -0.1406, -0.0580,\n",
      "         0.1146, -0.1243, -0.2172,  0.0230, -0.1305, -0.1094, -0.0471, -0.1914,\n",
      "        -0.2406, -0.0772,  0.3562,  0.1774,  0.1235, -0.1290,  0.2798, -0.0875,\n",
      "         0.1998, -0.1238, -0.1865, -0.2998,  0.1222,  0.0272, -0.0118,  0.2590,\n",
      "         0.0969,  0.1813,  0.1503,  0.1066,  0.0304,  0.0055, -0.0280, -0.1733,\n",
      "        -0.1716,  0.1035, -0.0071,  0.2032, -0.0097,  0.1127, -0.3787,  0.1318,\n",
      "         0.1514,  0.1162,  0.0473,  0.2220,  0.0988,  0.0393, -0.0457,  0.0242,\n",
      "        -0.2225, -0.0798, -0.1814, -0.1694, -0.1733, -0.0081,  0.1295,  0.0728,\n",
      "         0.0219, -0.0533,  0.0822,  0.0954,  0.1514,  0.0911, -0.2659,  0.0215,\n",
      "         0.2659,  0.1423, -0.0922, -0.1449,  0.0363, -0.0311, -0.0511,  0.1838,\n",
      "        -0.0051, -0.0524,  0.0242,  0.0808, -0.1274,  0.0854,  0.0057, -0.0615,\n",
      "        -0.0175, -0.1787,  0.1801, -0.0500, -0.3206,  0.1511,  0.1549,  0.0665,\n",
      "         0.0493, -0.1692,  0.0599,  0.0595,  0.0565,  0.0415, -0.0233, -0.3469,\n",
      "        -0.0263, -0.3782, -0.0726,  0.0886,  0.2284,  0.0097,  0.2266,  0.3293,\n",
      "         0.1203,  0.2338,  0.1301,  0.2249,  0.0881, -0.1248,  0.2444,  0.1208,\n",
      "         0.1279,  0.1676,  0.1829,  0.0576,  0.0240,  0.1385,  0.2009,  0.1024,\n",
      "        -0.0609,  0.0764,  0.1086,  0.3516,  0.3462,  0.2281,  0.2910,  0.1290,\n",
      "         0.2032, -0.2524,  0.0381, -0.0246,  0.1206, -0.0936,  0.1985,  0.0789,\n",
      "         0.2778, -0.1154, -0.0191, -0.2323,  0.1249,  0.0433,  0.1687,  0.1011,\n",
      "         0.0980, -0.0484, -0.0027,  0.0219,  0.1366, -0.0604,  0.2170,  0.0737,\n",
      "         0.3093,  0.0316,  0.1433,  0.2490,  0.0901,  0.2416,  0.1439,  0.2041,\n",
      "         0.2148,  0.1934, -0.0765,  0.2859,  0.0285, -0.1653, -0.2311,  0.2883,\n",
      "         0.0353, -0.2318,  0.0241,  0.1549, -0.0858, -0.1785, -0.0425,  0.0942,\n",
      "         0.1528,  0.0925,  0.1713,  0.0155, -0.0297, -0.0594, -0.0222, -0.2408,\n",
      "        -0.1711,  0.0051, -0.2106, -0.2267,  0.2295, -0.1852, -0.0078, -0.1862,\n",
      "        -0.0590,  0.0534, -0.1024,  0.2893,  0.1039,  0.0515,  0.0916, -0.0538,\n",
      "        -0.0201,  0.1274,  0.0054, -0.2957,  0.1525, -0.0097, -0.0392,  0.1946,\n",
      "        -0.0214,  0.0592, -0.0030, -0.0512,  0.2156,  0.0051,  0.0461, -0.2443,\n",
      "        -0.0029, -0.0101,  0.0609, -0.0069, -0.0757,  0.1388,  0.1411, -0.0222,\n",
      "         0.1652,  0.0949, -0.0252,  0.0452,  0.1396,  0.1945,  0.0724, -0.0208,\n",
      "         0.0907,  0.1954,  0.2306,  0.1844, -0.0330, -0.1310, -0.0804,  0.0585,\n",
      "         0.1022, -0.0837,  0.0459, -0.0482,  0.1984,  0.0163,  0.1758,  0.0442,\n",
      "         0.0817,  0.1731,  0.0414,  0.0638,  0.2837,  0.1003, -0.0064, -0.0453,\n",
      "        -0.0519, -0.0220,  0.2268,  0.0392,  0.0421, -0.2764,  0.0444, -0.1622,\n",
      "         0.1406,  0.1116,  0.0983,  0.2338,  0.0388,  0.1443, -0.2654, -0.1083,\n",
      "         0.0009,  0.2363,  0.0374,  0.0685, -0.0238, -0.0326, -0.0063,  0.0400,\n",
      "         0.0640,  0.2289,  0.1094, -0.1066, -0.1086,  0.0485, -0.0113, -0.1846,\n",
      "        -0.0538, -0.0589,  0.0856,  0.0463, -0.0994,  0.1512,  0.0086,  0.0775,\n",
      "         0.0864,  0.0041, -0.1054,  0.1644,  0.1144, -0.1203,  0.0233, -0.1670,\n",
      "        -0.0877,  0.1396,  0.1403, -0.1207,  0.2407,  0.0673,  0.1798,  0.0953,\n",
      "         0.0372,  0.1032, -0.3110,  0.1543, -0.1752,  0.0065,  0.0736, -0.1339,\n",
      "         0.0969,  0.0963, -0.1740, -0.0044,  0.0740, -0.1536,  0.1046,  0.0829,\n",
      "        -0.2399, -0.0657,  0.0042,  0.0829, -0.0288, -0.3293, -0.0145, -0.0331,\n",
      "        -0.2450, -0.1985, -0.0838,  0.1456,  0.0472, -0.0695,  0.1262, -0.0991,\n",
      "        -0.0854, -0.1113, -0.2081, -0.3770, -0.3438, -0.1381, -0.1224, -0.0354,\n",
      "         0.0202,  0.0896, -0.3076,  0.0822,  0.0214, -0.2932, -0.0343, -0.0609,\n",
      "        -0.2646, -0.3730, -0.3594, -0.2595,  0.0215,  0.0490,  0.0843, -0.0103,\n",
      "         0.0237, -0.0649,  0.0803, -0.0954, -0.0841, -0.1754, -0.1605,  0.0737,\n",
      "        -0.0258, -0.2301,  0.1650,  0.1145, -0.0876,  0.1390, -0.1157, -0.1554,\n",
      "        -0.1904, -0.0835, -0.2365, -0.2052, -0.1311,  0.0852,  0.1153,  0.0930,\n",
      "        -0.2012, -0.1713, -0.0577,  0.1262, -0.0516, -0.2717, -0.0718, -0.0298,\n",
      "        -0.2216,  0.0078,  0.0349, -0.1482, -0.2954, -0.0535,  0.1382,  0.0621,\n",
      "         0.0190, -0.2532, -0.0756, -0.0178,  0.1770, -0.1605,  0.0069, -0.0631,\n",
      "         0.1332,  0.1689,  0.0010, -0.0942, -0.0281, -0.0183,  0.0316],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0095,  0.0958,  0.0909,  ..., -0.2309, -0.0128,  0.0012],\n",
      "         [ 0.2745, -0.0796,  0.1065,  ..., -0.1392,  0.1413,  0.1151],\n",
      "         [-0.0348, -0.0004,  0.0144,  ..., -0.3373,  0.0580, -0.0080],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.3318, 0.7319, 0.3640,  ..., 0.4441, 0.3613, 0.2394], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.0948, -0.1427, -0.0024,  ..., -0.1937, -0.1567,  0.1458],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0271,  0.0533,  0.0027,  ..., -0.0984,  0.0054,  0.0210],\n",
      "         [ 0.0503,  0.0668, -0.0884,  ...,  0.1275,  0.2120,  0.1743],\n",
      "         [ 0.1135,  0.1519,  0.0464,  ..., -0.2732, -0.0816,  0.2107],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0159,  ..., -0.0929, -0.0364, -0.0815],\n",
      "         [-0.0186,  0.0704, -0.0159,  ..., -0.0929, -0.0364, -0.0815],\n",
      "         [-0.0186,  0.0704, -0.0159,  ..., -0.0929, -0.0364, -0.0815]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2316, 0.2751, 0.3850, 0.2878, 0.4250, 0.2413, 0.4246, 0.2216, 0.1669,\n",
      "        0.3813, 0.3530, 0.3975, 0.3362, 0.4312, 0.3914, 0.2272],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1562,  0.0514, -0.0792,  0.0439, -0.0931,  0.0599, -0.1152,  0.1024,\n",
      "        -0.0668, -0.0067, -0.0508,  0.0942, -0.0231, -0.1278, -0.1510,  0.1597],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.1 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00000: avg_val_f1 reached 0.10000 (best 0.10000), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_0.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0663,  0.1833,  0.0425,  ..., -0.1799, -0.0455,  0.0573],\n",
      "         [ 0.0212,  0.0288,  0.1438,  ...,  0.0761, -0.0073,  0.1867],\n",
      "         [ 0.1192,  0.0402,  0.2935,  ..., -0.0481, -0.0195, -0.0030],\n",
      "         ...,\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811],\n",
      "         [-0.0186,  0.0704, -0.0158,  ..., -0.0927, -0.0364, -0.0811]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.2942,  0.3970,  0.3918,  0.5664,  0.2206,  0.5327,  0.3860,  0.3218,\n",
      "         0.3308,  0.3730,  0.1725,  0.3340,  0.3926,  0.3015,  0.4238,  0.2847,\n",
      "         0.4805,  0.2920,  0.2003,  0.0180,  0.3167,  0.6821,  0.2450,  0.5864,\n",
      "         0.3069,  0.2092, -0.0895,  0.3838,  0.5908,  0.3064,  0.2695, -0.0490,\n",
      "         0.4480,  0.5112,  0.6382,  0.2932,  0.3884,  0.2433,  0.5015,  0.5527,\n",
      "         0.1809,  0.3926,  0.2104,  0.2192,  0.5801,  0.3389,  0.3972,  0.3862,\n",
      "         0.3821,  0.3669,  0.2474,  0.5527,  0.1046,  0.5913,  0.4854,  0.2491,\n",
      "         0.2085,  0.2059,  0.3398,  0.4500,  0.2756,  0.3301,  0.6084,  0.2183,\n",
      "         0.6929,  0.5059,  0.7163,  0.6270,  0.5825,  0.6650,  0.3347,  0.3364,\n",
      "         0.0456,  0.4597,  0.5020,  0.5278,  0.2181,  0.2362,  0.4595,  0.5728,\n",
      "         0.3604,  0.5361,  0.5117,  0.3025,  0.4036,  0.5352,  0.4412,  0.1735,\n",
      "         0.7886,  0.5381,  0.7773,  0.4419,  0.5400,  0.5688,  0.3545,  0.3389,\n",
      "         0.3689,  0.2426,  0.4109,  0.0859,  0.1161,  0.5317,  0.3813,  0.1897,\n",
      "         0.3469,  0.3506,  0.3899,  0.3342,  0.1114,  0.2284,  0.3972,  0.4470,\n",
      "         0.3540,  0.1927,  0.1979,  0.3933,  0.5015,  0.2303,  0.1844,  0.2939,\n",
      "         0.4387,  0.5537,  0.4424,  0.3938,  0.5059,  0.4966,  0.2080,  0.5151,\n",
      "         0.4485,  0.4590,  0.4976,  0.4255,  0.5952,  0.2629,  0.3635,  0.3845,\n",
      "         0.3257,  0.5908,  0.2842,  0.4744,  0.3616,  0.3250,  0.4333,  0.4565,\n",
      "         0.3672,  0.6001,  0.4348,  0.6626,  0.2993,  0.2251,  0.3848,  0.7026,\n",
      "         0.3147,  0.2683,  0.7148,  0.6216,  0.4204,  0.3176,  0.6709,  0.2542,\n",
      "         0.3362,  0.1114,  0.4619,  0.3264,  0.4922,  0.6250,  0.5132,  0.4668,\n",
      "         0.8120,  0.3362,  0.5469,  0.6001,  0.6655,  0.5342,  0.4517,  0.7505,\n",
      "         0.3042,  0.4702,  0.6177,  0.1814,  0.7393,  0.1486,  0.2153,  0.3062,\n",
      "         0.6191,  0.2896,  0.4224,  0.2700,  0.3091,  0.5649,  0.3020,  0.4072,\n",
      "         0.4683,  0.4597,  0.5703,  0.4829,  0.5688,  0.6543,  0.4575,  0.4824,\n",
      "         0.0963,  0.2683,  0.0830,  0.4534,  0.6738,  0.3625,  0.1840,  0.4570,\n",
      "         0.4329,  0.4294,  0.4971,  0.6924,  0.2629,  0.6685,  0.4160,  0.4453,\n",
      "         0.2688,  0.2263,  0.5420,  0.4922,  0.5840,  0.1287,  0.2722,  0.6938,\n",
      "         0.3743,  0.3049,  0.0976,  0.4128,  0.5200,  0.3179,  0.2737,  0.4517,\n",
      "         0.4937,  0.6890,  0.3459,  0.3169,  0.4185,  0.1273,  0.3809,  0.6807,\n",
      "         0.2883,  0.2812,  0.4543,  0.2367,  0.3010,  0.4814,  0.2321,  0.3350,\n",
      "         0.2369,  0.4138,  0.1854,  0.4888,  0.1223,  0.0861,  0.4829,  0.2360,\n",
      "         0.4331,  0.1642,  0.4741,  0.1103,  0.0881,  0.5410,  0.2496,  0.0959,\n",
      "         0.6396,  0.3398,  0.4619,  0.5078,  0.4675,  0.5015,  0.4810,  0.3093,\n",
      "         0.5742,  0.1876,  0.0081,  0.1104,  0.5293,  0.1033,  0.0429,  0.4629,\n",
      "         0.1538,  0.4543,  0.2078,  0.3655,  0.4463,  0.2908,  0.5220,  0.5381,\n",
      "         0.3535,  0.6704,  0.6426,  0.3042,  0.3181,  0.6201,  0.3374,  0.3850,\n",
      "         0.2004,  0.2737,  0.3369,  0.4819,  0.3191,  0.4202,  0.1947,  0.0829,\n",
      "         0.2615,  0.1837,  0.6240,  0.7886,  0.5513,  0.6152,  0.5864,  0.4053,\n",
      "         0.3887,  0.1181,  0.0152,  0.1984,  0.1342,  0.5571,  0.7656,  0.5747,\n",
      "         0.5547,  0.6113,  0.2135,  0.5142,  0.2350,  0.2671,  0.5659,  0.5562,\n",
      "         0.3142,  0.4727,  0.2396,  0.1350,  0.2151,  0.0597,  0.1556,  0.3494,\n",
      "         0.2479,  0.1158,  0.1583,  0.1613,  0.3584,  0.0983,  0.5161,  0.3035,\n",
      "         0.3782,  0.4062,  0.4680,  0.5591,  0.4705,  0.5044,  0.4502,  0.1987,\n",
      "         0.0381,  0.1954,  0.1180,  0.5273,  0.4607,  0.6318,  0.2832,  0.3699,\n",
      "         0.3640,  0.4868,  0.2408,  0.4504,  0.3049,  0.5752,  0.5771,  0.4661,\n",
      "         0.3179,  0.1993,  0.3911,  0.4883,  0.6196,  0.1754,  0.1572,  0.1566,\n",
      "         0.1510,  0.3240,  0.3557,  0.3516,  0.4824,  0.6030,  0.3938,  0.7446,\n",
      "         0.1936,  0.3232,  0.3635,  0.4097,  0.5186,  0.3535,  0.3562,  0.5156,\n",
      "         0.0948,  0.2167,  0.6406,  0.3894,  0.4487,  0.4563,  0.4695,  0.3308,\n",
      "         0.4956,  0.2861,  0.3625,  0.2834,  0.4331,  0.3020,  0.6475,  0.3979,\n",
      "         0.2245,  0.3892,  0.1646,  0.2042,  0.2262,  0.4333,  0.2937,  0.4976,\n",
      "         0.3333,  0.4084,  0.2583,  0.3333,  0.2249,  0.2610,  0.2632,  0.4426,\n",
      "         0.2482,  0.3149,  0.3376,  0.3503,  0.4475,  0.5571,  0.3486,  0.1168,\n",
      "         0.0136,  0.0106,  0.5737,  0.2727,  0.1967,  0.3147,  0.3953,  0.5049,\n",
      "         0.2130,  0.7900,  0.0792,  0.3865,  0.3589,  0.2190,  0.3525,  0.2629,\n",
      "         0.3674,  0.5444,  0.2683,  0.2979,  0.4111,  0.3857,  0.2279,  0.4336,\n",
      "         0.1938,  0.3799,  0.2588,  0.6328,  0.3481,  0.1975,  0.3315,  0.3540,\n",
      "         0.1028,  0.4358,  0.1736,  0.3462,  0.3042,  0.3083,  0.1792,  0.3132,\n",
      "         0.2888,  0.3547,  0.0539,  0.0995,  0.6768,  0.1632,  0.6196,  0.2039,\n",
      "         0.2264,  0.3977,  0.2747,  0.3911,  0.4390,  0.3176,  0.5015,  0.4521,\n",
      "         0.5864,  0.2507,  0.2048,  0.2961,  0.4941,  0.3547,  0.2377,  0.3848,\n",
      "         0.4097,  0.3098,  0.0174,  0.1958,  0.1225,  0.4397,  0.2080,  0.4163,\n",
      "         0.1992,  0.3311,  0.3347,  0.4380,  0.2534,  0.2284,  0.4385,  0.4346,\n",
      "         0.2496,  0.4622,  0.3704,  0.2996,  0.3013,  0.3984,  0.2357,  0.2717,\n",
      "         0.4360,  0.3188,  0.3423,  0.2423,  0.2651,  0.3450,  0.2202,  0.3550,\n",
      "         0.3499,  0.3811,  0.2708,  0.1733,  0.4265,  0.5181,  0.3411,  0.3054,\n",
      "         0.3843,  0.3994,  0.4070,  0.3601,  0.2330,  0.3369,  0.3408,  0.6274,\n",
      "         0.3591,  0.3416,  0.4316,  0.2001,  0.3267,  0.3210,  0.1637,  0.0970,\n",
      "         0.4390,  0.4690,  0.3660,  0.3262,  0.2103,  0.1870,  0.2908,  0.4272,\n",
      "         0.1913,  0.3145,  0.1990,  0.5410,  0.3059,  0.1882,  0.0735,  0.2281,\n",
      "         0.2517,  0.1641,  0.5239,  0.4294,  0.3210,  0.5073,  0.3333,  0.2725,\n",
      "         0.1562,  0.5010,  0.3247,  0.4309,  0.3279,  0.5308,  0.3076,  0.2318,\n",
      "         0.3074,  0.1272,  0.4309,  0.3669,  0.2893,  0.4106,  0.3699,  0.1592,\n",
      "         0.4690,  0.1449,  0.4653,  0.3472,  0.7251,  0.1536,  0.3518,  0.3933,\n",
      "         0.4893,  0.6138,  0.5996,  0.2612,  0.6787,  0.4927,  0.3525,  0.1076,\n",
      "         0.3167,  0.1863,  0.2705,  0.1277,  0.3972,  0.4583,  0.3821,  0.4429,\n",
      "         0.3835,  0.2037,  0.1542,  0.0778,  0.3833,  0.4990,  0.3672,  0.3049,\n",
      "         0.1705,  0.5215,  0.1556,  0.3118,  0.4458,  0.3213,  0.7354,  0.4980,\n",
      "         0.2710,  0.2321,  0.4102,  0.2747,  0.3494,  0.4229,  0.3171,  0.3894,\n",
      "         0.5767,  0.1951,  0.4048,  0.2678,  0.3081,  0.0829,  0.2905,  0.2313,\n",
      "         0.3071,  0.2194,  0.1542,  0.1306,  0.2703,  0.2360,  0.4082,  0.4678,\n",
      "         0.2396,  0.3613,  0.4507,  0.3755,  0.4075,  0.5381,  0.3621,  0.3521,\n",
      "         0.5659,  0.1781,  0.3625,  0.1678,  0.4097,  0.3916,  0.1927,  0.4519,\n",
      "         0.3269,  0.3613,  0.3667,  0.2815,  0.7861,  0.5776,  0.3623,  0.6294,\n",
      "         0.3569,  0.5713,  0.4626,  0.3945,  0.5601,  0.2239,  0.3457,  0.4490,\n",
      "         0.3203,  0.5737,  0.4226,  0.3496,  0.6851,  0.4026,  0.0635, -0.0215,\n",
      "         0.2063,  0.6929,  0.5649,  0.4739,  0.3855,  0.5400,  0.1323,  0.5586,\n",
      "         0.3337,  0.7803,  0.3472,  0.5298,  0.5693,  0.5874,  0.7168,  0.2749,\n",
      "         0.4504,  0.6055,  0.7456,  0.3293,  0.4197,  0.4004,  0.5830,  0.5078,\n",
      "         0.3809,  0.5015,  0.5322,  0.5356,  0.6011,  0.5420,  0.5825,  0.5215,\n",
      "         0.4922,  0.4265,  0.7046,  0.5674,  0.6040,  0.7402,  0.5464,  0.2646,\n",
      "         0.3821,  0.7944,  0.4656,  0.2690,  0.4517,  0.4626,  0.2866,  0.3735,\n",
      "         0.3826,  0.4504,  0.8716,  0.7212,  0.6475,  0.5654,  0.0632,  0.5176,\n",
      "         0.3330,  0.3293,  0.6045,  0.3096,  0.2137,  0.3691,  0.3054,  0.3584,\n",
      "         0.6689,  0.4006,  0.5547,  0.2576,  0.5366,  0.4717,  0.5664,  0.3765,\n",
      "         0.2396,  0.6421,  0.4387,  0.4270,  0.5664,  0.5649,  0.3879,  0.7349,\n",
      "         0.3789,  0.5586,  0.6177,  0.6079,  0.4158,  0.5566,  0.3979,  0.4854,\n",
      "         0.4695,  0.3811,  0.3564,  0.3755,  0.6011,  0.2971,  0.5986,  0.3499,\n",
      "         0.4053,  0.4026,  0.5732,  0.4082,  0.4153,  0.4504,  0.5396,  0.2413,\n",
      "         0.4976,  0.2686,  0.4043,  0.6987,  0.4434,  0.2494,  0.4419,  0.4497,\n",
      "         0.2881,  0.4207,  0.3206,  0.4026,  0.4451, -0.0312,  0.4299,  0.2472,\n",
      "         0.6377,  0.6895,  0.2003,  0.3738,  0.2263,  0.2119,  0.3909, -0.2712,\n",
      "         0.1827,  0.5781,  0.1483,  0.3589,  0.2147,  0.4658,  0.5986,  0.5918,\n",
      "         0.5859,  0.1968,  0.3931,  0.1826,  0.5835,  0.6836,  0.1749,  0.6685,\n",
      "         0.3696,  0.4272,  0.3196,  0.4094,  0.2208,  0.3733,  0.1906,  0.6138,\n",
      "         0.2097,  0.5542,  0.6221,  0.3564,  0.6499,  0.3018,  0.2411,  0.2246,\n",
      "         0.5845,  0.5190,  0.4272,  0.5913,  0.6348,  0.4680,  0.3901,  0.3254,\n",
      "         0.2284], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 9.2773e-02, -1.2128e-01, -3.2446e-01, -2.0532e-01, -2.7881e-01,\n",
      "        -2.5928e-01, -2.2083e-01,  4.9095e-03, -1.3708e-01, -2.1094e-01,\n",
      "        -1.2976e-01, -8.0200e-02,  2.3523e-01, -1.2054e-01, -1.5637e-01,\n",
      "        -1.8359e-01,  5.1514e-02,  9.2773e-02, -1.5991e-02,  6.3896e-03,\n",
      "        -4.1565e-02, -2.2449e-01,  1.0535e-01,  2.7832e-02,  2.9282e-02,\n",
      "        -2.0264e-02, -7.0862e-02, -1.1737e-01,  1.4392e-01,  4.1168e-02,\n",
      "         6.7101e-03, -5.7434e-02, -8.1665e-02, -2.0642e-01, -5.7129e-02,\n",
      "        -2.5439e-01, -1.4087e-01, -1.1224e-01, -1.9958e-02, -1.4880e-01,\n",
      "        -1.7566e-01,  1.4844e-01, -4.7852e-02, -2.2571e-01, -6.7505e-02,\n",
      "         3.9124e-02,  2.8275e-02,  1.0315e-01,  8.4839e-03,  7.1167e-02,\n",
      "        -3.0176e-01, -7.6477e-02, -1.5808e-02, -3.8391e-02, -2.7197e-01,\n",
      "         1.4209e-01, -7.2021e-02, -1.2238e-01,  1.8884e-01, -2.1252e-01,\n",
      "        -2.0251e-01, -1.4258e-01, -3.7354e-02, -1.0345e-01,  1.8176e-01,\n",
      "        -4.4678e-02, -1.3550e-01,  5.8594e-02, -1.5210e-01,  3.6774e-02,\n",
      "         1.1359e-01, -2.4231e-02, -2.5909e-02, -8.0200e-02,  5.5267e-02,\n",
      "        -1.5479e-01, -2.4597e-01, -6.5552e-02,  4.4708e-02, -1.9360e-01,\n",
      "        -1.5405e-01, -1.9824e-01, -2.5513e-01, -2.6270e-01, -3.6353e-01,\n",
      "        -4.3848e-01, -2.2351e-01, -7.3242e-02,  7.1716e-02, -1.6443e-01,\n",
      "         1.1981e-01, -4.6143e-02, -6.8115e-02, -3.6804e-02, -2.7100e-01,\n",
      "        -1.6589e-01, -5.7275e-01, -6.2256e-01, -1.1053e-01, -8.1604e-02,\n",
      "        -2.0972e-01, -7.6965e-02, -3.9526e-01, -4.1187e-01, -1.1139e-01,\n",
      "        -9.6252e-02, -1.2238e-01, -4.1187e-01, -3.8574e-01, -1.7480e-01,\n",
      "        -6.8018e-01, -3.5669e-01, -3.2959e-01, -3.5156e-01, -3.5095e-02,\n",
      "        -3.9966e-01, -3.1445e-01, -2.9004e-01, -3.4229e-01, -1.8591e-01,\n",
      "        -3.8867e-01, -8.7952e-02, -2.9224e-01, -1.2659e-01, -2.5562e-01,\n",
      "        -1.5625e-01, -1.6797e-01, -2.0557e-01, -1.7737e-01, -2.5342e-01,\n",
      "        -1.2189e-01, -1.7871e-01,  1.2915e-01, -1.5454e-01, -1.4294e-01,\n",
      "        -8.3252e-02, -9.2834e-02, -1.8396e-01, -3.0908e-01, -1.6541e-01,\n",
      "        -9.6741e-02, -1.1311e-03, -1.9531e-01, -1.8213e-01, -7.0068e-02,\n",
      "        -1.4453e-01, -2.7563e-01, -2.4353e-01, -4.7070e-01, -3.4814e-01,\n",
      "        -3.4033e-01,  1.8597e-03, -1.3098e-01, -1.1237e-01, -1.6492e-01,\n",
      "        -1.7810e-01, -2.3840e-01,  1.0162e-01, -3.3130e-01, -2.5488e-01,\n",
      "        -1.0565e-01, -2.3376e-01, -4.4861e-02, -3.4473e-01, -1.2201e-01,\n",
      "        -1.3696e-01, -4.0283e-01, -3.0103e-01, -4.0820e-01, -5.2490e-02,\n",
      "        -2.5269e-02, -2.9150e-01, -9.2468e-02, -4.3970e-01, -2.5317e-01,\n",
      "        -5.4053e-01, -5.0720e-02,  1.6968e-02, -2.9810e-01, -1.7822e-01,\n",
      "        -6.6223e-02, -2.6807e-01,  9.5886e-02,  1.7517e-02, -2.9663e-01,\n",
      "        -4.9658e-01, -1.0870e-01,  1.7319e-02, -3.4131e-01, -3.4180e-01,\n",
      "        -3.4351e-01,  2.5122e-01, -1.3989e-01, -6.2744e-02, -1.3196e-01,\n",
      "        -2.2400e-01, -4.6118e-01, -1.2189e-01, -4.2920e-01, -2.4963e-01,\n",
      "        -4.3018e-01, -1.5552e-01, -2.5732e-01, -6.6956e-02, -6.0693e-01,\n",
      "        -2.9517e-01, -1.2274e-01, -9.0088e-02,  1.6577e-01, -1.7676e-01,\n",
      "        -1.3232e-01, -4.2749e-01, -3.3130e-01, -1.3000e-01, -3.7598e-01,\n",
      "        -1.5320e-01, -5.3174e-01,  9.1614e-02, -1.8982e-01, -8.1360e-02,\n",
      "        -1.3110e-01,  3.8177e-02, -1.3928e-01, -5.0391e-01, -3.0298e-01,\n",
      "        -2.5146e-01, -2.4133e-01, -4.1992e-02,  1.6510e-02, -6.0364e-02,\n",
      "        -1.1810e-01, -2.5586e-01, -2.0471e-01, -2.3132e-02, -2.5610e-01,\n",
      "        -1.1902e-02, -1.6760e-01, -5.5542e-02, -2.5439e-01,  3.7994e-02,\n",
      "        -1.5503e-01, -1.8262e-01,  3.5992e-03,  4.2908e-02, -1.2299e-01,\n",
      "         3.4363e-02, -4.4434e-02, -4.0332e-01,  1.1328e-01, -1.3184e-01,\n",
      "        -1.3428e-01, -8.1726e-02, -9.0759e-02, -1.5533e-02, -1.0724e-01,\n",
      "        -2.3083e-01,  3.3081e-02, -1.0944e-01, -1.4978e-01, -1.2006e-01,\n",
      "        -7.5745e-02, -1.4514e-01, -2.2302e-01, -1.9397e-01, -1.0883e-01,\n",
      "        -2.5085e-02, -2.7148e-01, -4.0894e-02, -2.2473e-01, -1.4026e-01,\n",
      "         1.0513e-02, -1.4014e-01, -1.4038e-01,  1.1230e-01,  4.6570e-02,\n",
      "        -2.2107e-01, -1.3708e-01, -4.1260e-02,  2.1790e-02, -7.1960e-02,\n",
      "        -1.2732e-01,  8.0322e-02, -1.3306e-01,  4.1107e-02,  7.4341e-02,\n",
      "         1.0034e-01, -4.4128e-02, -5.6091e-02,  6.1493e-02, -8.5571e-02,\n",
      "        -1.2201e-01, -2.3572e-01, -7.1472e-02,  2.0508e-01, -1.4355e-01,\n",
      "         3.2928e-02,  3.5767e-02, -1.2030e-01, -3.7622e-01, -9.9915e-02,\n",
      "         1.3779e-02,  2.7069e-02, -3.3630e-02, -7.9529e-02, -1.5527e-01,\n",
      "        -2.4353e-01, -3.1543e-01, -3.2886e-01, -4.1382e-02, -2.6025e-01,\n",
      "        -1.8787e-01,  5.8990e-02,  9.5032e-02,  1.0330e-02, -4.3701e-02,\n",
      "        -1.2433e-01, -2.2864e-01, -2.9614e-01, -3.6377e-01, -4.5410e-02,\n",
      "        -2.4426e-01, -1.8396e-01, -1.7346e-01,  5.2826e-02, -2.4231e-02,\n",
      "        -2.3590e-02, -8.3130e-02,  5.4810e-02, -1.9287e-02, -5.0342e-01,\n",
      "        -1.0675e-01, -1.9775e-01, -2.4426e-01,  3.5522e-02, -2.5903e-01,\n",
      "        -1.5283e-01, -2.6611e-01,  2.5314e-02, -8.9722e-02, -3.6694e-01,\n",
      "        -6.2073e-02, -1.9946e-01, -2.3840e-01, -2.6978e-01,  2.9126e-01,\n",
      "        -1.6138e-01, -4.8462e-02, -5.9143e-02, -1.9946e-01, -2.1606e-01,\n",
      "         7.9803e-03, -1.3702e-02, -7.5684e-02, -8.7769e-02, -2.1985e-01,\n",
      "        -3.4277e-01, -2.2131e-01, -4.9194e-02, -1.5771e-01,  2.4622e-01,\n",
      "        -1.8213e-01, -1.7822e-01, -2.9175e-01,  2.6611e-01, -4.4617e-02,\n",
      "        -2.5830e-01, -2.3108e-01, -3.4253e-01,  1.0208e-02, -1.2396e-01,\n",
      "         2.1378e-02, -3.4351e-01, -3.0103e-01, -3.4399e-01, -6.9275e-02,\n",
      "        -2.1692e-01, -2.2937e-01, -1.4404e-02,  2.3657e-01, -1.0004e-01,\n",
      "        -6.5308e-02, -3.6060e-01,  9.8328e-02,  1.4954e-01,  7.0007e-02,\n",
      "        -9.4910e-02,  1.4270e-01, -7.6904e-02,  3.7048e-02, -1.9165e-01,\n",
      "         1.3596e-02, -5.7678e-02,  1.3342e-01, -4.3701e-02, -3.9038e-01,\n",
      "        -1.6541e-01, -3.9551e-02, -5.5481e-02, -1.1719e-02, -1.4973e-03,\n",
      "         1.0870e-01,  2.7863e-02,  2.3145e-01, -1.2585e-01, -1.2164e-01,\n",
      "        -3.3997e-02,  7.7698e-02, -1.8164e-01,  7.0679e-02,  1.2109e-01,\n",
      "        -1.5173e-01, -5.9387e-02, -1.0663e-01,  2.8442e-01, -2.0752e-02,\n",
      "        -1.4563e-01,  1.9424e-02,  8.7280e-02, -1.7395e-01,  2.7649e-02,\n",
      "        -1.2634e-01,  2.0898e-01,  2.4158e-01,  2.9465e-02,  5.9753e-02,\n",
      "         8.1787e-02,  2.5977e-01, -7.2205e-02,  5.9471e-03, -2.2986e-01,\n",
      "        -1.2903e-01, -3.4399e-01,  1.5991e-01, -4.9927e-02, -1.4197e-01,\n",
      "        -1.1920e-01,  1.7175e-01, -1.0516e-01, -2.8961e-02, -1.5063e-01,\n",
      "         5.7098e-02, -2.0300e-01, -9.9426e-02,  6.5422e-03, -9.5825e-03,\n",
      "         6.2866e-02,  1.3379e-01, -3.7292e-02,  1.4725e-02, -7.8552e-02,\n",
      "        -3.3142e-02,  4.0703e-03, -1.8713e-01, -1.8591e-01,  8.1543e-02,\n",
      "        -5.7983e-02,  7.6866e-03,  4.8004e-02, -6.2866e-02,  3.2288e-02,\n",
      "        -2.2131e-01,  1.6003e-03,  4.1595e-02,  1.6159e-02,  4.4373e-02,\n",
      "         2.8540e-01, -9.6619e-02, -3.5095e-02,  1.3745e-01,  1.1230e-01,\n",
      "        -5.0537e-02, -2.2400e-02,  8.3801e-02, -7.1144e-03,  1.9678e-01,\n",
      "         9.8816e-02, -1.4740e-02, -3.3081e-02,  3.9043e-03, -2.5708e-01,\n",
      "         1.3123e-01,  1.6394e-01, -1.8481e-01, -1.1078e-01,  8.3160e-03,\n",
      "         2.4902e-02,  4.1016e-02,  5.0232e-02, -7.5439e-02, -1.2347e-01,\n",
      "        -3.4199e-03, -1.9043e-01, -4.6204e-02,  3.6377e-02,  2.1057e-02,\n",
      "         8.5693e-02, -1.0345e-02,  1.0229e-01, -1.9690e-01, -3.6499e-02,\n",
      "        -9.1003e-02, -2.2797e-02, -9.7595e-02,  9.4727e-02, -2.0093e-01,\n",
      "         2.5586e-01, -1.5759e-01, -1.4600e-01, -2.9468e-01,  1.3159e-01,\n",
      "        -1.2311e-01, -1.5759e-01,  1.2671e-01, -4.1846e-01, -1.2708e-01,\n",
      "        -1.6357e-01, -3.1641e-01, -1.2433e-01, -3.5706e-02, -1.2836e-03,\n",
      "        -8.1665e-02, -7.6416e-02,  1.1060e-01,  8.8623e-02, -1.0919e-01,\n",
      "         2.2705e-01, -2.4841e-02, -1.4819e-01, -2.2186e-02, -8.0383e-02,\n",
      "        -1.1945e-01, -2.3518e-03, -9.8206e-02, -5.6213e-02, -1.8030e-01,\n",
      "         9.7198e-03,  1.0754e-01, -1.4648e-01, -5.4054e-03,  1.2207e-02,\n",
      "        -1.3672e-02,  1.2549e-01,  3.8574e-02, -1.2805e-01, -1.3770e-01,\n",
      "         2.1631e-01, -1.0284e-01, -3.2673e-03,  3.1464e-02, -1.4001e-01,\n",
      "         2.3270e-02,  1.6647e-02, -2.1411e-01, -5.5847e-02, -5.0049e-02,\n",
      "         2.1777e-01,  1.0229e-01,  1.1703e-02,  2.0142e-01,  1.4575e-01,\n",
      "         1.2457e-01,  1.4868e-01,  1.0425e-01, -7.0429e-04,  4.6265e-02,\n",
      "         1.4374e-02,  5.2338e-02,  4.4212e-03,  8.4473e-02,  5.8258e-02,\n",
      "        -1.1493e-01, -2.7856e-01, -2.1143e-01, -4.7119e-01, -3.8135e-01,\n",
      "        -1.9275e-01, -2.0618e-01, -1.5906e-01, -1.7627e-01, -2.5635e-01,\n",
      "        -2.0911e-01, -4.7705e-01, -3.9771e-01,  1.3062e-01, -2.2815e-01,\n",
      "        -2.4451e-01, -3.3936e-02,  2.5562e-01, -1.6663e-01, -1.4062e-01,\n",
      "        -2.4524e-01, -2.3035e-01, -3.8025e-02, -2.8076e-01, -1.2494e-01,\n",
      "        -2.8516e-01, -5.8167e-02, -2.7979e-01,  2.1692e-01, -2.2839e-01,\n",
      "        -1.5662e-01, -1.2164e-01, -1.9019e-01, -1.5759e-01, -6.6589e-02,\n",
      "        -1.1969e-01, -4.6143e-02, -1.7603e-01, -4.6777e-01, -4.2139e-01,\n",
      "        -3.6963e-01, -3.6035e-01,  3.5381e-03,  2.8210e-03,  5.0262e-02,\n",
      "        -1.7102e-01,  5.4596e-02,  2.3315e-02, -1.8518e-01, -2.0093e-01,\n",
      "        -2.3914e-01, -1.3391e-01,  3.6682e-02, -2.0764e-01, -5.4980e-01,\n",
      "        -4.2334e-01, -1.7200e-01, -3.1421e-01, -2.2980e-02,  4.1161e-03,\n",
      "        -1.9348e-01, -1.9800e-01, -3.2422e-01,  1.3342e-01,  8.9050e-02,\n",
      "        -2.8735e-01, -2.4182e-01,  1.0223e-01, -2.5757e-01, -1.8665e-01,\n",
      "        -9.4421e-02, -9.7961e-02, -2.7252e-02, -1.6711e-01,  3.8818e-02,\n",
      "        -2.0435e-01, -6.7444e-02, -1.3123e-01, -2.5000e-01, -2.8125e-01,\n",
      "        -1.3025e-01, -1.2335e-01, -1.3214e-02, -1.0809e-01, -8.4656e-02,\n",
      "        -9.8694e-02, -6.5552e-02,  3.5583e-02, -5.3894e-02, -1.1798e-01,\n",
      "        -1.8567e-01,  8.8806e-02, -2.6587e-01, -1.5942e-01, -1.0199e-01,\n",
      "        -7.0618e-02, -2.3102e-02, -1.2030e-01, -2.7368e-01, -3.5571e-01,\n",
      "        -4.1553e-01, -1.3855e-01, -2.3328e-01, -2.2937e-01, -3.9575e-01,\n",
      "        -2.9272e-01, -1.1896e-01, -2.2009e-01, -3.4229e-01, -2.5391e-02,\n",
      "        -2.2327e-01, -1.6565e-01,  1.3184e-01, -2.7539e-01, -2.6270e-01,\n",
      "        -2.9150e-01, -1.3074e-01, -1.7786e-01, -2.5562e-01, -3.0396e-01,\n",
      "        -5.9021e-02, -5.3894e-02, -2.6440e-01, -3.6108e-01, -6.5430e-01,\n",
      "        -1.7883e-01, -1.9739e-01, -5.9631e-02, -2.5098e-01,  1.9006e-01,\n",
      "        -2.5220e-01, -1.9238e-01, -2.4524e-01, -2.2961e-01, -1.6931e-01,\n",
      "        -1.9531e-01,  2.5497e-02, -9.8450e-02, -1.3367e-01, -2.9248e-01,\n",
      "        -6.9702e-02, -1.6785e-01, -2.8900e-02, -2.7808e-01,  2.5879e-02,\n",
      "        -1.5991e-01, -7.6538e-02, -2.9206e-04, -2.5952e-01,  1.7487e-02,\n",
      "        -5.5580e-03, -9.2468e-02, -2.5879e-01, -1.0272e-01, -2.9816e-02,\n",
      "        -2.4597e-01, -4.7607e-02, -1.3306e-01, -2.3303e-01, -2.0483e-01,\n",
      "        -2.2913e-01, -2.2620e-01, -1.9788e-01, -6.4636e-02, -5.1819e-02,\n",
      "        -1.5979e-01, -2.1753e-01, -3.8025e-02, -2.0264e-01, -8.3557e-02,\n",
      "        -1.0590e-01, -3.1567e-01, -3.8940e-02, -6.6772e-02,  1.3257e-01,\n",
      "        -1.0248e-01, -3.0737e-01, -2.2742e-01, -2.7979e-01,  6.1676e-02,\n",
      "        -1.5063e-01,  2.0227e-01, -1.8542e-01,  2.4918e-02, -2.1301e-01,\n",
      "        -1.9666e-01, -3.3472e-01, -2.7295e-01, -2.2986e-01, -2.7930e-01,\n",
      "        -2.5610e-01, -4.2114e-01, -2.4280e-01, -3.7256e-01,  2.0410e-01,\n",
      "        -2.4011e-01, -1.9739e-01, -6.3721e-01, -1.8457e-01, -7.5012e-02,\n",
      "        -9.8450e-02, -1.4233e-01, -1.8665e-01, -4.8193e-01, -4.5898e-01,\n",
      "        -2.6099e-01, -1.1700e-01, -1.1139e-01, -2.5269e-01, -2.0044e-01,\n",
      "        -3.8721e-01,  1.9019e-01, -1.1603e-01, -1.4697e-01, -4.5801e-01,\n",
      "         9.8267e-02,  1.4633e-02, -1.5198e-01, -1.6223e-01, -1.7151e-01,\n",
      "        -1.5942e-01,  8.5205e-02, -2.5220e-01, -1.9531e-01, -8.6060e-02,\n",
      "        -2.1411e-01,  2.1985e-01, -2.1765e-01, -1.3184e-01, -2.4915e-01,\n",
      "        -1.1652e-01, -3.0688e-01, -1.9995e-01, -1.0577e-01, -1.0345e-01,\n",
      "         2.7954e-02, -1.7310e-01, -1.0455e-01, -7.4829e-02, -2.5122e-01,\n",
      "        -3.5327e-01, -3.4809e-03, -3.7476e-02,  6.3416e-02, -9.7595e-02,\n",
      "         8.0566e-03, -6.8298e-02, -2.3938e-01, -1.1249e-01, -1.8286e-01,\n",
      "        -2.6074e-01, -3.1274e-01,  9.1064e-02, -4.8096e-02, -4.7913e-02,\n",
      "        -2.7051e-01, -1.3477e-01, -1.7920e-01, -2.0789e-01, -5.0720e-02,\n",
      "        -1.0907e-01, -4.6509e-02,  4.5807e-02, -3.3283e-03, -6.9153e-02,\n",
      "         2.4139e-02, -2.3071e-02, -2.1301e-01, -4.2358e-02, -6.4514e-02,\n",
      "        -1.5149e-01, -1.3123e-02, -2.6155e-04,  8.1726e-02,  1.3184e-01,\n",
      "         7.9498e-03, -1.2115e-01, -8.2397e-02, -7.6538e-02, -5.0537e-02,\n",
      "        -3.4839e-01, -3.3472e-01, -4.4482e-01, -3.6206e-01, -2.2864e-01,\n",
      "        -2.7002e-01, -1.3098e-01, -3.2715e-02,  1.2894e-02, -1.5198e-01,\n",
      "        -9.4421e-02, -1.6077e-01, -9.1431e-02, -9.8328e-02,  1.3330e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(11.4970, device='cuda:0')\tavg_answer_loss:  tensor(5.8085, device='cuda:0')\tavg_type_loss:  tensor(1.1377, device='cuda:0')\tavg_val_f1:  0.1\tavg_val_em:  0.1\tavg_val_prec:  0.1\tavg_val_recall:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.1859,  0.0764,  0.2055,  ..., -0.6835,  0.0025,  0.0574],\n",
      "         [ 0.0640,  0.0970,  0.0309,  ...,  0.1110, -0.0145,  0.0610],\n",
      "         [ 0.1025,  0.0869,  0.1140,  ..., -0.4251,  0.0130,  0.0722],\n",
      "         ...,\n",
      "         [ 0.0090,  0.0281, -0.0159,  ..., -0.0875, -0.0340, -0.0656],\n",
      "         [-0.0216,  0.0542, -0.0062,  ..., -0.1194, -0.0420, -0.0815],\n",
      "         [-0.0298,  0.0814, -0.0124,  ..., -0.0831, -0.0368, -0.0776]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0444,  0.1711, -0.0157,  ..., -0.2621, -0.0267, -0.0017],\n",
      "         [-0.1525,  0.0745, -0.1663,  ...,  0.2539,  0.3201,  0.2647],\n",
      "         [ 0.0060, -0.0174,  0.1135,  ..., -0.3279, -0.0007,  0.0074],\n",
      "         ...,\n",
      "         [-0.0304,  0.1814,  0.0314,  ..., -0.0934, -0.0515, -0.1460],\n",
      "         [-0.0843,  0.1059, -0.0484,  ..., -0.0783, -0.0454,  0.0616],\n",
      "         [-0.0134,  0.0666, -0.0117,  ..., -0.0887, -0.0358, -0.0647]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0158,  0.1036,  0.0145,  ..., -0.2913,  0.0829,  0.0608],\n",
      "         [ 0.0395,  0.1639, -0.0726,  ...,  0.0869,  0.0509,  0.2191],\n",
      "         [ 0.0760,  0.1813, -0.0067,  ..., -0.3000,  0.1315,  0.0657],\n",
      "         ...,\n",
      "         [-0.0491,  0.2005,  0.0251,  ...,  0.0566, -0.0644, -0.0497],\n",
      "         [-0.0091,  0.0662,  0.0107,  ..., -0.1055, -0.0471, -0.0729],\n",
      "         [-0.0076,  0.0594, -0.0069,  ..., -0.0954, -0.0221, -0.0614]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0359,  0.0270,  0.1166,  ..., -0.2062,  0.0201,  0.0087],\n",
      "         [ 0.1896,  0.1286,  0.3207,  ..., -0.1967,  0.0558,  0.0282],\n",
      "         [ 0.0161,  0.0536,  0.0859,  ..., -0.1762,  0.0134, -0.1095],\n",
      "         ...,\n",
      "         [ 0.3229,  0.4985,  0.2074,  ..., -0.0602,  0.0481, -0.2901],\n",
      "         [-0.0223,  0.1035, -0.0209,  ..., -0.1036, -0.0445, -0.0767],\n",
      "         [ 0.0396,  0.1142, -0.0659,  ..., -0.1846, -0.0494, -0.2707]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0513,  0.2113,  0.0572,  ...,  0.0127,  0.0167,  0.0784],\n",
      "         [ 0.1228,  0.3032,  0.0946,  ...,  0.2847,  0.3212,  0.1375],\n",
      "         [ 0.1387,  0.0060,  0.3263,  ..., -0.3284,  0.0818,  0.1274],\n",
      "         ...,\n",
      "         [-0.0218,  0.0638, -0.0194,  ..., -0.1075, -0.0341, -0.0777],\n",
      "         [-0.0054,  0.0687, -0.0247,  ..., -0.0912, -0.0177, -0.0720],\n",
      "         [-0.0519,  0.0970, -0.0333,  ..., -0.0668, -0.0225, -0.1895]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.3204, -0.1221,  0.0330,  ..., -0.6374,  0.0840,  0.3009],\n",
      "         [ 0.1450, -0.1108,  0.2634,  ..., -0.5214,  0.0334,  0.4174],\n",
      "         [ 0.0396,  0.1091,  0.1425,  ..., -0.1868,  0.0090,  0.3352],\n",
      "         ...,\n",
      "         [-0.0232,  0.0714, -0.0146,  ..., -0.0858, -0.0272, -0.0779],\n",
      "         [-0.0119,  0.0276, -0.0129,  ..., -0.0972, -0.0331, -0.0634],\n",
      "         [-0.0021,  0.0669, -0.0156,  ..., -0.0904, -0.0334, -0.0755]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0427, -0.0922,  0.0048,  ..., -0.1105,  0.1090,  0.0849],\n",
      "         [ 0.0804,  0.0978,  0.2077,  ..., -0.2628,  0.0704,  0.0448],\n",
      "         [ 0.1183,  0.1522,  0.0477,  ..., -0.3810, -0.0661,  0.0318],\n",
      "         ...,\n",
      "         [ 0.0123,  0.0679, -0.0237,  ..., -0.0901, -0.0442, -0.0685],\n",
      "         [ 0.0189,  0.0611,  0.0068,  ..., -0.0754, -0.0451, -0.0693],\n",
      "         [ 0.1982,  0.0428,  0.1684,  ...,  0.7246,  0.4108, -0.7206]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0824,  0.1835,  0.1532,  ..., -0.0866,  0.0632,  0.0912],\n",
      "         [ 0.2323,  0.3266,  0.0714,  ...,  0.1836,  0.1433,  0.1818],\n",
      "         [ 0.2262, -0.2031,  0.2709,  ..., -0.5009,  0.5099,  0.0050],\n",
      "         ...,\n",
      "         [ 0.1311,  0.2932, -0.1140,  ..., -0.1217, -0.1048, -0.2131],\n",
      "         [-0.0259,  0.0978, -0.0147,  ..., -0.0848, -0.0307, -0.0702],\n",
      "         [ 0.1363,  0.5691, -0.2372,  ..., -0.3083,  0.1257, -0.2082]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0438,  0.1829,  0.1014,  ..., -0.2927,  0.1353,  0.0151],\n",
      "         [ 0.1653,  0.1186, -0.0061,  ..., -0.0321,  0.0376,  0.1563],\n",
      "         [ 0.0343,  0.2890,  0.0760,  ..., -0.3711,  0.1702,  0.0741],\n",
      "         ...,\n",
      "         [-0.0375,  0.1129,  0.0424,  ..., -0.0786, -0.1071, -0.2581],\n",
      "         [-0.0471,  0.0943, -0.0711,  ..., -0.0596,  0.0098, -0.2003],\n",
      "         [-0.0186,  0.0388, -0.0177,  ..., -0.1000, -0.0339, -0.0577]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0607,  0.1022, -0.0269,  ..., -0.2914, -0.0012,  0.0179],\n",
      "         [ 0.4471, -0.1331,  0.1839,  ...,  0.1248, -0.0319,  0.0707],\n",
      "         [ 0.0851, -0.0045,  0.0880,  ..., -0.5129,  0.1317,  0.2081],\n",
      "         ...,\n",
      "         [-0.0273,  0.0588, -0.0134,  ..., -0.1215, -0.0343, -0.0795],\n",
      "         [-0.0089,  0.1562,  0.0248,  ..., -0.0563, -0.0495, -0.2417],\n",
      "         [-0.0260,  0.0548, -0.0176,  ..., -0.0915, -0.0421, -0.0711]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0860,  0.1673,  0.0201,  ..., -0.2797,  0.0405, -0.0207],\n",
      "         [-0.0236,  0.2471,  0.1597,  ...,  0.1262,  0.0627,  0.0933],\n",
      "         [-0.1090,  0.2075,  0.0447,  ..., -0.1160,  0.0170,  0.0985],\n",
      "         ...,\n",
      "         [-0.0199,  0.0951,  0.0013,  ..., -0.1070, -0.0442, -0.0867],\n",
      "         [-0.0232,  0.0609, -0.0210,  ..., -0.0923, -0.0358, -0.0647],\n",
      "         [-0.0098,  0.0547, -0.0109,  ..., -0.1009, -0.0366, -0.0526]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0375,  0.0223,  0.0567,  ..., -0.1943,  0.0141,  0.0576],\n",
      "         [ 0.1532,  0.0569,  0.0410,  ...,  0.0859,  0.0179,  0.0226],\n",
      "         [ 0.0208,  0.0620, -0.0051,  ..., -0.3572, -0.0012,  0.1692],\n",
      "         ...,\n",
      "         [-0.0167,  0.0710, -0.0087,  ..., -0.0720, -0.0329, -0.0716],\n",
      "         [-0.0263,  0.0765, -0.0056,  ..., -0.0912, -0.0166, -0.0793],\n",
      "         [-0.0267,  0.0660,  0.0015,  ..., -0.0885, -0.0354, -0.0750]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0513,  0.0134,  0.1627,  ..., -0.3383,  0.0364, -0.0091],\n",
      "         [ 0.0806,  0.1739,  0.1854,  ..., -0.1448,  0.0481,  0.1880],\n",
      "         [-0.0538, -0.0210,  0.2008,  ..., -0.3539,  0.0451,  0.0538],\n",
      "         ...,\n",
      "         [-0.1439,  0.4259,  0.0401,  ..., -0.2410, -0.0386,  0.0300],\n",
      "         [-0.0195,  0.0511, -0.0159,  ..., -0.0867, -0.0296, -0.0723],\n",
      "         [-0.0116,  0.0619, -0.0067,  ..., -0.1229, -0.0287, -0.0744]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1170,  0.2873,  0.2286,  ..., -0.1053,  0.0324,  0.2998],\n",
      "         [ 0.1789,  0.0974, -0.0020,  ...,  0.0683,  0.0563,  0.1618],\n",
      "         [ 0.1299,  0.0970,  0.0220,  ..., -0.2040, -0.0409,  0.0981],\n",
      "         ...,\n",
      "         [ 0.0106,  0.0958, -0.0207,  ..., -0.0896, -0.0474, -0.0816],\n",
      "         [-0.0075,  0.0672, -0.0170,  ..., -0.0945, -0.0313, -0.0751],\n",
      "         [-0.0303,  0.0644, -0.0207,  ..., -0.0834, -0.0530, -0.0685]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0510,  0.2579, -0.0522,  ..., -0.1358,  0.0450,  0.0876],\n",
      "         [-0.0813,  0.6990,  0.1218,  ..., -0.1754, -0.0606,  0.4114],\n",
      "         [ 0.0931,  0.2986,  0.0270,  ..., -0.2926,  0.0093, -0.0450],\n",
      "         ...,\n",
      "         [-0.0195,  0.0584, -0.0170,  ..., -0.0894, -0.0517, -0.0733],\n",
      "         [ 0.0233, -0.0487, -0.0562,  ..., -0.0736, -0.0439, -0.0661],\n",
      "         [-0.0188,  0.0675,  0.0014,  ..., -0.0766, -0.0381, -0.0938]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.1999,  0.2642,  0.1672,  ..., -0.1471, -0.1170,  0.1020],\n",
      "         [ 0.2310,  0.0437,  0.1292,  ..., -0.2120, -0.0975,  0.0749],\n",
      "         [ 0.0569,  0.1956,  0.1972,  ..., -0.1718,  0.0027,  0.2915],\n",
      "         ...,\n",
      "         [ 0.1939,  0.0186, -0.1417,  ..., -0.0310,  0.1161, -0.3413],\n",
      "         [ 0.0012,  0.0457, -0.0361,  ..., -0.1245, -0.0364,  0.0171],\n",
      "         [ 0.2234,  0.1468, -0.0326,  ...,  0.2886,  0.0867, -0.6932]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.1831,  0.0545,  0.0870,  ..., -0.1931,  0.0375,  0.1390],\n",
      "         [ 0.1683,  0.0567,  0.0087,  ...,  0.0240,  0.0380,  0.2619],\n",
      "         [ 0.1197,  0.0156,  0.0086,  ..., -0.3103,  0.0470, -0.0255],\n",
      "         ...,\n",
      "         [ 0.0277,  0.0684, -0.0016,  ..., -0.1731,  0.0150, -0.1974],\n",
      "         [-0.0186,  0.0611, -0.0138,  ..., -0.0954, -0.0240,  0.0144],\n",
      "         [-0.0627,  0.4782, -0.1911,  ..., -0.0936, -0.0167,  0.1106]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0850,  0.1186,  0.0839,  ..., -0.1629,  0.0492,  0.0376],\n",
      "         [ 0.1619,  0.1938, -0.0497,  ..., -0.0494,  0.1374,  0.2272],\n",
      "         [ 0.0791,  0.0335,  0.0385,  ..., -0.2464,  0.1319,  0.0212],\n",
      "         ...,\n",
      "         [-0.0100,  0.0422, -0.0201,  ..., -0.0829, -0.0432, -0.0722],\n",
      "         [-0.0195,  0.0592, -0.0166,  ..., -0.0989, -0.0289,  0.0127],\n",
      "         [-0.0138,  0.0641,  0.0013,  ..., -0.0889, -0.0259,  0.0142]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0135,  0.0654,  0.0896,  ..., -0.0511,  0.1356, -0.0448],\n",
      "         [ 0.1825,  0.0162,  0.1575,  ..., -0.1861,  0.0263, -0.2469],\n",
      "         [ 0.0830, -0.1018, -0.0637,  ..., -0.2576,  0.1421, -0.2110],\n",
      "         ...,\n",
      "         [-0.0061,  0.0840, -0.0169,  ..., -0.1036, -0.0319, -0.0741],\n",
      "         [-0.0006,  0.0709, -0.0353,  ..., -0.0871, -0.0405, -0.0809],\n",
      "         [-0.0027,  0.0691, -0.0123,  ..., -0.0806, -0.0329, -0.0809]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1128,  0.0529,  0.0887,  ..., -0.2374,  0.1420,  0.0387],\n",
      "         [-0.0152,  0.0791,  0.0486,  ..., -0.3400,  0.2168,  0.1974],\n",
      "         [ 0.0250,  0.1187, -0.0459,  ..., -0.4974,  0.1337, -0.0276],\n",
      "         ...,\n",
      "         [-0.0165,  0.0583,  0.0023,  ..., -0.0952, -0.0382, -0.0591],\n",
      "         [-0.0310,  0.1273, -0.0261,  ..., -0.0686, -0.0319, -0.1582],\n",
      "         [ 0.0018,  0.0723, -0.1252,  ...,  0.1948,  0.1149, -0.7713]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0908,  0.1342,  0.0958,  ..., -0.1774,  0.0582, -0.0054],\n",
      "         [ 0.3556,  0.2061,  0.1294,  ..., -0.1249,  0.0951,  0.0402],\n",
      "         [ 0.1433,  0.1518,  0.1384,  ..., -0.4199,  0.0166,  0.0597],\n",
      "         ...,\n",
      "         [-0.0142,  0.0616,  0.0070,  ..., -0.0922, -0.0243, -0.0877],\n",
      "         [-0.0186,  0.0587, -0.0179,  ..., -0.1113, -0.0248, -0.0965],\n",
      "         [-0.0153,  0.0569, -0.0231,  ..., -0.1101, -0.0455, -0.0769]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0178,  0.0923,  0.0666,  ..., -0.2101,  0.0852,  0.0909],\n",
      "         [ 0.2373, -0.0673,  0.2868,  ...,  0.0539,  0.0780,  0.2392],\n",
      "         [ 0.0718,  0.1215,  0.1135,  ..., -0.1211, -0.0030,  0.1509],\n",
      "         ...,\n",
      "         [-0.0128,  0.3342, -0.2573,  ..., -0.0416,  0.0142, -0.1783],\n",
      "         [-0.0123,  0.0616, -0.0140,  ..., -0.0838, -0.0330, -0.0779],\n",
      "         [-0.0206,  0.0710, -0.0187,  ..., -0.0861, -0.0381, -0.0838]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0309,  0.1194,  0.0613,  ...,  0.0251,  0.0754,  0.1129],\n",
      "         [ 0.1766,  0.6015, -0.0216,  ...,  0.8266,  0.0681,  0.4800],\n",
      "         [ 0.0097,  0.0650,  0.0198,  ..., -0.1311, -0.0329,  0.1343],\n",
      "         ...,\n",
      "         [ 0.3911,  0.2313, -0.0164,  ...,  0.6793,  0.0787, -0.2684],\n",
      "         [ 0.0472,  0.0493, -0.0199,  ..., -0.0331, -0.0285, -0.1162],\n",
      "         [-0.0062,  0.0622, -0.0141,  ..., -0.1038, -0.0180, -0.0807]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0655,  0.0639, -0.0875,  ..., -0.1711,  0.0906, -0.0395],\n",
      "         [ 0.5090, -0.2409,  0.1388,  ...,  0.2215,  0.0449,  0.0409],\n",
      "         [ 0.1993,  0.1500,  0.0729,  ..., -0.1263,  0.0734,  0.1084],\n",
      "         ...,\n",
      "         [-0.0139,  0.0555, -0.0152,  ..., -0.0761, -0.0486, -0.0731],\n",
      "         [-0.0051,  0.0634, -0.0118,  ..., -0.0912, -0.0319, -0.0840],\n",
      "         [-0.0121,  0.0672, -0.0238,  ..., -0.0888, -0.0395, -0.0842]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1115,  0.0821, -0.0502,  ..., -0.2088,  0.1160,  0.0157],\n",
      "         [ 0.0931, -0.1346,  0.2682,  ..., -0.0549,  0.0697,  0.0818],\n",
      "         [ 0.2163,  0.1482,  0.0268,  ..., -0.2341,  0.0911,  0.1270],\n",
      "         ...,\n",
      "         [-0.0195,  0.0555, -0.0184,  ..., -0.0936, -0.0382, -0.0664],\n",
      "         [-0.0158,  0.0602, -0.0096,  ..., -0.0804, -0.0317, -0.0784],\n",
      "         [-0.0280,  0.0629, -0.0340,  ..., -0.1095, -0.0353, -0.0732]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0701,  0.1314, -0.0039,  ..., -0.2327,  0.1324,  0.0910],\n",
      "         [ 0.2741, -0.1161,  0.2202,  ...,  0.2688,  0.0395,  0.0323],\n",
      "         [-0.0339, -0.0293, -0.0418,  ..., -0.3100,  0.1287, -0.0881],\n",
      "         ...,\n",
      "         [-0.0139,  0.0655, -0.0095,  ..., -0.0920, -0.0310, -0.0780],\n",
      "         [-0.0159,  0.0545,  0.0031,  ..., -0.0849, -0.0327, -0.0754],\n",
      "         [-0.0129,  0.0599, -0.0196,  ..., -0.0803, -0.0264, -0.0769]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0751, -0.0207,  0.0336,  ..., -0.1598,  0.1977,  0.1036],\n",
      "         [ 0.1559,  0.0947,  0.0068,  ..., -0.0487,  0.1759, -0.0221],\n",
      "         [-0.1138,  0.0456,  0.0465,  ..., -0.3735, -0.0425, -0.0132],\n",
      "         ...,\n",
      "         [-0.0156,  0.0884, -0.0097,  ..., -0.0901, -0.0310, -0.0744],\n",
      "         [-0.0045,  0.0579, -0.0356,  ..., -0.1040, -0.0312, -0.0894],\n",
      "         [-0.0152,  0.0633, -0.0192,  ..., -0.0915, -0.0313, -0.0830]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2764, -0.0448,  0.3002,  ..., -0.5139,  0.3604,  0.1546],\n",
      "         [ 0.3599,  0.1557,  0.2217,  ..., -0.1203,  0.1426, -0.0009],\n",
      "         [ 0.2293,  0.1146,  0.0116,  ..., -0.6301,  0.0405,  0.2483],\n",
      "         ...,\n",
      "         [-0.0166,  0.0587, -0.0215,  ..., -0.0823, -0.0156, -0.0636],\n",
      "         [-0.0204,  0.0644, -0.0197,  ..., -0.0892, -0.0404, -0.0702],\n",
      "         [ 0.1299,  0.3048, -0.1622,  ...,  0.0555,  0.2830, -0.2763]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.6171e-01,  3.4881e-02, -5.1720e-02,  ..., -2.4049e-01,\n",
      "           1.5279e-01, -1.4118e-02],\n",
      "         [ 3.8788e-02, -1.9134e-04, -8.1281e-02,  ..., -6.4114e-02,\n",
      "           7.9496e-02,  1.1751e-01],\n",
      "         [ 1.5991e-01,  2.9999e-02, -1.2600e-01,  ..., -5.6333e-02,\n",
      "           7.7244e-02, -1.9170e-01],\n",
      "         ...,\n",
      "         [-2.2548e-02,  6.9625e-02, -9.6909e-03,  ..., -8.9280e-02,\n",
      "          -3.2773e-02, -5.8158e-02],\n",
      "         [-1.1564e-02,  3.9690e-02, -8.3267e-03,  ..., -8.1041e-02,\n",
      "          -2.7225e-02, -7.9886e-02],\n",
      "         [-3.1839e-02,  1.8794e-01,  6.5750e-03,  ..., -6.0129e-02,\n",
      "          -1.1567e-01, -4.0019e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-1.6716e-02, -1.2039e-02,  7.7290e-02,  ..., -3.5930e-02,\n",
      "          -5.2241e-02, -7.6785e-02],\n",
      "         [ 1.8216e-01,  2.4145e-02,  1.9294e-01,  ...,  2.0552e-01,\n",
      "           1.8419e-02,  4.1145e-01],\n",
      "         [ 2.1591e-01,  7.5667e-02, -8.9904e-02,  ..., -3.8261e-01,\n",
      "          -5.0946e-03,  1.7489e-01],\n",
      "         ...,\n",
      "         [ 2.3400e-01,  1.9518e-01,  7.7449e-02,  ...,  1.6696e-01,\n",
      "           1.5230e-01, -1.6516e-01],\n",
      "         [-3.0596e-04,  1.7388e-01,  4.6796e-02,  ..., -7.8945e-02,\n",
      "          -6.3338e-02, -2.1226e-01],\n",
      "         [ 9.1958e-02,  9.4786e-02, -1.1533e-01,  ...,  4.1527e-02,\n",
      "           1.2078e-01, -4.2282e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0282,  0.2974, -0.0823,  ..., -0.5420,  0.0999, -0.1948],\n",
      "         [ 0.0519, -0.1114,  0.0782,  ...,  0.2539,  0.4219,  0.1886],\n",
      "         [ 0.3457,  0.1588,  0.1203,  ..., -0.3499,  0.0104,  0.0287],\n",
      "         ...,\n",
      "         [-0.0519,  0.1001, -0.0083,  ..., -0.0656, -0.0201, -0.1546],\n",
      "         [-0.0169,  0.0517, -0.0038,  ..., -0.0863, -0.0187, -0.0580],\n",
      "         [ 0.0143,  0.0572, -0.0426,  ..., -0.0839, -0.0258, -0.0761]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1269, -0.0148, -0.0245,  ...,  0.0291,  0.1846,  0.0312],\n",
      "         [ 0.3442,  0.0309,  0.3859,  ...,  0.2971,  0.1869,  0.6119],\n",
      "         [ 0.1726,  0.4360,  0.2627,  ...,  0.8076,  0.4598,  0.3641],\n",
      "         ...,\n",
      "         [-0.0138,  0.0632, -0.0210,  ..., -0.1141, -0.0381, -0.0783],\n",
      "         [-0.0222,  0.0653,  0.0014,  ..., -0.1054, -0.0365, -0.0664],\n",
      "         [-0.0195,  0.0688, -0.0189,  ..., -0.0877, -0.0151, -0.0806]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.1405,  0.0895,  0.1838,  ...,  0.1170, -0.0174,  0.1241],\n",
      "         [ 0.2825,  0.6054,  0.1008,  ...,  0.0097,  0.0835,  0.1038],\n",
      "         [ 0.1066,  0.1532,  0.0255,  ..., -0.0878,  0.0477,  0.2086],\n",
      "         ...,\n",
      "         [ 0.0815,  0.2211, -0.2196,  ...,  0.0359,  0.1751, -0.3208],\n",
      "         [ 0.2812, -0.0049, -0.0682,  ...,  0.0484, -0.1942, -0.2855],\n",
      "         [-0.0322,  0.0670, -0.0011,  ..., -0.0813, -0.0214, -0.0668]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1984, -0.0069,  0.0494,  ..., -0.1475,  0.0410, -0.0261],\n",
      "         [ 0.2761, -0.0885,  0.0832,  ...,  0.0098,  0.2261,  0.2155],\n",
      "         [ 0.2108,  0.0800,  0.0455,  ..., -0.2717, -0.0021,  0.2553],\n",
      "         ...,\n",
      "         [-0.0201,  0.0650, -0.0118,  ..., -0.0861, -0.0302, -0.0723],\n",
      "         [-0.0254,  0.2081, -0.0191,  ..., -0.0673, -0.1078, -0.0733],\n",
      "         [ 0.0208,  0.1500, -0.0465,  ..., -0.0359, -0.1411, -0.1907]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1397,  0.0428,  0.0429,  ..., -0.2178,  0.1962,  0.0831],\n",
      "         [ 0.3261, -0.0624,  0.1483,  ...,  0.2003,  0.0209,  0.1627],\n",
      "         [ 0.3218,  0.0420,  0.0195,  ..., -0.3824, -0.0573, -0.0388],\n",
      "         ...,\n",
      "         [-0.0334,  0.0959, -0.0099,  ..., -0.0505, -0.0292, -0.1877],\n",
      "         [ 0.0066,  0.0584, -0.0223,  ..., -0.1001, -0.0335, -0.0943],\n",
      "         [ 0.0034,  0.1656, -0.0335,  ..., -0.0894, -0.0515, -0.1773]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1340,  0.1506,  0.0998,  ...,  0.0412,  0.2166, -0.0476],\n",
      "         [ 0.3266, -0.1153,  0.2556,  ...,  0.0851,  0.2009,  0.0265],\n",
      "         [ 0.1962,  0.1127,  0.0613,  ..., -0.1751,  0.0238,  0.0259],\n",
      "         ...,\n",
      "         [ 0.0261,  0.0926, -0.0262,  ..., -0.0833, -0.0281, -0.0830],\n",
      "         [-0.0175,  0.0555, -0.0167,  ..., -0.0967, -0.0318, -0.0818],\n",
      "         [-0.0029,  0.0621, -0.0238,  ..., -0.0885, -0.0347, -0.0772]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0649,  0.1883,  0.0405,  ...,  0.0108,  0.1366,  0.0295],\n",
      "         [ 0.1207,  0.2270,  0.2592,  ...,  0.2660, -0.0086,  0.1965],\n",
      "         [ 0.3095,  0.4535,  0.1851,  ...,  0.5552,  0.1370,  0.2808],\n",
      "         ...,\n",
      "         [-0.0061,  0.0561, -0.0087,  ..., -0.0912, -0.0302, -0.0684],\n",
      "         [-0.0200,  0.0667, -0.0209,  ..., -0.1098, -0.0266, -0.0775],\n",
      "         [ 0.0543,  0.0767, -0.1547,  ...,  0.1239, -0.0218, -0.0933]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2046,  0.0819, -0.0118,  ...,  0.0589,  0.1098,  0.1526],\n",
      "         [-0.2581,  0.1003,  0.2353,  ..., -0.7012,  0.3977, -0.2912],\n",
      "         [ 0.3602, -0.1568, -0.2349,  ..., -0.1707, -0.0201,  0.1727],\n",
      "         ...,\n",
      "         [ 0.0112,  0.0572, -0.0224,  ..., -0.1134, -0.0458, -0.0841],\n",
      "         [-0.0258,  0.1536,  0.0144,  ..., -0.0831, -0.1279, -0.2605],\n",
      "         [-0.0134,  0.0657, -0.0019,  ..., -0.0934, -0.0202, -0.0845]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1410,  0.1250,  0.1264,  ..., -0.0247,  0.1974,  0.0604],\n",
      "         [ 0.3020,  0.0245,  0.1516,  ...,  0.1659,  0.1293,  0.0097],\n",
      "         [ 0.1914, -0.0964,  0.1100,  ..., -0.1358,  0.1781, -0.1327],\n",
      "         ...,\n",
      "         [-0.0259,  0.0704, -0.0321,  ..., -0.0871, -0.0394, -0.0664],\n",
      "         [-0.0284,  0.3923, -0.0681,  ..., -0.2425,  0.0498,  0.0658],\n",
      "         [-0.0048,  0.1473, -0.0901,  ..., -0.0789, -0.1232, -0.1876]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4770,  0.0525,  0.2307,  ...,  0.1585,  0.3753,  0.2453],\n",
      "         [ 0.3502,  0.1354,  0.0315,  ...,  0.1831,  0.0862,  0.2331],\n",
      "         [ 0.2010,  0.0375,  0.0258,  ..., -0.2344,  0.2208,  0.1404],\n",
      "         ...,\n",
      "         [-0.0202,  0.0893, -0.0293,  ..., -0.0946, -0.0365,  0.0165],\n",
      "         [-0.0109,  0.0851, -0.0262,  ..., -0.0833, -0.0274, -0.0741],\n",
      "         [-0.0210,  0.0594, -0.0224,  ..., -0.0779, -0.0370, -0.0736]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1827, -0.0247,  0.1092,  ..., -0.1131,  0.2517,  0.2510],\n",
      "         [ 0.4249,  0.0485, -0.1866,  ...,  0.3481,  0.2355,  0.0929],\n",
      "         [ 0.4272,  0.1155, -0.0512,  ...,  0.0157,  0.3098,  0.0841],\n",
      "         ...,\n",
      "         [ 0.1774,  0.0937, -0.1931,  ..., -0.1546,  0.1455, -0.0659],\n",
      "         [ 0.0134,  0.1794, -0.0200,  ..., -0.0355, -0.1388, -0.1721],\n",
      "         [-0.0532,  0.0877, -0.0264,  ..., -0.0419, -0.0236, -0.2010]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1287,  0.2031,  0.0310,  ..., -0.0511,  0.2230,  0.0407],\n",
      "         [ 0.4771,  0.1058,  0.0075,  ...,  0.3249,  0.3073, -0.0181],\n",
      "         [ 0.2256,  0.2046, -0.0447,  ..., -0.1867,  0.2250,  0.0049],\n",
      "         ...,\n",
      "         [-0.0049,  0.1479, -0.0650,  ..., -0.0398, -0.0385, -0.2274],\n",
      "         [-0.0482,  0.0164, -0.0073,  ..., -0.0428,  0.0032, -0.1856],\n",
      "         [-0.0401,  0.0739, -0.0620,  ..., -0.0533, -0.0276, -0.1793]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2222,  0.1049,  0.0300,  ...,  0.0772,  0.1701,  0.0165],\n",
      "         [ 0.4014,  0.2345,  0.3220,  ...,  0.3333,  0.1214,  0.0351],\n",
      "         [ 0.3539,  0.3450,  0.0802,  ..., -0.3594, -0.0232, -0.0866],\n",
      "         ...,\n",
      "         [-0.0111,  0.0668, -0.0160,  ..., -0.0859, -0.0309, -0.0761],\n",
      "         [-0.0171,  0.0734, -0.0278,  ..., -0.0873, -0.0143, -0.0792],\n",
      "         [-0.0095,  0.0564, -0.0173,  ..., -0.0847, -0.0250, -0.0777]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1870,  0.2430,  0.0034,  ...,  0.0199,  0.0233,  0.0361],\n",
      "         [ 0.6463, -0.0965, -0.0161,  ..., -0.2260,  0.2627,  0.3049],\n",
      "         [ 0.0608, -0.2113,  0.0359,  ..., -0.0917,  0.2268, -0.0383],\n",
      "         ...,\n",
      "         [-0.0118,  0.0624, -0.0231,  ..., -0.0853, -0.0262, -0.0868],\n",
      "         [-0.0024,  0.0523, -0.0138,  ..., -0.0935, -0.0134, -0.0522],\n",
      "         [-0.0054,  0.0622, -0.0179,  ..., -0.0977, -0.0274, -0.0788]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2836,  0.1136,  0.0764,  ..., -0.1762,  0.1878,  0.0094],\n",
      "         [ 0.4640, -0.0998,  0.3452,  ...,  0.4801,  0.3597,  0.2061],\n",
      "         [ 0.7472, -0.1370,  0.2432,  ..., -0.9926,  0.4646, -0.1439],\n",
      "         ...,\n",
      "         [-0.0055,  0.2123, -0.0486,  ..., -0.0721, -0.1170, -0.2398],\n",
      "         [-0.0088,  0.0651, -0.0087,  ..., -0.0796, -0.0294, -0.0800],\n",
      "         [-0.0073,  0.0581, -0.0157,  ..., -0.0810, -0.0294, -0.0760]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1149,  0.0772, -0.0283,  ..., -0.2309,  0.2259, -0.0581],\n",
      "         [ 0.1726,  0.3074,  0.1186,  ...,  0.3286,  0.1778,  0.0623],\n",
      "         [ 0.2454,  0.0635, -0.1273,  ...,  0.0603, -0.0069,  0.0918],\n",
      "         ...,\n",
      "         [-0.0106,  0.0507, -0.0111,  ..., -0.0895, -0.0292, -0.0790],\n",
      "         [ 0.3372,  0.3838, -0.2188,  ...,  0.3540, -0.0561, -0.8992],\n",
      "         [-0.0287,  0.3103,  0.0556,  ..., -0.1763, -0.1752, -0.2977]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0823,  0.0993,  0.1145,  ...,  0.1964,  0.1164,  0.0127],\n",
      "         [ 0.1092,  0.5254,  0.3290,  ...,  0.5678,  0.3369, -0.1418],\n",
      "         [ 0.1885,  0.2438, -0.1646,  ..., -0.1278,  0.0874, -0.0342],\n",
      "         ...,\n",
      "         [-0.0796,  0.2495, -0.1228,  ..., -0.3767,  0.0431, -0.0155],\n",
      "         [-0.0093,  0.0476, -0.0273,  ..., -0.0910, -0.0081, -0.0626],\n",
      "         [-0.0190,  0.0916, -0.0106,  ..., -0.0851, -0.0284, -0.0721]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1961,  0.1476,  0.0698,  ..., -0.0820,  0.1367,  0.0578],\n",
      "         [ 0.1084,  0.3569,  0.1216,  ...,  0.1897,  0.2651,  0.0789],\n",
      "         [ 0.1482,  0.1641, -0.0423,  ..., -0.6289,  0.1862,  0.0412],\n",
      "         ...,\n",
      "         [ 0.0228,  0.1052, -0.0868,  ..., -0.0766, -0.0020, -0.2658],\n",
      "         [ 0.0083,  0.0688, -0.0158,  ..., -0.1032, -0.0310, -0.0871],\n",
      "         [-0.0100,  0.0659, -0.0243,  ..., -0.1069, -0.0088, -0.1057]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.2363,  0.0471,  0.0650,  ..., -0.0751,  0.2553,  0.0113],\n",
      "         [ 0.4234,  0.1217, -0.1270,  ...,  0.3187,  0.2892, -0.2034],\n",
      "         [ 0.4666,  0.1575,  0.1247,  ..., -0.1435,  0.3462, -0.0532],\n",
      "         ...,\n",
      "         [ 0.2332,  0.1112, -0.0104,  ...,  0.2847,  0.3248, -0.5097],\n",
      "         [-0.0200,  0.0394, -0.0177,  ..., -0.0795, -0.0293, -0.0887],\n",
      "         [-0.0183,  0.0610, -0.0030,  ..., -0.0895, -0.0255, -0.0777]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1197,  0.1446,  0.0796,  ..., -0.1333,  0.2879,  0.1157],\n",
      "         [ 0.3822,  0.7559,  0.1129,  ...,  0.8122,  0.3084,  0.1141],\n",
      "         [ 0.3090,  0.1661, -0.1014,  ..., -0.2855,  0.2641, -0.0575],\n",
      "         ...,\n",
      "         [ 0.0416,  0.4867,  0.0490,  ..., -0.3852,  0.1419, -0.2999],\n",
      "         [-0.0092,  0.0624, -0.0077,  ..., -0.0641, -0.0202, -0.1217],\n",
      "         [-0.0091,  0.0632, -0.0170,  ..., -0.1115, -0.0314, -0.0777]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0341,  0.0259, -0.0114,  ...,  0.0613,  0.2132,  0.0718],\n",
      "         [ 0.3004,  0.2074,  0.0143,  ..., -0.0727,  0.2407,  0.0534],\n",
      "         [ 0.2706, -0.0772, -0.1561,  ...,  0.0287,  0.1764,  0.2073],\n",
      "         ...,\n",
      "         [-0.0157,  0.0653, -0.0155,  ..., -0.0708, -0.0306,  0.0127],\n",
      "         [-0.0016,  0.0599, -0.0261,  ..., -0.1081, -0.0333, -0.1153],\n",
      "         [-0.0038,  0.0620, -0.0191,  ..., -0.0973, -0.0388, -0.0820]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2065,  0.0359,  0.0839,  ..., -0.1128,  0.2130,  0.0113],\n",
      "         [ 0.4085,  0.1807,  0.0392,  ...,  0.2151,  0.2941, -0.0158],\n",
      "         [ 0.0630,  0.2194,  0.0614,  ...,  0.1825,  0.2841,  0.4140],\n",
      "         ...,\n",
      "         [ 0.3334, -0.0904,  0.0813,  ...,  0.7550, -0.1452,  0.3289],\n",
      "         [ 0.0238,  0.2843,  0.0961,  ...,  0.3586,  0.1329, -0.3550],\n",
      "         [ 0.0158,  0.0521, -0.0136,  ..., -0.0814, -0.0190, -0.0620]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3012,  0.1170, -0.1099,  ...,  0.0575,  0.3556, -0.0502],\n",
      "         [ 0.4155,  0.2822,  0.0786,  ...,  0.4783,  0.3336,  0.5038],\n",
      "         [ 0.3017,  0.0371,  0.0272,  ...,  0.3066,  0.2134, -0.0612],\n",
      "         ...,\n",
      "         [-0.0156,  0.0737, -0.0185,  ..., -0.0978, -0.0289, -0.0805],\n",
      "         [-0.0492,  0.1469,  0.0377,  ..., -0.1156, -0.0671, -0.2224],\n",
      "         [-0.0307,  0.4318, -0.1989,  ..., -0.3828,  0.0502, -0.0090]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0097,  0.2722, -0.0179,  ...,  0.3988,  0.1952, -0.0619],\n",
      "         [ 0.0381, -0.0356,  0.1402,  ...,  0.5704,  0.1377, -0.1878],\n",
      "         [-0.0130, -0.0809, -0.0433,  ...,  0.9158,  0.2999, -0.1895],\n",
      "         ...,\n",
      "         [-0.0088,  0.0630, -0.0233,  ..., -0.0779, -0.0273, -0.0772],\n",
      "         [-0.0172,  0.0769,  0.0031,  ..., -0.0774, -0.0297, -0.0857],\n",
      "         [-0.0239,  0.0723, -0.0153,  ..., -0.0924, -0.0310, -0.0877]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2885,  0.1840, -0.0164,  ..., -0.1573,  0.1909, -0.0713],\n",
      "         [ 0.3583,  0.3595, -0.1902,  ...,  0.6382,  0.2131,  0.0183],\n",
      "         [ 0.7323,  0.5110, -0.0178,  ..., -0.1410,  0.6930, -0.4282],\n",
      "         ...,\n",
      "         [-0.0212,  0.0607, -0.0165,  ..., -0.0840, -0.0268, -0.0835],\n",
      "         [ 0.0069,  0.0692, -0.0048,  ..., -0.0705, -0.0129, -0.0966],\n",
      "         [-0.0083,  0.0939, -0.0228,  ..., -0.0954, -0.0395, -0.0790]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0013,  0.2853,  0.0663,  ..., -0.0904,  0.2096, -0.1298],\n",
      "         [ 0.3529,  0.6778, -0.0330,  ...,  0.1710,  0.5194,  0.3012],\n",
      "         [ 0.1636,  0.3572,  0.1043,  ...,  0.0458,  0.3994, -0.1161],\n",
      "         ...,\n",
      "         [-0.0074,  0.0476, -0.0205,  ..., -0.0807, -0.0299, -0.0859],\n",
      "         [-0.0148,  0.0862, -0.0161,  ..., -0.0815, -0.0309, -0.0910],\n",
      "         [-0.0203,  0.0629, -0.0182,  ..., -0.0753, -0.0239, -0.0875]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1801,  0.1646, -0.1169,  ..., -0.1172,  0.3311,  0.0755],\n",
      "         [ 0.0930,  0.0893, -0.1151,  ...,  0.4587, -0.1311, -0.0089],\n",
      "         [ 0.3139,  0.3505, -0.0808,  ..., -0.0928,  0.2637,  0.0059],\n",
      "         ...,\n",
      "         [ 0.1238,  0.0494, -0.0105,  ..., -0.0023,  0.0073, -0.2483],\n",
      "         [-0.0105,  0.0805, -0.0118,  ..., -0.0909, -0.0353, -0.0733],\n",
      "         [ 0.0193,  0.0593, -0.0253,  ..., -0.1510,  0.0232, -0.1206]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0275,  0.1555,  0.0610,  ...,  0.1671,  0.1288,  0.0165],\n",
      "         [ 0.7162,  0.1461,  0.1423,  ...,  0.7615,  0.5738, -0.2563],\n",
      "         [ 0.1604,  0.0114,  0.0100,  ...,  0.4150,  0.0960, -0.1498],\n",
      "         ...,\n",
      "         [-0.0009,  0.0728, -0.0334,  ..., -0.0592,  0.0057, -0.1290],\n",
      "         [-0.0131,  0.0593, -0.0181,  ..., -0.0789, -0.0303, -0.0724],\n",
      "         [-0.0143,  0.0597, -0.0255,  ..., -0.0872, -0.0317, -0.0849]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.1546e-01,  3.5434e-01, -7.9876e-02,  ...,  6.1187e-01,\n",
      "           8.3908e-01, -8.9060e-02],\n",
      "         [ 2.4267e-01,  4.2243e-01, -3.0087e-01,  ...,  5.4283e-01,\n",
      "           6.0168e-01, -1.0496e-01],\n",
      "         [ 2.0663e-01,  3.2892e-02, -3.0994e-01,  ...,  5.3640e-02,\n",
      "           9.1642e-01, -1.7069e-01],\n",
      "         ...,\n",
      "         [-1.6461e-02,  5.9860e-02, -2.2564e-02,  ..., -8.1611e-02,\n",
      "          -1.1911e-02,  1.5872e-02],\n",
      "         [-1.1389e-02,  6.9367e-02, -2.4664e-02,  ..., -9.0689e-02,\n",
      "          -3.7253e-02, -8.1754e-02],\n",
      "         [-4.1578e-04,  5.7804e-02, -1.2537e-02,  ..., -8.7643e-02,\n",
      "          -2.3458e-02, -8.1977e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0889, -0.1094, -0.0118,  ...,  0.0900,  0.3934, -0.0149],\n",
      "         [ 0.2503,  0.3252, -0.3659,  ...,  0.2250,  0.3034, -0.0524],\n",
      "         [ 0.2539, -0.0246, -0.0899,  ..., -0.1058,  0.1688, -0.0874],\n",
      "         ...,\n",
      "         [-0.0014,  0.1360, -0.0632,  ..., -0.0367, -0.1096, -0.2199],\n",
      "         [ 0.3530,  0.1304, -0.4383,  ...,  0.4945,  0.3159, -0.4706],\n",
      "         [-0.0054,  0.0634, -0.0165,  ..., -0.0827, -0.0399, -0.0713]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2027,  0.1380,  0.0156,  ..., -0.0528,  0.3623, -0.1249],\n",
      "         [ 0.1199, -0.0150, -0.1441,  ...,  1.1471,  0.3617, -0.1928],\n",
      "         [ 0.0211,  0.1427,  0.0591,  ...,  0.2555,  0.4332, -0.0212],\n",
      "         ...,\n",
      "         [ 0.0134,  0.1404,  0.0109,  ..., -0.0606, -0.0499, -0.1794],\n",
      "         [-0.0093,  0.0903, -0.0132,  ..., -0.0992, -0.0294, -0.0857],\n",
      "         [-0.0061,  0.0633, -0.0187,  ..., -0.0785, -0.0351, -0.0889]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3996,  0.1678,  0.0472,  ...,  0.2078,  0.4057, -0.0365],\n",
      "         [ 0.4752,  0.5028, -0.1151,  ...,  0.8557,  0.4394, -0.1954],\n",
      "         [ 0.4515, -0.1445,  0.0313,  ...,  0.2640,  0.4467, -0.4133],\n",
      "         ...,\n",
      "         [ 0.3431,  0.6137, -0.1151,  ...,  0.3930,  0.2079, -0.4170],\n",
      "         [ 0.0300,  0.0892, -0.0658,  ..., -0.1104, -0.0823, -0.0855],\n",
      "         [ 0.0013,  0.0484, -0.0207,  ..., -0.0803, -0.0276, -0.0765]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2985,  0.2822,  0.0458,  ...,  0.2456,  0.3599, -0.0796],\n",
      "         [ 0.4005,  0.4437, -0.0539,  ...,  0.7587,  0.4573, -0.0433],\n",
      "         [ 0.6696,  0.1394, -0.3559,  ...,  0.6324,  0.3685, -0.0620],\n",
      "         ...,\n",
      "         [-0.0105,  0.0574, -0.0258,  ..., -0.0777, -0.0366, -0.0774],\n",
      "         [ 0.0021,  0.0511, -0.0114,  ..., -0.0866, -0.0207, -0.0724],\n",
      "         [ 0.0607,  0.3294, -0.0468,  ...,  0.3551,  0.2283, -0.1918]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1093,  0.1787, -0.0085,  ...,  0.3339,  0.4532, -0.0225],\n",
      "         [ 0.2263,  0.6546,  0.0422,  ...,  0.3650,  0.0799, -0.3441],\n",
      "         [ 0.1671,  0.3387,  0.2565,  ..., -0.3855,  0.4450, -0.3760],\n",
      "         ...,\n",
      "         [-0.0376,  0.0631, -0.0865,  ..., -0.0994,  0.0105, -0.2205],\n",
      "         [ 0.0105,  0.0665, -0.0198,  ..., -0.0716, -0.0396, -0.0758],\n",
      "         [-0.0007,  0.0727, -0.0331,  ..., -0.0869, -0.0437, -0.1093]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.4225,  0.3332,  0.2126,  ...,  0.4765,  0.4471,  0.0838],\n",
      "         [ 0.5194,  0.3376,  0.2280,  ...,  0.4349,  0.4595, -0.2030],\n",
      "         [ 0.3891,  0.3231,  0.0514,  ...,  0.2178,  0.3575, -0.0947],\n",
      "         ...,\n",
      "         [-0.0138,  0.0372, -0.0232,  ..., -0.1130, -0.0278, -0.0801],\n",
      "         [-0.0051,  0.0591, -0.0041,  ..., -0.0844, -0.0104, -0.0745],\n",
      "         [-0.0103,  0.0629, -0.0331,  ..., -0.1286, -0.0398, -0.0958]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1482,  0.1358, -0.1100,  ...,  0.2970,  0.4281, -0.0730],\n",
      "         [ 0.1143,  0.2225,  0.2172,  ...,  0.4145,  0.3701, -0.0077],\n",
      "         [ 0.1705,  0.0473, -0.1973,  ...,  0.4140,  0.2683, -0.0671],\n",
      "         ...,\n",
      "         [ 0.0009,  0.0292, -0.0061,  ..., -0.0833, -0.0189, -0.0711],\n",
      "         [ 0.0893,  0.1077, -0.1411,  ...,  0.0437,  0.2046, -0.2814],\n",
      "         [-0.0171,  0.0573, -0.0157,  ..., -0.1120, -0.0316, -0.0858]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.3237e-01,  3.3195e-01, -2.3262e-01,  ...,  6.3729e-01,\n",
      "           8.1546e-01,  8.6537e-03],\n",
      "         [ 4.4221e-01,  3.1419e-01,  1.0762e-01,  ...,  9.3184e-01,\n",
      "           2.9321e-01, -4.6485e-01],\n",
      "         [ 3.2811e-01,  1.5544e-03, -1.0658e-01,  ...,  3.3616e-01,\n",
      "           3.8450e-01, -1.3092e-01],\n",
      "         ...,\n",
      "         [-4.9846e-04,  1.6829e-01,  5.3234e-02,  ..., -7.6255e-02,\n",
      "          -8.6027e-02, -2.3272e-01],\n",
      "         [-1.6340e-02,  5.6264e-02, -4.1231e-03,  ..., -8.7932e-02,\n",
      "          -3.1641e-02, -7.4495e-02],\n",
      "         [ 1.2323e-02,  1.6991e-01, -2.0284e-02,  ..., -5.4035e-02,\n",
      "          -9.8187e-02, -2.4192e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 3.6808e-02,  2.0989e-01,  1.8999e-01,  ...,  4.7196e-01,\n",
      "           5.1984e-01, -6.5450e-03],\n",
      "         [ 9.4752e-01,  7.0144e-01, -6.8537e-02,  ...,  8.9581e-01,\n",
      "           1.9470e-01,  4.4651e-02],\n",
      "         [ 6.9568e-01,  6.0463e-01, -1.4144e-01,  ...,  9.5591e-01,\n",
      "           6.5001e-02, -1.0484e-02],\n",
      "         ...,\n",
      "         [ 3.4987e-03,  1.0903e-01,  1.8482e-02,  ..., -4.4662e-02,\n",
      "          -1.5322e-02, -1.6086e-01],\n",
      "         [-1.6868e-02,  5.0335e-02, -2.2756e-02,  ..., -7.0283e-02,\n",
      "          -2.8921e-02, -8.1119e-02],\n",
      "         [ 2.4272e-05,  2.1875e-02, -7.3295e-02,  ..., -6.8671e-02,\n",
      "          -2.2072e-03, -1.9566e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1485,  0.0800,  0.3144,  ...,  0.4100,  0.4668, -0.1346],\n",
      "         [ 0.4202,  0.2041,  0.0530,  ...,  0.6909,  0.2546, -0.0306],\n",
      "         [ 0.4405,  0.2421, -0.2505,  ...,  0.4120,  0.4380,  0.1102],\n",
      "         ...,\n",
      "         [ 0.0015,  0.0540, -0.0413,  ..., -0.1456,  0.0079, -0.1414],\n",
      "         [-0.0236,  0.0346, -0.0288,  ..., -0.0847, -0.0230, -0.0810],\n",
      "         [-0.0013,  0.0523, -0.0463,  ..., -0.1052, -0.0131, -0.0872]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7124,  0.1609,  0.0409,  ...,  0.2834,  0.6865, -0.1944],\n",
      "         [ 0.8703,  0.0687, -0.1253,  ...,  0.4673,  0.7598, -0.4900],\n",
      "         [ 0.3712,  0.1226, -0.1910,  ...,  0.2330,  0.9214, -0.1530],\n",
      "         ...,\n",
      "         [-0.0099,  0.0600, -0.0015,  ..., -0.1010, -0.0270, -0.0592],\n",
      "         [ 0.0263,  0.0864,  0.0033,  ..., -0.0891, -0.0349, -0.0528],\n",
      "         [-0.0147,  0.0590, -0.0138,  ..., -0.0810, -0.0228, -0.0838]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7212,  0.2070,  0.2456,  ...,  0.6506,  0.9810, -0.1611],\n",
      "         [ 0.5825,  0.1904, -0.2025,  ...,  0.9764,  1.2217,  0.5940],\n",
      "         [ 0.7091,  0.1192, -0.1288,  ...,  0.5235,  1.0848,  0.0036],\n",
      "         ...,\n",
      "         [ 0.0187,  0.0413, -0.0523,  ..., -0.1302, -0.0061, -0.0862],\n",
      "         [-0.0087,  0.0627, -0.0141,  ..., -0.0792, -0.0279, -0.0895],\n",
      "         [-0.0115,  0.0616, -0.0231,  ..., -0.0821, -0.0280, -0.0835]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3635,  0.0809, -0.2996,  ...,  0.6003,  0.3387, -0.2840],\n",
      "         [ 0.4749,  0.1987, -0.2703,  ...,  0.6546,  0.4914,  0.2017],\n",
      "         [ 0.7486, -0.0215, -0.2878,  ...,  0.4851,  0.5309, -0.3595],\n",
      "         ...,\n",
      "         [ 0.3053,  0.3604, -0.0545,  ...,  0.0396, -0.0466, -0.1809],\n",
      "         [ 0.0064,  0.0609, -0.0308,  ..., -0.0732, -0.0291, -0.1023],\n",
      "         [-0.1756,  0.1363, -0.1067,  ..., -0.1982, -0.0402, -0.2367]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4595,  0.5205, -0.3272,  ...,  0.9209,  0.8570, -0.4714],\n",
      "         [ 0.4248,  0.0278, -0.1670,  ...,  0.9262,  0.5366, -0.2137],\n",
      "         [ 0.3988,  0.2128, -0.3455,  ..., -0.0648,  0.7784, -0.4588],\n",
      "         ...,\n",
      "         [-0.0193,  0.0635, -0.0218,  ..., -0.0807, -0.0285, -0.0706],\n",
      "         [-0.1321,  0.4981, -0.0816,  ..., -0.0439,  0.0794,  0.0459],\n",
      "         [-0.0102,  0.0618, -0.0214,  ..., -0.0856, -0.0243, -0.0760]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3223,  0.3262, -0.1119,  ...,  0.7353,  0.6767,  0.0408],\n",
      "         [ 0.8236,  0.1266,  0.3743,  ...,  0.4190,  0.6839, -0.1883],\n",
      "         [ 0.8784,  0.1329, -0.2625,  ...,  0.6649,  0.7941, -0.3026],\n",
      "         ...,\n",
      "         [-0.0107,  0.0676,  0.0048,  ..., -0.0655, -0.0346, -0.0912],\n",
      "         [-0.0110,  0.0538, -0.0171,  ..., -0.0764, -0.0305, -0.0834],\n",
      "         [-0.0354,  0.2611,  0.0666,  ..., -0.0810, -0.0534, -0.4046]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.9786e-01,  3.2524e-01, -2.7544e-01,  ...,  6.2275e-01,\n",
      "           7.1669e-01, -4.0266e-01],\n",
      "         [ 2.5962e-01,  4.4438e-01, -2.5457e-02,  ...,  1.0381e+00,\n",
      "           5.4265e-01, -4.0101e-01],\n",
      "         [ 8.4151e-01,  2.8516e-01, -2.6327e-01,  ...,  6.5405e-01,\n",
      "           2.9383e-01, -2.8287e-01],\n",
      "         ...,\n",
      "         [-1.0700e-02,  5.7043e-02, -2.3577e-02,  ..., -7.7103e-02,\n",
      "          -3.1115e-02, -8.1774e-02],\n",
      "         [-3.7424e-02,  1.6488e-01, -3.9811e-02,  ..., -1.3071e-01,\n",
      "          -4.3918e-02, -2.0236e-01],\n",
      "         [ 9.5065e-04,  8.2619e-02, -1.8138e-02,  ..., -8.8996e-02,\n",
      "          -3.2040e-02, -7.2272e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7341,  0.0821, -0.3529,  ...,  0.8505,  0.8398, -0.0171],\n",
      "         [ 0.5958, -0.0447, -0.2445,  ...,  0.3449,  0.8304,  0.0964],\n",
      "         [ 0.7109, -0.0448, -0.1632,  ...,  0.2453,  0.6289, -0.3169],\n",
      "         ...,\n",
      "         [-0.0480,  0.1648,  0.0683,  ..., -0.0321, -0.0442, -0.1186],\n",
      "         [-0.0639,  0.3008,  0.2142,  ...,  0.0335, -0.1097, -0.0905],\n",
      "         [ 0.0126,  0.0456, -0.0089,  ..., -0.1171, -0.0217, -0.0845]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.0545e-01,  3.0312e-01, -6.2641e-02,  ..., -2.6545e-02,\n",
      "           6.9023e-01, -4.9769e-03],\n",
      "         [ 5.5384e-01,  4.5371e-01, -3.0617e-02,  ...,  6.1530e-01,\n",
      "           8.7786e-01,  4.3574e-01],\n",
      "         [ 1.0436e+00,  5.9946e-01, -1.6769e-01,  ...,  1.0402e-01,\n",
      "           5.7099e-01, -5.3694e-01],\n",
      "         ...,\n",
      "         [-1.7521e-02,  7.2456e-02, -5.8297e-03,  ..., -7.6879e-02,\n",
      "          -3.8750e-02, -9.5630e-02],\n",
      "         [ 1.3690e-01,  2.4721e-01,  6.8969e-02,  ..., -1.4203e-01,\n",
      "           7.0204e-04, -3.8586e-01],\n",
      "         [-2.0030e-02,  3.2397e-02, -1.2618e-02,  ..., -1.2206e-01,\n",
      "          -1.1636e-02, -8.6054e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6917,  0.5398, -0.3435,  ...,  0.3334,  0.5039, -0.0748],\n",
      "         [ 0.5295, -0.0072, -0.6066,  ...,  0.3292,  0.3692, -0.0449],\n",
      "         [-0.0314, -0.3859, -0.1306,  ...,  0.2851,  0.6345, -0.0181],\n",
      "         ...,\n",
      "         [ 0.0553,  0.2705,  0.0840,  ..., -0.0336, -0.0702, -0.2894],\n",
      "         [-0.0223,  0.0605, -0.0222,  ..., -0.0880, -0.0221, -0.0799],\n",
      "         [-0.0127,  0.0931, -0.0178,  ..., -0.0745, -0.0272, -0.0848]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0048,  0.2223,  0.2800,  ...,  1.0965,  0.9936, -0.1188],\n",
      "         [ 0.6232,  0.3993, -0.2823,  ...,  0.8489,  0.6691, -0.2686],\n",
      "         [ 0.9424,  0.0930,  0.0453,  ..., -0.1757,  0.6387, -0.3387],\n",
      "         ...,\n",
      "         [ 0.0345,  0.0889, -0.0562,  ..., -0.0879, -0.0215, -0.1139],\n",
      "         [ 0.0096,  0.0420, -0.0100,  ..., -0.0889, -0.0219, -0.0732],\n",
      "         [ 0.0185,  0.1030,  0.0189,  ..., -0.0262, -0.0986, -0.2068]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.0223,  0.3396, -0.1347,  ...,  0.5082,  0.8095, -0.4008],\n",
      "         [ 0.6568,  0.3263, -0.1988,  ...,  0.5111,  0.8512, -0.3325],\n",
      "         [ 0.6520,  0.4192, -0.3758,  ...,  0.1031,  0.5662, -0.3194],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.5625, 0.9736, 0.6450,  ..., 0.9556, 0.9062, 1.1201], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.2179, -0.2306,  0.1387,  ..., -0.0881, -0.1506, -0.1293],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8477,  0.2139, -0.2595,  ...,  0.7509,  0.8763, -0.3711],\n",
      "         [ 0.7618,  0.2192, -0.2317,  ...,  0.9280,  0.6420, -0.2407],\n",
      "         [ 0.6398, -0.2689, -0.2806,  ...,  0.1074,  1.0586, -0.3097],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 7.5732e-01,  1.2471e+00,  8.9453e-01,  1.0146e+00,  8.9795e-01,\n",
      "         6.4111e-01,  5.2051e-01,  5.9375e-01,  6.3281e-01,  7.8906e-01,\n",
      "         9.2578e-01,  5.7959e-01,  1.0078e+00,  6.7236e-01,  6.4453e-01,\n",
      "         8.4229e-01,  7.9492e-01,  7.7197e-01,  9.5459e-01,  4.4580e-01,\n",
      "         8.3643e-01,  4.2456e-01,  8.7695e-01,  3.8306e-01,  8.0762e-01,\n",
      "         9.9414e-01,  6.6260e-01,  7.1094e-01,  2.6758e-01,  1.8051e-02,\n",
      "         4.5557e-01,  1.2275e+00,  1.4197e-01,  4.8535e-01,  6.0742e-01,\n",
      "         4.1675e-01,  6.1670e-01,  4.4287e-01,  5.9521e-01,  6.9531e-01,\n",
      "         4.4238e-01,  1.0703e+00,  9.6240e-01,  9.5996e-01,  8.8574e-01,\n",
      "         7.9590e-01,  3.9062e-01,  7.6611e-01,  8.3887e-01,  8.4375e-01,\n",
      "         6.7676e-01,  3.3740e-01,  4.8926e-01,  1.9897e-01,  4.7534e-01,\n",
      "         7.4170e-01,  4.3091e-01,  9.1699e-01,  5.4688e-01,  6.2646e-01,\n",
      "         4.9902e-01,  7.0947e-01,  4.5020e-01,  1.1406e+00,  1.0195e+00,\n",
      "         9.9609e-01,  7.5879e-01,  7.5488e-01,  7.4658e-01,  7.7588e-01,\n",
      "         8.9258e-01,  7.3096e-01,  6.5039e-01,  7.0801e-01,  6.0156e-01,\n",
      "         1.0557e+00,  6.0596e-01,  7.8418e-01,  6.9141e-01,  6.1426e-01,\n",
      "         6.5381e-01,  6.0547e-01,  1.1533e+00,  6.7920e-01,  9.3945e-01,\n",
      "         8.7012e-01,  7.5879e-01,  2.6611e-01,  7.5049e-01,  9.0088e-01,\n",
      "         7.3828e-01,  5.1025e-01,  8.9307e-01,  8.5498e-01,  6.3721e-01,\n",
      "         4.2554e-01,  9.5947e-01,  6.1035e-01,  1.1045e+00,  8.9014e-01,\n",
      "         6.1865e-01,  6.6064e-01,  9.5264e-01,  5.0391e-01,  7.4170e-01,\n",
      "         8.9600e-01,  1.0264e+00,  6.9336e-01,  9.1260e-01,  7.5391e-01,\n",
      "         8.5400e-01,  6.5430e-01,  4.8584e-02,  1.1017e-01,  2.4695e-01,\n",
      "         5.9668e-01,  3.9453e-01,  6.8994e-01,  7.4023e-01,  9.2432e-01,\n",
      "         1.4404e-01,  5.6641e-01,  5.1514e-01,  5.0928e-01,  6.3623e-01,\n",
      "         4.4556e-01,  6.6992e-01,  6.2646e-01,  9.5557e-01,  8.0811e-01,\n",
      "         5.2148e-01,  1.1006e+00,  1.1328e+00,  6.3379e-01,  5.6738e-01,\n",
      "         4.2432e-01,  8.3740e-01,  7.7148e-01,  4.8535e-01,  6.8018e-01,\n",
      "         8.6279e-01,  7.6270e-01,  9.1846e-01,  4.6655e-01,  6.0010e-01,\n",
      "         7.9834e-01,  6.4941e-01,  9.2773e-01,  8.6914e-01,  1.1133e+00,\n",
      "         7.5195e-01,  1.0195e+00,  6.2451e-01,  4.1675e-01,  6.3525e-01,\n",
      "         5.0781e-01,  1.3232e+00,  1.0068e+00,  6.8945e-01,  6.7188e-01,\n",
      "         8.8013e-02,  7.9248e-01,  7.1973e-01,  7.5879e-01,  1.1465e+00,\n",
      "         8.5840e-01,  6.5967e-01,  1.2461e+00,  9.7754e-01,  6.2354e-01,\n",
      "         7.4951e-01,  1.0732e+00,  6.7432e-01,  5.1611e-01,  9.5801e-01,\n",
      "         8.1396e-01,  8.2617e-01,  4.6387e-01,  1.0020e+00,  8.9844e-01,\n",
      "         9.4287e-01,  1.1484e+00,  7.7881e-01,  7.5879e-01,  1.0127e+00,\n",
      "         1.1055e+00,  8.9990e-01,  5.3760e-01,  8.8574e-01,  6.8701e-01,\n",
      "         9.3311e-01,  4.9951e-01,  8.0420e-01,  6.4795e-01,  8.4082e-01,\n",
      "         5.5713e-01,  3.5059e-01,  7.6123e-01,  7.9443e-01,  3.7500e-01,\n",
      "         3.3844e-02,  3.7476e-01,  9.7949e-01,  6.5430e-01,  9.2188e-01,\n",
      "         5.8740e-01,  6.7529e-01,  7.9980e-01,  1.0654e+00,  5.2686e-01,\n",
      "         9.1895e-01,  3.8623e-01,  7.2656e-01,  9.7852e-01,  1.0693e+00,\n",
      "         8.9746e-01,  1.0205e+00,  1.0264e+00,  9.5605e-01,  1.0068e+00,\n",
      "         6.2842e-01,  8.1494e-01,  1.1094e+00,  5.0830e-01,  9.7363e-01,\n",
      "         6.8848e-01,  1.2683e-01,  5.8984e-01,  9.8145e-01,  7.1777e-01,\n",
      "         5.0879e-01,  5.3760e-01,  8.3594e-01,  5.8057e-01,  6.2891e-01,\n",
      "         5.7324e-01,  9.1895e-01,  4.5605e-01,  7.8271e-01,  6.7236e-01,\n",
      "         4.7241e-01,  8.6768e-01,  3.8477e-01,  6.5771e-01,  4.9756e-01,\n",
      "         4.9609e-01,  8.1006e-01,  4.6509e-01,  8.8721e-01,  4.5630e-01,\n",
      "         1.0254e+00,  3.9941e-01,  5.9326e-01,  1.3086e+00,  1.3213e+00,\n",
      "         1.0469e+00,  9.7754e-01,  8.8525e-01,  1.0459e+00,  4.2944e-01,\n",
      "         1.1133e+00,  1.1211e+00,  7.8271e-01,  7.2119e-01,  4.2749e-01,\n",
      "         9.7266e-01,  1.1973e+00,  1.0273e+00,  7.8662e-01,  6.2207e-01,\n",
      "         5.6299e-01,  7.6172e-01,  4.7534e-01,  8.0225e-01,  6.3916e-01,\n",
      "         1.0527e+00,  7.1436e-01,  4.6509e-01,  6.8604e-01,  6.4746e-01,\n",
      "         8.3105e-01,  4.6436e-01,  7.1680e-01,  5.3760e-01,  8.3057e-01,\n",
      "         1.0615e+00,  5.5420e-01,  5.3516e-01,  7.8955e-01,  4.1406e-01,\n",
      "         3.2532e-02,  5.9180e-01,  3.2397e-01,  7.6562e-01,  2.5244e-01,\n",
      "         3.9160e-01,  6.2744e-01,  5.0244e-01,  3.0981e-01,  6.5527e-01,\n",
      "         3.3887e-01,  3.5596e-01,  5.0391e-01,  5.9766e-01,  7.1973e-01,\n",
      "         3.6621e-01,  3.6890e-01,  4.1846e-01,  1.0211e-01,  7.1826e-01,\n",
      "         1.6516e-01,  5.0879e-01,  4.8779e-01,  4.1577e-01,  8.3008e-01,\n",
      "         4.1309e-01,  7.9834e-01,  5.2490e-01,  7.4463e-01,  5.6689e-01,\n",
      "         1.1221e+00,  7.6904e-01,  1.0352e+00,  9.2480e-01,  9.5654e-01,\n",
      "         5.2881e-01,  6.4404e-01,  3.2397e-01,  9.6777e-01,  5.9570e-01,\n",
      "         1.1152e+00,  1.0039e+00,  1.0957e+00,  7.2266e-01,  6.7383e-01,\n",
      "         7.4121e-01,  7.9248e-01,  8.0420e-01,  5.2051e-01,  7.8955e-01,\n",
      "         3.3911e-01,  6.3770e-01,  5.2686e-01,  7.5049e-01,  5.7812e-01,\n",
      "         6.4502e-01,  5.5859e-01,  5.4492e-01,  7.4512e-01,  1.0049e+00,\n",
      "         6.5137e-01,  8.9746e-01,  4.0869e-01,  8.1104e-01,  9.6045e-01,\n",
      "         9.5850e-01,  9.8730e-01,  7.0068e-01,  9.2529e-01,  6.9531e-01,\n",
      "         5.2686e-01,  9.2432e-01,  9.7949e-01,  3.3716e-01,  7.9492e-01,\n",
      "         4.9121e-01,  9.9414e-01,  9.4824e-01,  1.0264e+00,  8.5400e-01,\n",
      "         5.5859e-01,  3.6670e-01,  9.9609e-01,  1.1318e+00,  9.7363e-01,\n",
      "         1.0234e+00,  1.1592e+00,  5.1660e-01,  7.3242e-01,  1.0811e+00,\n",
      "         3.0591e-01,  7.8564e-01,  2.7295e-01,  8.7158e-01,  3.5742e-01,\n",
      "         8.5400e-01,  3.1201e-01,  4.1553e-01,  2.1655e-01,  6.4307e-01,\n",
      "         4.3530e-01,  3.1226e-01,  8.4717e-01,  2.9712e-01,  5.0598e-02,\n",
      "         9.3164e-01,  2.5049e-01,  4.0918e-01,  8.2422e-01,  4.6533e-01,\n",
      "         3.0542e-01,  7.6221e-01,  5.7959e-01,  3.0563e-02,  1.0195e+00,\n",
      "         9.3799e-01,  7.0117e-01,  9.1602e-01,  4.3750e-01,  8.8818e-01,\n",
      "         7.0264e-01,  1.6516e-01,  5.3809e-01,  2.6782e-01,  7.7881e-01,\n",
      "         9.1309e-01,  4.3311e-01,  9.2725e-01,  3.9551e-02,  5.0293e-01,\n",
      "         5.0879e-01,  2.9712e-01,  4.7754e-01,  7.1533e-01,  9.4287e-01,\n",
      "         6.8896e-01,  9.1064e-01,  5.4883e-01,  7.1777e-01,  9.0137e-01,\n",
      "         5.7764e-01,  7.5293e-01,  3.4253e-01,  6.4258e-01,  6.9922e-01,\n",
      "         6.7676e-01,  8.5938e-01,  6.8115e-01,  6.6895e-01,  8.0908e-01,\n",
      "         1.0098e+00,  4.6387e-01,  6.5723e-01,  2.9492e-01,  9.1504e-01,\n",
      "         9.0820e-01,  6.4209e-01,  7.1436e-01,  7.8271e-01,  8.1641e-01,\n",
      "         5.9814e-01,  9.6387e-01,  1.1078e-01,  4.1699e-01, -1.4699e-04,\n",
      "         5.5957e-01,  6.3428e-01,  5.6885e-02,  7.9297e-01,  5.1367e-01,\n",
      "         6.8408e-01,  9.5215e-01,  6.4941e-01,  5.4248e-01,  8.0469e-01,\n",
      "         3.8647e-01,  9.3457e-01,  8.0420e-01,  1.0020e+00,  8.3691e-01,\n",
      "         7.2217e-01,  5.2637e-01,  6.9336e-01,  9.7852e-01,  4.2847e-01,\n",
      "         7.6221e-01,  1.0869e+00,  6.4453e-01,  6.2842e-01,  8.3154e-01,\n",
      "         3.6035e-01,  5.5469e-01,  8.6084e-01,  4.8975e-01,  6.3184e-01,\n",
      "         4.7119e-01,  6.4453e-01,  6.1377e-01,  5.8887e-01,  3.7695e-01,\n",
      "         6.1914e-01,  3.1885e-01,  4.8145e-01,  6.1621e-01,  4.4189e-01,\n",
      "         8.6377e-01,  5.3271e-01,  8.7646e-01,  6.2158e-01,  9.6533e-01,\n",
      "         3.6914e-01, -1.4771e-01,  2.0740e-01,  4.3140e-01,  2.7661e-01,\n",
      "         4.4727e-01,  1.9385e-01,  3.7476e-01,  4.4922e-01,  6.5918e-01,\n",
      "         7.3193e-01,  8.3984e-01,  7.1045e-01,  1.2344e+00,  1.0186e+00,\n",
      "         9.1602e-01,  9.0039e-01,  8.0664e-01,  9.6143e-01,  6.6650e-01,\n",
      "         7.9736e-01,  9.3750e-01,  8.8525e-01,  4.5435e-01,  6.3672e-01,\n",
      "         9.2920e-01,  6.0400e-01,  8.6133e-01,  6.0791e-01,  8.0859e-01,\n",
      "         4.7632e-01,  7.0703e-01,  4.8242e-01,  9.3262e-01,  8.7109e-01,\n",
      "         8.2471e-01,  4.8730e-01,  6.4258e-01,  6.7969e-01,  3.1128e-01,\n",
      "         1.9067e-01,  6.7871e-01,  4.8438e-01,  8.0566e-01,  5.7080e-01,\n",
      "         5.9326e-01,  5.9131e-01,  9.7559e-01,  2.6343e-01,  5.7275e-01,\n",
      "         9.2432e-01,  4.2505e-01,  8.6572e-01,  5.1904e-01,  6.5918e-01,\n",
      "         4.9756e-01,  8.3057e-01,  7.4951e-01,  8.6182e-01,  5.6006e-01,\n",
      "         5.6006e-01,  6.8506e-01,  8.2861e-01,  7.5391e-01,  4.3384e-01,\n",
      "         6.1572e-01,  6.0400e-01,  7.8418e-01,  5.7715e-01,  1.6492e-01,\n",
      "         7.5928e-01,  6.3770e-01,  7.4072e-01,  4.0332e-01,  7.5244e-01,\n",
      "         9.0283e-01,  4.7119e-01,  4.7900e-01,  6.3916e-01,  1.0928e+00,\n",
      "         6.0889e-01,  8.0518e-01,  7.0166e-01,  8.1885e-01,  6.1133e-01,\n",
      "         7.9834e-01,  4.7339e-01,  6.6553e-01,  8.8330e-01,  6.9580e-01,\n",
      "         7.7148e-01,  5.6982e-01,  9.7656e-01,  7.1240e-01,  8.2422e-01,\n",
      "         7.6758e-01,  9.6875e-01,  7.2705e-01,  6.9482e-01,  1.0840e+00,\n",
      "         8.6963e-01,  9.7559e-01,  6.7188e-01,  7.5830e-01,  7.1777e-01,\n",
      "         7.7832e-01,  6.8164e-01,  7.1338e-01,  9.4971e-01,  7.9297e-01,\n",
      "         5.2100e-01,  6.4844e-01,  7.7734e-01,  1.0938e+00,  4.4946e-01,\n",
      "         7.4414e-01,  8.6328e-01,  7.4463e-01,  4.4727e-01,  5.1953e-01,\n",
      "         1.0459e+00,  8.4521e-01,  8.0078e-01,  5.9082e-01,  9.4873e-01,\n",
      "         8.3740e-01,  7.6318e-01,  7.1777e-01,  9.7119e-01,  8.3301e-01,\n",
      "         1.1934e+00,  1.1240e+00,  8.6182e-01,  4.0283e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([-5.6686e-03, -5.6494e-01, -5.5713e-01, -2.3279e-01, -9.1748e-01,\n",
      "        -5.2051e-01, -6.1377e-01, -2.9956e-01, -5.3906e-01, -4.5361e-01,\n",
      "        -2.3450e-01, -6.6589e-02,  1.5588e-01, -2.7124e-01, -1.9775e-01,\n",
      "         1.7715e-02, -3.0444e-01,  1.0040e-01, -1.9507e-01,  2.0374e-01,\n",
      "        -6.6406e-02,  6.6162e-02,  3.1641e-01, -2.7197e-01, -9.1309e-02,\n",
      "        -6.4453e-02, -3.3966e-02, -4.1699e-01,  3.2251e-01,  3.9722e-01,\n",
      "        -9.4666e-02,  1.5149e-01,  1.4368e-01, -3.5913e-01, -1.7731e-02,\n",
      "         3.2013e-02,  2.4573e-01,  1.6785e-01,  2.7847e-02,  1.7896e-01,\n",
      "         2.5098e-01,  3.9038e-01,  1.3928e-01, -3.9886e-02,  5.7434e-02,\n",
      "        -1.2720e-01, -2.7271e-01,  1.1456e-01, -2.5757e-02, -2.0905e-02,\n",
      "        -1.4343e-01, -2.5781e-01, -2.7466e-02,  1.6919e-01, -3.4814e-01,\n",
      "         1.4514e-01,  1.8005e-01, -3.6530e-02,  2.9443e-01, -1.7914e-02,\n",
      "        -1.7078e-01, -2.8503e-02,  1.7749e-01,  1.2830e-01,  3.4882e-02,\n",
      "        -1.9287e-01,  3.0542e-01,  1.0175e-01,  3.5278e-02, -3.9038e-01,\n",
      "         1.3330e-01, -1.9617e-01, -2.9240e-03,  1.5259e-01, -1.1711e-02,\n",
      "        -1.3647e-01,  2.5586e-01,  2.9761e-01, -1.4551e-01, -5.4657e-02,\n",
      "         9.1125e-02,  2.8717e-02, -3.9111e-01, -6.2469e-02,  1.9006e-01,\n",
      "         2.2229e-01,  3.5229e-01, -3.6475e-01,  2.3511e-01,  1.1665e-02,\n",
      "        -2.3376e-01,  1.6650e-01, -2.3743e-01, -5.4004e-01,  7.6599e-02,\n",
      "        -1.1261e-01, -2.5439e-01,  2.0660e-02, -4.5807e-02, -1.3647e-01,\n",
      "         8.0688e-02, -3.1299e-01,  2.4146e-01, -1.2657e-02, -1.6431e-01,\n",
      "         1.2769e-01, -1.3452e-01, -8.9233e-02,  8.1543e-02,  1.6016e-01,\n",
      "         3.3844e-02, -2.9785e-01, -6.1963e-01, -5.4245e-03, -1.6345e-01,\n",
      "         9.8999e-02, -2.2034e-02, -1.4519e-02, -8.7952e-02, -2.6880e-01,\n",
      "         7.0251e-02, -1.5747e-01, -2.9736e-01, -6.9336e-01,  7.3862e-04,\n",
      "        -2.8125e-01,  1.0431e-01,  1.3220e-01,  3.4180e-02,  7.8735e-02,\n",
      "        -2.5854e-01, -6.4209e-01, -1.3098e-01, -3.0908e-01, -3.1641e-01,\n",
      "        -2.3718e-01, -4.1870e-01, -1.8164e-01, -2.0172e-02, -2.4792e-01,\n",
      "         4.4937e-03,  4.3106e-03,  9.0210e-02, -5.5084e-02, -1.9836e-01,\n",
      "        -8.7036e-02,  6.0516e-02,  2.5620e-02,  6.0303e-02, -8.1482e-02,\n",
      "         7.6523e-03,  1.3135e-01, -1.9226e-02, -1.9897e-01,  1.0443e-01,\n",
      "        -1.3220e-01,  4.4647e-02, -2.2107e-01, -3.8013e-01, -2.2742e-01,\n",
      "        -1.8579e-01, -2.8534e-02, -5.4779e-02, -3.7354e-01, -5.5878e-02,\n",
      "        -1.1700e-01, -6.0010e-01, -7.1777e-01, -2.2839e-01,  2.6566e-02,\n",
      "        -3.5205e-01,  1.9263e-01, -9.6863e-02,  1.5747e-01, -1.5295e-01,\n",
      "        -1.1803e-02,  7.3425e-02,  9.4788e-02,  1.7834e-01,  1.1023e-01,\n",
      "        -2.7686e-01,  1.6626e-01,  2.4353e-01,  5.9021e-02, -3.2617e-01,\n",
      "        -3.6914e-01, -2.2571e-01, -2.1606e-02,  1.7212e-01, -3.7817e-01,\n",
      "        -6.0089e-02, -1.0887e-02,  2.0215e-01, -2.8906e-01,  1.1806e-03,\n",
      "         2.7435e-02, -2.4414e-02, -3.9014e-01, -2.5342e-01, -1.7102e-01,\n",
      "         2.7637e-01, -2.3108e-01, -6.1859e-02, -9.6359e-03,  1.2976e-01,\n",
      "        -2.8906e-01, -7.1582e-01,  6.5247e-02,  2.0068e-01,  3.1006e-02,\n",
      "         3.6072e-02, -2.7954e-01,  3.1006e-02, -6.2939e-01, -4.7168e-01,\n",
      "        -9.4299e-02, -1.9165e-01,  8.5266e-02, -8.1848e-02, -8.2764e-01,\n",
      "         3.5370e-02, -1.5417e-01,  2.7603e-02,  1.7910e-03, -1.4734e-01,\n",
      "        -1.1261e-01, -1.4648e-01,  7.2556e-03, -1.7041e-01, -3.6646e-01,\n",
      "        -1.1224e-01, -2.4536e-02,  1.4062e-01, -2.1582e-01, -3.2715e-01,\n",
      "        -1.7090e-02, -7.3047e-01,  7.9346e-02, -2.1252e-01,  1.6052e-02,\n",
      "         1.9363e-02, -7.1594e-02,  5.9052e-02,  9.2834e-02, -3.7479e-03,\n",
      "        -2.6855e-01, -5.1483e-02, -2.0599e-02,  1.2805e-01, -2.0740e-01,\n",
      "        -5.4541e-01,  1.8445e-01, -1.6504e-01, -2.7930e-01, -4.9561e-01,\n",
      "        -2.7979e-01, -8.4473e-01, -6.4209e-02,  3.2013e-02, -1.1845e-03,\n",
      "         1.1224e-01, -3.1372e-01, -3.7354e-01, -3.7201e-02, -1.9885e-01,\n",
      "        -1.4294e-01, -3.0664e-01, -2.6099e-01,  1.3405e-02,  3.4851e-02,\n",
      "        -1.9714e-02, -1.0260e-01,  1.4900e-02,  1.9214e-01, -2.6611e-01,\n",
      "        -1.4551e-01, -9.0698e-02,  4.7852e-02, -4.1656e-02, -1.2842e-01,\n",
      "        -2.9248e-01, -1.2781e-01,  1.2128e-01,  9.5139e-03, -5.6992e-03,\n",
      "        -5.3076e-01, -3.9600e-01, -1.0797e-01, -2.9816e-02, -1.4160e-01,\n",
      "        -3.6060e-01,  5.2872e-03, -3.4821e-02,  7.7209e-02, -3.9355e-01,\n",
      "        -1.0944e-01,  9.1431e-02, -2.7026e-01, -7.6416e-02, -3.0518e-02,\n",
      "        -1.5027e-01,  1.9617e-01, -2.5732e-01, -1.4286e-03, -4.5142e-01,\n",
      "         5.7526e-02, -3.3667e-01,  9.8511e-02,  7.8491e-02, -7.4829e-02,\n",
      "         7.5134e-02, -3.6963e-01,  2.8906e-01, -4.7333e-02, -6.7139e-01,\n",
      "         1.8347e-01, -6.2286e-02,  2.4948e-02,  1.6223e-01, -1.5967e-01,\n",
      "        -2.3035e-01, -3.3966e-02,  1.6785e-01,  8.7524e-02,  4.5068e-01,\n",
      "        -6.1670e-01,  5.7678e-02,  1.9971e-01, -2.0416e-02,  4.9866e-02,\n",
      "         1.9128e-01, -5.7037e-02,  2.5122e-01, -1.3084e-02, -2.7161e-02,\n",
      "         1.7139e-01, -2.2717e-01,  1.2158e-01,  8.3496e-02,  1.4758e-01,\n",
      "        -1.2915e-01, -2.9388e-02,  9.2041e-02,  1.9324e-01,  2.8439e-03,\n",
      "         1.9385e-01,  1.8054e-01, -2.7148e-01, -6.4514e-02,  1.9556e-01,\n",
      "        -1.2360e-01, -4.6631e-01,  1.8335e-01, -3.2690e-01, -4.8340e-01,\n",
      "        -1.6699e-01, -4.2480e-01, -6.9580e-01, -7.9834e-02, -2.2131e-01,\n",
      "        -4.0918e-01, -4.8926e-01, -7.4036e-02, -4.8535e-01, -3.3472e-01,\n",
      "        -3.3667e-01, -3.0591e-01, -1.3782e-01, -4.4580e-01, -6.0400e-01,\n",
      "        -1.8042e-01, -5.4932e-01, -7.2266e-01, -3.6646e-01, -2.9590e-01,\n",
      "        -5.3955e-01, -4.2993e-01, -2.1460e-01,  2.5781e-01,  3.9520e-02,\n",
      "        -3.1226e-01, -1.0760e-01, -4.2633e-02,  1.2451e-01, -3.1860e-01,\n",
      "        -3.9291e-03, -1.1375e-02, -1.1497e-02, -8.3203e-01, -1.6382e-01,\n",
      "        -4.1089e-01, -4.4971e-01, -4.5654e-01,  6.4636e-02,  1.8494e-01,\n",
      "        -1.4030e-02, -5.8398e-01,  1.2012e-01, -5.5029e-01, -1.0651e-01,\n",
      "         4.1260e-01,  1.2398e-02, -3.2910e-01, -2.0203e-01, -3.4399e-01,\n",
      "        -3.4790e-01, -7.8186e-02, -9.7595e-02, -4.5923e-01, -3.6133e-01,\n",
      "        -6.2042e-02,  1.9287e-01, -1.8689e-01, -2.1533e-01, -9.6130e-02,\n",
      "        -2.6514e-01, -5.3760e-01, -1.0223e-01,  1.7664e-01,  3.4821e-02,\n",
      "         1.1725e-01, -1.9666e-01, -2.4597e-01, -3.9648e-01, -2.7661e-01,\n",
      "        -3.4521e-01,  2.2644e-01, -1.5564e-01, -1.2927e-01,  2.5464e-01,\n",
      "        -5.8258e-02,  2.0142e-01, -9.3506e-02, -1.0577e-01, -1.0663e-01,\n",
      "        -6.4551e-01, -5.4047e-02, -2.3474e-01, -4.3671e-02,  2.4048e-01,\n",
      "         2.0276e-01,  2.3376e-02, -1.6394e-01, -2.6929e-01,  2.6392e-01,\n",
      "        -2.7051e-01,  4.1168e-02,  2.3804e-01,  1.7676e-01,  5.1416e-01,\n",
      "        -5.0098e-01, -3.0029e-01, -4.7559e-01,  4.3854e-02,  1.5613e-01,\n",
      "         5.9174e-02,  1.7798e-01, -3.2074e-02, -7.4997e-03,  1.5295e-01,\n",
      "        -2.1454e-02,  2.8589e-01,  3.1677e-02,  4.6356e-02, -4.1724e-01,\n",
      "         1.7114e-01, -3.0396e-01,  5.2948e-02, -2.1851e-01,  1.9543e-01,\n",
      "        -8.5840e-01, -2.1985e-01, -4.1962e-02,  1.6370e-01,  4.9561e-02,\n",
      "         1.6388e-02, -2.8052e-01, -3.6163e-02, -4.0112e-01,  3.0322e-01,\n",
      "        -9.6375e-02, -7.9102e-02,  2.0850e-01, -2.3523e-01, -1.0460e-02,\n",
      "         3.3081e-02,  5.7129e-02, -1.4929e-01,  1.6858e-01,  7.4585e-02,\n",
      "         4.3628e-01,  4.7791e-02, -1.9177e-01, -7.6111e-02, -1.6333e-01,\n",
      "        -5.6427e-02,  2.4780e-02,  3.0835e-01, -1.5112e-01, -2.9102e-01,\n",
      "        -5.4492e-01, -1.6614e-01, -4.8047e-01, -1.1115e-01, -1.7932e-01,\n",
      "         3.2544e-01, -1.1017e-01,  2.7808e-01, -4.0259e-01, -4.6484e-01,\n",
      "        -2.2327e-01, -5.1465e-01, -8.5678e-03, -6.1768e-01, -3.9673e-01,\n",
      "        -3.4595e-01, -8.6084e-01, -3.4082e-01, -2.8149e-01, -7.1411e-02,\n",
      "        -4.2383e-01, -2.5415e-01, -4.1211e-01, -7.0618e-02, -2.9150e-01,\n",
      "        -9.9670e-02,  6.1523e-02,  2.3682e-01, -1.4844e-01, -1.2993e-02,\n",
      "        -4.7314e-01, -1.5259e-01, -2.6440e-01, -7.4585e-02,  1.1652e-01,\n",
      "         8.3862e-02,  2.8101e-01,  1.8152e-01,  6.4453e-02,  9.4223e-03,\n",
      "        -3.9276e-02,  3.2684e-02,  6.7017e-02,  4.7485e-02,  1.9861e-01,\n",
      "         3.2520e-01, -3.0975e-02,  1.3268e-02,  1.7712e-01, -1.6632e-02,\n",
      "         1.4026e-01, -7.9285e-02, -1.2769e-01,  1.7529e-01,  7.8735e-02,\n",
      "         1.0938e-01, -4.1333e-01, -3.5718e-01,  2.6636e-01, -1.9666e-01,\n",
      "         1.9202e-01,  1.4014e-01,  2.0288e-01, -8.4900e-02, -6.0059e-01,\n",
      "         3.9444e-03, -4.2188e-01, -4.1724e-01,  1.2512e-01, -1.2054e-01,\n",
      "        -9.6130e-02,  2.9419e-01, -2.3962e-01, -4.8431e-02, -2.2644e-01,\n",
      "        -5.5634e-02, -4.2648e-03, -2.2522e-01, -1.8066e-01, -4.1650e-01,\n",
      "        -3.6646e-01,  1.0852e-01,  9.0881e-02,  3.6133e-01,  7.6843e-02,\n",
      "        -5.1361e-02,  3.6221e-03, -1.3232e-01,  1.4404e-01,  8.7097e-02,\n",
      "         3.2928e-02, -2.4622e-01, -2.9150e-01, -1.7493e-01, -6.0349e-03,\n",
      "        -2.2278e-01,  7.8354e-03, -2.8833e-01, -3.6133e-01,  1.0669e-01,\n",
      "         1.7993e-01,  1.4270e-01, -3.7750e-02, -1.6504e-01, -1.0246e-02,\n",
      "        -2.4036e-01,  1.5674e-01, -1.7468e-01, -2.5818e-02,  3.4937e-01,\n",
      "         1.2305e-01, -3.0127e-01, -3.5962e-01, -1.3268e-02,  5.2887e-02,\n",
      "        -1.2756e-01, -7.9529e-02,  1.6833e-01, -4.6973e-01, -5.9473e-01,\n",
      "         1.7664e-01,  1.7419e-01,  2.4023e-01, -3.3862e-01, -1.0248e-01,\n",
      "        -2.5787e-02,  8.8730e-03, -1.9702e-01, -1.1505e-01, -1.7883e-01,\n",
      "        -3.7231e-01, -2.1375e-01, -1.1420e-01,  1.6663e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.1426,  0.2751, -0.2163,  ...,  0.6804,  1.0310, -0.4941],\n",
      "         [ 0.5030,  0.3127, -0.2535,  ...,  0.5890,  0.9000, -0.4712],\n",
      "         [ 1.0748,  0.0566, -0.1791,  ...,  0.5423,  1.2207, -0.5283],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.5415,  1.0410,  0.7490,  0.7383,  0.2020,  0.0218,  0.5757,  0.7017,\n",
      "         0.4163,  0.3774,  0.4607,  0.4436,  0.6440,  0.5107,  0.7314,  0.5200,\n",
      "         0.3264,  0.7104,  0.8984,  0.7607,  0.7368,  0.9087,  0.5386,  0.7446,\n",
      "         0.6011,  0.1897,  0.3958,  0.7280,  0.7910,  0.4961,  0.7529,  0.6177,\n",
      "         0.5337,  0.4719,  0.4214,  0.5273,  0.5010,  0.7285,  0.3296,  1.0322,\n",
      "         0.3108,  0.8940,  0.2595,  0.9536,  0.6538,  0.4189,  0.6357,  0.1074,\n",
      "         0.7539,  0.3872,  0.7217,  0.4260,  0.5771,  0.3899,  0.7974,  0.5225,\n",
      "         0.2939,  0.7256,  0.6016, -0.0121, -0.0937,  0.0588,  0.6245,  0.0373,\n",
      "         0.9844,  0.9595,  0.6509,  0.6558,  0.6392,  0.4324,  0.6602,  0.5737,\n",
      "         0.4285,  0.9990,  0.5186,  0.9531,  0.5269,  0.9570,  0.6143,  0.8340,\n",
      "         0.7993,  0.8120,  0.6523,  0.9722,  0.6567,  0.7441,  0.9971,  0.7944,\n",
      "         0.6704,  1.0986,  0.6997,  0.5454,  0.6919,  0.0654,  0.0144,  0.8408,\n",
      "         0.8804,  0.7266,  0.7490,  0.5322,  0.9321,  1.0654,  0.6733,  0.6602,\n",
      "         0.5601,  0.9487,  0.9893,  0.9844,  0.7554,  0.8799,  0.6284,  0.2522,\n",
      "         0.4353,  0.6855,  0.6680,  0.4966,  0.6816,  0.5708,  0.4751,  0.7690,\n",
      "         0.6191,  0.7993,  0.6416,  0.6304,  0.8296,  0.7329,  0.3394,  0.4373,\n",
      "         0.7798,  0.2771,  0.7969,  0.4177,  0.8833,  0.5396,  0.7974,  0.9321,\n",
      "         0.7163,  0.7358,  0.6064,  1.2930,  0.6768,  0.8237,  0.6533,  0.6685,\n",
      "         1.1221,  0.8955,  0.5088,  0.7715,  0.2710,  0.7021,  0.5249,  0.6812,\n",
      "         0.8208,  0.4548,  0.7920,  0.6792,  0.8013,  0.4299,  0.7734,  0.9243,\n",
      "         1.0459,  0.6431,  0.6470,  0.7827,  0.8813,  0.5723,  0.9395,  0.6152,\n",
      "         0.7651,  0.7036,  0.7500,  0.9160,  0.5518,  0.6445,  0.3301,  0.5796,\n",
      "         0.7222,  0.8950,  0.5840,  1.1406,  0.9741,  0.9741,  0.6982,  0.8667,\n",
      "         1.0254,  1.0801,  0.9253,  0.0360,  0.4106,  1.0801,  1.0029,  0.6353,\n",
      "         0.5942,  0.4419,  0.4221,  0.9912,  1.0283,  0.6919,  0.4744,  1.1855,\n",
      "         0.8818,  0.7441,  0.8149,  0.7749,  0.1187,  0.4419,  0.0468,  0.3264,\n",
      "         0.7549,  0.5405,  0.6968,  0.5620,  0.7656,  0.4509,  0.6997,  0.7695,\n",
      "         0.4033,  0.8799,  0.5962,  0.7158,  0.6333,  0.7764,  0.9141,  1.0117,\n",
      "         0.6299,  0.8384,  0.3630,  0.1992,  0.7837,  0.7300,  0.7344,  0.7944,\n",
      "         0.8496,  0.8721,  0.7407,  0.8760,  0.4360,  0.8428,  0.3152,  1.0146,\n",
      "         0.8457,  1.2158,  1.0361,  0.7417,  1.0791,  0.7534,  0.9805,  0.6519,\n",
      "         0.5615,  0.8818,  0.0583,  0.3516,  0.9424,  0.5620,  0.9873,  0.4172,\n",
      "         1.0791,  0.7856,  1.0527,  0.7632,  0.8950,  0.7061,  0.5952,  0.5166,\n",
      "         0.5503,  0.1321,  0.2141,  0.1755,  0.5508,  0.6313,  0.5708,  0.5508,\n",
      "         1.2002,  0.8555,  0.5103,  0.3005,  0.2798,  0.4429,  0.7515,  0.4119,\n",
      "         0.9941,  0.8970,  0.3904,  0.6006,  0.3767,  0.8008,  0.6748,  0.3879,\n",
      "         0.8286,  0.7085,  0.7163,  0.4854,  0.7378,  0.3950,  0.9570,  0.7085,\n",
      "         0.7100,  0.5239,  0.8608,  0.5020,  0.9463,  0.7910,  0.9014,  0.7993,\n",
      "         1.1611,  1.0859,  0.5991,  0.4878,  1.1582,  0.8721,  0.3884,  1.1104,\n",
      "         0.7222,  0.5981,  0.6753,  1.1689,  0.8120,  0.9766,  0.5405, -0.1230,\n",
      "         0.7734,  0.2788,  0.0303,  0.4639,  0.7373,  0.2642,  0.4404,  0.8135,\n",
      "         1.0713,  0.7202,  0.9570,  0.8740,  0.4402,  0.2382,  0.3840,  1.0420,\n",
      "         0.8271,  0.9912,  0.8184,  0.9785,  0.6123,  0.6372,  0.9116,  0.7979,\n",
      "         0.7612,  0.5361,  0.7715,  0.8154,  0.6938,  0.8286,  0.4592,  0.0365,\n",
      "         0.5972,  0.1315,  0.6235,  0.7617,  0.9092,  0.8003,  1.1426,  0.4487,\n",
      "         0.4395,  0.2360,  0.4231,  0.5889,  0.3372,  0.9146,  0.3721,  0.7163,\n",
      "         1.0068,  1.1406,  0.9961,  0.6504,  0.6924,  0.8486,  0.7197,  0.1847,\n",
      "         0.2693,  0.3557,  0.4688,  0.7954,  0.8252,  0.1860,  0.6191,  0.7852,\n",
      "         0.9404,  0.7461,  1.1533,  0.8706,  0.2964,  0.1466,  0.5068,  0.7739,\n",
      "         0.4072,  0.8589,  0.3508,  0.8354,  0.5107,  0.5010,  0.2460,  0.6050,\n",
      "         0.7832,  0.8765,  0.8354,  0.6431,  0.7646,  1.0225,  0.4292,  0.3750,\n",
      "         0.7241,  0.2534,  0.8735,  0.3247,  0.5439,  0.9106,  1.1523,  0.4470,\n",
      "         0.1532,  0.4902,  0.4917,  0.5210,  0.0378,  0.5557,  0.7397,  0.5298,\n",
      "         0.3210,  0.5693,  0.2979,  0.8008,  0.9214,  0.7847,  0.3831,  0.8794,\n",
      "         0.9561,  0.8989,  0.5396,  0.3137,  0.6396,  0.3154,  0.9561,  0.6309,\n",
      "         0.7798,  0.4590,  0.9526,  0.4326,  0.8706,  0.7021,  0.7573,  0.3354,\n",
      "         0.2705,  0.5908,  1.0518,  0.7915,  0.3450,  0.7368,  0.2230,  0.5327,\n",
      "         1.1436,  0.4741,  0.8389,  0.6699,  0.7856,  0.1874,  0.8730,  0.9658,\n",
      "         0.2423,  0.6387,  0.6167,  0.5464,  0.9048,  0.5723,  0.4580,  0.1990,\n",
      "         0.2017,  0.0206,  0.4148,  0.4375,  1.0215,  0.5708,  0.7427,  0.6548,\n",
      "         0.5840, -0.0514,  0.8872,  0.4250,  0.9443,  0.5972,  0.5835,  0.2705,\n",
      "         0.3535,  0.7925,  0.5903,  0.0663,  0.9541,  0.4009,  0.8203,  0.0905,\n",
      "         0.2377,  0.0693,  0.6191,  0.5581,  0.3330,  0.2842,  0.5020,  0.9722,\n",
      "         0.8330,  0.9204,  0.3923,  0.8848,  0.8569,  0.2377, -0.1415,  0.2952,\n",
      "         0.4824,  0.4250,  0.7070,  0.6597,  0.3601,  0.4648,  0.4724,  0.7358,\n",
      "         0.3328,  0.2822,  0.6646,  0.5776,  0.4229,  0.3970,  0.7222,  0.4602,\n",
      "         0.6978,  0.8340,  0.6670,  0.1595,  0.4868,  0.8711,  0.5859,  0.3599,\n",
      "         0.8237,  0.4722,  0.7354,  0.4578,  0.1670,  0.2377,  0.5762,  0.9263,\n",
      "         0.4568,  0.8584,  0.8599,  0.4729,  0.4014,  0.7466,  0.4670,  0.6572,\n",
      "         0.6382,  0.2817,  0.8242,  0.6206,  0.8447,  0.8115,  0.7256,  0.4844,\n",
      "         0.2976,  0.4917,  0.6758,  0.8740,  0.7090,  0.9189,  0.5127,  0.7876,\n",
      "         0.1946,  0.7178,  0.3296,  0.4575,  0.1306,  0.7861,  0.9424,  0.6426,\n",
      "         0.7759,  0.1409,  0.4902,  0.5596,  0.6343,  0.8335,  0.7300,  0.8467,\n",
      "         0.8242,  0.6738,  0.3823,  0.5669,  0.6519,  0.4927,  0.5020,  0.4075,\n",
      "         0.1471, -0.1259,  0.4868,  0.3247,  0.5225,  0.5083,  0.5869,  0.7373,\n",
      "         0.6943,  0.7363,  0.6978,  0.3909,  0.5864,  0.7026,  0.6455,  0.5044,\n",
      "         0.5269,  0.5488,  0.2717,  0.2810,  0.4041,  1.0967,  0.8369,  0.8452,\n",
      "         0.3733], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 2.0962e-03, -3.1519e-01, -1.0822e-01, -2.9419e-01, -2.8931e-01,\n",
      "        -4.5654e-01, -5.4395e-01, -3.2007e-01, -7.9346e-01, -6.7725e-01,\n",
      "        -6.4795e-01, -1.9580e-01, -1.2042e-01, -3.9673e-01,  1.9989e-02,\n",
      "        -2.2107e-01, -2.2156e-01, -9.1431e-02, -3.7793e-01, -6.5979e-02,\n",
      "        -2.9419e-02, -7.6111e-02,  8.5678e-03, -3.2074e-02,  3.7671e-01,\n",
      "        -3.1348e-01, -2.0081e-02, -4.0747e-01,  1.3573e-02,  1.5625e-01,\n",
      "         1.5295e-01, -2.5098e-01,  1.3708e-01,  1.2537e-01,  2.1484e-01,\n",
      "         3.7988e-01,  8.6731e-02, -1.2323e-01, -1.8036e-02,  3.7048e-02,\n",
      "        -3.9697e-01, -2.7417e-01, -1.5552e-01, -1.6101e-01, -2.9816e-02,\n",
      "         1.8188e-02, -3.1348e-01, -1.4844e-01,  7.9834e-02,  7.3303e-02,\n",
      "         6.3538e-02, -5.2948e-02,  2.2171e-02, -8.2886e-02,  3.3887e-01,\n",
      "         1.3130e-02,  1.7053e-01,  4.8462e-02, -5.5573e-02, -3.5522e-01,\n",
      "         1.4748e-02, -3.3051e-02, -1.8738e-01, -2.6685e-01,  1.0797e-01,\n",
      "         2.6221e-01,  2.3633e-01,  1.0876e-01,  2.5497e-02,  7.7271e-02,\n",
      "        -6.0150e-02,  4.6436e-01,  9.3445e-02,  6.4697e-02,  6.6833e-02,\n",
      "         2.4121e-01,  1.2305e-01,  9.7778e-02,  1.0658e-02, -1.7761e-01,\n",
      "        -3.5718e-01, -1.3770e-01,  1.2439e-01, -1.6101e-01, -1.6150e-01,\n",
      "         3.5715e-04, -5.1544e-02,  8.2886e-02, -1.5247e-01, -2.3547e-01,\n",
      "         2.0288e-01, -1.8750e-01,  1.2646e-01,  5.4297e-01, -1.0211e-01,\n",
      "        -3.6011e-01,  7.7133e-03, -1.2421e-01,  2.3779e-01,  1.9824e-01,\n",
      "         8.5999e-02,  9.2087e-03, -1.7737e-01, -2.8442e-02, -2.8638e-01,\n",
      "         1.4600e-01,  1.7297e-01, -6.0272e-02, -2.4939e-01, -3.7354e-01,\n",
      "         3.8815e-03, -5.8624e-02, -2.2498e-01, -9.6863e-02, -6.0089e-02,\n",
      "         1.2001e-02,  3.3844e-02, -1.7114e-01, -3.2745e-02,  2.0337e-01,\n",
      "         6.2988e-02,  5.9296e-02,  1.0205e-01, -2.1610e-03, -3.3350e-01,\n",
      "        -6.2622e-02, -2.6172e-01, -2.4866e-01, -2.1265e-01, -4.6722e-02,\n",
      "        -3.7628e-02, -2.6270e-01,  5.6274e-02, -2.4662e-03, -4.1968e-01,\n",
      "        -4.2749e-01, -3.6304e-01, -3.0078e-01, -3.8745e-01, -1.5527e-01,\n",
      "         3.6255e-01, -2.0459e-01, -1.6235e-01, -1.3708e-01, -5.5957e-01,\n",
      "        -5.2887e-02, -8.1787e-02, -1.5771e-01, -2.1216e-01,  1.2581e-02,\n",
      "        -9.8694e-02,  2.0972e-01,  1.7810e-01, -4.2944e-01,  3.9825e-02,\n",
      "         5.2783e-01,  1.0962e-01,  7.8003e-02,  2.1667e-01,  7.8506e-03,\n",
      "        -3.9526e-01,  3.1982e-02,  1.5137e-02, -1.8774e-01, -4.2145e-02,\n",
      "        -5.3809e-01,  2.1826e-01,  1.7810e-01, -5.8319e-02, -1.5027e-01,\n",
      "        -1.8127e-01, -8.1406e-03, -5.3418e-01, -8.2422e-01, -7.8418e-01,\n",
      "         2.3962e-01, -1.1920e-01,  5.2872e-03, -1.2421e-01,  1.9714e-01,\n",
      "        -4.0741e-02,  1.5625e-01, -2.7539e-01,  2.0312e-01, -4.1406e-01,\n",
      "        -4.4507e-01, -3.4814e-01, -2.5415e-01, -4.0991e-01, -3.4149e-02,\n",
      "        -4.0283e-01, -6.4600e-01,  1.5613e-01, -3.0200e-01, -4.3652e-01,\n",
      "         4.6411e-01,  2.4451e-01, -3.0981e-01, -1.1957e-01,  2.2083e-01,\n",
      "        -1.3904e-01, -1.6199e-01, -4.3259e-03,  1.0156e-01, -6.2402e-01,\n",
      "        -4.8877e-01, -2.9785e-01, -4.0601e-01, -3.7750e-02,  2.3608e-01,\n",
      "        -2.7905e-01, -2.3621e-01,  4.2603e-01, -1.8884e-01,  5.7434e-02,\n",
      "        -5.2197e-01,  1.8530e-01, -4.0063e-01, -1.7529e-01, -7.4219e-02,\n",
      "         3.5614e-02,  3.9795e-01, -1.3074e-01, -2.4719e-01, -1.6992e-01,\n",
      "        -3.0835e-01,  3.0859e-01,  6.6345e-02,  1.4880e-01, -7.7881e-02,\n",
      "        -2.0642e-01,  1.0891e-03, -9.0256e-03,  1.9495e-01, -2.5562e-01,\n",
      "        -3.9722e-01, -3.2861e-01, -4.3970e-01, -4.2188e-01, -2.2742e-01,\n",
      "        -1.9910e-01, -6.2622e-02,  2.9492e-01, -2.1655e-01, -2.7979e-01,\n",
      "         1.3770e-01,  1.7419e-01, -1.7908e-01, -4.1699e-01, -1.0931e-01,\n",
      "        -3.0566e-01, -5.3418e-01,  8.4412e-02, -3.9526e-01,  5.1193e-03,\n",
      "        -9.8328e-02, -5.1074e-01, -2.7148e-01, -2.1277e-01, -2.0789e-01,\n",
      "        -2.0105e-01, -2.3041e-02, -4.5532e-01, -2.1326e-01, -7.0435e-02,\n",
      "        -3.6377e-01, -5.1178e-02, -2.4255e-01, -1.3904e-01, -1.7017e-01,\n",
      "        -5.2002e-01, -5.0830e-01, -2.9468e-01, -1.7578e-01, -4.5483e-01,\n",
      "        -3.5309e-02, -2.8467e-01, -5.2539e-01,  1.7297e-01, -1.0480e-01,\n",
      "         1.9519e-01, -4.9951e-01,  1.7993e-01, -5.6738e-01, -4.3488e-02,\n",
      "        -1.8579e-01, -4.4312e-01,  7.6050e-02, -1.1261e-01, -5.9424e-01,\n",
      "        -2.3450e-01, -1.5112e-01,  3.7866e-01, -2.4567e-02,  3.6450e-01,\n",
      "        -6.2042e-02,  4.7607e-02, -3.1763e-01,  2.6560e-04,  5.3955e-02,\n",
      "        -1.2433e-01,  1.3245e-01, -7.1655e-02,  1.5144e-02,  2.9932e-01,\n",
      "         2.5244e-01,  1.9867e-02, -9.7107e-02, -1.6699e-01,  1.9241e-02,\n",
      "        -3.5010e-01,  4.3433e-01,  4.6289e-01, -2.6392e-01, -2.9004e-01,\n",
      "         8.3801e-02, -1.7102e-01,  7.7820e-02, -2.0605e-01, -1.5283e-01,\n",
      "        -6.1328e-01, -6.4160e-01, -1.0992e-01, -6.1914e-01, -1.3940e-01,\n",
      "        -2.6147e-01,  2.7222e-01, -4.8877e-01,  1.1517e-01, -2.0496e-01,\n",
      "         8.3923e-02, -5.4980e-01, -5.0488e-01, -2.8961e-02, -3.5498e-01,\n",
      "        -2.8656e-02, -2.7008e-02, -1.6455e-01,  9.7580e-03, -5.9418e-02,\n",
      "         2.0190e-01, -1.7224e-01, -7.1350e-02,  1.6248e-01, -4.5197e-02,\n",
      "        -1.8909e-01, -1.7810e-01, -2.9224e-01, -9.1016e-01, -6.3574e-01,\n",
      "        -7.2266e-01, -2.5244e-01, -2.6978e-01, -3.9111e-01, -2.5586e-01,\n",
      "        -1.8420e-01,  9.9121e-02, -4.3018e-01, -7.9407e-02, -4.1602e-01,\n",
      "        -1.2793e-01,  4.2450e-02, -6.8115e-01, -4.0747e-01, -5.3467e-01,\n",
      "        -1.6248e-01, -1.7041e-01, -4.3774e-01, -9.4238e-02, -1.5259e-01,\n",
      "        -1.3672e-01,  4.9042e-02, -2.4036e-01, -6.2549e-01, -4.8975e-01,\n",
      "        -1.5955e-01, -1.6809e-01, -9.2957e-02, -2.5049e-01, -3.5107e-01,\n",
      "        -5.5127e-01, -1.1432e-01,  3.6530e-02, -1.2512e-01, -1.3635e-01,\n",
      "        -2.7295e-01, -1.2360e-01, -4.9512e-01, -1.0078e+00, -5.6299e-01,\n",
      "        -3.8086e-01, -1.7590e-01, -8.5986e-01, -4.4189e-01, -3.2837e-01,\n",
      "        -5.7770e-02, -2.5830e-01, -2.7954e-01, -2.1484e-01,  3.7476e-01,\n",
      "         2.0844e-02, -5.8740e-01, -4.1553e-01,  4.1870e-02, -2.7441e-01,\n",
      "        -3.5954e-03, -3.3105e-01,  1.6772e-01, -4.1602e-01, -1.5686e-01,\n",
      "         3.5858e-02,  1.6882e-01,  3.3783e-02,  2.0532e-01, -9.5642e-02,\n",
      "         2.6294e-01, -1.6907e-02, -7.5500e-02, -3.1226e-01,  2.9297e-01,\n",
      "         9.3933e-02, -1.7078e-01,  1.3757e-01, -7.7576e-02,  1.0712e-01,\n",
      "        -3.3545e-01, -2.9370e-01,  1.3464e-01,  3.6230e-01,  5.2686e-01,\n",
      "        -8.3130e-02,  1.6321e-01,  2.6514e-01,  8.2374e-05, -2.8833e-01,\n",
      "         1.6956e-01, -3.7451e-01, -3.3032e-01,  1.4978e-01, -2.6221e-01,\n",
      "         2.5342e-01, -8.3801e-02,  2.2107e-01, -2.1875e-01,  5.4245e-03,\n",
      "        -1.1700e-01, -1.0736e-01, -5.0684e-01, -4.5361e-01, -2.9248e-01,\n",
      "        -4.0436e-02, -9.9060e-02, -5.0977e-01,  6.2180e-03, -3.2501e-02,\n",
      "        -2.4255e-01, -5.6305e-02, -1.4758e-01, -2.4597e-02, -6.2549e-01,\n",
      "        -3.4497e-01, -1.8921e-02,  4.8462e-02, -1.9983e-01, -1.2769e-01,\n",
      "        -2.3468e-02, -2.7515e-01,  7.5378e-02, -3.4027e-02, -3.5986e-01,\n",
      "        -3.0737e-01, -3.1006e-01, -4.2847e-01, -1.5222e-01,  3.3179e-01,\n",
      "        -1.3086e-01, -1.3318e-01,  1.1475e-01, -3.9764e-02, -5.7892e-02,\n",
      "        -1.9507e-01, -5.1465e-01, -7.1472e-02, -2.6416e-01, -4.6600e-02,\n",
      "        -4.6460e-01, -2.1820e-02, -5.9570e-01, -4.4580e-01,  3.5305e-03,\n",
      "        -6.7322e-02, -4.5190e-01,  1.4702e-02, -8.3130e-02,  3.6224e-02,\n",
      "        -5.0781e-01,  2.2949e-01,  2.7905e-01, -4.5190e-01, -2.7368e-01,\n",
      "        -1.7139e-01,  2.0386e-01, -1.1017e-01, -1.0663e-01, -5.8563e-02,\n",
      "        -2.5024e-02, -5.1904e-01, -9.7839e-02, -8.4473e-02, -2.9150e-01,\n",
      "        -1.6711e-01, -5.9229e-01, -1.8127e-02, -2.9517e-01, -2.6929e-01,\n",
      "        -5.1270e-01,  1.5430e-01, -2.7832e-01, -2.8345e-01, -1.7883e-01,\n",
      "        -1.9910e-01, -5.0928e-01, -2.2253e-01, -8.8965e-01, -3.8086e-01,\n",
      "        -5.1807e-01, -8.2825e-02,  1.3390e-02, -2.8247e-01, -2.0471e-01,\n",
      "        -8.8013e-02, -1.5710e-01, -1.4551e-01, -1.6797e-01,  2.7026e-01,\n",
      "        -8.5693e-02, -1.9250e-01, -3.5181e-01, -7.8418e-01,  8.1116e-02,\n",
      "        -3.8788e-02, -7.2937e-02, -9.0454e-02, -5.3375e-02, -1.1469e-01,\n",
      "         8.1604e-02, -3.2080e-01,  9.3872e-02, -2.7759e-01, -6.6772e-02,\n",
      "        -3.8379e-01, -5.6006e-01, -1.4038e-01, -7.1411e-02,  4.9377e-02,\n",
      "        -5.0000e-01, -3.5547e-01, -3.1421e-01,  2.2766e-02, -1.4661e-01,\n",
      "         1.1401e-01,  9.5947e-02, -4.0308e-01, -1.7639e-01,  4.7607e-02,\n",
      "        -3.1616e-01, -1.7120e-02, -2.2961e-01, -4.8126e-02,  1.3623e-01,\n",
      "        -4.1113e-01,  2.1240e-01,  2.6718e-02,  1.7688e-01, -3.8110e-01,\n",
      "        -5.1953e-01, -5.9277e-01, -3.6035e-01, -1.9653e-02,  1.4319e-01,\n",
      "         8.6426e-02,  5.6641e-02, -2.6807e-01,  3.5767e-01, -1.4343e-01,\n",
      "         8.5693e-02, -4.8877e-01,  3.5156e-02, -1.5068e-02, -1.4270e-01,\n",
      "        -1.8689e-01, -5.0928e-01, -2.1667e-01, -2.4817e-01, -5.0635e-01,\n",
      "        -4.1162e-01, -3.9825e-02,  1.6064e-01,  8.6121e-02,  1.1597e-01,\n",
      "        -3.1299e-01, -4.6606e-01, -2.6493e-03, -5.7617e-01, -5.5573e-02,\n",
      "        -8.1238e-02, -3.7671e-01, -4.6582e-01,  2.4048e-01,  6.9641e-02,\n",
      "        -4.6289e-01,  1.8665e-01, -1.1884e-01, -6.4990e-01, -4.0918e-01,\n",
      "        -3.0542e-01,  1.4624e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.4663,  0.5444, -0.1731,  ...,  0.7758,  1.0408, -0.3350],\n",
      "         [ 0.3271,  0.4586, -0.0025,  ...,  0.8093,  0.7110, -0.1714],\n",
      "         [ 0.4099,  0.1884,  0.1555,  ..., -0.0059,  0.5852,  0.1898],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 3.7305e-01,  9.8242e-01,  6.6602e-01,  8.1641e-01,  2.3499e-01,\n",
      "         6.4746e-01,  5.4150e-01,  6.3818e-01,  4.8389e-01,  7.7441e-01,\n",
      "         2.2961e-01,  6.9971e-01,  5.4785e-01,  5.7373e-01,  4.7705e-01,\n",
      "         3.1860e-01,  8.8232e-01,  5.2686e-01,  4.2798e-01,  2.7905e-01,\n",
      "         5.1611e-01,  9.0527e-01,  4.0283e-01,  4.4824e-01,  2.3840e-01,\n",
      "         4.6753e-01,  7.3145e-01,  4.0259e-01,  6.2207e-01,  2.5195e-01,\n",
      "         5.3857e-01,  2.6270e-01,  2.9443e-01,  4.7900e-01, -9.9121e-02,\n",
      "         6.0596e-01,  2.7466e-01,  1.6687e-01,  1.3245e-01,  9.8828e-01,\n",
      "         6.3086e-01,  5.4590e-01,  3.1445e-01,  2.1204e-01,  5.3174e-01,\n",
      "         3.9233e-01,  2.0459e-01,  1.1957e-01, -3.1799e-02,  4.1235e-01,\n",
      "         4.5557e-01,  3.3887e-01,  7.2070e-01,  2.4036e-01,  6.5137e-01,\n",
      "         4.5605e-01,  4.5020e-01,  4.8157e-02,  7.7393e-01,  2.8564e-01,\n",
      "         6.0498e-01,  8.5303e-01,  3.3594e-01,  4.6436e-01,  2.6221e-01,\n",
      "         2.6587e-01,  1.0547e+00,  7.4609e-01,  5.2197e-01,  4.1211e-01,\n",
      "         5.9277e-01,  3.2764e-01,  9.0674e-01,  6.9238e-01,  3.1079e-01,\n",
      "         5.8044e-02,  3.4009e-01,  4.1846e-01,  7.0850e-01,  4.0747e-01,\n",
      "         4.7510e-01,  4.3726e-01,  3.8354e-01,  3.2324e-01,  8.2520e-01,\n",
      "         3.5010e-01,  8.0127e-01,  5.6885e-02,  1.4221e-01,  2.7441e-01,\n",
      "         6.8018e-01,  9.7656e-01,  2.2180e-01,  4.5117e-01,  1.5881e-01,\n",
      "         7.2266e-01,  3.0615e-01,  9.4336e-01,  3.3203e-01,  4.8730e-01,\n",
      "         3.0908e-01,  6.9727e-01,  2.3596e-01,  4.8584e-01,  5.3809e-01,\n",
      "         9.5117e-01,  3.9404e-01,  8.2617e-01,  5.9619e-01,  9.6924e-02,\n",
      "         4.8779e-01,  5.0635e-01,  4.9268e-01,  7.2119e-01,  4.2334e-01,\n",
      "         6.0303e-01,  7.2021e-01,  5.1465e-01,  3.1714e-01,  3.0078e-01,\n",
      "         2.8183e-02,  4.4580e-01,  4.9072e-01,  3.6621e-01,  8.0371e-01,\n",
      "         3.7695e-01,  1.4673e-01,  4.3091e-01,  5.1562e-01, -6.9695e-03,\n",
      "         2.7173e-01,  3.8354e-01,  6.4160e-01,  8.0273e-01,  3.7720e-01,\n",
      "         3.7012e-01,  9.4629e-01,  4.6558e-01,  1.7126e-01,  4.5972e-01,\n",
      "         3.7256e-01,  4.2798e-01, -1.0166e-03,  4.3774e-01,  9.1553e-01,\n",
      "         2.5391e-01, -3.6469e-02,  2.9224e-01,  3.2080e-01,  1.5881e-01,\n",
      "         6.1279e-01,  4.6851e-01,  2.8809e-01,  2.7856e-01,  3.4973e-02,\n",
      "         4.9561e-01,  4.9902e-01,  3.8550e-01,  8.4717e-01,  8.4277e-01,\n",
      "         6.0693e-01,  1.4392e-01,  7.8125e-01,  3.9087e-01,  7.1240e-01,\n",
      "         4.0649e-01,  3.8086e-01,  3.7402e-01,  5.0928e-01,  1.5587e-02,\n",
      "         9.0234e-01,  4.5532e-01,  1.0039e+00,  8.2910e-01,  8.0762e-01,\n",
      "         3.3154e-01,  1.1224e-01,  4.3433e-01,  9.4531e-01,  6.2061e-01,\n",
      "         2.6392e-01,  3.5376e-01,  3.4692e-01,  7.1338e-01,  2.2620e-01,\n",
      "         6.2646e-01,  4.6680e-01,  1.0846e-01,  4.1846e-01,  2.2058e-01,\n",
      "         3.4692e-01,  2.3840e-01,  9.6484e-01,  2.1960e-01,  5.9717e-01,\n",
      "         7.7588e-01,  4.4995e-01,  8.4277e-01,  7.7295e-01,  5.9473e-01,\n",
      "         2.0850e-01,  6.6455e-01,  4.5044e-01,  3.1079e-01,  2.6025e-01,\n",
      "         4.6899e-01,  2.0093e-01,  8.2080e-01,  3.2788e-01,  5.9033e-01,\n",
      "         4.3164e-01,  6.0547e-01,  3.1567e-01,  2.5684e-01,  7.2900e-01,\n",
      "         8.7402e-02,  6.8848e-01,  8.8721e-01,  3.9673e-01,  3.1030e-01,\n",
      "         1.3440e-01,  3.6597e-01,  6.0352e-01,  4.4556e-01,  2.8003e-01,\n",
      "         1.8518e-01,  2.2815e-01,  1.6846e-01,  4.6948e-01,  6.0547e-01,\n",
      "         4.7705e-01,  4.5752e-01,  3.2324e-01,  1.0029e+00,  7.2461e-01,\n",
      "         5.8838e-01,  4.2041e-01,  3.4521e-01,  9.2090e-01,  5.3369e-01,\n",
      "         5.6396e-01,  3.5986e-01,  2.6172e-01,  6.2695e-01,  3.9600e-01,\n",
      "         1.3037e-01,  3.5010e-01,  9.9902e-01,  6.2988e-01,  5.6348e-01,\n",
      "         3.3398e-01,  4.5166e-01,  6.3086e-01,  3.3716e-01,  3.7207e-01,\n",
      "         5.7178e-01,  7.9297e-01,  3.6719e-01,  4.8438e-01,  2.6318e-01,\n",
      "         2.0923e-01,  1.0264e+00,  7.9150e-01,  5.0928e-01,  4.2505e-01,\n",
      "         5.4541e-01,  2.1851e-01,  7.6221e-01,  5.9033e-01,  3.6621e-01,\n",
      "         7.8467e-01,  4.8242e-01,  4.7363e-01,  1.8311e-01,  3.9990e-01,\n",
      "         1.4624e-01,  2.9077e-01,  1.0205e+00,  3.2593e-01,  4.8242e-01,\n",
      "         3.0762e-01,  6.8555e-01,  3.4253e-01,  4.1309e-01,  2.5439e-01,\n",
      "         8.3057e-01,  3.4912e-01,  4.9414e-01,  6.7139e-01,  4.7852e-01,\n",
      "         6.2793e-01,  4.9561e-01,  8.8379e-01,  7.6953e-01,  4.7266e-01,\n",
      "         3.5864e-01,  5.2979e-01,  4.2603e-01,  5.5908e-01,  7.6709e-01,\n",
      "         7.3047e-01,  5.7471e-01,  7.4707e-01,  1.0020e+00,  1.8433e-01,\n",
      "         9.2090e-01,  4.2603e-01,  6.0596e-01,  3.4912e-01,  2.8760e-01,\n",
      "        -7.1838e-02,  7.9041e-02,  1.8713e-01,  4.7852e-01,  3.2959e-01,\n",
      "         3.2300e-01,  1.5454e-01,  8.3740e-02, -8.7708e-02,  1.0254e+00,\n",
      "         6.6602e-01,  3.3252e-01,  3.8135e-01,  4.7437e-01,  4.1089e-01,\n",
      "         7.2314e-01,  4.8340e-01,  8.4326e-01,  5.0977e-01,  3.3765e-01,\n",
      "         8.5400e-01,  2.8076e-01,  8.3691e-01,  2.9712e-01,  2.4670e-01,\n",
      "         7.9248e-01,  6.5283e-01,  8.7402e-01,  6.8945e-01,  3.7183e-01,\n",
      "         7.1484e-01,  6.1401e-02,  9.2896e-02,  6.8054e-02,  6.3867e-01,\n",
      "         2.6514e-01,  1.8457e-01,  1.7114e-01,  5.5176e-01,  3.9893e-01,\n",
      "         4.1260e-01,  5.4053e-01,  3.3008e-01,  8.8428e-01,  5.8691e-01,\n",
      "         5.1855e-01,  4.1162e-01,  5.5908e-01,  8.2617e-01,  4.7803e-01,\n",
      "         4.9512e-01,  3.6450e-01,  4.4238e-01,  4.9854e-01,  3.2690e-01,\n",
      "         2.9541e-01,  2.7271e-01,  1.0205e+00,  6.5479e-01,  5.3760e-01,\n",
      "         3.7231e-01,  8.0518e-01,  3.5620e-01,  3.9087e-01,  1.3782e-01,\n",
      "         9.3506e-01,  3.2397e-01,  8.5596e-01,  1.8616e-01,  4.6484e-01,\n",
      "         5.4199e-01,  3.7720e-01,  7.4951e-01,  5.7812e-01,  6.6455e-01,\n",
      "         5.7764e-01,  3.1274e-01,  5.2100e-01,  3.1934e-01,  4.5142e-01,\n",
      "         6.3965e-01,  3.4497e-01,  7.9883e-01,  4.6240e-01,  7.8906e-01,\n",
      "         2.6489e-01,  9.9707e-01,  7.4023e-01,  4.7705e-01,  4.3164e-01,\n",
      "         6.4648e-01,  1.4868e-01,  8.4863e-01,  3.1250e-01,  9.2676e-01,\n",
      "         2.6367e-01,  3.0127e-01,  7.4268e-01,  1.9141e-01,  3.7427e-01,\n",
      "         3.5938e-01,  4.4629e-01,  3.1421e-01,  6.1377e-01,  3.7427e-01,\n",
      "         4.0503e-01,  5.8154e-01,  3.7573e-01,  7.8467e-01,  2.2021e-01,\n",
      "         3.3740e-01,  5.3906e-01,  3.0762e-01,  2.7759e-01,  2.4695e-01,\n",
      "        -3.2397e-01, -5.6549e-02,  5.3467e-02,  2.8149e-01,  1.7114e-01,\n",
      "         2.5293e-01,  5.5481e-02, -5.0903e-02, -1.7896e-01,  9.9219e-01,\n",
      "         6.4404e-01,  3.2715e-01,  3.2666e-01,  3.7720e-01,  3.3838e-01,\n",
      "         6.7480e-01,  3.5815e-01,  4.1772e-01,  7.2461e-01,  4.3921e-01,\n",
      "         5.1953e-01,  3.5205e-01,  5.4150e-01,  2.9370e-01,  3.3252e-01,\n",
      "         6.2646e-01,  3.7061e-01,  4.6826e-01,  2.5269e-01,  6.4307e-01,\n",
      "         4.1553e-01,  4.3237e-01,  2.9077e-01,  3.0811e-01,  6.0986e-01,\n",
      "         4.2969e-01,  5.7861e-01,  3.3130e-01,  5.2930e-01,  3.4082e-01,\n",
      "         6.0693e-01,  4.7656e-01,  4.6606e-01,  5.5664e-01,  6.2646e-01,\n",
      "         4.0015e-01,  3.2007e-01,  9.4238e-01,  2.2327e-01,  4.6948e-01,\n",
      "         2.9028e-01,  3.2446e-01,  6.5869e-01,  4.9512e-01,  6.3770e-01,\n",
      "         6.2158e-01,  5.3125e-01,  2.1545e-01,  4.9170e-01,  3.3594e-01,\n",
      "         7.5134e-02,  6.1816e-01,  5.5518e-01,  3.2520e-01,  7.2314e-01,\n",
      "         2.2717e-01,  6.5088e-01,  3.1079e-01,  2.6001e-01,  5.3955e-01,\n",
      "         2.5244e-01,  2.9004e-01,  4.7070e-01,  7.4805e-01,  2.5928e-01,\n",
      "         6.3330e-01,  3.3618e-01,  3.3765e-01,  6.5283e-01,  5.8008e-01,\n",
      "         6.6260e-01,  6.0352e-01, -5.0842e-02,  4.4092e-01, -1.9836e-01,\n",
      "         8.3691e-01,  6.1816e-01,  3.1250e-01,  4.1089e-01,  6.5723e-01,\n",
      "         5.3760e-01,  3.9600e-01,  6.9238e-01,  7.2168e-01,  6.8555e-01,\n",
      "         6.1572e-01,  4.9902e-01,  3.5339e-02,  3.9795e-01, -1.9165e-01,\n",
      "         7.2754e-01,  4.6729e-01,  5.1172e-01,  3.2788e-01,  5.9814e-01,\n",
      "         2.9712e-01,  3.4546e-01,  7.4902e-01,  6.1572e-01,  7.5342e-01,\n",
      "         6.8604e-01,  2.7539e-01,  7.8760e-01,  1.9568e-01,  8.0811e-01,\n",
      "         2.8906e-01,  2.6196e-01,  8.4033e-01,  7.6904e-01,  3.8745e-01,\n",
      "        -1.4915e-02,  7.2949e-01,  4.6240e-01,  6.2744e-01,  6.9336e-01,\n",
      "         4.1504e-02,  7.6709e-01,  3.6011e-01,  7.1875e-01,  7.4805e-01,\n",
      "         7.7490e-01,  4.9854e-01,  5.7080e-01,  6.1963e-01,  2.2693e-01,\n",
      "         7.8955e-01,  3.5547e-01,  1.8201e-01,  2.7344e-01,  3.5840e-01,\n",
      "         4.0942e-01,  7.8955e-01,  5.9326e-01,  5.0293e-01,  2.8394e-01,\n",
      "         4.7583e-01,  9.2834e-02,  8.4521e-01,  4.9609e-01,  5.4199e-01,\n",
      "         3.0200e-01,  3.4863e-01,  6.8909e-02,  7.3926e-01,  4.3872e-01,\n",
      "         6.7334e-01,  2.8174e-01,  6.4648e-01,  4.2871e-01,  3.6914e-01,\n",
      "         4.0894e-01,  2.4792e-01,  3.5254e-01,  6.3525e-01,  2.8516e-01,\n",
      "         2.0801e-01,  5.3418e-01,  2.9614e-01,  3.5034e-01,  7.2266e-01,\n",
      "         2.1411e-01,  7.0703e-01,  3.5522e-01,  2.3230e-01,  9.8730e-01,\n",
      "         3.7329e-01,  2.6953e-01,  5.2783e-01,  1.3843e-01,  3.3008e-01,\n",
      "         5.0781e-01,  6.1914e-01,  8.3838e-01,  4.7510e-01,  6.7578e-01,\n",
      "         2.5684e-01, -1.0687e-01,  4.7046e-01,  1.0537e+00,  4.8633e-01,\n",
      "         6.8311e-01,  4.0552e-01,  5.8350e-01,  2.2131e-01,  3.4863e-01,\n",
      "         7.9248e-01,  4.3286e-01,  2.0312e-01,  3.1641e-01,  3.7476e-01,\n",
      "         3.9893e-01,  4.1895e-01,  9.4434e-01,  5.5664e-02,  7.1924e-01,\n",
      "         4.7900e-01,  3.3203e-01,  7.6855e-01,  4.3823e-01,  1.0156e+00,\n",
      "         3.7622e-01, -3.9948e-02,  3.0298e-01,  8.8037e-01,  5.0781e-01,\n",
      "         1.0273e+00,  5.3076e-01,  6.5723e-01,  6.1035e-01,  9.4824e-01,\n",
      "         3.8232e-01,  4.7021e-01,  8.5449e-01,  6.7236e-01,  5.7227e-01,\n",
      "         4.0039e-01,  5.7129e-01,  6.5137e-01,  6.3330e-01,  3.7524e-01,\n",
      "         5.4492e-01,  8.9307e-01,  4.2236e-01,  7.5488e-01,  2.1411e-01,\n",
      "         3.8672e-01,  6.9287e-01,  5.8789e-01,  4.4214e-01,  7.3438e-01,\n",
      "         4.3579e-01,  3.0347e-01,  2.0154e-01,  9.3896e-01,  5.5859e-01,\n",
      "         5.4102e-01,  3.4790e-01,  5.9717e-01,  4.5923e-01,  4.7656e-01,\n",
      "         8.5107e-01,  5.3955e-01,  5.0537e-01,  3.1592e-01,  5.8789e-01,\n",
      "         8.0566e-01,  4.8291e-01,  5.6934e-01,  3.8550e-01,  5.3027e-01,\n",
      "         5.5029e-01,  4.1919e-01,  3.5010e-01,  3.8892e-01,  9.6387e-01,\n",
      "         6.4404e-01,  5.3174e-01,  3.5742e-01,  8.9307e-01,  2.5146e-01,\n",
      "         6.3818e-01,  9.3262e-01,  2.4792e-01,  7.2998e-01,  5.1416e-01,\n",
      "         6.7090e-01,  2.9004e-01,  5.4639e-01,  2.2290e-01,  7.5488e-01,\n",
      "         4.0308e-01,  5.2393e-01,  3.0176e-01,  6.2891e-01,  5.7861e-01,\n",
      "         4.5532e-01,  1.1517e-01,  7.6123e-01,  7.2119e-01,  5.9961e-01,\n",
      "         2.9248e-01,  5.2051e-01,  3.8477e-01,  4.3726e-01,  6.3672e-01,\n",
      "         8.2031e-01,  5.4102e-01,  5.7324e-01,  3.3398e-01,  3.1104e-01,\n",
      "         1.3635e-01,  1.5967e-01,  5.8838e-01,  3.9990e-01,  8.1592e-01,\n",
      "         7.6904e-01,  6.3428e-01,  7.4268e-01,  2.7173e-01,  7.4463e-01,\n",
      "         3.1348e-01,  6.7627e-01,  4.6533e-01,  3.3960e-01,  7.9395e-01,\n",
      "         8.1104e-01,  6.1963e-01,  5.2246e-01,  6.0352e-01,  6.7676e-01,\n",
      "         5.7715e-01,  6.1951e-02,  3.5327e-01,  1.6809e-01,  3.9844e-01,\n",
      "         1.0439e+00,  8.0176e-01,  6.1865e-01,  3.9502e-01,  7.0166e-01,\n",
      "         7.5537e-01,  2.4963e-01,  7.8418e-01,  4.5068e-01,  4.5752e-01,\n",
      "         7.2363e-01,  5.5029e-01,  4.6143e-01,  6.7627e-01,  1.0928e+00,\n",
      "         5.4199e-01,  3.3057e-01,  7.7100e-01,  7.5098e-01,  5.0488e-01,\n",
      "         7.7979e-01,  6.8408e-01,  4.3628e-01,  8.2178e-01,  5.4785e-01,\n",
      "         6.5088e-01,  3.4644e-01,  4.8047e-01,  8.2861e-01,  5.4541e-01,\n",
      "         4.7632e-01,  9.7754e-01,  8.1934e-01,  5.3516e-01,  6.8750e-01,\n",
      "         8.0273e-01,  3.0786e-01,  7.0996e-01,  8.3252e-01,  6.1621e-01,\n",
      "         9.2822e-01,  3.3765e-01,  5.2979e-01,  4.2993e-01,  1.0272e-01,\n",
      "         3.6621e-01,  3.7256e-01,  1.0077e-01,  8.8574e-01,  5.7666e-01,\n",
      "         5.9326e-02,  6.7627e-01,  4.7583e-01,  5.3809e-01,  1.0088e+00,\n",
      "         2.2717e-01,  2.2314e-01,  5.0244e-01,  4.2603e-01,  2.4622e-01,\n",
      "         5.2002e-01,  5.8301e-01,  5.4688e-01,  5.4443e-01,  3.5376e-01,\n",
      "         4.6167e-01,  6.1035e-01,  5.4443e-01,  2.1887e-01,  3.2715e-01,\n",
      "         3.5889e-01,  7.4170e-01,  6.2805e-02,  5.0977e-01,  1.1542e-01,\n",
      "         4.2114e-01,  2.7441e-01,  1.3318e-01,  9.5312e-01,  5.6641e-01,\n",
      "         2.6660e-01,  4.8877e-01,  7.3738e-03,  3.5156e-01,  4.1235e-01,\n",
      "         7.5244e-01,  6.1182e-01,  7.8467e-01,  6.5967e-01,  6.4453e-01,\n",
      "         4.7144e-01,  6.1914e-01,  8.1543e-02,  3.4790e-01,  5.5127e-01,\n",
      "         6.5479e-01,  7.3975e-01,  7.6953e-01,  7.2314e-01,  7.8418e-01,\n",
      "         8.3398e-01,  3.2446e-01,  7.9883e-01,  3.4668e-01,  3.3008e-01,\n",
      "         8.6377e-01,  6.0449e-01,  4.8926e-01,  2.8101e-01,  7.4121e-01,\n",
      "         7.7100e-01,  4.7485e-01,  4.8291e-01,  2.2717e-01,  5.9814e-01,\n",
      "         3.8745e-01,  2.5610e-01,  4.5068e-01,  1.8542e-01,  8.2520e-01,\n",
      "         6.0254e-01,  5.3955e-01,  2.0496e-01,  3.5352e-01,  7.3975e-01,\n",
      "         5.8350e-01,  6.1035e-01,  4.4849e-01,  2.2363e-01,  4.1357e-01,\n",
      "         2.7393e-01,  2.7246e-01,  6.7773e-01,  6.7139e-01,  3.5815e-01,\n",
      "         3.8892e-01,  3.8025e-02,  1.1383e-01,  8.2568e-01,  6.7969e-01,\n",
      "         6.6309e-01,  2.9980e-01,  5.1416e-01,  1.0468e-01,  6.4355e-01,\n",
      "         2.3425e-01,  5.0244e-01,  3.2642e-01,  3.5742e-01, -4.8218e-02,\n",
      "         3.6682e-02,  8.6230e-01,  6.7871e-01,  5.2393e-01,  2.8613e-01,\n",
      "         6.2109e-01,  1.8481e-01,  7.0654e-01,  5.5225e-01,  4.4043e-01,\n",
      "         2.2263e-02,  8.3252e-01,  6.0107e-01,  4.5190e-01,  5.1270e-01,\n",
      "         5.4395e-01,  1.4526e-01,  3.4961e-01,  4.1382e-01,  5.3320e-01,\n",
      "         3.1201e-01,  1.4844e-01,  6.3135e-01,  5.1172e-01,  4.5605e-01,\n",
      "         9.0088e-01,  6.7041e-01,  6.3574e-01,  3.2104e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.3098e-01, -3.8794e-01,  1.0307e-02,  4.7760e-02, -4.7412e-01,\n",
      "         1.7163e-01, -1.0992e-01,  1.0699e-01,  8.4290e-02, -1.5552e-01,\n",
      "        -4.1992e-01, -2.2693e-01, -2.2278e-01, -1.9958e-01, -1.0028e-01,\n",
      "         1.6687e-01, -2.6465e-01, -7.7698e-02,  7.4707e-02,  1.6992e-01,\n",
      "         2.5781e-01, -2.3279e-01, -9.3750e-02,  2.5620e-02,  1.8884e-01,\n",
      "         2.3340e-01, -2.7710e-01, -2.1637e-02,  2.6196e-01,  1.3257e-01,\n",
      "         5.9586e-03,  1.4244e-02,  1.1920e-01,  1.8835e-01, -1.4417e-01,\n",
      "         2.7808e-01,  1.7249e-01,  1.8127e-01, -6.4453e-01, -1.1774e-01,\n",
      "        -7.1228e-02, -1.5007e-02,  1.3672e-01, -4.2725e-01,  2.6367e-01,\n",
      "         2.5146e-01,  2.0300e-01,  8.7158e-02,  9.7351e-02,  3.6328e-01,\n",
      "         3.6865e-02,  1.0370e-01,  1.2688e-02, -2.4768e-01, -5.1260e-04,\n",
      "         1.8835e-01, -1.7188e-01, -5.0977e-01,  4.0601e-01,  6.4087e-02,\n",
      "        -4.2511e-02, -2.4841e-01, -2.8372e-04, -2.3572e-01,  8.7769e-02,\n",
      "        -1.7419e-01, -1.8750e-01, -8.4778e-02,  2.2900e-01,  1.8372e-01,\n",
      "         4.9561e-02,  1.4246e-01, -1.4417e-01,  1.3525e-01,  1.5601e-01,\n",
      "        -1.2225e-01,  1.1591e-01, -2.2864e-01,  1.2866e-01,  1.1865e-01,\n",
      "        -1.6125e-01, -5.9540e-02, -1.8738e-01, -2.3499e-01,  4.2139e-01,\n",
      "         1.6205e-02,  1.3485e-03, -3.4155e-01, -2.9468e-01,  3.5181e-01,\n",
      "        -7.1411e-02, -6.3770e-01, -3.4058e-01, -6.8970e-02, -2.6392e-01,\n",
      "        -7.9285e-02, -1.9934e-01, -1.3013e-01, -7.7271e-02,  3.2275e-01,\n",
      "         3.0117e-03, -1.2384e-01,  3.2739e-01, -1.5454e-01,  9.0576e-02,\n",
      "        -6.6147e-03,  8.1711e-03,  7.0679e-02,  3.4189e-04,  7.7881e-02,\n",
      "        -7.9407e-02, -1.6418e-02, -3.3447e-01,  1.5540e-01, -9.5032e-02,\n",
      "        -9.5032e-02,  1.6064e-01,  2.8345e-01,  2.1606e-01,  1.9852e-02,\n",
      "         7.6843e-02,  3.5205e-01,  8.2016e-03,  8.0627e-02,  9.2773e-02,\n",
      "        -1.6040e-01, -1.5759e-01, -1.9617e-01,  8.7097e-02,  1.1652e-01,\n",
      "        -1.9196e-02, -4.5679e-01, -2.1631e-01, -1.3125e-04,  2.3804e-01,\n",
      "        -1.3025e-01,  3.6914e-01,  6.7017e-02,  1.1902e-01, -2.4933e-02,\n",
      "         1.8713e-01,  2.7563e-01, -6.4316e-03,  2.3291e-01, -2.6184e-02,\n",
      "         1.3695e-02, -3.5376e-01,  1.3354e-01, -8.0566e-01, -5.2979e-01,\n",
      "         1.9397e-01,  2.0178e-01,  1.6125e-01, -5.0812e-02,  1.4626e-02,\n",
      "         3.2690e-01, -1.6479e-02,  6.2347e-02, -3.8700e-03, -2.1082e-01,\n",
      "        -2.6636e-01, -2.4915e-01, -2.6025e-01,  1.0687e-01, -4.4214e-01,\n",
      "        -3.1812e-01,  8.2581e-02,  4.4739e-02,  1.8115e-01, -2.5879e-01,\n",
      "         3.5376e-01,  7.9285e-02, -5.2887e-02, -5.1416e-01,  4.2236e-02,\n",
      "         6.6406e-02, -2.2034e-01,  7.9834e-02, -5.6250e-01,  1.6528e-01,\n",
      "         1.0645e-01,  4.5197e-02, -3.1250e-01,  2.1057e-01,  2.9736e-01,\n",
      "         5.5023e-02,  6.0791e-02, -3.8843e-01,  1.1731e-01, -7.6562e-01,\n",
      "        -8.5999e-02, -1.6040e-01, -2.0312e-01, -5.7422e-01,  2.2119e-01,\n",
      "        -2.6660e-01, -3.4814e-01,  1.1237e-01, -1.7120e-02, -3.4399e-01,\n",
      "        -2.0349e-01, -1.3037e-01,  4.5837e-02, -9.4666e-02, -2.6493e-03,\n",
      "         1.6064e-01, -2.6318e-01, -4.1772e-01, -3.5864e-01,  2.9938e-02,\n",
      "         3.7140e-02, -1.5027e-01, -3.4814e-01, -4.0161e-01,  3.5675e-02,\n",
      "        -3.4106e-01, -3.2397e-01, -3.5815e-01, -7.2314e-01, -3.0957e-01,\n",
      "        -1.7676e-01, -2.9614e-01, -3.4332e-02,  3.7537e-02,  3.0289e-02,\n",
      "        -1.3611e-01, -3.7140e-02, -6.3623e-01, -5.2414e-03, -2.9199e-01,\n",
      "        -2.0166e-01, -6.4746e-01,  1.7407e-01, -3.8647e-01, -9.2651e-02,\n",
      "         1.0796e-02,  1.4099e-01,  1.2445e-01, -2.6147e-01, -1.1078e-01,\n",
      "         1.8524e-02,  1.8164e-01,  1.1487e-01,  1.3440e-01,  6.8665e-02,\n",
      "         2.3694e-01, -7.8418e-01, -2.2766e-01, -1.9714e-01, -5.0278e-03,\n",
      "         4.4617e-02, -4.0845e-01,  2.3730e-01,  1.2952e-01,  2.7026e-01,\n",
      "         2.1667e-02, -1.7358e-01,  4.2572e-02, -5.5695e-02,  2.3880e-02,\n",
      "        -5.5518e-01, -1.6992e-01, -1.3635e-01,  1.6650e-01,  9.3445e-02,\n",
      "         1.5601e-01, -3.3838e-01,  3.7134e-01,  1.5527e-01,  8.8623e-02,\n",
      "         8.1299e-02,  2.5244e-01,  6.8436e-03, -3.0566e-01, -3.2056e-01,\n",
      "         7.3669e-02, -5.6396e-01, -8.5022e-02, -4.0436e-02,  3.5498e-01,\n",
      "         3.5278e-02, -1.1188e-01,  1.3538e-01, -1.0760e-01, -4.3481e-01,\n",
      "         2.2998e-01,  2.3901e-01,  5.1758e-02,  8.5999e-02, -5.7159e-02,\n",
      "        -1.7566e-01,  4.9744e-02, -1.8768e-02, -6.4941e-02,  1.5918e-01,\n",
      "        -2.4255e-01,  3.0493e-01,  3.4210e-02, -2.0923e-01,  4.9408e-02,\n",
      "         2.3975e-01,  9.1125e-02, -3.7174e-03, -1.4441e-01, -2.1802e-01,\n",
      "        -5.4474e-02,  1.4404e-01,  3.1934e-01,  1.2815e-04,  3.6526e-03,\n",
      "        -3.2642e-01, -1.7798e-01,  9.2087e-03, -1.3550e-01,  1.2262e-01,\n",
      "         3.0441e-02, -3.0371e-01, -2.2839e-01, -5.5273e-01, -1.1151e-01,\n",
      "        -8.0811e-02,  1.2634e-01,  7.8812e-03, -7.2559e-01, -3.6401e-01,\n",
      "         1.1757e-02, -1.4099e-01,  2.9068e-02, -1.5613e-01, -6.0938e-01,\n",
      "        -2.6172e-01, -5.8008e-01, -3.9032e-02, -2.5543e-02, -1.7615e-01,\n",
      "         4.2175e-02,  2.6343e-01, -8.4839e-02,  2.5894e-02,  2.9736e-01,\n",
      "         3.4302e-01,  1.1639e-01,  2.7420e-02, -6.9153e-02,  5.8685e-02,\n",
      "        -9.7351e-02, -2.4628e-02, -1.2854e-01,  3.0566e-01,  7.9468e-02,\n",
      "         1.5564e-01, -3.5522e-01,  1.7395e-01, -3.6084e-01, -1.2683e-01,\n",
      "        -4.5380e-02,  1.8787e-01,  4.2999e-02, -2.0459e-01, -8.7585e-02,\n",
      "         3.5767e-02,  2.5415e-01,  8.4961e-02,  1.7834e-01,  1.9629e-01,\n",
      "        -9.0515e-02, -7.9785e-01, -1.5479e-01, -1.0992e-01,  4.5685e-02,\n",
      "         1.4636e-01, -6.0449e-01, -8.5068e-03, -5.8643e-01, -1.0394e-01,\n",
      "         6.3477e-02,  1.1633e-01,  9.7595e-02,  6.4148e-02, -3.3966e-02,\n",
      "        -9.5444e-03, -2.2986e-01,  9.0881e-02, -3.9154e-02,  2.2937e-01,\n",
      "        -2.0154e-01, -2.1167e-01,  2.8125e-01,  8.6548e-02,  2.3840e-01,\n",
      "         6.1920e-02, -2.8027e-01,  1.3611e-01, -3.4424e-01,  6.6528e-02,\n",
      "        -1.7896e-01, -1.4270e-01, -1.4490e-01,  2.0740e-01,  2.1362e-01,\n",
      "         2.7393e-01, -2.6294e-01,  1.3220e-01,  2.0239e-01,  4.2175e-02,\n",
      "         1.0260e-01, -7.6828e-03,  2.2241e-01, -5.8929e-02,  1.7480e-01,\n",
      "         1.3049e-01, -4.6844e-02, -1.6418e-01,  3.1250e-01,  1.6211e-01,\n",
      "         3.0566e-01,  1.2231e-01, -2.5977e-01,  1.4868e-01,  7.7942e-02,\n",
      "        -5.3809e-01, -7.5745e-02, -1.8738e-02,  2.6050e-01, -2.9083e-02,\n",
      "        -3.6133e-01, -1.5393e-01,  3.6194e-02, -3.0975e-02,  2.3840e-01,\n",
      "         1.5637e-01, -2.8491e-01, -1.6016e-01, -5.3955e-01, -1.6882e-01,\n",
      "        -8.4473e-02,  1.2408e-01,  9.2224e-02, -7.7588e-01,  1.7542e-01,\n",
      "         1.8520e-03,  2.2229e-01, -2.8229e-02,  1.3977e-01,  3.6548e-01,\n",
      "         8.9722e-02, -5.0079e-02,  9.6680e-02,  2.6709e-01,  4.0054e-03,\n",
      "         1.6101e-01,  3.2617e-01,  2.2009e-01, -1.1833e-02, -1.3977e-01,\n",
      "        -4.8920e-02,  2.3987e-01,  2.5049e-01,  2.3861e-03,  1.3538e-01,\n",
      "         1.4868e-01, -1.7578e-01,  1.8262e-01,  2.9126e-01, -5.8203e-01,\n",
      "        -2.4646e-01, -3.7018e-02, -1.0590e-01, -4.3823e-01, -4.2188e-01,\n",
      "         1.0345e-01,  6.4636e-02, -5.9433e-03, -2.9755e-02,  2.2937e-01,\n",
      "         1.8152e-01, -4.3427e-02,  1.0468e-01,  9.8633e-02,  1.4417e-01,\n",
      "         7.5317e-02,  1.9556e-01, -1.3770e-01,  1.6711e-01,  1.5857e-01,\n",
      "         1.6028e-01,  1.5979e-01,  2.7222e-01, -2.0837e-01,  1.3342e-01,\n",
      "         1.9495e-01,  2.6270e-01,  1.5845e-01, -7.8674e-02,  4.2383e-01,\n",
      "         2.9004e-01,  1.7944e-01,  2.7828e-03, -2.7466e-02, -2.7714e-03,\n",
      "         1.2097e-01,  2.0251e-01, -1.1833e-02,  1.8921e-01,  3.4277e-01,\n",
      "        -4.8187e-02, -2.1704e-01, -4.7070e-01, -1.7517e-01, -1.7920e-01,\n",
      "        -1.8433e-01, -1.9495e-01,  1.9824e-01, -7.3166e-03,  7.5562e-02,\n",
      "         1.5332e-01, -2.8198e-01,  1.9800e-01, -7.2388e-02,  2.2375e-01,\n",
      "         1.6333e-01, -2.0789e-01, -5.3857e-01, -7.9773e-02, -2.2327e-01,\n",
      "        -1.6504e-01, -2.0477e-02,  2.0447e-01,  2.8223e-01,  3.0005e-01,\n",
      "        -6.6699e-01,  1.7932e-01,  1.1542e-01,  2.8882e-01,  9.2834e-02,\n",
      "         2.0447e-01, -6.8164e-01, -1.4636e-01, -6.2695e-01, -1.0765e-02,\n",
      "        -5.0995e-02, -2.3645e-01, -7.7393e-02, -4.3854e-02,  2.2949e-01,\n",
      "        -1.7273e-01,  2.3792e-01, -7.1411e-02, -3.7256e-01,  5.4596e-02,\n",
      "         1.3283e-02,  1.0260e-01,  6.3904e-02, -1.1981e-01,  7.2266e-02,\n",
      "         1.8921e-01, -7.3291e-01,  8.0383e-02, -2.3474e-01, -7.0312e-01,\n",
      "        -4.9347e-02, -6.9824e-02, -2.2498e-01, -4.3115e-01, -7.5488e-01,\n",
      "        -3.9893e-01, -4.4556e-01, -1.4453e-01,  4.2191e-03,  1.9849e-01,\n",
      "        -2.5879e-02,  1.3867e-01, -2.9150e-01, -1.3196e-01,  5.5054e-02,\n",
      "         2.2205e-01, -2.1454e-02,  1.0358e-01, -1.7468e-01, -1.5186e-01,\n",
      "         2.7368e-01, -1.6174e-02, -9.9060e-02, -6.4026e-02,  1.3257e-01,\n",
      "        -8.5373e-03,  2.0483e-01, -7.2119e-01, -1.8585e-02,  3.0249e-01,\n",
      "         1.0950e-01, -1.0577e-01,  1.0522e-01, -5.0049e-01,  1.8463e-02,\n",
      "         9.2651e-02,  6.7932e-02, -1.6418e-02, -2.4792e-01,  4.1235e-01,\n",
      "        -1.7609e-02,  9.0088e-02,  2.4292e-01,  3.4375e-01,  1.1200e-01,\n",
      "         3.5950e-02,  1.2367e-02, -1.2177e-01, -4.4525e-02, -5.4639e-01,\n",
      "         8.9493e-03, -9.5398e-02, -4.9414e-01, -2.3523e-01, -1.4929e-01,\n",
      "         2.5439e-01,  8.0017e-02, -8.3191e-02,  1.8665e-01, -4.0918e-01,\n",
      "        -1.6125e-01, -1.1920e-01, -3.2104e-01,  2.4011e-01, -4.6729e-01,\n",
      "        -3.6426e-01, -3.4546e-01,  9.5215e-02, -3.9526e-01,  1.1499e-01,\n",
      "         1.9666e-01,  4.4678e-02,  5.8197e-02,  2.8229e-02,  1.6907e-02,\n",
      "         1.1646e-01, -4.3121e-02, -6.1279e-01, -7.2632e-02, -1.0199e-01,\n",
      "        -4.0479e-01,  2.1375e-01, -2.4500e-01, -3.9215e-02, -1.3013e-01,\n",
      "        -2.8906e-01, -3.3252e-01, -7.0557e-02,  1.7590e-01, -4.7150e-02,\n",
      "        -1.5686e-02, -2.5024e-01,  2.5464e-01, -2.6660e-01, -2.4780e-02,\n",
      "         1.6394e-01, -6.7505e-02,  1.1975e-01,  3.5303e-01, -7.9773e-02,\n",
      "         1.0699e-01,  9.2468e-02,  1.5283e-01, -5.3314e-02,  7.2266e-02,\n",
      "         1.2006e-01,  3.4497e-01, -4.4116e-01, -6.0272e-02, -4.3121e-02,\n",
      "         1.3928e-01,  2.1790e-01,  8.5571e-02, -7.1924e-01, -3.9697e-01,\n",
      "        -4.9219e-01, -1.4795e-01,  2.5360e-02,  1.4172e-01, -7.2510e-02,\n",
      "        -1.9519e-01, -1.1322e-01,  8.8135e-02,  2.3035e-01, -1.1041e-01,\n",
      "         1.5112e-01,  4.8462e-02, -2.5146e-01, -8.2666e-01, -1.0602e-01,\n",
      "        -8.2397e-02,  5.4382e-02,  1.4514e-01, -6.1523e-01, -1.1395e-01,\n",
      "        -3.1323e-01,  5.0385e-02, -1.8646e-02,  9.3201e-02,  4.6875e-02,\n",
      "        -9.3262e-02,  8.0322e-02,  8.1238e-02, -1.6174e-01, -1.1406e-02,\n",
      "        -3.4943e-02,  2.9282e-02, -1.4099e-01,  1.6675e-01, -2.5940e-02,\n",
      "        -1.6479e-01,  5.9052e-02,  1.3062e-01,  1.7712e-01, -1.5918e-01,\n",
      "        -2.4329e-01,  3.9355e-01,  2.3804e-02,  2.7686e-01,  1.7810e-01,\n",
      "        -6.5369e-02,  6.5796e-02,  5.1544e-02,  3.2886e-01,  1.6190e-02,\n",
      "        -1.8463e-02, -6.8848e-01, -4.3631e-04, -1.0797e-01, -2.6154e-02,\n",
      "        -1.7529e-01, -1.6895e-01, -4.5703e-01, -6.6699e-01, -2.4915e-01,\n",
      "        -2.3987e-01, -2.9736e-01,  9.6619e-02, -1.9812e-01, -1.6998e-02,\n",
      "        -2.2400e-01, -2.1655e-01, -2.3450e-01,  1.7960e-02, -4.2633e-02,\n",
      "         2.4915e-01, -2.6196e-01,  2.2778e-01, -7.1167e-02, -4.9316e-01,\n",
      "        -9.0149e-02, -1.5894e-01,  1.5833e-01,  2.4304e-01,  1.3159e-01,\n",
      "        -3.6035e-01, -1.2067e-01, -7.4036e-02, -3.0469e-01, -3.3350e-01,\n",
      "         1.5906e-01,  1.3574e-01, -1.3660e-01,  3.0249e-01, -4.2038e-03,\n",
      "        -4.5361e-01,  1.7273e-01, -4.1724e-01, -8.2336e-02, -1.2585e-01,\n",
      "        -4.4238e-01, -6.7322e-02, -1.9080e-01, -1.8555e-01, -1.1676e-01,\n",
      "         2.3657e-01, -8.4290e-02, -6.4026e-02,  8.8623e-02, -1.4197e-01,\n",
      "        -8.7891e-01, -1.6650e-01, -2.2742e-01, -7.0508e-01, -5.7617e-01,\n",
      "        -3.0225e-01, -1.8518e-01, -9.1370e-02,  4.1382e-02, -2.2058e-01,\n",
      "        -3.5400e-01,  1.2433e-01, -1.4610e-02,  1.4553e-03, -6.8555e-01,\n",
      "        -2.9688e-01, -5.6104e-01, -3.1787e-01,  5.4016e-02, -1.8481e-01,\n",
      "        -3.9160e-01, -3.5571e-01, -3.5400e-01, -1.4368e-01, -3.4961e-01,\n",
      "         1.3098e-01, -4.4385e-01, -1.0979e-02, -1.0699e-01, -2.9028e-01,\n",
      "        -2.1667e-01, -9.0515e-02, -1.3892e-01, -9.0881e-02, -1.8579e-01,\n",
      "        -2.2937e-01, -8.4045e-02, -2.1021e-01,  2.3010e-01, -5.0928e-01,\n",
      "        -6.8787e-02, -2.9932e-01,  1.6174e-01,  2.3267e-01,  2.1887e-01,\n",
      "         1.0168e-01, -1.6138e-01, -1.8140e-01,  8.3801e-02, -5.2582e-02,\n",
      "        -3.2422e-01, -2.3230e-01, -7.1533e-01, -5.7959e-01,  5.3162e-02,\n",
      "         1.6638e-01, -6.6162e-02, -2.2131e-01, -3.2135e-02, -1.5137e-01,\n",
      "        -5.5084e-02, -1.3818e-01, -2.9761e-01,  1.2659e-01, -2.5421e-02,\n",
      "        -8.9233e-02, -4.6729e-01,  1.1654e-03,  1.9971e-01, -1.5173e-01,\n",
      "        -2.1375e-01, -3.7305e-01, -5.0201e-02, -4.1162e-01,  1.7859e-01,\n",
      "        -3.5107e-01, -2.0154e-01,  1.1853e-01,  2.6514e-01, -1.4038e-01,\n",
      "        -8.1299e-02, -1.5100e-01,  1.8164e-01,  3.5400e-01, -8.4106e-02,\n",
      "         2.9614e-01,  1.2061e-01, -1.1310e-01, -7.3730e-01, -4.5013e-02,\n",
      "        -1.2866e-01,  8.7830e-02,  2.0996e-01, -4.1040e-01,  1.6101e-01,\n",
      "         2.9648e-02,  3.1860e-01, -1.1664e-01, -1.7090e-01,  3.9966e-01,\n",
      "         1.4702e-02,  2.9175e-01,  5.5603e-02,  3.7746e-03,  3.5248e-02,\n",
      "         1.8481e-01,  2.3779e-01, -4.0356e-01, -1.0223e-01, -2.1460e-01,\n",
      "         1.4563e-01,  2.6123e-01,  1.3940e-01, -4.5020e-01,  1.7346e-01,\n",
      "         1.5793e-02, -1.6467e-01, -9.4299e-02, -2.7905e-01,  2.7856e-01,\n",
      "        -3.4180e-01, -9.0027e-02, -1.4124e-01,  2.5415e-01,  2.3828e-01,\n",
      "         1.7810e-01, -3.4521e-01,  4.4586e-02,  9.3933e-02,  3.4497e-01,\n",
      "        -3.4131e-01,  3.7378e-01, -1.8646e-02,  1.0883e-01,  1.1450e-01,\n",
      "         1.8994e-01, -3.2440e-02, -3.1543e-01,  3.5596e-01,  1.7590e-01,\n",
      "        -3.6670e-01, -4.2798e-01,  2.1130e-01,  6.0089e-02,  4.2065e-01,\n",
      "        -7.1045e-02, -1.7822e-02, -6.9763e-02,  1.6895e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8266,  0.1181, -0.1437,  ...,  0.5815,  0.9904, -0.3220],\n",
      "         [ 0.6936,  0.3592, -0.2110,  ...,  0.7675,  0.9359, -0.2581],\n",
      "         [ 0.9139,  0.1309, -0.2078,  ...,  0.4796,  0.9767, -0.3912],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.6079,  1.0527,  0.9092,  0.9741,  0.8452,  0.2532,  0.3232,  0.9648,\n",
      "         0.6201,  0.6338,  0.3694,  0.5596,  0.5566,  0.2749,  1.0059,  0.3599,\n",
      "         0.6514,  0.4392,  0.7178,  0.7759,  0.1261,  0.5068,  0.0789,  0.4839,\n",
      "         0.7666,  0.4697,  1.0713,  0.8247,  0.6299,  0.8442,  0.7114,  0.8662,\n",
      "         0.3779,  0.6348,  0.3445,  0.2316,  0.7866,  0.6455,  0.2927,  0.2069,\n",
      "         0.8140, -0.0258,  0.5015,  0.0589,  0.8584, -0.1604,  0.1794,  0.5073,\n",
      "         0.6606,  0.0820,  0.5801,  0.8149,  0.1337,  0.6128,  0.3276,  0.2377,\n",
      "         0.1110,  0.3906,  0.5469, -0.0185,  0.8862,  0.6611,  0.4146,  0.5488,\n",
      "         0.5571,  0.5869,  0.0318,  0.4573,  0.8218,  0.0858,  0.9722,  0.6167,\n",
      "        -0.0772,  0.8389,  0.0730,  0.4338,  0.0017,  1.0908,  0.4810,  1.0488,\n",
      "         0.3982,  0.4448,  0.1421,  0.8887,  0.2578,  0.5337,  0.4067,  0.5615,\n",
      "         0.0797,  0.7090,  0.4258,  0.5098,  1.0137,  0.3235,  0.8550,  0.6021,\n",
      "         1.0244,  0.9395,  0.2527,  0.2595,  0.6670,  0.5649,  0.2686,  0.5439,\n",
      "         0.6636,  0.5034, -0.2122,  0.3813,  0.4854,  0.6675,  0.1000,  0.7251,\n",
      "         0.3706,  0.4272,  0.9854,  0.4050,  0.8623,  0.4695,  0.7065,  0.7461,\n",
      "         0.7710,  0.8413,  0.5293,  0.3245,  0.7896,  0.9106,  0.7373,  0.7617,\n",
      "         0.8027,  0.1350,  0.7432,  0.3787,  0.4363,  0.7065,  0.8179,  0.4302,\n",
      "         0.4041,  0.8989,  0.1028,  0.6460, -0.2605,  0.6250,  1.0723,  0.3154,\n",
      "         0.3179,  0.2817,  0.4873,  0.2299,  0.4839,  0.7715,  0.8135,  0.3801,\n",
      "         0.8037,  0.0146,  0.8276,  0.7407,  0.9990,  0.7832,  0.6035,  0.7520,\n",
      "         0.1626,  0.3960,  0.3486, -0.1353,  0.4009,  0.2085,  0.4460,  0.7666,\n",
      "         0.2428,  0.8682,  0.7236,  0.9844,  0.7324,  1.1328,  0.3296, -0.0943,\n",
      "         0.7710, -0.0327,  0.4343,  0.3201,  0.6665,  0.5791,  0.5874,  0.7261,\n",
      "         0.6191,  0.0901,  0.9312,  0.4263,  0.6753,  0.5879,  0.3936,  0.8403,\n",
      "         0.8037,  0.3909,  0.2445,  0.7915,  0.4365,  0.6831,  0.0286,  0.3647,\n",
      "         0.7202,  0.1071,  0.6992,  0.3313,  0.4729,  0.5640,  0.3171,  0.8257,\n",
      "         0.8691,  1.0049,  0.8535,  0.6055,  0.8936,  1.0215,  1.2695,  0.9517,\n",
      "         0.7017,  0.7144,  0.8184,  0.9346,  0.9585,  1.0859,  1.0498,  1.2598,\n",
      "         0.8447,  0.6489,  0.6934,  0.8164,  0.9565,  0.8442,  1.1064,  1.0967,\n",
      "         0.6353,  0.5542,  1.4326,  0.9150,  0.8472,  0.4771,  1.0566,  0.7729,\n",
      "         1.1641,  1.1660,  0.9072,  0.8052,  1.1738,  0.4592,  0.4429,  0.4023,\n",
      "         0.6875, -0.0724,  1.0381,  0.0701,  0.6528,  0.3718,  0.4702,  0.7935,\n",
      "        -0.1372,  0.4631,  0.2026,  0.3015,  0.8867,  0.4399,  0.3000,  1.1465,\n",
      "         0.8804,  0.9097,  0.8032,  0.9009,  1.0479,  1.0898,  1.0977,  0.7397,\n",
      "         0.8677,  0.6543,  0.8623,  0.4009,  0.4011,  0.4287,  0.5366,  0.5513,\n",
      "         0.1880,  0.9053,  0.6221,  0.7905,  0.9238,  0.8276,  0.1642,  0.5981,\n",
      "         0.5605,  0.7842,  1.0869,  0.4639,  0.6616,  0.3826,  0.2803,  0.8794,\n",
      "         0.3110,  1.0381,  0.7134,  0.3459,  0.9775,  0.4985,  0.4766, -0.0566,\n",
      "         0.4326,  0.6440,  0.8682,  0.7939,  0.6465,  0.6968,  0.9204,  0.8252,\n",
      "         0.6826,  0.9971,  0.8203,  1.1260,  0.4504,  1.0742,  0.6284,  0.2269,\n",
      "         0.3579,  1.0635,  0.5581,  0.8354,  0.6284,  0.6294,  1.2510,  0.9619,\n",
      "         0.7681,  0.6392,  0.8022,  0.6699,  1.0723,  0.9043,  0.7769,  0.4031,\n",
      "         0.7974,  0.8989,  1.0391,  0.4656,  0.6562,  0.8223,  0.6509,  0.6787,\n",
      "         1.0166,  1.0918,  1.3467,  0.9834,  1.0020,  0.5537,  0.7656,  0.9053,\n",
      "         0.7871,  0.4546,  0.6758,  0.8052,  0.7881,  0.5391,  0.6841,  0.3208,\n",
      "         0.2603,  0.0295,  0.4985,  0.2111,  0.4248,  0.8242,  0.9307,  0.5708,\n",
      "         0.8164,  1.0283,  0.7476,  0.3767,  0.7437,  0.8203,  0.4819,  0.9102,\n",
      "         1.0957,  0.5679,  1.0820,  0.3589,  0.9531,  0.5493,  0.6255,  0.7251,\n",
      "         0.3984,  0.8442,  0.7031,  1.4316,  0.9961,  0.7583,  1.5225,  0.6909,\n",
      "         0.3411,  0.4966,  0.7837,  0.9497,  1.3545,  0.8896,  0.5337,  0.1519,\n",
      "         1.1680,  0.7246,  0.6621,  0.6250,  0.6470,  0.6201,  0.4873,  0.8716,\n",
      "         0.6226,  1.2275,  0.8647,  0.6094,  0.5947,  0.8013,  0.8140,  0.7773,\n",
      "         1.0547,  0.9434,  0.8213,  0.4102,  0.9482,  0.7524,  0.6929,  1.0645,\n",
      "         0.7065,  0.6875,  0.8525,  0.9854,  0.3501,  0.4106,  0.6646,  0.8252,\n",
      "         0.6387,  1.0146,  0.7476,  1.0566,  0.3936,  1.0039,  0.5151,  0.1534,\n",
      "         0.7095,  1.0820,  1.2432,  0.8687,  0.5503,  0.6201,  0.8091,  0.7949,\n",
      "         0.7007,  1.0479,  0.9277,  0.7959,  0.4131,  0.9292,  0.5142,  0.3813,\n",
      "         0.7256,  0.4736,  0.6533,  0.7422,  0.6143,  0.9590,  0.5083,  1.1006,\n",
      "         0.5952,  0.3530,  0.6587,  0.7109,  0.5464,  0.4343,  0.2352,  0.2966,\n",
      "         0.3826,  0.6997,  0.7466,  0.6646,  0.9121,  0.4985,  0.7368,  1.1953,\n",
      "         0.7007,  0.5684,  0.7085,  1.1338,  0.5498,  1.0713,  0.4001,  1.0010,\n",
      "         0.6914,  0.5410,  0.5996,  0.7920,  0.6577,  0.8550,  0.9023,  0.6016,\n",
      "         0.9863,  0.8696,  0.4663,  1.1982,  0.5879,  0.9473,  0.5791,  0.7510,\n",
      "         0.8403,  0.3623,  0.6240,  1.0752,  0.6475,  0.4382,  0.6504,  0.7788,\n",
      "         0.5942,  0.7515,  0.8643,  0.6831,  0.6562,  0.5156,  0.5146,  0.2832,\n",
      "         0.1936,  0.6724,  0.8184,  0.3799,  0.5708,  0.2668,  0.8491,  0.3975,\n",
      "         0.7163,  0.8540,  0.7441,  0.8906,  0.4707,  0.3904,  0.4282,  0.6025,\n",
      "         0.6631,  0.8525,  0.8267,  0.9805,  1.1670,  0.3020,  0.6479,  0.5605,\n",
      "         0.4097,  0.7583,  0.9604,  0.7119,  0.8638,  0.5776,  0.8569,  0.4075,\n",
      "         0.3037,  0.2979,  0.5552,  0.6558,  0.3591,  0.5825,  0.2966,  0.0604,\n",
      "         0.4246,  0.1323,  0.1252,  0.2866,  0.4707,  0.2668,  0.7759,  1.0859,\n",
      "         1.2461,  0.9517,  1.1768,  1.3398,  0.9424,  0.8193,  0.6846,  0.7363,\n",
      "         1.0527,  1.0615,  0.2864,  0.2098,  0.5869,  1.4287,  0.9863,  0.7256,\n",
      "         0.7041,  0.9214,  0.9229,  0.8628,  1.1104,  1.0166,  0.5117,  0.1154,\n",
      "         1.2139,  0.8750,  0.7554,  1.2812,  0.7212,  0.5708,  0.4622,  0.3638,\n",
      "         0.9067,  0.6401,  0.8486,  1.0020,  0.4382,  0.7119,  0.6250,  1.2949,\n",
      "         0.7378,  0.7095,  0.2566,  1.3145,  0.9595,  0.7466,  0.6826,  0.8003,\n",
      "         0.9194,  1.2852,  0.8765,  0.4185,  1.3125,  0.9902,  0.5728,  1.3262,\n",
      "         0.9814,  1.0059,  0.3989], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1239, -0.2664, -0.1713, -0.1823, -0.1154,  0.0171, -0.4211, -0.1108,\n",
      "        -0.2502, -0.7500, -0.3247, -0.3979,  0.0051, -0.2125, -0.0717, -0.4431,\n",
      "        -0.4097,  0.2125, -0.0437, -0.0866, -0.0712,  0.7002, -0.1267, -0.3328,\n",
      "         0.0872, -0.3110,  0.1057,  0.1019,  0.0423, -0.2479,  0.1603, -0.2520,\n",
      "         0.1962, -0.2028,  0.2776, -0.2328, -0.1515, -0.3669,  0.3230, -0.2039,\n",
      "        -0.1226, -0.6484,  0.2900,  0.0220, -0.0112, -0.2489,  0.0193,  0.1302,\n",
      "         0.2474, -0.2942,  0.0175,  0.2148, -0.3555,  0.0171, -0.0213, -0.1482,\n",
      "        -0.0490, -0.0268, -0.3545, -0.2886, -0.3489, -0.4358,  0.1648, -0.2571,\n",
      "        -0.7314, -0.3865, -0.3762, -0.3130, -0.4067, -0.3572, -0.2798, -0.3245,\n",
      "        -0.2000, -0.4282, -0.6748,  0.0440, -0.2201,  0.0217, -0.3865, -0.5210,\n",
      "        -0.1193, -0.1553, -0.3354, -0.4883, -0.6338, -0.3184, -0.2969, -0.3962,\n",
      "        -0.2607, -0.5088,  0.1388, -0.2372, -0.6943, -0.1432,  0.2163,  0.2421,\n",
      "        -0.0103, -0.0153, -0.2074, -0.1065, -0.2491,  0.1720, -0.3198, -0.2445,\n",
      "        -0.1711, -0.1203,  0.0286, -0.2401, -0.1466, -0.0564, -0.2615, -0.4399,\n",
      "         0.1931, -0.1694, -0.1415, -0.7656, -0.0859, -0.0993,  0.3477,  0.2240,\n",
      "        -0.1038,  0.2642,  0.1589, -0.3196,  0.3140,  0.0177,  0.1268,  0.0083,\n",
      "        -0.0560, -0.6299,  0.0184,  0.2313,  0.3521, -0.0641, -0.4426,  0.1844,\n",
      "        -0.1841, -0.2124, -0.8052,  0.0175, -0.4182, -0.1632, -0.0876, -0.4946,\n",
      "        -0.1711, -0.0528, -0.1726, -0.6758, -0.3325, -0.2988, -0.6162,  0.1991,\n",
      "        -0.2032, -0.1444,  0.1151,  0.1190, -0.1177, -0.2551, -0.0131, -0.2223,\n",
      "        -0.4717, -0.0286, -0.0767, -0.3767,  0.1379, -0.0890,  0.0168,  0.0533,\n",
      "        -0.1142,  0.1711,  0.2214, -0.1261, -0.2651, -0.2759, -0.2568, -0.5933,\n",
      "        -0.0662, -0.6050,  0.0856, -0.3936, -0.0298,  0.0372,  0.0246, -0.0950,\n",
      "        -0.3477, -0.1741,  0.0258,  0.0046,  0.3240,  0.0230, -0.0986,  0.0180,\n",
      "         0.5317, -0.0308, -0.4104, -0.1055, -0.5283, -0.1559, -0.6230, -0.0384,\n",
      "        -0.1599, -0.3762, -0.2615,  0.0108, -0.6421,  0.0398, -0.1394,  0.1150,\n",
      "        -0.0145, -0.4172,  0.0502, -0.1700, -0.4902, -0.2832, -0.1782,  0.4458,\n",
      "        -0.1987,  0.2676,  0.1052,  0.3694, -0.2637, -0.2910,  0.1091, -0.1252,\n",
      "         0.4031, -0.1478,  0.2272,  0.0098,  0.4768, -0.2639, -0.2568,  0.0545,\n",
      "        -0.7627, -0.0178, -0.3142, -0.0518, -0.1273, -0.0859,  0.0906,  0.1376,\n",
      "        -0.1974,  0.0512, -0.0317, -0.0626, -0.3716, -0.7192, -0.2001, -0.4417,\n",
      "        -0.1807, -0.3147, -0.5449, -0.4539, -0.4373,  0.2142, -0.0305, -0.1595,\n",
      "        -0.3328,  0.6685, -0.0493, -0.3616,  0.1357, -0.2019, -0.2761,  0.0025,\n",
      "        -0.6035, -0.3232, -0.2242, -0.0489, -0.2125, -0.1859, -0.1860,  0.0300,\n",
      "        -0.0356,  0.0600, -0.2269, -0.4756, -0.0302,  0.0040,  0.0407, -0.1997,\n",
      "        -0.5518, -0.2103,  0.0276, -0.0992, -0.3352, -0.0690, -0.0980,  0.1700,\n",
      "        -0.0489, -0.0948, -0.1698,  0.1109, -0.1960,  0.1160, -0.1931, -0.3921,\n",
      "        -0.4785,  0.0561, -0.0026, -0.2654,  0.1337,  0.0565, -0.0258, -0.4607,\n",
      "         0.3374, -0.0170, -0.4185, -0.2688,  0.0260, -0.0998,  0.1022, -0.0117,\n",
      "         0.1586, -0.3318,  0.0787, -0.1823,  0.2275,  0.1383,  0.7744,  0.1203,\n",
      "        -0.3833, -0.0250, -0.1124,  0.1145,  0.1013,  0.1846, -0.1378,  0.1403,\n",
      "        -0.2137,  0.0980,  0.3110, -0.1053,  0.1887,  0.2354, -0.5234,  0.2096,\n",
      "        -0.3342, -0.1058,  0.1194,  0.1594,  0.0444,  0.0460, -0.4065, -0.0467,\n",
      "        -0.0948, -0.3650,  0.1289,  0.0040,  0.1436, -0.1247,  0.1445, -0.2406,\n",
      "         0.2095,  0.1884,  0.0166,  0.0086, -0.0839,  0.0136,  0.1023,  0.0319,\n",
      "         0.0486, -0.1558,  0.7534,  0.1080, -0.3848, -0.0607,  0.3115,  0.0296,\n",
      "         0.3525,  0.0865, -0.0633, -0.0517,  0.1947,  0.2367, -0.1138,  0.0871,\n",
      "        -0.0118, -0.0790, -0.0767, -0.4294,  0.0158, -0.1014,  0.0406, -0.4805,\n",
      "         0.2081, -0.1032, -0.1923, -0.1456, -0.1224, -0.2328, -0.0853, -0.6567,\n",
      "         0.0102, -0.1077, -0.1081, -0.1026,  0.0066,  0.1726, -0.0775, -0.0175,\n",
      "         0.1746,  0.1160,  0.0527, -0.4375,  0.2539,  0.0877, -0.4333,  0.1130,\n",
      "        -0.2864, -0.0112,  0.5347, -0.0796,  0.3608,  0.1393,  0.4438, -0.1454,\n",
      "        -0.1301,  0.3242, -0.3965,  0.2133, -0.0049,  0.0396, -0.0137,  0.1993,\n",
      "         0.1604,  0.0569,  0.0972,  0.2373,  0.0964, -0.1013,  0.0600,  0.1139,\n",
      "        -0.4653,  0.0387,  0.2242,  0.0309, -0.1793,  0.0516,  0.2235,  0.4414,\n",
      "         0.2085,  0.1285,  0.0853,  0.6001, -0.1279,  0.4014,  0.2352,  0.5811,\n",
      "        -0.0288, -0.1151,  0.3733, -0.2930,  0.2113, -0.1140,  0.0476, -0.1931,\n",
      "         0.0218,  0.0759,  0.3481,  0.3418,  0.1522, -0.0414,  0.1147, -0.3586,\n",
      "        -0.1332,  0.0576, -0.5864,  0.2469, -0.5269,  0.1470, -0.0678, -0.0068,\n",
      "         0.0124,  0.2820,  0.2274,  0.1709,  0.0313,  0.0958,  0.0434,  0.0828,\n",
      "         0.1605,  0.2267,  0.0839,  0.0365,  0.1979,  0.2710, -0.0498, -0.1508,\n",
      "         0.2517, -0.0324,  0.0598,  0.2109,  0.3774,  0.2391,  0.3157,  0.2198,\n",
      "         0.2433,  0.2625, -0.1058,  0.3135, -0.0497,  0.3782, -0.0935, -0.0360,\n",
      "        -0.0262,  0.1946,  0.3933,  0.1456, -0.1836,  0.1141,  0.2225, -0.3416,\n",
      "         0.1344, -0.4153, -0.0804, -0.4558, -0.0991, -0.6138, -0.2778, -0.0316,\n",
      "        -0.5903,  0.1075,  0.1616,  0.1315,  0.6875,  0.0750,  0.0168, -0.1858,\n",
      "         0.0195, -0.2749, -0.2030, -0.5107, -0.1002, -0.5898, -0.0054, -0.7056,\n",
      "        -0.5225,  0.0583,  0.2035,  0.3127,  0.1239, -0.0339, -0.1362, -0.2089,\n",
      "        -0.5171, -0.0357,  0.1447, -0.0847, -0.1213,  0.2025,  0.2042,  0.3743,\n",
      "         0.0844,  0.2839,  0.0065, -0.1385,  0.0842,  0.2974,  0.4014, -0.0850,\n",
      "         0.2952,  0.0201,  0.0210,  0.0227,  0.4785,  0.0237, -0.4707, -0.3560,\n",
      "        -0.0361,  0.0154, -0.2228, -0.0186, -0.0056, -0.8018, -0.3398, -0.3579,\n",
      "         0.0644, -0.0556, -0.6714, -0.0506, -0.1060,  0.0017,  0.5220, -0.0755,\n",
      "         0.3789,  0.2142,  0.3726, -0.2189, -0.1249,  0.1892, -0.2825, -0.3005,\n",
      "         0.0942,  0.0932, -0.0481, -0.1805, -0.7227,  0.0207, -0.0980, -0.0015,\n",
      "         0.1162, -0.4985, -0.4917,  0.1016, -0.0778,  0.0712,  0.3647,  0.4583,\n",
      "         0.5430,  0.2634,  0.1359, -0.0568,  0.6084, -0.0955,  0.4482,  0.3240,\n",
      "        -0.1129, -0.0239,  0.1240, -0.0478,  0.2075, -0.2311, -0.1126, -0.1622,\n",
      "        -0.2177, -0.2515,  0.2020], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.8750,  0.2417, -0.2270,  ...,  0.5883,  0.7821, -0.4677],\n",
      "         [ 0.3378,  0.4438, -0.1509,  ...,  0.3848,  0.4370, -0.2615],\n",
      "         [ 0.8610,  0.2989, -0.3066,  ...,  0.4715,  0.5705, -0.5287],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.5913, 1.0000, 0.7070,  ..., 0.9492, 0.9248, 0.4658], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.3171,  0.0235, -0.0295,  ..., -0.1465, -0.1620,  0.1923],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.6201, -0.0659, -0.4060,  ...,  0.4224,  0.5636, -0.4233],\n",
      "         [ 0.8755, -0.0316,  0.0660,  ...,  0.7669,  0.5863, -0.2242],\n",
      "         [ 0.3931,  0.1254,  0.0950,  ...,  0.6467,  0.4413, -0.0400],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.7012,  0.8735,  0.7056,  0.5254,  0.5913,  0.8735,  0.3716,  0.8867,\n",
      "         0.6211,  0.7612,  0.6235,  0.9590,  0.5923,  0.6528,  0.9561,  0.3501,\n",
      "         0.9990,  0.9077,  0.7852,  0.4172,  0.8574,  0.6758,  0.8262,  0.9414,\n",
      "         0.7349,  0.6846,  0.1148,  0.1703,  0.1666,  1.0117,  0.8701,  0.7051,\n",
      "         0.4167,  0.9121,  0.4353,  0.8716,  0.1450,  0.2737,  0.4480,  0.7539,\n",
      "         0.5879,  0.6318,  1.0820,  0.8608,  0.6572,  0.0243,  0.4668, -0.0132,\n",
      "         0.2430,  0.2345,  0.4041,  0.4312,  0.8574,  0.6895,  0.6895,  0.7690,\n",
      "         0.9033,  0.7222,  0.1897,  0.4404,  0.8003,  0.6074,  0.9106,  0.5249,\n",
      "         0.1614,  0.8433,  0.2815,  0.8672,  0.6729,  0.2408,  0.3599,  0.0992,\n",
      "         0.2725,  0.0111,  0.8472,  0.1418,  0.2469,  0.3816,  0.7046,  0.5068,\n",
      "         0.0842,  0.6479,  0.4321,  0.6841,  0.6812,  0.3689,  0.9526,  0.6895,\n",
      "         1.0010,  0.3220,  0.9214,  0.8066,  0.5986,  0.9351,  0.2190,  0.2979,\n",
      "         0.3894,  0.8911,  0.7017,  0.4363,  0.5977,  0.1813,  0.9165,  0.1190,\n",
      "         0.9990,  0.3420,  1.0449,  0.7920,  0.3311,  0.5898,  0.6138,  0.2756,\n",
      "         0.8154,  0.7671,  0.8281,  0.7759,  0.8979,  0.7710,  0.9077,  0.4380,\n",
      "         0.8589,  0.6855,  0.8418,  0.5244,  0.2703,  0.2991,  0.7827,  0.2134,\n",
      "         0.5098,  0.6880,  0.0986,  0.4597,  0.3594,  0.3970,  0.8999,  0.1750,\n",
      "         0.3098,  0.3706,  0.8169,  0.4922,  0.4014,  0.9204,  0.2211,  0.5718,\n",
      "         0.7783,  0.9961,  0.3025,  0.5903,  0.8872,  0.5874,  0.9277,  0.2734,\n",
      "         0.3347,  0.4521,  0.8022,  0.3379,  0.5996,  1.0586,  1.2812,  0.9199,\n",
      "         0.7559,  0.7417,  0.8872,  0.3533,  0.5283,  0.6362,  1.1973,  0.8696,\n",
      "         0.4946,  0.9736,  0.9116,  0.5635,  0.5830,  0.2759,  0.5635,  0.6807,\n",
      "         0.1498,  0.8701,  0.7773,  0.7930,  0.7080,  0.8296,  0.8237,  0.9980,\n",
      "         0.5771,  0.9355,  0.7134,  0.8271,  0.4636,  0.3179,  0.2230,  0.9321,\n",
      "         0.4026,  0.8481,  0.7764,  0.1022,  0.5112,  0.2131,  0.4514,  0.1749,\n",
      "         0.4563,  0.9238,  0.1625,  0.2942,  0.3987,  0.7612,  0.5776,  0.7061,\n",
      "         1.0303,  0.7432,  1.1133,  0.7734,  0.2646,  0.3347,  0.3733,  1.0361,\n",
      "         0.5137,  0.4902,  0.7310,  0.1998,  1.1025,  0.3552,  0.6504,  0.6865,\n",
      "         1.1113,  0.7969,  0.5439,  0.4995,  0.3999,  0.5488,  0.9844,  0.2886,\n",
      "         0.3491,  0.5562,  0.8101,  0.4937,  0.3792,  0.2235,  1.1250,  0.6685,\n",
      "         0.3145,  0.8091,  0.1998,  1.1172,  0.3071,  0.6758,  0.7539,  0.9360,\n",
      "         0.7217,  0.2603,  0.2603,  1.0352,  0.4951,  0.5723,  0.0433,  1.0381,\n",
      "         0.6240,  0.6650,  0.8594,  1.0293,  0.3623,  0.6548,  0.3772,  0.3540,\n",
      "         0.7466,  0.7744,  0.7876,  0.2202,  1.0029,  0.8813,  1.0391,  1.0537,\n",
      "         1.0430,  0.3337,  0.6436,  0.5059,  0.2966,  0.8135,  0.6948,  0.3906,\n",
      "         0.3181,  0.7690,  0.6094,  0.9043,  0.7451,  0.3765,  1.4004,  0.7715,\n",
      "         0.5200,  0.3640,  0.5112,  0.9448,  1.1484,  0.4744,  0.7871,  0.4902,\n",
      "         0.5732,  0.1054,  0.5049,  0.0604,  0.7632,  0.2773,  0.4031,  0.9009,\n",
      "         0.7471,  0.5449,  1.0117,  1.2070,  0.8501,  0.8457,  1.1885,  0.4006,\n",
      "         0.4363,  0.7446,  0.0552,  0.7671,  0.9634,  0.5771,  0.2654,  0.6909,\n",
      "         0.5205,  0.9653,  0.7520,  0.7656,  0.7656,  0.9375,  0.1949,  0.9199,\n",
      "         0.6694,  0.7573,  0.2603,  0.7554,  0.8413,  0.6187,  0.4634,  0.5049,\n",
      "         1.0029,  0.4824,  0.7124,  0.6001,  1.1631,  0.9590,  0.5083,  0.7266,\n",
      "         0.9961,  0.4829,  0.9038,  0.5552,  0.9473,  1.0811,  1.1807,  0.4907,\n",
      "         0.5747,  0.9526,  0.9844,  0.9609,  0.5986,  0.1338,  1.0781,  0.8760,\n",
      "         1.0176,  0.8218,  0.5928,  0.4937,  0.8550,  0.7827,  0.5161,  0.2401,\n",
      "         1.0059,  0.9556,  0.9038,  0.8428,  0.6006,  0.3806,  0.6436,  0.8110,\n",
      "         0.3938,  0.4583,  0.3357,  0.6426,  0.7119,  0.5015,  0.8755,  0.4031,\n",
      "         0.3120,  1.1660,  0.2766,  0.9243,  0.4011,  0.6650,  1.0088,  0.5249,\n",
      "         0.5283,  0.5903,  0.4873,  0.5635,  0.9326,  0.5005,  0.4780, -0.0583,\n",
      "         0.3684,  0.4634,  0.2703,  0.0307,  1.0645,  0.7686,  0.9150,  0.8018,\n",
      "         0.1416,  0.1774,  0.9600,  0.9663,  0.5723,  0.3931,  0.4941,  0.7646,\n",
      "         0.4292,  0.5190,  0.7114,  0.7246,  0.7812,  0.4995,  0.7100,  0.2303,\n",
      "         0.1945,  0.7144,  0.8276,  0.4844,  0.5400,  0.4604,  0.5024,  0.7139,\n",
      "         0.4180,  0.7861,  0.5239,  0.6768,  0.6196,  0.4336,  0.7007,  0.1166,\n",
      "         0.1348,  0.5752,  0.5239,  0.6587,  0.6494,  0.1793,  0.8735,  0.8848,\n",
      "         0.4561,  0.7349,  0.5728,  0.8789,  0.5459,  0.3835,  0.1799,  0.1904,\n",
      "         0.2347,  0.9814,  0.4246,  1.0859,  0.9692,  0.5596,  0.1591,  0.8477,\n",
      "         0.4639,  0.1848,  0.1388,  0.8159,  0.1884,  0.6274,  0.3616,  1.0498,\n",
      "         0.6719,  0.7305,  0.9043,  0.1737,  0.4128,  0.3389,  0.5308,  0.1633,\n",
      "         0.1270,  0.6558,  0.3933,  0.0294,  0.2710, -0.1781,  0.1088,  0.4648,\n",
      "         0.6011,  0.7075,  0.6641,  0.5718,  0.3730,  0.8247,  0.5459,  0.6387,\n",
      "         0.6216,  0.8022,  0.8301,  0.5713,  0.6538,  0.6240,  0.8252,  0.5161,\n",
      "         0.6060,  0.7139,  0.5322,  0.4443, -0.0547,  0.0828,  0.9429,  0.5552,\n",
      "         0.6440,  0.5859,  0.7378,  0.5786,  0.5635,  0.5527,  0.5210,  0.2834,\n",
      "         0.7817,  0.4646,  0.4524,  0.6958,  0.8711,  0.6372,  0.2969,  0.5562,\n",
      "         0.3503,  0.3662,  0.7891,  0.5806,  0.7061,  0.6519,  0.8613,  0.5874,\n",
      "         0.7026,  0.9067,  0.4492,  0.7520,  0.4446,  0.6938,  0.5283,  1.1357,\n",
      "         0.9219,  0.2974,  0.6689,  0.4927,  1.0068,  0.6655,  0.6455,  1.0234,\n",
      "         0.7163,  0.7998,  0.5146,  0.8408,  0.5713,  0.8750,  0.3193,  0.7065,\n",
      "         0.2073, -0.1075,  0.6064,  0.8545,  0.0643,  1.0156,  0.6128,  0.6748,\n",
      "         0.0413,  0.7944,  0.4651,  0.3408,  0.3782,  0.4512,  0.4753,  0.3193,\n",
      "         0.7886,  0.8545,  0.4353,  0.5161, -0.0427,  0.7222,  0.2166,  0.7900,\n",
      "         0.4597,  0.9678,  0.5366,  0.9385,  0.4966,  0.2411,  0.7812,  0.0564,\n",
      "         0.5493,  0.6777,  0.6011, -0.1464,  0.5972,  0.2094,  0.6870,  0.8652,\n",
      "         0.2952,  0.7759,  0.6455,  0.3582,  0.7329,  0.2959,  0.5127,  0.7549,\n",
      "         0.4805,  0.7339,  0.6968,  0.2842,  0.4358,  0.7676,  0.5273,  0.6802,\n",
      "         0.5273,  0.6401,  0.2605,  0.5825,  0.4819,  0.4473,  0.5762,  0.5405,\n",
      "         1.1240,  0.9312,  0.5210,  0.6763,  0.4219,  0.3811,  0.5200,  0.5942,\n",
      "         0.8325,  1.0771,  0.7842,  0.4526,  0.7007,  0.3108,  0.5581,  0.2482,\n",
      "         0.4578,  0.7705,  0.4932,  0.5747,  0.1030,  0.3357,  0.7075,  0.6572,\n",
      "         0.8857,  0.3496,  0.5405,  0.2416,  0.4150,  0.2705,  0.8481,  0.2849,\n",
      "         0.3606,  0.4424,  0.7671,  0.8677,  0.3550,  0.8818,  0.1348,  0.9429,\n",
      "         0.6069,  0.4109,  0.6235,  0.7422,  0.6323,  0.2325,  0.1437,  0.6768,\n",
      "         0.1407,  0.7559,  0.6079,  0.5381,  0.8877,  0.6304,  0.7300,  0.7778,\n",
      "         0.7725,  0.3306,  0.3137,  0.4666,  0.3032,  0.5552,  0.8364,  0.6963,\n",
      "         0.6680,  0.8657,  0.7744,  0.6699,  0.7344,  0.6084,  0.4751,  0.7119,\n",
      "         0.7275,  0.5576,  0.5752,  0.4932,  0.9863,  0.9390,  0.7568,  0.5049,\n",
      "         0.8042,  0.6118,  0.7422,  1.1045,  0.8535,  0.9185,  0.9663],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.2021, -0.1726,  0.3303,  0.0771,  0.1847,  0.0298, -0.4976,  0.2842,\n",
      "        -0.0163,  0.4717,  0.1022,  0.0113, -0.5518, -0.3926, -0.1141, -0.3689,\n",
      "        -0.2455, -0.1813, -0.0733,  0.2209,  0.0938,  0.2373,  0.0314,  0.0914,\n",
      "         0.1266,  0.0866, -0.5908,  0.1277, -0.4431,  0.5806, -0.0091,  0.0039,\n",
      "        -0.6987,  0.1211,  0.0059,  0.1154,  0.0140,  0.1787,  0.1219, -0.2131,\n",
      "         0.1398, -0.1025,  0.5972,  0.0406, -0.4912, -0.4431, -0.0410,  0.3398,\n",
      "         0.1012, -0.6255, -0.6406, -0.4331,  0.2861,  0.0887,  0.4478,  0.2578,\n",
      "         0.0816,  0.7651,  0.0875,  0.2920,  0.6934, -0.0291,  0.3232,  0.0957,\n",
      "        -0.0993,  0.5815,  0.2949,  0.0363, -0.0446, -0.7388, -0.0955, -0.0139,\n",
      "        -0.1931,  0.3860,  0.0409, -0.0673,  0.1221,  0.0823, -0.0545,  0.3022,\n",
      "        -0.2445, -0.2744, -0.3601, -0.2230,  0.2214,  0.2242, -0.0965, -0.0511,\n",
      "         0.3147,  0.2239, -0.1279, -0.0592, -0.8970, -0.1148, -0.1138,  0.0750,\n",
      "         0.0042, -0.4194,  0.0453, -0.7026, -0.0307, -0.5854, -0.2839, -0.1445,\n",
      "         0.3972,  0.2137, -0.0523,  0.1093, -0.4272,  0.1931, -0.0608, -0.4043,\n",
      "         0.2615,  0.0620,  0.4709,  0.1953,  0.5464,  0.0164,  0.3611,  0.1140,\n",
      "         0.1591,  0.1223,  0.7432,  0.0093,  0.1088, -0.0167,  0.7070,  0.0597,\n",
      "         0.0972, -0.5239, -0.4775, -0.3232,  0.3914, -0.1059, -0.1792, -0.1506,\n",
      "         0.0590,  0.0146, -0.1945, -0.5444,  0.2068,  0.4167, -0.0427,  0.0053,\n",
      "         0.0635,  0.4612, -0.0508, -0.0502,  0.0344, -0.9307, -0.0521, -0.0666,\n",
      "         0.1182,  0.0335,  0.1688,  0.0484, -0.0015, -0.2175, -0.1119, -0.1030,\n",
      "        -0.0270, -0.1234,  0.5601,  0.0436,  0.1199,  0.3169,  0.0363,  0.1903,\n",
      "         0.1289,  0.3733, -0.0247, -0.3418,  0.0658, -0.6606, -0.4521,  0.3799,\n",
      "        -0.1197,  0.3130,  0.0742,  0.4885,  0.2186,  0.5874,  0.0129,  0.3862,\n",
      "         0.0392,  0.0672,  0.0811,  0.7188,  0.0652,  0.1598, -0.0314,  0.5273,\n",
      "         0.2419,  0.0035, -0.6489, -0.5586, -0.2333, -0.1406, -0.2384,  0.4021,\n",
      "         0.0559, -0.0417, -0.1237,  0.0789,  0.0309, -0.1226, -0.4453, -0.2009,\n",
      "        -0.1065,  0.0699, -0.1497,  0.0600, -0.8599, -0.1085, -0.7271,  0.4873,\n",
      "        -0.0080, -0.0163,  0.0947, -0.5645,  0.2910,  0.0411,  0.3862,  0.3894,\n",
      "        -0.3276, -0.0522, -0.8325,  0.0847,  0.2896, -0.0019, -0.0070, -0.0335,\n",
      "         0.1884,  0.0530,  0.0437,  0.0645, -0.0075, -0.1122, -0.2098,  0.0244,\n",
      "        -0.1711,  0.0927, -0.1071,  0.2964,  0.0812,  0.4231,  0.3967, -0.1255,\n",
      "         0.1821, -0.2559, -0.3623,  0.2155, -0.0679, -0.0886,  0.0531,  0.1813,\n",
      "         0.1626,  0.0365, -0.1181, -0.0457, -0.7261, -0.2600, -0.2152, -0.4541,\n",
      "        -0.2109, -0.0097, -0.0991, -0.7090,  0.0931,  0.0950, -0.0117, -0.7490,\n",
      "        -0.0139, -0.6318,  0.2174,  0.0062, -0.6255, -0.3884, -0.0773, -0.3428,\n",
      "        -0.1689, -0.1890, -0.2036,  0.2417, -0.0077, -0.2008, -0.4810, -0.3328,\n",
      "         0.1840, -0.2097, -0.5244, -0.1294, -0.3992, -0.4148, -0.1536, -0.3848,\n",
      "        -0.3750, -0.1231,  0.5073,  0.0580,  0.1772, -0.2893,  0.2878,  0.0240,\n",
      "         0.1482, -0.3560, -0.3640, -0.4397,  0.1046,  0.1168, -0.0066,  0.0692,\n",
      "        -0.1102,  0.1992, -0.0201,  0.2238,  0.0322, -0.4692, -0.2600, -0.3484,\n",
      "        -0.5630,  0.3101,  0.0542,  0.4355,  0.1514,  0.3032, -0.3503,  0.2910,\n",
      "         0.2961,  0.4285,  0.2595,  0.3367,  0.2993,  0.1848,  0.0796,  0.2578,\n",
      "        -0.0481, -0.4863, -0.2573, -0.2771,  0.0528,  0.2047,  0.3728,  0.0690,\n",
      "         0.1949, -0.1700,  0.0318,  0.0036,  0.2778,  0.1737, -0.3914, -0.0902,\n",
      "         0.2286,  0.2915, -0.4231, -0.2435,  0.0591, -0.0084, -0.0819,  0.2656,\n",
      "         0.0522, -0.0414,  0.0745,  0.1058, -0.0828,  0.1237, -0.0027, -0.3933,\n",
      "         0.1774, -0.1125,  0.2316, -0.0043, -0.4050,  0.2028,  0.2401, -0.0634,\n",
      "         0.0536, -0.2009,  0.1520, -0.1305, -0.0218,  0.1860,  0.0144, -0.5649,\n",
      "         0.0132, -0.5718, -0.1174,  0.3215,  0.3967,  0.2559,  0.5400,  0.4463,\n",
      "         0.3630,  0.2438,  0.1948,  0.4995,  0.1774, -0.2067,  0.2040,  0.1765,\n",
      "         0.1753,  0.2312,  0.1159,  0.0380,  0.0768,  0.2720,  0.2451,  0.2590,\n",
      "        -0.1098,  0.0349,  0.0880,  0.6387,  0.4351,  0.4011,  0.4036,  0.0018,\n",
      "         0.3250, -0.2708, -0.1252, -0.1060,  0.3118,  0.1124,  0.3247,  0.1724,\n",
      "         0.3977,  0.0533,  0.1210, -0.5396,  0.0768, -0.1135, -0.1404,  0.0514,\n",
      "        -0.0136, -0.1804, -0.0883,  0.0839,  0.3066,  0.0860,  0.2445,  0.1525,\n",
      "         0.3823,  0.1392,  0.0470,  0.3835,  0.1333, -0.0998,  0.1943,  0.2075,\n",
      "         0.0818,  0.4316, -0.0505,  0.4409, -0.0503, -0.4343, -0.3862,  0.2688,\n",
      "        -0.1967, -0.1615,  0.0198,  0.1388, -0.0398, -0.2484, -0.2007,  0.1655,\n",
      "         0.2214,  0.0895,  0.1964, -0.0087, -0.4155,  0.0015, -0.1443, -0.1368,\n",
      "        -0.3740,  0.0397, -0.3096, -0.5254,  0.2666, -0.3989, -0.1317, -0.3706,\n",
      "        -0.2959, -0.0498, -0.1389,  0.3103,  0.0934, -0.2515,  0.1032, -0.0542,\n",
      "        -0.0772,  0.2289,  0.0570, -0.3870,  0.2003,  0.0709,  0.0276,  0.3718,\n",
      "         0.1215,  0.2585,  0.0884, -0.0199,  0.3564,  0.1132,  0.2634, -0.4424,\n",
      "         0.0809, -0.1209,  0.2324,  0.0043, -0.1588,  0.0811,  0.4104,  0.0211,\n",
      "         0.2471,  0.1990,  0.1013,  0.2218,  0.1825,  0.3594,  0.1578,  0.1044,\n",
      "         0.1536,  0.2266,  0.3777,  0.2466, -0.0207, -0.2255, -0.4189,  0.1650,\n",
      "         0.2474, -0.3799,  0.1320,  0.0061,  0.3945,  0.2468,  0.4392, -0.0318,\n",
      "         0.1454,  0.3899,  0.1564,  0.1302,  0.4841,  0.2014, -0.1797, -0.0315,\n",
      "        -0.0792, -0.0293,  0.2988,  0.0671,  0.1084, -0.2810, -0.1680, -0.2450,\n",
      "         0.0800, -0.0523,  0.0262,  0.3931, -0.2452,  0.1868, -0.5400, -0.1247,\n",
      "        -0.1422,  0.2045,  0.0080,  0.0577, -0.2242, -0.1046, -0.0548, -0.0254,\n",
      "        -0.2311,  0.3140,  0.1826, -0.3899, -0.1246,  0.1124, -0.1131, -0.3350,\n",
      "        -0.1857, -0.1754,  0.0854,  0.0865, -0.2125,  0.3496,  0.0585,  0.2849,\n",
      "         0.2424,  0.0453, -0.2418,  0.3040,  0.0933, -0.2673,  0.0333, -0.3296,\n",
      "        -0.0509,  0.1969,  0.2583, -0.3394,  0.3521,  0.3010,  0.0880,  0.2028,\n",
      "        -0.0222,  0.3516, -0.4106,  0.2057, -0.1146,  0.0964,  0.2400, -0.0308,\n",
      "         0.1809,  0.3838, -0.1405,  0.0964,  0.2455, -0.2020,  0.1691,  0.3828,\n",
      "        -0.0252, -0.1033,  0.1875,  0.2546,  0.0530,  0.0060,  0.0272,  0.0283,\n",
      "        -0.1956, -0.2754, -0.1891,  0.2384,  0.2410,  0.0181,  0.2878,  0.0466,\n",
      "        -0.0298,  0.0196, -0.2274, -0.5537, -0.2338, -0.1727,  0.0411, -0.1748,\n",
      "         0.0569,  0.2847, -0.3796,  0.2068,  0.2335, -0.4746,  0.1991, -0.0071,\n",
      "        -0.3188, -0.4336, -0.4119, -0.3411,  0.0833, -0.0881,  0.2025, -0.1187,\n",
      "         0.0387, -0.0858,  0.1415,  0.0282, -0.1422, -0.2323, -0.1636,  0.3147,\n",
      "         0.1492, -0.3669,  0.2576,  0.2532, -0.2362,  0.2881, -0.1103, -0.1324,\n",
      "        -0.1869,  0.1248, -0.0429, -0.2783, -0.1049,  0.1465,  0.3323,  0.1121,\n",
      "        -0.2864, -0.4775,  0.0211,  0.2068,  0.0033, -0.3799, -0.0302,  0.0670,\n",
      "        -0.3167,  0.0143,  0.1051, -0.0612, -0.4473, -0.2007,  0.1782,  0.1957,\n",
      "         0.0307, -0.2544, -0.0163,  0.0329,  0.4182, -0.0178,  0.1032, -0.0410,\n",
      "         0.1927,  0.2898,  0.0279, -0.0368, -0.0495,  0.0030, -0.0635],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8432,  0.3602, -0.1824,  ...,  0.7218,  0.9796, -0.3414],\n",
      "         [ 0.6983,  0.5394, -0.2286,  ...,  0.5531,  0.9049, -0.3008],\n",
      "         [ 0.4615,  0.3878, -0.0460,  ...,  0.3157,  0.6673, -0.2929],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.6313, 1.0059, 0.6304,  ..., 0.7769, 0.6836, 0.3706], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1091, -0.2133, -0.0573,  ..., -0.3413, -0.3120,  0.1642],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0013,  0.1042,  0.0375,  ...,  0.1164,  0.0759, -0.0362],\n",
      "         [ 0.0764,  0.2245, -0.0606,  ...,  0.4174,  0.3616, -0.0617],\n",
      "         [ 0.2419,  0.1449,  0.0871,  ..., -0.0859,  0.1202,  0.0367],\n",
      "         ...,\n",
      "         [-0.0096,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0973],\n",
      "         [-0.0096,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0973],\n",
      "         [-0.0096,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0973]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2654, 0.4917, 0.4626, 0.2922, 0.5044, 0.3042, 0.5112, 0.3286, 0.2028,\n",
      "        0.5630, 0.6260, 0.5483, 0.5444, 0.5527, 0.5791, 0.2566],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1454, -0.0472, -0.1583,  0.0421, -0.1952, -0.0135, -0.1722,  0.0456,\n",
      "        -0.1633, -0.0594, -0.1048,  0.0308, -0.1715, -0.2588, -0.2551,  0.1543],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.5211,  0.3019, -0.0091,  ...,  1.0275,  0.5569, -0.3526],\n",
      "         [ 0.6394,  0.1978,  0.2088,  ...,  1.1765,  0.6148, -0.0973],\n",
      "         [ 0.6445,  0.2563,  0.3489,  ...,  1.1763,  0.6887, -0.3393],\n",
      "         ...,\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972],\n",
      "         [-0.0097,  0.0644, -0.0218,  ..., -0.0799, -0.0289, -0.0972]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 4.1943e-01,  8.3447e-01,  7.0996e-01,  1.0205e+00,  8.7830e-02,\n",
      "         7.1045e-01,  5.7861e-01,  4.4824e-01,  4.9756e-01,  6.9678e-01,\n",
      "         4.3555e-01,  4.3262e-01,  6.8994e-01,  4.0894e-01,  5.2197e-01,\n",
      "         4.0625e-01,  8.6523e-01,  4.6289e-01,  2.4304e-01,  1.7615e-01,\n",
      "         2.5708e-01,  1.0244e+00,  3.5571e-01,  9.7949e-01,  2.9980e-01,\n",
      "         1.9238e-01,  1.5915e-02,  3.6011e-01,  9.6484e-01,  3.3350e-01,\n",
      "         2.6465e-01,  7.8430e-02,  4.9512e-01,  7.2168e-01,  9.4580e-01,\n",
      "         3.7158e-01,  3.5767e-01,  3.4888e-01,  8.8281e-01,  7.9102e-01,\n",
      "         5.0293e-01,  5.6299e-01,  1.2433e-01,  2.0117e-01,  8.2373e-01,\n",
      "         5.3418e-01,  4.5654e-01,  6.1279e-01,  4.9463e-01,  3.6230e-01,\n",
      "         4.8486e-01,  1.0020e+00,  1.9666e-01,  7.5000e-01,  6.8457e-01,\n",
      "         2.1851e-01,  2.9199e-01,  3.4497e-01,  4.9512e-01,  5.8447e-01,\n",
      "         1.5454e-01,  3.1494e-01,  7.8271e-01,  1.1237e-01,  6.3818e-01,\n",
      "         4.6460e-01,  9.3115e-01,  9.5947e-01,  6.9092e-01,  1.1201e+00,\n",
      "         4.9707e-01,  3.6938e-01,  1.8372e-01,  6.4355e-01,  6.3916e-01,\n",
      "         6.1572e-01,  5.6885e-01,  2.7588e-01,  5.1025e-01,  6.8701e-01,\n",
      "         5.4443e-01,  7.5977e-01,  7.2559e-01,  4.1943e-01,  5.2588e-01,\n",
      "         6.9531e-01,  4.2651e-01,  2.7267e-02,  1.0547e+00,  7.2510e-01,\n",
      "         1.0986e+00,  4.0820e-01,  7.0312e-01,  7.4756e-01,  9.0527e-01,\n",
      "         8.0176e-01,  3.1958e-01,  3.0713e-01,  6.6309e-01, -6.9695e-03,\n",
      "         3.1052e-02,  4.5044e-01,  2.9785e-01,  1.2225e-01,  4.3921e-01,\n",
      "         3.5620e-01,  2.9272e-01,  1.7139e-01,  1.9852e-02,  2.6929e-01,\n",
      "         4.3359e-01,  6.6211e-01,  3.7305e-01,  1.4978e-01,  1.5906e-01,\n",
      "         3.2739e-01,  6.1279e-01,  3.1445e-01,  2.0801e-01,  4.7583e-01,\n",
      "         7.8027e-01,  1.0039e+00,  6.0596e-01,  5.5176e-01,  7.3730e-01,\n",
      "         9.6484e-01,  5.1904e-01,  7.1826e-01,  4.5508e-01,  6.1230e-01,\n",
      "         5.8301e-01,  5.0781e-01,  8.8330e-01,  2.6270e-01,  4.6606e-01,\n",
      "         4.7998e-01,  6.7480e-01,  7.9199e-01,  7.1680e-01,  6.8311e-01,\n",
      "         4.6924e-01,  4.9902e-01,  5.1953e-01,  7.2461e-01,  4.6631e-01,\n",
      "         4.6973e-01,  3.6841e-01,  1.0098e+00,  4.4873e-01,  1.2225e-01,\n",
      "         3.3643e-01,  1.1182e+00,  3.4277e-01,  3.3447e-01,  9.2139e-01,\n",
      "         7.5098e-01,  6.1426e-01,  1.5771e-01,  7.1826e-01,  1.1310e-01,\n",
      "         6.9287e-01,  2.9590e-01,  6.6162e-01,  7.6562e-01,  7.5000e-01,\n",
      "         1.1885e+00,  7.7441e-01,  6.9043e-01,  9.6826e-01,  6.0449e-01,\n",
      "         6.1426e-01,  7.3975e-01,  1.2441e+00,  7.0312e-01,  6.0156e-01,\n",
      "         8.7158e-01,  5.3906e-01,  6.1523e-01,  6.9238e-01,  1.5625e-01,\n",
      "         1.1123e+00,  2.6685e-01,  2.9688e-01,  3.9185e-01,  7.1240e-01,\n",
      "         4.8486e-01,  5.8496e-01,  2.1753e-01,  2.8491e-01,  4.8584e-01,\n",
      "         3.7354e-01,  6.9629e-01,  5.5127e-01,  3.8794e-01,  8.4473e-01,\n",
      "         6.6211e-01,  8.0859e-01,  1.1211e+00,  6.6357e-01,  5.5859e-01,\n",
      "         1.8152e-01,  6.4502e-01,  2.3743e-01,  5.2783e-01,  8.8428e-01,\n",
      "         4.5996e-01,  1.7410e-02,  6.1523e-01,  8.4375e-01,  5.1709e-01,\n",
      "         5.2539e-01,  7.8711e-01,  2.0288e-01,  1.0723e+00,  6.1328e-01,\n",
      "         5.5371e-01,  2.2241e-01,  2.9004e-01,  3.8940e-01,  4.9805e-01,\n",
      "         1.0977e+00,  3.8867e-01,  3.4521e-01,  8.0273e-01,  3.7354e-01,\n",
      "         6.9824e-01,  3.0127e-01,  5.8984e-01,  6.1182e-01,  4.6094e-01,\n",
      "         3.1689e-01,  4.6680e-01,  3.1836e-01,  1.0195e+00,  5.2783e-01,\n",
      "         2.5854e-01,  8.1348e-01,  2.2693e-01,  3.2812e-01,  1.1045e+00,\n",
      "         3.3179e-01,  3.4985e-01,  7.5146e-01,  3.3911e-01,  4.6411e-01,\n",
      "         7.5781e-01,  2.7539e-01,  7.4512e-01,  3.5693e-01,  7.3828e-01,\n",
      "         2.4939e-01,  7.0801e-01,  1.6138e-01,  1.6016e-01,  6.2891e-01,\n",
      "         2.2046e-01,  7.0117e-01,  1.8445e-01,  6.7236e-01,  1.4294e-01,\n",
      "         1.5808e-01,  7.3438e-01,  2.2546e-01,  4.5337e-01,  1.0283e+00,\n",
      "         1.8958e-01,  4.0845e-01,  5.3076e-01,  4.6216e-01,  6.4795e-01,\n",
      "         5.0000e-01,  3.2935e-01,  7.1338e-01,  1.6296e-01, -2.3422e-02,\n",
      "         2.7490e-01,  6.2061e-01,  9.3811e-02,  1.0870e-01,  5.3955e-01,\n",
      "         9.8755e-02,  3.9160e-01,  2.1106e-01,  4.8047e-01,  5.7666e-01,\n",
      "         1.4478e-01,  6.0791e-01,  4.7949e-01,  2.8345e-01,  5.8936e-01,\n",
      "         7.4463e-01,  2.6685e-01,  3.3667e-01,  5.5273e-01,  4.4043e-01,\n",
      "         6.6309e-01,  1.7871e-01,  4.6143e-01,  7.6904e-01,  9.1113e-01,\n",
      "         6.0449e-01,  8.2861e-01,  5.1123e-01,  1.9165e-01,  5.6836e-01,\n",
      "         3.7207e-01,  8.0713e-01,  9.8438e-01,  9.7656e-01,  7.3535e-01,\n",
      "         7.0068e-01,  7.3193e-01,  7.2998e-01,  4.0112e-01,  9.4788e-02,\n",
      "         4.2700e-01,  2.8491e-01,  6.9531e-01,  9.5801e-01,  9.3066e-01,\n",
      "         6.4160e-01,  7.1777e-01,  3.5474e-01,  9.8828e-01,  4.7803e-01,\n",
      "         3.5864e-01,  6.4062e-01,  9.0479e-01,  5.6201e-01,  6.8799e-01,\n",
      "         5.4834e-01,  2.8760e-01,  3.6914e-01,  2.4182e-01,  3.7695e-01,\n",
      "         4.5386e-01,  3.4692e-01,  2.8516e-01,  1.7664e-01,  4.8340e-01,\n",
      "         4.8926e-01, -5.7465e-02,  5.8203e-01,  2.8101e-01,  6.8994e-01,\n",
      "         5.2295e-01,  3.9575e-01,  8.2227e-01,  6.8213e-01,  7.1484e-01,\n",
      "         8.5205e-01,  4.6606e-01,  1.1639e-01,  3.8672e-01,  2.3767e-01,\n",
      "         6.2891e-01,  5.9082e-01,  7.3975e-01,  3.8647e-01,  7.1289e-01,\n",
      "         4.7607e-01,  5.5957e-01,  3.2373e-01,  7.1631e-01,  4.2236e-01,\n",
      "         6.4502e-01,  7.9736e-01,  6.6797e-01,  6.6650e-01,  3.2275e-01,\n",
      "         3.6206e-01,  4.5679e-01,  7.8662e-01,  2.2815e-01,  2.0264e-01,\n",
      "         9.8145e-02, -4.6730e-04,  4.1797e-01,  7.3047e-01,  4.1553e-01,\n",
      "         5.2002e-01,  7.9199e-01,  4.7314e-01,  9.3408e-01,  8.1909e-02,\n",
      "         3.2910e-01,  2.4500e-01,  3.9478e-01,  5.2197e-01,  3.3496e-01,\n",
      "         4.7852e-01,  9.4434e-01,  2.8101e-01,  2.9004e-01,  7.9785e-01,\n",
      "         5.5322e-01,  9.6240e-01,  6.5625e-01,  8.3008e-01,  4.5703e-01,\n",
      "         8.0078e-01,  4.9121e-01,  7.5244e-01,  4.3872e-01,  5.3418e-01,\n",
      "         4.4165e-01,  1.0752e+00,  6.3770e-01,  3.9966e-01,  7.7734e-01,\n",
      "         3.3887e-01,  3.5083e-01,  3.6108e-01,  6.6553e-01,  4.0430e-01,\n",
      "         6.1621e-01,  5.2637e-01,  4.2700e-01,  3.7646e-01,  5.6348e-01,\n",
      "         2.0581e-01,  2.0593e-01,  2.3743e-01,  4.3286e-01,  2.6123e-01,\n",
      "         4.7217e-01,  7.1680e-01,  4.2236e-01,  5.2197e-01,  7.8174e-01,\n",
      "         7.3389e-01,  1.4429e-01, -7.9102e-02, -1.7624e-02,  1.0576e+00,\n",
      "         4.7900e-01,  5.8008e-01,  4.4214e-01,  4.7754e-01,  7.0264e-01,\n",
      "         3.1689e-01,  1.0312e+00,  1.3953e-01,  8.0078e-01,  5.6250e-01,\n",
      "         4.5386e-01,  5.0098e-01,  4.0161e-01,  4.5581e-01,  7.1143e-01,\n",
      "         3.1250e-01,  3.9160e-01,  6.7139e-01,  5.1367e-01,  3.0762e-01,\n",
      "         6.4404e-01,  2.7832e-01,  5.9521e-01,  2.1277e-01,  1.0420e+00,\n",
      "         5.8887e-01,  4.5044e-01,  6.6846e-01,  5.0342e-01,  1.9287e-01,\n",
      "         6.9775e-01,  3.8379e-01,  5.3516e-01,  5.3809e-01,  5.8838e-01,\n",
      "         2.7954e-01,  4.6436e-01,  2.5220e-01,  4.7437e-01, -1.0181e-01,\n",
      "         6.8176e-02,  7.9639e-01,  6.8054e-02,  7.6953e-01,  5.0781e-01,\n",
      "         3.3545e-01,  9.0430e-01,  4.4482e-01,  5.6982e-01,  6.0254e-01,\n",
      "         2.9492e-01,  6.3184e-01,  6.2939e-01,  1.0059e+00,  5.7178e-01,\n",
      "         4.1260e-01,  2.8125e-01,  4.3286e-01,  2.5415e-01,  2.7979e-01,\n",
      "         5.9473e-01,  4.3750e-01,  2.7100e-01, -1.2073e-01,  1.8384e-01,\n",
      "        -1.8347e-01,  5.1807e-01,  6.2988e-02,  5.1025e-01,  1.6919e-01,\n",
      "         6.3574e-01,  4.4775e-01,  5.5225e-01,  5.9131e-01,  3.4204e-01,\n",
      "         7.0557e-01,  8.0078e-01,  3.8330e-01,  6.6016e-01,  5.3760e-01,\n",
      "         3.8916e-01,  3.5840e-01,  7.9492e-01,  5.1221e-01,  4.7705e-01,\n",
      "         7.7393e-01,  5.0879e-01,  5.4541e-01,  3.4570e-01,  4.0747e-01,\n",
      "         6.9580e-01,  4.1699e-01,  4.8193e-01,  5.8154e-01,  7.1436e-01,\n",
      "         4.5947e-01,  4.3188e-01,  6.4648e-01,  8.0713e-01,  5.2979e-01,\n",
      "         4.3164e-01,  4.8340e-01,  6.7920e-01,  6.4258e-01,  5.0342e-01,\n",
      "         3.6011e-01,  5.2539e-01,  4.1943e-01,  9.9023e-01,  5.2686e-01,\n",
      "         4.2944e-01,  7.8223e-01,  5.2441e-01,  4.2480e-01,  6.2744e-01,\n",
      "         3.3472e-01,  1.5283e-01,  6.8359e-01,  7.2705e-01,  5.2197e-01,\n",
      "         4.6362e-01,  5.3369e-01,  3.6987e-01,  3.5059e-01,  7.6318e-01,\n",
      "         1.9910e-01,  4.1309e-01,  1.5601e-01,  7.8955e-01,  4.5801e-01,\n",
      "         3.5889e-01,  1.4746e-01,  3.1592e-01,  2.9004e-01,  2.5220e-01,\n",
      "         7.8271e-01,  7.1045e-01,  4.6777e-01,  6.8604e-01,  4.3628e-01,\n",
      "         2.7588e-01,  4.5776e-02,  6.7139e-01,  4.4189e-01,  7.2852e-01,\n",
      "         4.6948e-01,  7.2168e-01,  6.2793e-01,  3.3789e-01,  6.6846e-01,\n",
      "         3.1592e-01,  5.6055e-01,  7.2314e-01,  4.4482e-01,  5.0635e-01,\n",
      "         7.7246e-01,  2.9761e-01,  6.4795e-01,  3.8794e-01,  5.5908e-01,\n",
      "         3.8354e-01,  8.4521e-01,  2.8687e-01,  7.2949e-01,  4.5508e-01,\n",
      "         6.9092e-01,  8.9404e-01,  7.2412e-01,  3.5620e-01,  9.7461e-01,\n",
      "         5.9082e-01,  6.3428e-01,  1.4185e-01,  2.8369e-01, -1.3367e-02,\n",
      "         5.4150e-01,  4.5044e-02,  6.8896e-01,  6.5137e-01,  6.0840e-01,\n",
      "         4.4312e-01,  4.6411e-01,  1.6223e-01,  1.5430e-01,  3.0869e-02,\n",
      "         4.6338e-01,  4.9854e-01,  5.0928e-01,  4.5947e-01,  2.1114e-03,\n",
      "         6.0107e-01,  1.0992e-01,  2.9126e-01,  6.0791e-01,  3.9307e-01,\n",
      "         6.6406e-01,  4.5605e-01,  5.4834e-01,  3.2812e-01,  8.4082e-01,\n",
      "         4.3579e-01,  4.6460e-01,  7.5781e-01,  4.5410e-01,  4.9023e-01,\n",
      "         9.3262e-01,  4.0479e-01,  6.0938e-01,  4.0625e-01,  4.9756e-01,\n",
      "        -1.5793e-02,  3.3398e-01,  2.7222e-01,  4.1162e-01,  2.6709e-01,\n",
      "         1.7175e-01,  1.6431e-01,  3.6743e-01,  1.9373e-01,  6.0254e-01,\n",
      "         8.7500e-01,  4.2358e-01,  6.9971e-01,  5.5078e-01,  3.8550e-01,\n",
      "         5.7568e-01,  7.7734e-01,  4.8291e-01,  4.7339e-01,  8.7549e-01,\n",
      "         3.6377e-01,  5.7031e-01,  3.2275e-01,  5.1758e-01,  3.6743e-01,\n",
      "         4.5215e-01,  6.5039e-01,  6.0938e-01,  3.8403e-01,  4.7339e-01,\n",
      "         3.7402e-01,  1.0000e+00,  6.3184e-01,  5.8154e-01,  7.0605e-01,\n",
      "         3.7134e-01,  7.6562e-01,  7.9736e-01,  6.1523e-01,  7.6318e-01,\n",
      "         3.1323e-01,  3.9868e-01,  6.1670e-01,  4.4629e-01,  9.0918e-01,\n",
      "         6.5771e-01,  4.8340e-01,  9.1113e-01,  7.3047e-01,  2.7054e-02,\n",
      "        -1.3684e-01,  9.8999e-02,  9.6680e-01,  6.7920e-01,  8.2812e-01,\n",
      "         6.2354e-01,  8.3691e-01,  3.8184e-01,  6.0547e-01,  5.3467e-01,\n",
      "         7.6953e-01,  4.3384e-01,  5.8740e-01,  6.7578e-01,  6.0205e-01,\n",
      "         1.0420e+00,  4.9170e-01,  6.3379e-01,  7.2363e-01,  9.8828e-01,\n",
      "         5.7129e-01,  5.1270e-01,  4.6289e-01,  6.3037e-01,  6.6650e-01,\n",
      "         4.4849e-01,  6.9580e-01,  6.5527e-01,  8.8965e-01,  7.3096e-01,\n",
      "         6.3818e-01,  9.0283e-01,  6.9873e-01,  5.9473e-01,  4.3994e-01,\n",
      "         8.3154e-01,  6.0400e-01,  9.5752e-01,  1.0215e+00,  6.5381e-01,\n",
      "         3.0640e-01,  3.9624e-01,  1.0869e+00,  6.5283e-01,  3.8770e-01,\n",
      "         5.6348e-01,  5.0586e-01,  5.2930e-01,  5.0830e-01,  6.7773e-01,\n",
      "         5.0488e-01,  1.2705e+00,  9.7266e-01,  8.3398e-01,  8.7012e-01,\n",
      "        -1.2976e-01,  8.2715e-01,  5.9326e-01,  5.3174e-01,  9.3457e-01,\n",
      "         2.9688e-01,  1.6785e-01,  3.6353e-01,  1.1688e-01,  6.5088e-01,\n",
      "         6.9629e-01,  3.8501e-01,  4.3506e-01,  1.1688e-01,  8.3936e-01,\n",
      "         6.4844e-01,  7.0166e-01,  5.0244e-01,  3.1641e-01,  7.5830e-01,\n",
      "         5.4785e-01,  4.5898e-01,  6.5527e-01,  4.1748e-01,  5.3906e-01,\n",
      "         8.1299e-01,  6.0059e-01,  8.1152e-01,  6.0449e-01,  5.3711e-01,\n",
      "         6.6846e-01,  7.6367e-01,  4.3848e-01,  5.8789e-01,  5.4053e-01,\n",
      "         4.6997e-01,  1.5833e-01,  4.3726e-01,  6.8018e-01,  2.0630e-01,\n",
      "         6.1133e-01,  2.9468e-01,  5.6934e-01,  4.7192e-01,  6.2500e-01,\n",
      "         2.7368e-01,  7.2363e-01,  5.7764e-01,  6.8457e-01,  2.6611e-01,\n",
      "         8.5059e-01,  4.4727e-01,  4.4385e-01,  9.2822e-01,  6.4062e-01,\n",
      "         2.7661e-01,  2.8979e-01,  5.3320e-01,  1.6479e-01,  4.7070e-01,\n",
      "         5.8252e-01,  7.1533e-01,  6.8408e-01, -8.1299e-02,  8.2568e-01,\n",
      "         6.2305e-01,  9.7266e-01,  7.6318e-01,  1.5698e-01,  3.4424e-01,\n",
      "         3.7915e-01,  4.1992e-01,  3.4204e-01, -3.7817e-01,  2.1021e-01,\n",
      "         6.3379e-01,  3.3630e-02,  3.1641e-01,  3.0615e-01,  7.5244e-01,\n",
      "         8.5742e-01,  8.3545e-01,  8.0469e-01,  2.7490e-01,  7.9248e-01,\n",
      "         4.5728e-01,  8.5889e-01,  8.2861e-01,  2.8564e-01,  1.0146e+00,\n",
      "         4.3896e-01,  5.7373e-01,  3.0347e-01,  3.9258e-01,  1.4978e-01,\n",
      "         4.1064e-01,  1.2140e-01,  6.6797e-01,  5.6274e-02,  7.4316e-01,\n",
      "         6.6553e-01,  5.8887e-01,  8.7305e-01,  6.2744e-01,  2.1338e-01,\n",
      "         1.5979e-01,  7.6611e-01,  6.3184e-01,  5.2539e-01,  7.1826e-01,\n",
      "         7.3340e-01,  9.8242e-01,  6.8506e-01,  6.6113e-01,  3.2471e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.1700e-01, -3.8989e-01, -5.6885e-01, -4.9268e-01, -3.9795e-01,\n",
      "        -3.4155e-01, -7.2559e-01, -6.8298e-02, -2.7881e-01, -2.2986e-01,\n",
      "        -1.6907e-01, -7.7759e-02,  3.7036e-01, -2.1875e-01, -1.8408e-01,\n",
      "        -4.4629e-01,  1.3269e-01,  8.9233e-02, -9.6619e-02,  4.6661e-02,\n",
      "        -9.1125e-02, -1.5515e-01,  1.4417e-01,  5.0537e-02,  9.6252e-02,\n",
      "        -8.1909e-02, -1.0307e-02, -9.0942e-02,  2.0105e-01,  1.0455e-01,\n",
      "        -1.2474e-02, -8.3847e-03, -1.9470e-02, -3.6963e-01,  5.7983e-02,\n",
      "        -2.0862e-01, -8.9722e-02, -1.5369e-01, -1.2323e-01, -1.9482e-01,\n",
      "        -4.2725e-01, -9.7473e-02, -2.1033e-01, -2.9175e-01, -9.3567e-02,\n",
      "         3.6072e-02,  7.7820e-02,  1.5771e-01,  2.7710e-02, -9.5032e-02,\n",
      "        -3.3813e-01, -8.4457e-03, -9.2896e-02, -9.3262e-02, -4.0625e-01,\n",
      "         2.1652e-02,  5.2185e-02, -1.3928e-01,  2.5806e-01, -2.9614e-01,\n",
      "        -4.6704e-01, -3.5718e-01, -1.1725e-01, -2.8052e-01,  2.0459e-01,\n",
      "        -8.9905e-02, -2.1423e-01,  6.0028e-02, -1.5576e-01,  8.8928e-02,\n",
      "         1.4258e-01, -8.5999e-02,  2.8366e-02, -7.0862e-02,  1.1041e-01,\n",
      "         2.7328e-02, -5.1318e-01, -1.1249e-01,  1.5845e-01, -1.3733e-01,\n",
      "        -3.2300e-01, -2.3816e-01, -1.8225e-01, -4.7705e-01, -4.1162e-01,\n",
      "        -6.1670e-01, -2.4915e-01, -5.5634e-02,  1.3684e-01, -1.2732e-01,\n",
      "         3.7744e-01,  7.3792e-02,  1.1230e-01, -3.6259e-03, -4.5703e-01,\n",
      "        -4.2676e-01, -7.2266e-01, -1.0117e+00, -7.5562e-02, -1.1070e-02,\n",
      "        -2.6147e-01,  9.9548e-02, -4.5972e-01, -6.2793e-01, -2.7734e-01,\n",
      "         1.5808e-02, -2.3474e-01, -5.0488e-01, -4.8145e-01, -9.6130e-02,\n",
      "        -9.2578e-01, -3.1445e-01, -2.8540e-01, -4.2920e-01, -1.5210e-01,\n",
      "        -6.8164e-01, -2.5415e-01, -2.6758e-01, -3.4277e-01, -2.3352e-01,\n",
      "        -5.5566e-01,  8.4473e-02, -2.9810e-01, -7.4036e-02, -3.7061e-01,\n",
      "        -2.8491e-01, -3.4546e-01, -1.2225e-01, -2.0300e-01, -3.5864e-01,\n",
      "        -7.7637e-02, -1.1163e-01,  3.7598e-01, -1.5845e-01, -2.5952e-01,\n",
      "        -6.8726e-02, -2.9932e-01, -2.5293e-01, -5.0537e-01, -3.6914e-01,\n",
      "        -2.5049e-01,  4.9805e-02, -3.2422e-01, -1.3306e-01,  1.4858e-03,\n",
      "         6.5689e-03, -3.0078e-01, -3.7158e-01, -6.9141e-01, -4.4165e-01,\n",
      "        -5.5908e-01,  1.9434e-01, -1.1102e-01, -1.1383e-01, -1.8518e-01,\n",
      "        -3.3545e-01, -2.9199e-01,  2.6172e-01, -4.7998e-01, -2.2742e-01,\n",
      "        -6.9153e-02, -1.4355e-01,  2.1877e-03, -4.8730e-01, -5.0879e-01,\n",
      "        -6.6406e-02, -3.7134e-01, -1.6113e-01, -4.6484e-01, -1.9806e-02,\n",
      "         6.0822e-02, -4.2212e-01, -2.8934e-03, -3.9258e-01, -2.1179e-01,\n",
      "        -5.3906e-01, -1.1864e-02,  9.5093e-02, -2.7954e-01, -2.7466e-01,\n",
      "         1.2646e-01, -2.3376e-01,  1.5710e-01,  1.4442e-02, -3.3325e-01,\n",
      "        -6.9922e-01, -1.5466e-01,  5.7159e-02, -4.3872e-01, -2.2913e-01,\n",
      "        -3.0908e-01,  4.7095e-01, -8.3862e-02,  9.0942e-02, -6.0577e-02,\n",
      "        -1.7249e-01, -6.4160e-01,  9.2773e-02, -2.9199e-01, -6.2500e-02,\n",
      "        -6.1816e-01, -5.0201e-02, -1.5271e-01,  9.4360e-02, -5.9863e-01,\n",
      "        -4.4897e-01, -2.5488e-01, -3.8090e-03,  2.6855e-01, -2.0337e-01,\n",
      "        -6.4697e-02, -4.2773e-01, -5.5615e-01,  5.5206e-02, -2.3816e-01,\n",
      "        -6.1981e-02, -6.7480e-01,  2.4731e-01, -2.8101e-01, -2.2595e-01,\n",
      "        -5.7770e-02,  8.0994e-02, -3.0487e-02, -4.4482e-01, -4.5508e-01,\n",
      "        -1.7639e-01, -1.5735e-01,  5.1910e-02,  4.5837e-02,  8.4763e-03,\n",
      "        -3.2501e-02, -4.1406e-01, -4.4312e-01, -6.5231e-03, -3.0591e-01,\n",
      "         1.1719e-01, -1.1859e-01, -1.3623e-01, -3.9087e-01,  2.9077e-01,\n",
      "        -1.3708e-01, -8.1543e-02,  1.4839e-02,  5.4901e-02, -1.2720e-01,\n",
      "         1.1426e-01, -4.3976e-02, -5.8350e-01,  1.4880e-01, -1.2573e-01,\n",
      "        -6.1737e-02, -4.0405e-01, -8.4351e-02, -2.3773e-02, -2.1362e-01,\n",
      "        -1.1469e-01,  6.6711e-02, -1.4336e-02, -4.2944e-01, -8.4412e-02,\n",
      "        -7.1777e-02, -2.2058e-01, -8.9172e-02, -4.8438e-01, -1.4783e-01,\n",
      "        -1.1017e-01, -3.0127e-01, -2.2400e-01, -1.7456e-01, -1.0040e-01,\n",
      "        -2.6367e-02, -1.0138e-01, -1.0712e-01, -8.5632e-02, -2.7191e-02,\n",
      "        -3.3911e-01, -2.0361e-01,  3.0136e-02,  6.2500e-02, -1.5930e-01,\n",
      "         4.3243e-02,  2.1094e-01,  4.7394e-02,  6.8054e-02,  3.0151e-01,\n",
      "         2.5098e-01,  1.4553e-03, -6.6772e-02,  2.4219e-01,  1.8958e-01,\n",
      "        -1.9165e-02, -7.8613e-02, -1.0590e-01,  4.4849e-01,  6.4392e-02,\n",
      "         2.0035e-02,  4.3030e-02, -2.9053e-01, -5.8740e-01, -4.1943e-01,\n",
      "        -4.1718e-02,  9.6985e-02,  9.8450e-02, -1.0675e-01, -2.2302e-01,\n",
      "        -2.4304e-01, -2.3157e-01, -3.8501e-01, -6.6895e-02, -2.4915e-01,\n",
      "        -2.2791e-01,  6.1157e-02,  2.2473e-01,  1.3635e-01, -3.4576e-02,\n",
      "        -1.1713e-01, -2.2327e-01, -1.9104e-01, -4.0283e-01, -5.7404e-02,\n",
      "        -2.3889e-01, -1.5723e-01, -3.0981e-01,  1.8652e-01,  3.9062e-02,\n",
      "         3.4424e-02, -9.6741e-02,  1.0754e-01, -2.8748e-02, -5.6592e-01,\n",
      "        -1.4502e-01, -3.0957e-01, -4.4067e-01, -9.0393e-02, -4.1797e-01,\n",
      "        -1.2054e-01, -2.2156e-01,  2.0645e-02, -2.0477e-02, -7.2803e-01,\n",
      "        -9.5764e-02, -2.3694e-01, -2.4670e-01, -3.3276e-01,  4.5142e-01,\n",
      "        -1.9421e-01,  6.5796e-02,  4.4327e-03, -1.4099e-01, -4.0601e-01,\n",
      "         1.9153e-01,  1.1639e-01, -6.4514e-02, -1.3232e-01, -2.0911e-01,\n",
      "        -1.4233e-01, -1.8262e-01, -4.1122e-03, -9.4910e-02,  4.1919e-01,\n",
      "        -2.3401e-01, -6.5308e-02, -4.3213e-01,  4.4019e-01, -9.8450e-02,\n",
      "        -2.8979e-01, -3.1104e-01, -7.1240e-01,  1.7776e-02, -2.2302e-01,\n",
      "         7.5684e-02, -4.8877e-01, -4.2334e-01, -7.1973e-01, -1.0504e-01,\n",
      "        -3.7036e-01, -6.0547e-01,  8.8257e-02,  3.4131e-01, -1.6028e-01,\n",
      "         1.2764e-02, -4.5264e-01, -5.9357e-02, -7.6721e-02, -1.9360e-01,\n",
      "        -1.0431e-01,  2.7930e-01, -3.1787e-01,  1.3525e-01, -3.8501e-01,\n",
      "         1.0785e-01, -5.5145e-02,  2.3267e-01,  4.4037e-02, -3.1055e-01,\n",
      "        -4.0747e-01, -1.6174e-01, -5.3131e-02, -4.7821e-02, -3.7567e-02,\n",
      "         6.5079e-03, -4.3869e-03,  2.1460e-01, -1.9458e-01, -2.2888e-01,\n",
      "        -2.1423e-02,  1.3867e-01, -8.4778e-02,  1.6125e-01,  2.9565e-01,\n",
      "        -1.1554e-01,  2.9190e-02, -7.3120e-02,  4.2749e-01, -6.0944e-02,\n",
      "        -2.4817e-01,  2.8793e-02, -1.5161e-01, -5.8496e-01,  3.9215e-02,\n",
      "        -2.1753e-01,  1.5295e-01,  2.2192e-01, -9.1736e-02, -3.6224e-02,\n",
      "         1.3489e-01,  3.7061e-01, -1.3110e-01,  7.7881e-02, -2.9199e-01,\n",
      "        -4.3286e-01, -6.6406e-01, -1.3512e-02, -2.2791e-01, -2.7881e-01,\n",
      "        -1.7651e-01,  3.3252e-01, -1.6223e-01,  3.2898e-02, -1.5698e-01,\n",
      "         5.5481e-02, -2.0312e-01, -4.6118e-01, -1.0704e-02, -2.3132e-02,\n",
      "         1.1890e-01,  1.7346e-01, -6.4392e-02,  2.0279e-02,  9.3002e-03,\n",
      "        -1.0052e-01,  1.2500e-01, -3.1372e-01, -9.6985e-02,  3.1769e-02,\n",
      "        -9.5276e-02, -4.3677e-01, -1.4880e-01, -1.7297e-01,  6.9641e-02,\n",
      "        -2.6880e-01, -8.4229e-02,  8.8989e-02,  1.0608e-01, -8.8379e-02,\n",
      "         4.5386e-01, -1.1530e-01, -7.4036e-02,  2.2437e-01,  1.9885e-01,\n",
      "         3.1185e-03, -2.2797e-02,  5.6549e-02, -8.8562e-02,  1.9104e-01,\n",
      "        -2.0477e-02,  7.6538e-02, -2.5342e-01,  1.3806e-01, -4.3018e-01,\n",
      "         1.6504e-01,  2.7344e-01, -2.0850e-01, -3.7567e-02, -4.2999e-02,\n",
      "        -5.4657e-02,  1.0602e-01,  6.0059e-02, -9.9304e-02, -5.2216e-02,\n",
      "        -2.8467e-01, -3.1006e-01, -1.8542e-01, -1.6968e-01, -7.2815e-02,\n",
      "         1.1243e-01,  5.6229e-03,  1.8970e-01, -3.6694e-01, -5.6671e-02,\n",
      "        -3.1763e-01, -1.6422e-03, -3.6060e-01,  2.3682e-01, -4.2310e-01,\n",
      "         4.1821e-01, -2.0264e-01, -1.6235e-02, -4.7852e-01,  1.6541e-01,\n",
      "        -1.5137e-01,  1.3557e-02,  1.8103e-01, -3.3081e-01, -2.5977e-01,\n",
      "        -1.8164e-01, -7.4658e-01, -1.1285e-01, -5.5878e-02,  5.0079e-02,\n",
      "        -7.0496e-02, -2.7313e-02,  2.0483e-01,  4.8950e-02, -1.7859e-01,\n",
      "         2.6172e-01, -1.6815e-02, -1.2695e-01,  3.7659e-02,  2.1103e-02,\n",
      "        -1.2213e-01,  8.6853e-02, -2.7026e-01,  7.2861e-03, -1.8872e-01,\n",
      "         2.2003e-02,  3.6224e-02, -1.7053e-01, -9.9670e-02, -4.2450e-02,\n",
      "         8.9493e-03,  1.5735e-01, -1.7151e-02, -2.1875e-01, -1.9800e-01,\n",
      "         3.1030e-01,  3.1952e-02, -4.3060e-02,  7.9407e-02, -1.3293e-01,\n",
      "         1.3330e-01,  1.7899e-02, -6.6406e-02, -8.2947e-02, -1.2976e-01,\n",
      "         1.8640e-01,  2.4780e-01,  4.0497e-02,  1.6895e-01,  6.0638e-02,\n",
      "         1.8909e-01,  1.9775e-01,  1.4124e-01, -2.9785e-02,  1.7224e-01,\n",
      "         1.3634e-02,  1.1383e-01, -4.4647e-02,  1.7452e-03,  1.5833e-01,\n",
      "        -5.8990e-02, -1.1859e-01, -1.2769e-01, -4.8193e-01, -6.7432e-01,\n",
      "        -5.2393e-01, -3.4644e-01, -2.2937e-01, -2.9175e-01, -1.1798e-01,\n",
      "        -1.2384e-01, -4.4946e-01, -5.3906e-01,  1.6211e-01, -2.6611e-01,\n",
      "        -7.5195e-02,  6.2561e-02,  4.1724e-01, -1.7029e-01, -5.5389e-02,\n",
      "        -2.9077e-01, -9.6008e-02,  5.1361e-02, -6.2695e-01, -1.1395e-01,\n",
      "        -4.8291e-01,  7.7820e-02, -2.5586e-01,  4.1650e-01, -2.2351e-01,\n",
      "        -4.3915e-02, -2.2278e-01, -1.3245e-01, -1.2494e-01,  8.1665e-02,\n",
      "         1.0138e-01,  7.1838e-02, -2.4524e-01, -7.2559e-01, -5.5420e-01,\n",
      "        -7.5439e-01, -9.0137e-01,  2.0889e-02, -5.3131e-02,  1.0486e-01,\n",
      "        -1.9287e-01, -1.2286e-01,  3.9276e-02, -2.5757e-01, -4.0112e-01,\n",
      "        -2.4597e-01, -5.8441e-02, -3.3051e-02, -4.0063e-01, -9.2236e-01,\n",
      "        -6.3525e-01, -2.7173e-01, -4.7021e-01,  2.2980e-02,  7.3242e-02,\n",
      "        -2.8320e-02, -1.1615e-01, -4.8096e-01,  1.6516e-01,  1.8225e-01,\n",
      "        -3.1152e-01, -1.7664e-01,  2.6782e-01, -2.4622e-01, -6.4819e-02,\n",
      "        -7.3303e-02, -4.4586e-02,  5.9784e-02,  1.0281e-03,  3.9185e-02,\n",
      "        -1.3220e-01, -2.4628e-02, -6.6101e-02, -4.1846e-01, -3.4961e-01,\n",
      "        -7.2876e-02, -1.3953e-01,  1.1456e-01,  3.4698e-02, -1.4539e-01,\n",
      "        -1.7212e-01, -4.6051e-02, -7.6721e-02,  8.4167e-02, -5.7037e-02,\n",
      "        -3.3130e-01,  2.6733e-01, -2.5171e-01, -5.4291e-02, -9.1187e-02,\n",
      "        -2.8015e-02,  3.9856e-02,  2.8046e-02, -2.4719e-01, -3.5400e-01,\n",
      "        -8.0322e-01, -2.0642e-01, -3.6401e-01, -3.6572e-01, -4.8242e-01,\n",
      "        -4.5825e-01,  4.7119e-02, -5.2277e-02, -6.8359e-01,  1.1884e-01,\n",
      "        -3.5132e-01, -1.2524e-01,  2.8540e-01, -2.8857e-01, -1.7224e-01,\n",
      "        -3.8770e-01, -2.5830e-01, -1.7773e-01, -3.3398e-01, -4.1040e-01,\n",
      "         6.3248e-03, -7.8735e-02, -2.1143e-01, -5.1367e-01, -9.2188e-01,\n",
      "        -1.3110e-01, -2.1802e-01,  1.0065e-01, -9.3994e-02,  3.1812e-01,\n",
      "        -2.8394e-01, -1.8811e-01, -2.1045e-01, -1.6638e-01, -2.7002e-01,\n",
      "         8.0750e-02,  2.2729e-01, -4.1840e-02,  7.9117e-03, -2.2278e-01,\n",
      "        -3.6530e-02, -6.8970e-02, -6.4819e-02, -5.7343e-02,  1.8311e-01,\n",
      "        -1.2396e-01,  2.3773e-02,  1.2976e-01, -1.1957e-01,  8.3618e-02,\n",
      "         1.4612e-01, -1.6724e-01, -2.0325e-01, -1.6370e-01,  8.5327e-02,\n",
      "        -7.7942e-02,  6.5247e-02, -6.5857e-02, -1.4539e-01, -3.1738e-01,\n",
      "        -6.0699e-02,  3.6041e-02, -3.1006e-01, -9.8938e-02, -5.5206e-02,\n",
      "        -1.0748e-01, -1.3550e-01,  5.5664e-02, -9.9548e-02,  4.0817e-03,\n",
      "         3.5553e-02, -3.5815e-01,  1.0272e-01, -4.5013e-02,  1.7212e-01,\n",
      "        -2.7252e-02, -3.3154e-01, -4.0552e-01, -6.9580e-01, -5.9662e-02,\n",
      "        -4.1943e-01,  4.0601e-01, -2.6880e-01, -3.2257e-02, -2.9932e-01,\n",
      "        -1.3513e-01, -8.6719e-01, -4.5947e-01, -6.3232e-01, -4.8438e-01,\n",
      "        -1.8835e-01, -5.7520e-01, -1.6882e-01, -4.2578e-01,  3.0908e-01,\n",
      "        -2.3181e-01, -2.3163e-02, -8.5547e-01, -2.6929e-01,  1.2573e-01,\n",
      "         1.1401e-01,  8.1116e-02, -3.1201e-01, -5.8643e-01, -9.1357e-01,\n",
      "        -2.0813e-01, -2.9272e-01, -4.6478e-02, -5.5878e-02, -1.8646e-02,\n",
      "        -4.8877e-01,  3.8281e-01, -1.6199e-01, -1.2469e-01, -7.8613e-01,\n",
      "         3.8509e-03,  1.2091e-01, -2.7148e-01, -2.8101e-01, -4.1235e-01,\n",
      "        -2.4500e-01,  1.5808e-01, -4.2188e-01, -4.1821e-01, -1.3281e-01,\n",
      "        -3.8892e-01,  3.9624e-01, -2.2644e-01,  6.0129e-04, -5.5859e-01,\n",
      "        -2.0740e-01, -5.8350e-01, -5.4688e-01, -1.1993e-01, -3.1421e-01,\n",
      "         1.2688e-02, -3.5229e-01, -1.3367e-01, -3.1097e-02, -4.0674e-01,\n",
      "        -5.1611e-01, -2.0789e-01, -2.7173e-01, -1.3806e-01, -4.9469e-02,\n",
      "         1.5259e-02, -1.0138e-01, -3.8062e-01, -3.9233e-01, -2.7319e-01,\n",
      "        -3.1250e-01, -6.5186e-01, -9.6359e-03, -3.8428e-01, -2.9932e-01,\n",
      "        -3.4253e-01, -5.1318e-01, -1.4099e-01, -3.8379e-01, -6.3538e-02,\n",
      "        -6.3477e-02, -9.6008e-02, -4.6967e-02, -5.0568e-02, -7.6843e-02,\n",
      "         5.0446e-02, -2.1851e-02, -3.6548e-01, -1.9666e-01, -1.0809e-01,\n",
      "        -2.9810e-01, -4.6295e-02, -9.9304e-02,  7.2937e-02,  1.0780e-02,\n",
      "        -1.3428e-01, -4.7266e-01, -2.3108e-01, -2.9395e-01, -7.1228e-02,\n",
      "        -4.0137e-01, -8.3057e-01, -5.8838e-01, -8.7793e-01, -5.5811e-01,\n",
      "        -3.4106e-01, -1.9556e-01, -2.8247e-01, -1.6040e-01, -4.4263e-01,\n",
      "        -2.3523e-01, -3.9990e-01, -1.6479e-01, -2.6758e-01,  1.6589e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(6.3444, device='cuda:0')\tavg_answer_loss:  tensor(5.7080, device='cuda:0')\tavg_type_loss:  tensor(0.1273, device='cuda:0')\tavg_val_f1:  0.12352941185235977\tavg_val_em:  0.1\tavg_val_prec:  0.11333333402872085\tavg_val_recall:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.12352941185235977 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00001: avg_val_f1 reached 0.12353 (best 0.12353), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_1.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.0407,  0.6361, -0.4510,  ...,  0.1840,  1.0630, -0.6297],\n",
      "         [ 1.3052,  0.8394, -0.2182,  ...,  1.6861,  1.1869, -0.2395],\n",
      "         [ 0.5660,  0.4964, -0.1579,  ...,  0.9108,  0.1426, -0.4224],\n",
      "         ...,\n",
      "         [-0.0386,  0.1728, -0.2173,  ...,  0.0388,  0.0231, -0.0278],\n",
      "         [ 0.0838,  0.2836, -0.0673,  ..., -0.3315,  0.0671, -0.1888],\n",
      "         [-0.0100,  0.0854, -0.0218,  ..., -0.0837, -0.0303, -0.0685]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0591,  0.2028, -0.1484,  ...,  0.6177,  1.0233,  0.0804],\n",
      "         [ 0.2748, -0.0835, -0.2139,  ...,  0.9176,  1.1098, -0.4463],\n",
      "         [ 0.8098,  0.2805, -0.0764,  ..., -0.0768,  0.6763, -0.2413],\n",
      "         ...,\n",
      "         [-0.0081,  0.0710, -0.0134,  ..., -0.0782, -0.0265, -0.0829],\n",
      "         [ 0.0076,  0.0603, -0.0085,  ..., -0.0803, -0.0264, -0.0804],\n",
      "         [ 0.0235,  0.5070,  0.0631,  ...,  0.0265, -0.0813, -0.3423]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0194,  0.2593, -0.1437,  ...,  0.8462,  1.0051, -0.1411],\n",
      "         [ 0.4614,  0.6593, -0.6463,  ...,  0.3109,  0.8734,  0.1522],\n",
      "         [ 0.6292,  0.2558, -0.2591,  ...,  0.4981,  0.8464, -0.5201],\n",
      "         ...,\n",
      "         [-0.0124,  0.0616, -0.0217,  ..., -0.0752, -0.0224, -0.0831],\n",
      "         [-0.0659,  0.0746, -0.3490,  ...,  0.5447,  0.0786, -0.0704],\n",
      "         [-0.0040,  0.0588,  0.0060,  ..., -0.0881, -0.0296, -0.0880]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7044,  0.6408, -0.2110,  ...,  0.0747,  1.1006, -0.4139],\n",
      "         [ 0.2202,  0.4452, -0.0795,  ...,  0.8461,  0.8584, -0.4289],\n",
      "         [ 0.8713,  0.4154, -0.2170,  ...,  0.4306,  0.6043, -0.3642],\n",
      "         ...,\n",
      "         [ 0.2913,  0.2905, -0.2169,  ...,  0.0478,  0.2183, -0.4125],\n",
      "         [-0.0086,  0.0879,  0.0028,  ..., -0.1100, -0.0292, -0.0862],\n",
      "         [-0.0078,  0.0759, -0.0381,  ..., -0.0836, -0.0313, -0.0838]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6420,  0.6647, -0.0658,  ...,  0.7648,  1.1552, -0.3013],\n",
      "         [ 0.4090,  0.4001, -0.1039,  ...,  0.9318,  1.0300,  0.2244],\n",
      "         [ 0.6246,  0.2345, -0.0846,  ...,  1.1143,  1.0363, -0.4644],\n",
      "         ...,\n",
      "         [-0.0222,  0.1466,  0.0575,  ..., -0.0469, -0.0498, -0.4218],\n",
      "         [ 0.0721, -0.0363, -0.1559,  ...,  0.2509,  0.4676, -0.5396],\n",
      "         [ 0.0111,  0.2863, -0.1326,  ...,  0.1966, -0.0239, -0.0551]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7550,  0.1914, -0.2108,  ...,  0.8936,  0.7870, -0.2893],\n",
      "         [ 0.3986,  0.5555, -0.2617,  ...,  1.2631,  0.9443, -0.4160],\n",
      "         [ 0.8539, -0.2016, -0.6082,  ..., -0.1959,  0.7511, -0.4956],\n",
      "         ...,\n",
      "         [-0.0107,  0.0629, -0.0085,  ..., -0.0781, -0.0117, -0.0816],\n",
      "         [-0.0168,  0.0776, -0.0127,  ..., -0.0785, -0.0306,  0.0173],\n",
      "         [-0.0191,  0.0976,  0.0159,  ..., -0.0892, -0.0506, -0.2430]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.1740e-01,  4.8918e-01,  1.5764e-01,  ...,  8.1237e-01,\n",
      "           1.1330e+00, -5.5039e-01],\n",
      "         [ 1.3992e+00,  1.7007e-01, -1.3702e-01,  ...,  8.7009e-01,\n",
      "           1.0158e+00, -4.8636e-01],\n",
      "         [ 1.3542e+00,  2.6668e-01, -5.1029e-03,  ...,  1.4681e-02,\n",
      "           8.5102e-01, -7.4369e-01],\n",
      "         ...,\n",
      "         [-4.5340e-02,  5.9959e-02,  2.8306e-02,  ..., -1.8381e-01,\n",
      "           2.0528e-02,  7.2008e-02],\n",
      "         [-1.3484e-02,  6.0944e-02,  2.1396e-03,  ..., -1.1141e-01,\n",
      "          -3.1959e-02, -7.8898e-02],\n",
      "         [-9.6082e-04,  5.9966e-02, -1.2320e-02,  ..., -8.6700e-02,\n",
      "          -2.5557e-02, -7.7693e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9133,  0.4469, -0.4661,  ...,  1.0059,  1.0545, -0.4029],\n",
      "         [ 0.2976,  0.3871, -0.5394,  ...,  0.7627,  1.3352, -0.5193],\n",
      "         [ 1.0739,  0.5867,  0.0575,  ...,  1.1143,  1.2100, -0.6219],\n",
      "         ...,\n",
      "         [-0.0035,  0.0565, -0.0271,  ..., -0.0758, -0.0228, -0.0843],\n",
      "         [-0.0116,  0.0597, -0.0168,  ..., -0.0693, -0.0221, -0.0752],\n",
      "         [-0.0037,  0.0386, -0.0151,  ..., -0.1129, -0.0358, -0.0750]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1615,  0.4877, -0.4618,  ...,  0.7155,  1.2178,  0.1429],\n",
      "         [ 0.5526,  0.3188, -0.4936,  ...,  0.6413,  0.4966, -0.7172],\n",
      "         [ 0.6426,  0.2296, -0.0581,  ...,  0.2903,  0.8616, -0.3487],\n",
      "         ...,\n",
      "         [-0.0026,  0.0493, -0.0186,  ..., -0.0814, -0.0177, -0.0674],\n",
      "         [ 0.1327,  0.0338,  0.0098,  ..., -0.2150,  0.0340, -0.2337],\n",
      "         [-0.0093,  0.0581, -0.0192,  ..., -0.0859, -0.0283, -0.0843]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2153e+00,  6.9839e-01, -3.6598e-01,  ...,  1.2333e+00,\n",
      "           1.3308e+00, -4.1130e-01],\n",
      "         [ 6.2010e-01,  2.3671e-01, -7.1730e-01,  ...,  1.2800e+00,\n",
      "           7.1990e-01, -2.0911e-01],\n",
      "         [ 5.0168e-01,  2.1771e-01, -4.8504e-01,  ...,  5.5660e-01,\n",
      "           1.0002e+00, -7.7984e-01],\n",
      "         ...,\n",
      "         [ 4.5043e-02, -1.9059e-01, -2.0454e-01,  ...,  3.8513e-01,\n",
      "           8.3380e-02, -5.3171e-01],\n",
      "         [-3.9793e-03,  4.4161e-02, -2.6590e-02,  ..., -8.4402e-02,\n",
      "          -2.6515e-02, -8.1360e-02],\n",
      "         [-3.8807e-04,  5.7965e-02, -2.1637e-02,  ..., -9.3782e-02,\n",
      "          -3.3088e-02, -8.2288e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1916,  0.3533, -0.1104,  ...,  0.9831,  0.6701, -0.5598],\n",
      "         [ 0.9414,  0.3943, -0.2545,  ...,  1.2872,  0.5700, -0.5347],\n",
      "         [ 0.3783,  0.5901, -0.1771,  ...,  0.3753,  0.6140, -0.2497],\n",
      "         ...,\n",
      "         [ 0.9060, -0.1036, -0.1245,  ...,  0.8549,  0.9012, -0.9076],\n",
      "         [ 0.0045,  0.0581, -0.0186,  ..., -0.0776, -0.0353,  0.0128],\n",
      "         [-0.0158,  0.0581, -0.0121,  ..., -0.0914, -0.0252, -0.0754]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2563,  0.5701, -0.2189,  ...,  1.4349,  1.1044, -0.4618],\n",
      "         [ 0.7020,  0.4016, -0.0174,  ...,  0.4260,  0.8250, -0.5661],\n",
      "         [ 0.6840,  0.4942, -0.1279,  ...,  0.6950,  0.5566, -0.1932],\n",
      "         ...,\n",
      "         [ 0.3443,  0.3383, -0.0814,  ...,  0.3122,  0.2059, -0.5347],\n",
      "         [-0.0239,  0.2673, -0.0528,  ..., -0.1803,  0.0038, -0.1924],\n",
      "         [ 0.0558,  0.1037,  0.0098,  ..., -0.1092, -0.0344, -0.2888]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8249,  0.5219, -0.0212,  ...,  0.8512,  0.8763,  0.0194],\n",
      "         [ 0.2557,  0.4548,  0.2236,  ...,  0.7021,  0.7457, -0.1243],\n",
      "         [ 0.5062,  0.4783, -0.1154,  ..., -0.6024,  0.2461, -0.5078],\n",
      "         ...,\n",
      "         [-0.0250,  0.1034, -0.0454,  ..., -0.0660, -0.0771, -0.2269],\n",
      "         [-0.0454,  0.1752, -0.0097,  ..., -0.0348, -0.0178, -0.1951],\n",
      "         [-0.0043,  0.1070, -0.0482,  ..., -0.0933, -0.1368, -0.2526]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6244,  0.6360, -0.2191,  ...,  1.0798,  1.4611, -0.6080],\n",
      "         [ 0.7874,  0.2862, -0.1813,  ...,  0.9123,  1.2162, -0.2985],\n",
      "         [ 0.9590,  0.0829, -0.1238,  ...,  0.5598,  1.1557, -0.3674],\n",
      "         ...,\n",
      "         [-0.0533,  0.1000, -0.0571,  ..., -0.0020, -0.0036, -0.2384],\n",
      "         [-0.0120,  0.0682, -0.0229,  ..., -0.0714, -0.0236,  0.0131],\n",
      "         [ 0.0125,  0.1055,  0.0215,  ..., -0.0109, -0.0065, -0.1992]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9872,  0.5321, -0.2641,  ...,  1.2206,  1.1598, -1.0721],\n",
      "         [ 0.5813,  0.8697,  0.3661,  ...,  0.2699,  0.7407, -0.5005],\n",
      "         [ 0.5946,  0.4581, -0.4083,  ...,  0.8637,  1.1185, -1.0025],\n",
      "         ...,\n",
      "         [-0.0054,  0.0699, -0.0389,  ..., -0.0554, -0.0227, -0.1531],\n",
      "         [ 0.0078,  0.0519, -0.0177,  ..., -0.0887, -0.0225, -0.0905],\n",
      "         [ 0.0134,  0.0891, -0.0160,  ..., -0.0587, -0.0433, -0.0981]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6824,  0.4025, -0.2723,  ...,  1.2294,  0.6228, -0.2823],\n",
      "         [ 0.4164, -0.0022,  0.0914,  ...,  1.1717,  0.4220, -0.5013],\n",
      "         [ 0.5415,  0.0133, -0.4117,  ...,  0.9704,  0.6233, -0.5103],\n",
      "         ...,\n",
      "         [-0.0084,  0.0365, -0.0213,  ..., -0.0909, -0.0159, -0.0779],\n",
      "         [-0.0119,  0.0864, -0.0174,  ..., -0.0776, -0.0268, -0.0756],\n",
      "         [ 0.0062,  0.0400, -0.0403,  ..., -0.0532, -0.0461, -0.1464]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.9628,  0.2310,  0.1129,  ...,  1.2420,  0.7260, -0.5020],\n",
      "         [ 0.7362,  0.6453,  0.0590,  ...,  1.2516,  1.0725, -0.3678],\n",
      "         [ 0.9597, -0.0383, -0.0713,  ...,  1.2838,  0.8871, -0.1194],\n",
      "         ...,\n",
      "         [ 0.0955,  0.5464, -0.1217,  ..., -0.1601,  0.0358,  0.1509],\n",
      "         [-0.0120,  0.0591, -0.0233,  ..., -0.0713, -0.0337, -0.0724],\n",
      "         [-0.0363,  0.0997, -0.0230,  ..., -0.0154, -0.0222, -0.1995]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0274,  0.3102, -0.1010,  ...,  0.9366,  0.7923, -0.5576],\n",
      "         [ 0.8415,  0.3468,  0.0041,  ...,  1.2645,  0.8070, -0.5860],\n",
      "         [ 1.0237,  0.1466,  0.0618,  ...,  0.3695,  0.5026, -0.2731],\n",
      "         ...,\n",
      "         [-0.0162,  0.0959,  0.0293,  ..., -0.0524, -0.0479, -0.1991],\n",
      "         [ 0.1632,  0.3823, -0.1205,  ...,  0.8019,  0.2673, -0.0791],\n",
      "         [ 0.0074,  0.4983, -0.2463,  ...,  0.0018,  0.2233, -0.1521]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7991,  0.5318, -0.3265,  ...,  1.3573,  0.6983, -0.5702],\n",
      "         [ 0.5911,  0.4145, -0.2470,  ...,  0.8838,  0.8489, -0.1629],\n",
      "         [ 0.6341,  0.3779,  0.0874,  ...,  0.1036,  0.9812, -0.6220],\n",
      "         ...,\n",
      "         [-0.0129,  0.1596,  0.0249,  ..., -0.0621, -0.0689, -0.2640],\n",
      "         [-0.0092,  0.0559, -0.0148,  ..., -0.0893, -0.0297, -0.0831],\n",
      "         [ 0.0886,  0.1094, -0.0877,  ...,  0.1622,  0.1169, -0.0770]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8417,  0.3113, -0.2170,  ...,  1.5530,  0.8150, -0.4987],\n",
      "         [ 0.5353,  0.2019, -0.0545,  ...,  1.1548,  0.8803, -0.1954],\n",
      "         [ 0.7983, -0.1982, -0.3433,  ...,  0.4496,  0.9423, -0.2330],\n",
      "         ...,\n",
      "         [-0.0200,  0.0606, -0.0175,  ..., -0.0884, -0.0364, -0.0799],\n",
      "         [-0.0104,  0.0587, -0.0140,  ..., -0.0877, -0.0277, -0.0795],\n",
      "         [-0.0132,  0.0492, -0.0199,  ..., -0.0805, -0.0143, -0.0873]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8904,  0.1094, -0.3417,  ...,  0.9333,  0.6351, -0.4844],\n",
      "         [ 0.5775,  0.2461,  0.1032,  ...,  0.3335,  0.9961, -0.4210],\n",
      "         [ 0.6397,  0.5480, -0.2821,  ...,  0.9479, -0.0868, -0.3592],\n",
      "         ...,\n",
      "         [-0.0412,  0.1648,  0.0366,  ..., -0.1164, -0.0749, -0.2504],\n",
      "         [-0.0080,  0.0586, -0.0186,  ..., -0.0739, -0.0289, -0.0767],\n",
      "         [ 0.2690,  0.4745,  0.0710,  ...,  0.6442,  0.2612, -0.3125]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9532,  0.4008, -0.0327,  ...,  0.5193,  0.9260, -0.1470],\n",
      "         [ 0.3600,  0.6187, -0.0883,  ...,  0.1982,  1.0470, -0.1424],\n",
      "         [ 0.2242,  0.1554,  0.0632,  ...,  0.1212,  0.4639, -0.4440],\n",
      "         ...,\n",
      "         [ 0.0871,  0.4837,  0.0366,  ...,  0.3152, -0.0278, -0.2834],\n",
      "         [ 0.0876,  0.4345,  0.3000,  ...,  1.1620,  0.4758, -0.0385],\n",
      "         [-0.0112,  0.0629, -0.0163,  ..., -0.0738, -0.0260, -0.0891]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.4578e-01,  3.6167e-01, -3.3237e-01,  ...,  1.5200e+00,\n",
      "           1.4272e+00, -5.2711e-01],\n",
      "         [ 5.6061e-01,  5.3527e-01, -2.1830e-01,  ...,  5.5177e-01,\n",
      "           1.0700e+00, -4.3180e-01],\n",
      "         [ 3.0467e-02,  4.1470e-01, -3.1468e-01,  ...,  1.2967e+00,\n",
      "           1.4293e+00, -9.0499e-01],\n",
      "         ...,\n",
      "         [-3.1369e-03,  6.3697e-02, -1.8590e-02,  ..., -1.1671e-01,\n",
      "          -2.6077e-02, -8.7158e-02],\n",
      "         [ 4.0157e-01,  5.0297e-01,  1.8649e-01,  ..., -1.7690e-01,\n",
      "           2.6384e-01, -5.4255e-01],\n",
      "         [-1.9052e-03,  6.6892e-02, -1.7972e-05,  ..., -3.8956e-02,\n",
      "           2.4627e-03, -2.1558e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0132,  0.1539,  0.0187,  ...,  0.5030,  0.9053, -0.7079],\n",
      "         [ 0.8068, -0.2115, -0.0897,  ...,  0.9742,  1.0150, -0.5735],\n",
      "         [ 0.3774, -0.0082,  0.2740,  ...,  0.7566,  0.5541, -0.2527],\n",
      "         ...,\n",
      "         [-0.0069,  0.0620, -0.0166,  ..., -0.0739, -0.0176, -0.0828],\n",
      "         [-0.0167,  0.0646, -0.0342,  ..., -0.0748, -0.0347, -0.0897],\n",
      "         [-0.0141,  0.0551, -0.0144,  ..., -0.0823, -0.0155, -0.0877]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7064,  0.5980,  0.2187,  ...,  1.7311,  1.0650, -0.8125],\n",
      "         [ 0.4731,  0.8145,  0.0840,  ...,  1.2575,  0.9329, -0.3260],\n",
      "         [ 0.6893,  0.3164, -0.2779,  ...,  1.0221,  0.7196, -0.4849],\n",
      "         ...,\n",
      "         [ 0.0262,  0.1444, -0.0660,  ..., -0.1193, -0.0918, -0.0561],\n",
      "         [-0.0049,  0.0779, -0.0207,  ..., -0.0713, -0.0346, -0.0995],\n",
      "         [-0.0065,  0.0755, -0.0292,  ..., -0.0152, -0.0230, -0.2208]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1779,  0.2549, -0.0790,  ...,  0.9877,  1.1110, -0.5498],\n",
      "         [ 0.7162,  0.3994, -0.3083,  ...,  0.0714,  0.9903, -0.5935],\n",
      "         [ 0.5698,  0.5138, -0.0469,  ...,  0.6581,  1.2833,  0.0110],\n",
      "         ...,\n",
      "         [ 0.0013,  0.0448, -0.0151,  ..., -0.0741, -0.0278,  0.0120],\n",
      "         [ 0.0912, -0.0165, -0.2552,  ..., -0.1932,  0.2627, -0.0418],\n",
      "         [-0.0035,  0.0610, -0.0310,  ..., -0.0792, -0.0440,  0.0025]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1280,  0.0896, -0.4899,  ...,  1.1849,  1.1710, -0.4570],\n",
      "         [ 0.4330, -0.1506,  0.0050,  ...,  1.6803,  1.0003,  0.3452],\n",
      "         [ 0.5454,  0.3039,  0.0971,  ...,  1.0197,  1.2372, -0.2321],\n",
      "         ...,\n",
      "         [-0.0086,  0.0499, -0.0137,  ..., -0.0732, -0.0219,  0.0167],\n",
      "         [-0.0073,  0.0577, -0.0037,  ..., -0.0819, -0.0228, -0.0699],\n",
      "         [ 0.0122,  0.0769,  0.0112,  ..., -0.1094, -0.0477, -0.2118]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9625,  0.6038, -0.1734,  ...,  1.0952,  1.1591, -0.0839],\n",
      "         [ 0.9335,  0.2791, -0.1921,  ...,  1.3533,  1.2440, -0.3052],\n",
      "         [ 0.7547,  0.2287,  0.2659,  ...,  0.9416,  0.8799, -0.1494],\n",
      "         ...,\n",
      "         [-0.0120,  0.0570, -0.0164,  ..., -0.0774, -0.0222, -0.0902],\n",
      "         [-0.0314,  0.0986, -0.0233,  ..., -0.0362,  0.0131, -0.1951],\n",
      "         [-0.0381,  0.1652, -0.0200,  ..., -0.0287, -0.0673, -0.2071]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7942, -0.1602, -0.2378,  ...,  0.7732,  1.0914, -0.3653],\n",
      "         [ 1.0307, -0.5721, -0.0861,  ...,  1.4481,  1.1183, -0.3998],\n",
      "         [ 1.0454,  0.3298, -0.1897,  ...,  0.3609,  0.8410, -0.2109],\n",
      "         ...,\n",
      "         [-0.0063,  0.0701, -0.0205,  ..., -0.0821, -0.0232, -0.0907],\n",
      "         [-0.0491,  0.1047, -0.0370,  ..., -0.0592, -0.0318, -0.2104],\n",
      "         [-0.0387,  0.1640, -0.1750,  ..., -0.0024,  0.0783, -0.1755]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1228,  0.6872, -0.2492,  ...,  1.0718,  1.0884, -0.5520],\n",
      "         [ 0.8853,  0.5361,  0.1224,  ...,  0.8956,  0.8815, -0.6267],\n",
      "         [ 0.8852,  0.7011, -0.2934,  ...,  0.8678,  0.6168, -0.4409],\n",
      "         ...,\n",
      "         [-0.0045,  0.0410, -0.0129,  ..., -0.0725, -0.0242, -0.0930],\n",
      "         [ 0.0044,  0.4778, -0.3935,  ...,  0.0458,  0.4603, -0.5952],\n",
      "         [-0.0121,  0.0823, -0.0079,  ..., -0.0859, -0.0308,  0.0187]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1187,  0.1536, -0.0604,  ...,  1.4162,  0.9762, -0.4217],\n",
      "         [ 1.1323, -0.1652,  0.0871,  ...,  1.6930,  0.8693,  0.0950],\n",
      "         [ 0.2026, -0.1436, -0.1799,  ...,  1.2129,  1.0371, -0.8770],\n",
      "         ...,\n",
      "         [ 0.1028,  0.1833, -0.4881,  ...,  0.4168,  0.0879, -0.5284],\n",
      "         [-0.0385,  0.0968,  0.0191,  ..., -0.0518, -0.0275, -0.2073],\n",
      "         [ 0.0534,  0.0891,  0.0030,  ..., -0.0401, -0.0792, -0.1534]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0515,  0.4041, -0.0142,  ...,  1.1574,  1.2245, -0.4408],\n",
      "         [ 0.5145,  0.5352, -0.0849,  ...,  0.3213,  1.2153, -0.6723],\n",
      "         [ 0.7091,  0.2165,  0.0345,  ...,  0.6280,  1.1580, -0.4817],\n",
      "         ...,\n",
      "         [-0.0089,  0.0385, -0.0209,  ..., -0.0832, -0.0251, -0.0857],\n",
      "         [ 0.2823,  0.0669,  0.3329,  ..., -0.3179,  0.1988, -0.3080],\n",
      "         [-0.0068,  0.0597, -0.0269,  ..., -0.0787, -0.0284, -0.0897]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.8053,  0.5932, -0.3753,  ...,  1.5756,  0.9069, -0.3600],\n",
      "         [ 0.1378,  0.6982,  0.3572,  ...,  0.6555,  0.8293, -0.5310],\n",
      "         [ 0.9699,  0.4706, -0.2236,  ...,  0.4150,  0.9506, -0.5914],\n",
      "         ...,\n",
      "         [-0.0149,  0.0520, -0.0321,  ..., -0.0783, -0.0355, -0.0902],\n",
      "         [-0.0057,  0.0615, -0.0205,  ..., -0.0699, -0.0264,  0.0103],\n",
      "         [ 0.0024,  0.0425, -0.0138,  ..., -0.0819, -0.0224, -0.0882]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1607,  0.3296, -0.1255,  ...,  0.8263,  0.6832, -0.2263],\n",
      "         [ 0.7910,  0.3932,  0.5196,  ...,  0.7003,  0.5438, -0.3387],\n",
      "         [ 0.6520,  0.2738,  0.3104,  ...,  0.4179,  0.5952, -0.1815],\n",
      "         ...,\n",
      "         [-0.0423,  0.0850, -0.0193,  ..., -0.0580,  0.0034, -0.1608],\n",
      "         [ 0.0759,  0.2300, -0.1408,  ...,  0.0186,  0.1009, -0.0883],\n",
      "         [-0.0591,  0.0999,  0.0160,  ..., -0.0593, -0.0186, -0.2039]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9862,  0.3316,  0.0976,  ...,  0.4255,  0.8874, -0.4236],\n",
      "         [ 0.9974,  0.5184,  0.2132,  ...,  1.2436,  0.4196, -0.1855],\n",
      "         [ 0.7978,  0.4546, -0.1208,  ...,  0.7603,  0.8611, -0.3721],\n",
      "         ...,\n",
      "         [-0.0134,  0.0588, -0.0135,  ..., -0.0723, -0.0208,  0.0175],\n",
      "         [-0.0157,  0.0596, -0.0270,  ..., -0.0844, -0.0297, -0.0810],\n",
      "         [-0.0048,  0.0645, -0.0101,  ..., -0.0723, -0.0258, -0.0905]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4808,  0.3667, -0.0070,  ...,  1.2266,  0.7677, -0.5883],\n",
      "         [ 1.1842,  0.1363,  0.1739,  ...,  1.5283,  0.9834, -0.4036],\n",
      "         [ 0.9635,  0.3869,  0.2168,  ...,  0.9425,  0.4385, -0.5264],\n",
      "         ...,\n",
      "         [-0.0107,  0.0551, -0.0210,  ..., -0.0982, -0.0300, -0.0910],\n",
      "         [ 0.0123,  0.0485, -0.0168,  ..., -0.0821, -0.0224, -0.0681],\n",
      "         [ 0.0039,  0.0593, -0.0204,  ..., -0.0821, -0.0321,  0.0117]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.3960e-01,  2.0408e-01, -4.2743e-02,  ...,  1.9111e-01,\n",
      "           9.2566e-01,  1.1426e-03],\n",
      "         [ 1.0646e+00,  2.3241e-01,  8.4688e-02,  ...,  1.3824e+00,\n",
      "           1.1996e+00, -3.3738e-01],\n",
      "         [ 6.5318e-01,  4.1830e-01, -1.1272e-01,  ...,  7.8517e-01,\n",
      "           1.1529e+00, -5.0827e-01],\n",
      "         ...,\n",
      "         [-4.0452e-02,  1.6697e-01, -4.0824e-02,  ..., -6.1119e-02,\n",
      "          -7.4295e-02, -4.7387e-02],\n",
      "         [ 1.6281e-01,  5.7679e-02,  4.6142e-02,  ...,  3.9544e-01,\n",
      "           3.6429e-01, -1.8530e-01],\n",
      "         [ 6.1533e-02,  2.9807e-01, -8.5807e-02,  ..., -2.4691e-01,\n",
      "           1.2218e-01, -1.8522e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8162,  0.3480, -0.2005,  ...,  1.6208,  1.2188, -0.4901],\n",
      "         [ 0.2923,  0.6812,  0.2464,  ...,  0.8091,  0.7678, -0.3206],\n",
      "         [ 0.1827,  0.1111, -0.0417,  ...,  0.3265,  0.5888, -0.2616],\n",
      "         ...,\n",
      "         [ 0.0319,  0.2053,  0.0622,  ..., -0.0294,  0.0141, -0.1766],\n",
      "         [-0.0095,  0.0621, -0.0224,  ..., -0.0838, -0.0377, -0.0859],\n",
      "         [ 0.0460,  0.4404,  0.0506,  ...,  0.2496,  0.1308, -0.2433]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9805,  0.2846, -0.1423,  ...,  1.1949,  0.9828, -0.4665],\n",
      "         [ 0.6463,  0.3424,  0.1255,  ...,  1.1304,  0.4614, -0.3327],\n",
      "         [ 0.7355,  0.3155,  0.0437,  ...,  1.6124,  0.9333, -0.0818],\n",
      "         ...,\n",
      "         [-0.0039,  0.1910, -0.0601,  ..., -0.3374, -0.0408, -0.1483],\n",
      "         [-0.0137,  0.0558, -0.0139,  ..., -0.0776, -0.0230, -0.0775],\n",
      "         [-0.0236,  0.1092, -0.0361,  ..., -0.0451, -0.0166, -0.2307]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.1532e-01,  3.0795e-01, -4.5264e-02,  ...,  7.2683e-01,\n",
      "           1.1179e+00, -4.7398e-01],\n",
      "         [ 6.0048e-01,  5.9772e-01,  2.0144e-01,  ...,  1.3317e+00,\n",
      "           2.0963e-01, -2.5898e-01],\n",
      "         [ 5.5961e-01,  2.3473e-01, -1.9163e-01,  ...,  1.3074e+00,\n",
      "           4.6712e-01, -1.5442e-01],\n",
      "         ...,\n",
      "         [-8.1678e-03,  7.7370e-02, -1.6122e-02,  ..., -6.7566e-02,\n",
      "          -2.4061e-02, -8.9077e-02],\n",
      "         [ 3.9809e-01,  5.6270e-01, -3.2628e-02,  ..., -1.9000e-01,\n",
      "           3.7228e-02, -4.7881e-01],\n",
      "         [ 9.5648e-05,  1.4884e-01,  2.2709e-02,  ..., -7.6618e-02,\n",
      "          -6.4728e-02, -4.4545e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.8957e-01,  3.0618e-01, -4.3876e-01,  ...,  1.2595e+00,\n",
      "           1.1274e+00, -1.8692e-01],\n",
      "         [ 1.0395e+00,  4.8257e-01,  8.2664e-02,  ...,  9.2955e-01,\n",
      "           3.4772e-01, -3.1068e-01],\n",
      "         [ 1.4185e+00,  5.8794e-01,  4.8039e-01,  ...,  5.3202e-01,\n",
      "           6.3303e-01, -3.3030e-01],\n",
      "         ...,\n",
      "         [ 2.7565e-04,  4.1634e-02,  1.6693e-03,  ..., -7.5316e-02,\n",
      "          -3.7281e-02, -8.9751e-02],\n",
      "         [ 2.6673e-01,  1.2918e-01, -5.3449e-01,  ...,  1.6699e-01,\n",
      "           2.7014e-01, -3.7228e-01],\n",
      "         [ 1.3039e-02,  2.4228e-01,  1.1913e-01,  ...,  2.0593e-02,\n",
      "          -8.2496e-02, -3.7550e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9360,  0.4032,  0.2165,  ...,  1.0368,  0.8366, -0.2925],\n",
      "         [ 0.2976,  0.8237, -0.0644,  ...,  0.7680,  0.4691, -0.2824],\n",
      "         [ 0.6088,  0.3612, -0.0615,  ...,  0.7464,  0.5536, -0.1471],\n",
      "         ...,\n",
      "         [-0.0074,  0.0580, -0.0263,  ..., -0.0726, -0.0245, -0.0652],\n",
      "         [ 0.0120,  0.0495, -0.0170,  ..., -0.0753, -0.0282, -0.0877],\n",
      "         [ 0.0049,  0.0592, -0.0155,  ..., -0.1120, -0.0274, -0.0824]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0143,  0.1268, -0.0749,  ...,  1.3534,  0.8281,  0.0648],\n",
      "         [ 0.8464,  0.2083, -0.0941,  ...,  1.0329,  0.9132, -0.3638],\n",
      "         [ 0.8621,  0.4213,  0.0621,  ...,  1.0524,  1.0782,  0.0568],\n",
      "         ...,\n",
      "         [-0.0037,  0.1518,  0.0023,  ..., -0.0645,  0.0146, -0.2145],\n",
      "         [-0.0148,  0.0485, -0.0140,  ..., -0.1080, -0.0158, -0.0871],\n",
      "         [-0.0049,  0.1893, -0.0710,  ...,  0.3399,  0.0357, -0.1899]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7630,  0.3108,  0.1957,  ...,  1.0932,  1.3776, -0.2949],\n",
      "         [ 0.7823,  0.3440,  0.4206,  ...,  1.1255,  0.7672, -0.3631],\n",
      "         [ 0.9963,  0.5061,  0.1245,  ...,  1.0788,  0.3546, -0.3898],\n",
      "         ...,\n",
      "         [-0.0109,  0.0667,  0.0039,  ..., -0.0815, -0.0303, -0.1001],\n",
      "         [ 0.2608,  0.0981, -0.3053,  ...,  0.1009,  0.1279, -0.2544],\n",
      "         [-0.0480,  0.1181, -0.1819,  ..., -0.0653,  0.0436, -0.2744]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8506,  0.4269, -0.2581,  ...,  1.3413,  1.1996, -0.1405],\n",
      "         [ 1.0478,  0.4243, -0.0308,  ...,  1.1764,  0.7634, -0.2946],\n",
      "         [ 1.2480,  0.3499,  0.0022,  ...,  0.8903,  1.0177, -0.5936],\n",
      "         ...,\n",
      "         [-0.0146,  0.0543, -0.0188,  ..., -0.0828, -0.0206, -0.0901],\n",
      "         [ 0.0067,  0.1865,  0.0325,  ..., -0.1012, -0.0703, -0.2792],\n",
      "         [-0.0411,  0.0701, -0.0134,  ..., -0.0590, -0.0242, -0.2136]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3122,  0.5364,  0.0194,  ...,  1.8172,  1.3799, -0.2151],\n",
      "         [ 0.5986,  0.4432,  0.2519,  ...,  1.3840,  0.8082, -0.1487],\n",
      "         [ 0.8785,  0.5459,  0.5352,  ...,  1.0871,  0.5250, -0.0043],\n",
      "         ...,\n",
      "         [-0.0052,  0.1720,  0.0102,  ..., -0.1066, -0.0932, -0.2617],\n",
      "         [ 0.0064,  0.0494, -0.0113,  ..., -0.0738, -0.0278, -0.0905],\n",
      "         [-0.0075,  0.0407, -0.0075,  ..., -0.0738, -0.0313, -0.0878]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7539,  0.0593,  0.2559,  ...,  0.9237,  0.4665, -0.5811],\n",
      "         [ 0.7359, -0.1477, -0.1154,  ...,  1.2075,  1.1331, -0.3128],\n",
      "         [ 0.0720,  0.6784, -0.1242,  ...,  0.8101,  0.4252, -0.2379],\n",
      "         ...,\n",
      "         [-0.0094,  0.0571, -0.0130,  ..., -0.0929, -0.0188, -0.0838],\n",
      "         [ 0.0074,  0.0490, -0.0159,  ..., -0.0664,  0.0049, -0.1014],\n",
      "         [-0.0119,  0.0550, -0.0177,  ..., -0.0812, -0.0169, -0.0859]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.7359,  0.5713,  0.0786,  ...,  1.1082,  1.0981, -0.4448],\n",
      "         [-0.3150,  0.8499,  0.0654,  ...,  1.0173,  0.3917, -0.2515],\n",
      "         [ 0.6741,  0.3060,  0.0153,  ...,  1.0013,  0.8577, -0.2408],\n",
      "         ...,\n",
      "         [ 0.1319, -0.0456, -0.0828,  ...,  0.0747,  0.1990, -0.2967],\n",
      "         [-0.0157,  0.0396, -0.0089,  ..., -0.0786, -0.0225, -0.0866],\n",
      "         [-0.1677,  0.1409, -0.2824,  ..., -0.3569,  0.1141, -0.1470]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9760,  0.2114, -0.1430,  ...,  1.3108,  1.0195, -0.2824],\n",
      "         [ 1.1105,  0.3662, -0.0510,  ...,  1.2458,  0.9543, -0.1670],\n",
      "         [ 1.3201,  0.2678, -0.0016,  ...,  1.0940,  0.9737, -0.3524],\n",
      "         ...,\n",
      "         [ 0.0533,  0.0508, -0.0430,  ..., -0.1184,  0.0296, -0.1566],\n",
      "         [-0.0094,  0.0401, -0.0067,  ..., -0.0677, -0.0183, -0.0832],\n",
      "         [-0.0049,  0.3526, -0.2197,  ...,  0.2241, -0.0423, -0.1289]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0596,  0.2766, -0.1088,  ...,  1.2444,  0.9859, -0.1316],\n",
      "         [ 0.2582,  0.3132, -0.0542,  ...,  0.7620,  0.7927, -0.1190],\n",
      "         [ 0.1383, -0.2945, -0.2587,  ...,  0.8203,  0.7624, -0.3844],\n",
      "         ...,\n",
      "         [ 0.0160,  0.0888,  0.0310,  ..., -0.0731, -0.0414, -0.2409],\n",
      "         [-0.0015,  0.0475, -0.0048,  ..., -0.0652, -0.0451, -0.1261],\n",
      "         [-0.0079,  0.0581, -0.0115,  ..., -0.0780, -0.0215, -0.0827]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.4620e+00,  2.5913e-01,  4.8252e-02,  ...,  1.4533e+00,\n",
      "           1.4701e+00, -5.7138e-01],\n",
      "         [ 8.8950e-01,  6.6995e-01, -1.9339e-01,  ...,  1.1141e+00,\n",
      "           1.1146e+00, -5.1603e-01],\n",
      "         [ 9.2086e-01,  6.4297e-01,  4.3546e-01,  ...,  1.1209e+00,\n",
      "           1.5316e+00, -6.0281e-01],\n",
      "         ...,\n",
      "         [-6.9959e-02,  6.2124e-02, -6.0457e-02,  ..., -6.9990e-02,\n",
      "          -5.7386e-02, -2.2821e-01],\n",
      "         [ 2.2376e-04,  1.5383e-01,  2.9214e-02,  ..., -9.9462e-02,\n",
      "          -6.8656e-02, -2.1456e-01],\n",
      "         [-1.2790e-02,  5.1070e-02, -2.3183e-02,  ..., -1.0944e-01,\n",
      "          -2.5567e-02, -6.7164e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1348,  0.2394,  0.1462,  ...,  1.3329,  1.7285, -0.0027],\n",
      "         [ 1.2803,  0.4503,  0.1952,  ...,  1.2222,  1.0096,  0.4189],\n",
      "         [ 0.4072,  0.6452,  0.2974,  ...,  0.6437,  0.5646,  0.0912],\n",
      "         ...,\n",
      "         [ 0.0082,  0.0523, -0.0132,  ..., -0.0775, -0.0152, -0.0765],\n",
      "         [ 0.0382,  0.1454,  0.0097,  ..., -0.0517, -0.0481, -0.1602],\n",
      "         [ 0.0075,  0.0589, -0.0171,  ..., -0.0716, -0.0255, -0.0659]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1700e+00,  2.8782e-01,  1.5474e-01,  ...,  1.5668e+00,\n",
      "           1.4613e+00, -4.0790e-01],\n",
      "         [ 1.2083e+00,  4.2463e-01,  7.5437e-03,  ...,  9.4963e-01,\n",
      "           9.8575e-01, -6.6175e-01],\n",
      "         [ 9.0708e-01,  7.4612e-02, -2.4073e-01,  ...,  4.0139e-03,\n",
      "           1.2582e+00, -6.9333e-01],\n",
      "         ...,\n",
      "         [ 2.1182e-02,  5.2633e-02, -3.5314e-02,  ..., -6.8837e-02,\n",
      "          -2.0180e-02, -1.2520e-01],\n",
      "         [ 1.2206e-01,  3.9418e-01, -2.4098e-01,  ...,  2.8333e-01,\n",
      "           3.5082e-02, -9.0851e-02],\n",
      "         [-1.5300e-03,  6.1422e-02, -1.6026e-02,  ..., -7.5815e-02,\n",
      "          -1.7053e-02, -8.7904e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2613,  0.5829,  0.0282,  ...,  1.0539,  1.1718,  0.1219],\n",
      "         [ 0.3274,  0.4726, -0.1026,  ...,  0.9927,  1.0228, -0.2596],\n",
      "         [ 0.9285,  0.1645,  0.0343,  ...,  0.6715,  0.8885, -0.2377],\n",
      "         ...,\n",
      "         [-0.0053,  0.0577, -0.0058,  ..., -0.0773, -0.0226, -0.0786],\n",
      "         [-0.0185,  0.5056, -0.2426,  ..., -0.0894,  0.0119, -0.4192],\n",
      "         [ 0.0059,  0.0556, -0.0252,  ..., -0.0953, -0.0422, -0.0883]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1600,  0.1670, -0.1848,  ...,  2.0090,  1.3336, -0.3697],\n",
      "         [ 0.9872,  0.2975,  0.3039,  ...,  0.9684,  1.1414, -0.8372],\n",
      "         [ 0.7642,  0.5020,  0.2215,  ...,  0.4235,  1.0499, -0.8453],\n",
      "         ...,\n",
      "         [-0.0029,  0.0619, -0.0088,  ..., -0.0720, -0.0233, -0.0811],\n",
      "         [-0.0158,  0.0601, -0.0186,  ..., -0.0805, -0.0274, -0.0895],\n",
      "         [-0.0071,  0.0510, -0.0147,  ..., -0.0697, -0.0341, -0.0753]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9532,  0.0897, -0.0482,  ...,  1.4524,  1.3873, -0.7237],\n",
      "         [ 0.6125,  0.3634,  0.0567,  ...,  0.7763,  0.7587, -0.4220],\n",
      "         [ 0.8974,  0.4193,  0.2446,  ...,  0.4927,  0.9559,  0.1061],\n",
      "         ...,\n",
      "         [-0.0093,  0.0499,  0.0031,  ..., -0.0784, -0.0245, -0.0810],\n",
      "         [ 0.0321,  0.0933,  0.0174,  ..., -0.0512, -0.1044,  0.0250],\n",
      "         [-0.0127,  0.0580, -0.0134,  ..., -0.0873, -0.0290,  0.0138]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.8491e-01,  1.4700e-01,  7.0399e-02,  ...,  4.1115e-01,\n",
      "           7.3303e-01, -4.8093e-01],\n",
      "         [ 7.7233e-01,  5.0138e-02,  9.7664e-02,  ...,  1.4969e+00,\n",
      "           9.5786e-01, -1.5820e-01],\n",
      "         [ 1.7164e-01,  3.2090e-01, -1.3947e-01,  ...,  1.3093e+00,\n",
      "           8.5006e-01, -1.4841e-01],\n",
      "         ...,\n",
      "         [-2.2894e-02,  4.2065e-02, -1.8788e-02,  ..., -5.8109e-02,\n",
      "          -3.5114e-02, -8.0745e-02],\n",
      "         [-3.3813e-05,  5.6528e-02, -1.6638e-02,  ..., -7.1948e-02,\n",
      "          -2.8854e-02, -8.7940e-02],\n",
      "         [-7.9828e-03,  5.2129e-02,  3.5747e-03,  ..., -8.3345e-02,\n",
      "          -2.5503e-02,  3.3598e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9319,  0.2654, -0.0738,  ...,  1.1101,  1.2978, -0.4186],\n",
      "         [ 0.7871,  0.5191,  0.2401,  ...,  0.6670,  1.0125, -0.2673],\n",
      "         [ 0.7959,  0.3289,  0.0753,  ...,  0.7269,  0.9367, -0.3691],\n",
      "         ...,\n",
      "         [-0.0059,  0.0399, -0.0168,  ..., -0.0842, -0.0175, -0.0833],\n",
      "         [ 0.0267,  0.3974, -0.2392,  ..., -0.1471, -0.0178, -0.2611],\n",
      "         [ 0.0901,  0.0928, -0.1497,  ...,  0.1064,  0.1882, -0.2663]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7112,  0.6356, -0.3816,  ...,  1.4851,  1.2158, -0.6340],\n",
      "         [ 0.8388,  0.5086, -0.1469,  ...,  2.0233,  1.2688,  0.4799],\n",
      "         [ 1.0515,  0.4166,  0.1556,  ...,  1.5823,  1.2028, -0.5902],\n",
      "         ...,\n",
      "         [ 0.1164,  0.1840, -0.1311,  ..., -0.1417,  0.2126, -0.1795],\n",
      "         [-0.0118,  0.0555, -0.0146,  ..., -0.0842, -0.0235, -0.0889],\n",
      "         [-0.0047,  0.0919, -0.0298,  ..., -0.1044, -0.0216, -0.0901]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.5399e+00,  4.6955e-01, -1.1336e-01,  ...,  1.5180e+00,\n",
      "           1.2656e+00, -1.8135e-01],\n",
      "         [ 7.0621e-01,  1.6240e-01, -1.1699e-01,  ...,  8.8832e-01,\n",
      "           1.0948e+00, -4.1624e-01],\n",
      "         [ 1.1836e+00, -1.2825e-01, -3.5618e-02,  ...,  1.3003e+00,\n",
      "           8.6395e-01,  1.7397e-01],\n",
      "         ...,\n",
      "         [-2.4573e-03,  6.6318e-02, -2.0163e-02,  ..., -1.2597e-01,\n",
      "           1.5188e-02,  4.2901e-02],\n",
      "         [ 1.6900e-02,  1.2121e-01,  1.2255e-03,  ..., -5.9906e-02,\n",
      "          -1.0077e-01, -2.4916e-01],\n",
      "         [-9.4362e-03,  6.9355e-02, -1.7203e-02,  ..., -1.1943e-01,\n",
      "          -3.1894e-02, -9.2307e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3132e+00,  1.3130e-01, -2.1898e-02,  ...,  1.2444e+00,\n",
      "           8.3900e-01, -4.3292e-01],\n",
      "         [ 1.0374e+00,  5.1094e-01, -1.2693e-01,  ..., -2.1095e-02,\n",
      "           1.1894e+00, -1.1228e-01],\n",
      "         [ 1.0807e+00,  3.4417e-01,  1.0416e-01,  ...,  1.2906e+00,\n",
      "           1.0867e+00, -5.3712e-01],\n",
      "         ...,\n",
      "         [ 5.1639e-04,  2.2344e-01, -5.6802e-02,  ...,  1.4592e-01,\n",
      "           8.6807e-03, -2.4286e-01],\n",
      "         [-9.5014e-03,  6.5617e-02, -3.6412e-02,  ..., -8.7473e-02,\n",
      "          -3.0601e-02,  3.3701e-02],\n",
      "         [-1.1898e-02,  3.5971e-02, -1.6875e-02,  ..., -8.3182e-02,\n",
      "          -3.1730e-02,  1.5182e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.3039,  0.2947,  0.1432,  ...,  1.6973,  1.3938, -0.6052],\n",
      "         [ 1.0280,  0.4648,  0.1354,  ...,  1.7786,  0.5980, -0.3349],\n",
      "         [ 0.7910,  0.4631,  0.0644,  ..., -0.1066,  0.4825, -0.4599],\n",
      "         ...,\n",
      "         [ 0.2163,  0.4885,  0.0340,  ..., -0.2448,  0.0860, -0.4909],\n",
      "         [-0.0107,  0.0406, -0.0186,  ..., -0.0962, -0.0300, -0.0975],\n",
      "         [-0.0085,  0.0965, -0.0146,  ..., -0.0717, -0.0312, -0.1008]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3692,  0.2845, -0.0499,  ...,  1.9485,  1.6124, -0.4309],\n",
      "         [ 0.8913,  0.1934,  0.0880,  ...,  1.6745,  1.1485, -0.2501],\n",
      "         [ 1.2670,  0.3391,  0.0875,  ...,  1.5089,  1.8497, -0.4457],\n",
      "         ...,\n",
      "         [ 0.0136,  0.1116, -0.0075,  ...,  0.0353,  0.0194, -0.2054],\n",
      "         [-0.1185,  0.2389, -0.0661,  ...,  0.1381, -0.2181,  0.0279],\n",
      "         [ 0.0084,  0.0518, -0.0338,  ..., -0.0729, -0.0266, -0.0799]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0968,  0.3766, -0.1900,  ...,  1.4939,  1.4258, -0.6458],\n",
      "         [ 0.7501,  0.4754,  0.0537,  ...,  1.1357,  1.1632, -0.6803],\n",
      "         [ 1.0504,  0.4202, -0.1373,  ...,  0.8956,  1.2673, -0.6207],\n",
      "         ...,\n",
      "         [-0.0068,  0.0464, -0.0180,  ..., -0.0701, -0.0287,  0.0046],\n",
      "         [-0.1665,  0.1542, -0.0814,  ...,  0.0671,  0.0790, -0.1949],\n",
      "         [ 0.0057,  0.1573, -0.0244,  ..., -0.0755, -0.0151, -0.2431]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2561,  0.2554, -0.3072,  ...,  1.5283,  0.7903, -0.6936],\n",
      "         [ 0.7053,  0.3323, -0.1234,  ...,  1.0149,  0.7799, -0.5679],\n",
      "         [ 0.9928,  0.1433,  0.1192,  ...,  1.0972,  1.0022, -0.6856],\n",
      "         ...,\n",
      "         [-0.0279,  0.2933, -0.0680,  ...,  0.1490,  0.2933, -0.3197],\n",
      "         [-0.1691,  0.1195, -0.0140,  ...,  0.3434,  0.1311, -0.1111],\n",
      "         [-0.0178,  0.0528, -0.0282,  ..., -0.0823, -0.0296, -0.0740]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1869,  0.5490,  0.0896,  ...,  0.7718,  1.1355, -0.5633],\n",
      "         [ 0.8930, -0.0695,  0.0086,  ...,  1.3286,  1.0554, -0.2178],\n",
      "         [ 0.8028,  0.3888, -0.1304,  ...,  0.8109,  0.9868, -0.4598],\n",
      "         ...,\n",
      "         [-0.0167,  0.0647, -0.0237,  ..., -0.1085, -0.0249, -0.0928],\n",
      "         [-0.0120,  0.0432, -0.0268,  ..., -0.0904, -0.0197, -0.0794],\n",
      "         [-0.0108,  0.0536, -0.0091,  ..., -0.1121, -0.0446, -0.0847]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 3.8947e-01,  5.3555e-01, -1.7158e-01,  ...,  2.1963e+00,\n",
      "           1.4557e+00, -6.9057e-01],\n",
      "         [ 6.5911e-01,  7.7045e-01,  2.1687e-01,  ...,  1.8243e+00,\n",
      "           8.1212e-01, -5.0879e-01],\n",
      "         [ 7.1189e-01,  3.5895e-01, -4.4142e-02,  ...,  6.6374e-01,\n",
      "           5.5135e-02, -3.8675e-01],\n",
      "         ...,\n",
      "         [ 1.3453e-02,  5.6112e-02, -6.9186e-02,  ..., -4.4532e-02,\n",
      "          -1.9114e-02, -1.4691e-01],\n",
      "         [ 3.3915e-03,  6.0338e-02,  9.1917e-04,  ..., -8.1967e-02,\n",
      "          -1.5342e-02, -8.2319e-02],\n",
      "         [ 1.0564e-03,  7.1244e-02,  2.3408e-03,  ..., -8.2808e-02,\n",
      "          -1.2759e-02, -7.5803e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7796,  0.7751, -0.2830,  ...,  1.3006,  1.0170, -0.5935],\n",
      "         [ 0.9732,  0.3829, -0.1198,  ...,  1.4385,  1.4177,  0.1100],\n",
      "         [ 0.9169,  0.5863,  0.2859,  ...,  1.3238,  1.2368,  0.3368],\n",
      "         ...,\n",
      "         [-0.1713,  0.4529,  0.0060,  ...,  0.3410,  0.1676, -0.1670],\n",
      "         [-0.0115,  0.0603, -0.0207,  ..., -0.0689, -0.0311, -0.0878],\n",
      "         [-0.0114,  0.0717, -0.0359,  ..., -0.0792, -0.0319, -0.0765]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2108,  0.3118, -0.2040,  ...,  1.4866,  1.0012,  0.0606],\n",
      "         [ 0.4407,  0.6422,  0.1516,  ...,  0.9579,  1.1605, -0.4007],\n",
      "         [ 1.1222,  0.2964,  0.5763,  ...,  1.4016,  0.1914, -0.5317],\n",
      "         ...,\n",
      "         [-0.0046,  0.2494,  0.1310,  ..., -0.0102, -0.0857, -0.4232],\n",
      "         [ 0.3555,  0.1070, -0.2055,  ...,  0.0043,  0.2012, -0.0632],\n",
      "         [-0.0172,  0.0810, -0.0631,  ..., -0.0250, -0.0289, -0.2451]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.6464e-01,  4.3407e-01,  2.1055e-01,  ...,  1.5267e+00,\n",
      "           1.1479e+00, -4.2143e-01],\n",
      "         [ 9.6197e-01, -1.8605e-01,  3.5636e-02,  ...,  1.5201e+00,\n",
      "           6.9590e-01, -2.6682e-01],\n",
      "         [ 1.0386e+00,  4.2219e-01, -1.8768e-01,  ...,  1.3817e+00,\n",
      "           9.3908e-01, -3.7923e-01],\n",
      "         ...,\n",
      "         [-1.1597e-02,  4.2397e-02,  2.7034e-03,  ..., -9.7445e-02,\n",
      "          -1.6595e-02, -9.1291e-02],\n",
      "         [-4.4835e-02,  4.7410e-02, -3.3826e-02,  ..., -5.1574e-02,\n",
      "          -3.4038e-04,  2.8901e-02],\n",
      "         [-1.2040e-02,  5.5401e-02, -2.2288e-02,  ..., -7.6700e-02,\n",
      "          -3.5504e-02, -8.5165e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2919,  0.6266,  0.2101,  ...,  1.3821,  1.4751, -0.7005],\n",
      "         [ 0.9412,  0.5853,  0.4665,  ...,  1.4753,  1.2433, -0.6937],\n",
      "         [ 0.8359,  0.4369,  0.1274,  ...,  0.8888,  1.1303, -0.4320],\n",
      "         ...,\n",
      "         [-0.0165,  0.0562, -0.0185,  ..., -0.0725, -0.0334, -0.0998],\n",
      "         [ 0.3328,  0.2866, -0.1284,  ...,  0.6114,  0.4289, -0.1337],\n",
      "         [-0.0061,  0.0541, -0.0117,  ..., -0.0599, -0.0264, -0.0852]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.5004e+00,  3.8489e-01, -4.6347e-01,  ...,  1.6632e+00,\n",
      "           1.4946e+00,  9.2052e-02],\n",
      "         [ 7.7897e-01,  5.1674e-01, -4.8360e-01,  ...,  1.6216e+00,\n",
      "           1.0863e+00, -6.5265e-01],\n",
      "         [ 1.2521e+00,  1.6922e-01, -2.4027e-01,  ...,  1.6537e+00,\n",
      "           1.3915e+00, -3.5726e-01],\n",
      "         ...,\n",
      "         [-2.3006e-04,  7.8210e-02,  2.7427e-02,  ..., -3.0103e-02,\n",
      "          -4.5510e-02, -1.2288e-01],\n",
      "         [-4.3949e-03,  5.4309e-02, -1.9230e-02,  ..., -8.0681e-02,\n",
      "          -2.4578e-02, -9.9384e-02],\n",
      "         [ 1.3772e-02,  6.4089e-02, -6.5862e-03,  ..., -8.9265e-02,\n",
      "          -2.6384e-02, -8.0396e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9003,  0.0451, -0.1248,  ...,  0.4693,  1.1584, -0.1085],\n",
      "         [ 0.8559,  0.4563, -0.3013,  ...,  0.8964,  0.7293, -0.4150],\n",
      "         [ 0.9939,  0.1341, -0.7086,  ...,  0.0252,  0.9311, -0.3788],\n",
      "         ...,\n",
      "         [ 0.0133,  0.0585, -0.0067,  ..., -0.1034, -0.0263, -0.0843],\n",
      "         [-0.0111,  0.0604, -0.0161,  ..., -0.0817, -0.0284, -0.1010],\n",
      "         [ 0.2237,  0.1373, -0.1252,  ...,  0.0452,  0.1549, -0.2798]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0423,  0.3487, -0.1217,  ...,  1.3539,  1.0235, -0.2274],\n",
      "         [ 0.7186,  0.6774,  0.4130,  ...,  0.1813,  0.4103, -0.3340],\n",
      "         [ 0.9484,  0.6101,  0.1347,  ...,  1.7367,  0.3626, -0.3117],\n",
      "         ...,\n",
      "         [-0.0131,  0.0615, -0.0332,  ..., -0.0526, -0.0439, -0.0890],\n",
      "         [ 0.0723, -0.0117, -0.0064,  ...,  0.6385,  0.0685, -0.4088],\n",
      "         [-0.0082,  0.0889, -0.0266,  ..., -0.0717, -0.0326, -0.0701]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3503e+00,  4.8499e-01, -1.5147e-01,  ...,  1.6180e+00,\n",
      "           1.2863e+00, -5.7248e-01],\n",
      "         [ 6.5069e-01,  5.9794e-01,  4.9136e-02,  ...,  1.0375e+00,\n",
      "           1.0340e+00, -6.7730e-01],\n",
      "         [ 7.1708e-01,  4.6311e-01,  1.1509e-01,  ...,  6.9593e-01,\n",
      "           8.4637e-01, -2.8382e-01],\n",
      "         ...,\n",
      "         [-2.9111e-04,  4.8385e-02, -1.4368e-02,  ..., -7.0207e-02,\n",
      "          -2.1357e-02, -8.0582e-02],\n",
      "         [-1.3131e-02,  6.4318e-02, -3.0318e-02,  ..., -7.9210e-02,\n",
      "          -2.6664e-02, -9.5206e-02],\n",
      "         [-1.6659e-02,  5.9593e-02, -2.2775e-02,  ..., -7.3997e-02,\n",
      "          -3.3596e-02, -8.5361e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3195,  0.9261, -0.2243,  ...,  2.0699,  1.7774, -0.0959],\n",
      "         [ 1.1607,  0.6004, -0.0532,  ...,  0.9395,  1.0322, -0.4917],\n",
      "         [ 0.9173,  0.4056, -0.0639,  ...,  1.1961,  0.5948, -0.6170],\n",
      "         ...,\n",
      "         [-0.0066,  0.0524, -0.0183,  ..., -0.0783, -0.0252, -0.0943],\n",
      "         [-0.0037,  0.0612, -0.0081,  ..., -0.0772, -0.0279, -0.0914],\n",
      "         [-0.0103,  0.0434, -0.0248,  ..., -0.0897, -0.0367, -0.1034]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 8.9312e-02,  6.1456e-01, -1.2298e-01,  ...,  1.2381e+00,\n",
      "           1.2110e+00, -3.7815e-01],\n",
      "         [ 6.0737e-01,  3.5307e-01,  1.5748e-01,  ...,  1.6307e+00,\n",
      "           7.5202e-01,  1.0683e-01],\n",
      "         [ 1.2999e+00,  6.6252e-01, -1.2422e-01,  ...,  1.4075e+00,\n",
      "           9.7229e-01, -3.4138e-01],\n",
      "         ...,\n",
      "         [-1.3730e-02,  6.4351e-02, -1.5283e-02,  ..., -9.2714e-02,\n",
      "          -3.6760e-02, -9.4972e-02],\n",
      "         [-2.6890e-01,  2.7367e-01, -2.6403e-01,  ...,  5.4986e-01,\n",
      "           4.3413e-01, -1.8509e-01],\n",
      "         [-1.0244e-03,  6.7412e-02, -9.9816e-03,  ..., -1.1962e-01,\n",
      "          -3.1569e-02, -8.0732e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9221,  0.8990, -0.1720,  ...,  1.4184,  1.1954, -0.8743],\n",
      "         [ 0.9929,  0.8247, -0.1251,  ...,  0.7776,  0.7754, -0.4708],\n",
      "         [ 0.6314,  0.9186, -0.1642,  ...,  0.4926,  1.2557, -0.7497],\n",
      "         ...,\n",
      "         [-0.0311,  0.3003, -0.0697,  ..., -0.0489, -0.0746, -0.4532],\n",
      "         [-0.0068,  0.0370,  0.0041,  ..., -0.0780, -0.0301, -0.0645],\n",
      "         [-0.0163,  0.0527, -0.0171,  ..., -0.0659, -0.0292, -0.0735]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.4106e-01,  3.8723e-01, -1.4603e-02,  ...,  1.2881e+00,\n",
      "           1.7964e+00, -1.0229e-01],\n",
      "         [ 7.4460e-01,  5.9497e-01,  2.6695e-02,  ...,  1.2655e+00,\n",
      "           5.9371e-01, -5.6250e-01],\n",
      "         [ 6.6315e-01,  5.6780e-01, -3.3380e-01,  ...,  1.1533e+00,\n",
      "           8.5608e-01, -3.2189e-01],\n",
      "         ...,\n",
      "         [-9.4515e-03,  5.5566e-02, -1.5543e-02,  ..., -7.2509e-02,\n",
      "          -2.2624e-02, -8.4111e-02],\n",
      "         [ 6.5386e-04,  5.5848e-02, -1.8825e-02,  ..., -7.4872e-02,\n",
      "          -2.5062e-02, -8.7922e-02],\n",
      "         [ 5.3334e-04,  1.7385e-01,  1.3707e-02,  ..., -6.2233e-02,\n",
      "          -5.0991e-02, -2.4913e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.4041,  0.4639, -0.1338,  ...,  1.0878,  1.2251, -0.4719],\n",
      "         [ 0.9215,  0.3331, -0.0241,  ...,  0.8388,  0.9310, -0.3577],\n",
      "         [ 0.9260,  0.4501, -0.3507,  ...,  0.5693,  0.8128, -0.3828],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.4683, 0.9619, 0.6436,  ..., 1.2500, 0.9233, 1.0811], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.4929, 0.0668, 0.4849,  ..., 0.3943, 0.5557, 0.2947], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.2395,  0.4042, -0.1763,  ...,  1.3481,  1.3576, -0.4009],\n",
      "         [ 0.9253,  0.3312, -0.0289,  ...,  1.2392,  0.7236, -0.2830],\n",
      "         [ 0.9195, -0.1568, -0.1738,  ...,  0.4042,  1.3419, -0.2287],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.6846,  1.0615,  0.7524,  1.1152,  0.3855,  0.3096,  0.1443,  0.3259,\n",
      "         0.2712,  0.5742,  1.4102,  0.8003,  1.4873,  0.8032,  0.2871,  0.6392,\n",
      "         0.9097,  0.4336,  0.8345,  0.4897,  1.5820,  0.6948,  1.2754,  0.4775,\n",
      "         0.8350,  1.6328,  0.9902,  0.4241,  0.0641, -0.4419,  0.1716,  1.1553,\n",
      "        -0.2776,  0.1875,  1.0459,  0.5190,  0.9629,  0.5684,  0.5859,  0.4622,\n",
      "         0.4634,  1.1367,  0.9165,  1.0977,  0.9424,  0.7266, -0.0598,  0.8604,\n",
      "         0.7944,  0.6387,  0.6665, -0.0902,  0.4656, -0.1212,  0.2156,  1.2939,\n",
      "         0.4668,  1.3418,  0.8701,  0.7007,  0.0810,  0.8877,  0.4487,  1.1445,\n",
      "         0.7749,  1.2295,  0.7803,  0.6211,  0.6328,  0.4951,  0.4543,  1.5342,\n",
      "         0.9663,  1.1396,  0.6421,  0.7866,  0.4771,  0.5576,  1.4082,  0.9014,\n",
      "         1.0742,  0.6641,  1.1221,  0.4634,  0.7612,  0.7568,  0.4990,  0.0095,\n",
      "         0.7778,  0.8149,  0.5415,  0.5234,  0.7075,  0.4519,  0.8984,  0.1137,\n",
      "         0.7275,  0.4568,  0.9033,  1.5947,  0.8413,  0.9028,  1.5186,  0.6196,\n",
      "         0.9243,  0.7900,  0.6641,  0.4001,  1.0869,  1.0049,  0.9131,  0.3389,\n",
      "        -0.2808,  0.0046, -0.0327,  1.0918,  0.6372,  1.1406,  0.9805,  0.7964,\n",
      "         0.0444,  0.4543,  0.3770,  0.1126,  0.8735,  0.1512,  1.1465,  0.7329,\n",
      "         1.4844,  0.8315,  0.1199,  0.8755,  1.1123,  0.2217,  0.4744,  0.1935,\n",
      "         0.5381,  0.9346,  0.4651,  0.5146,  1.4639,  1.0723,  0.6890,  0.7993,\n",
      "         0.6841,  1.1318,  0.7764,  0.9102,  1.0957,  1.6553,  0.9541,  1.4531,\n",
      "         0.6455,  0.1421,  1.1123,  0.6289,  1.3789,  1.1045,  0.2676,  0.2578,\n",
      "        -0.1227,  1.3545,  0.9067,  0.3396,  1.4951,  0.8184,  0.1486,  0.8159,\n",
      "         1.7324,  0.8657,  1.0068,  1.6855,  0.7852,  0.1884,  0.9922,  0.7793,\n",
      "         0.8936,  0.2527,  1.3359,  0.8672,  0.6050,  1.7705,  1.1533,  0.6670,\n",
      "         0.8140,  0.6763,  1.6982,  0.8403,  1.3115,  0.8555,  1.5498,  0.8428,\n",
      "         1.3135,  0.8330,  0.7075,  1.0166,  0.4604,  0.6816,  0.7544,  0.1592,\n",
      "        -0.4153, -0.0059,  1.4717,  0.8105,  1.4844,  0.7905,  0.3206,  1.2871,\n",
      "         1.4688,  0.6343,  0.9404, -0.0756,  0.5542,  0.7319,  0.9189,  0.9062,\n",
      "         0.7163,  0.8906,  0.8379,  0.4695,  0.6929,  1.2012,  1.4600,  0.5991,\n",
      "         1.1025,  0.8794,  0.0310,  0.3940,  0.7192,  0.5127,  0.9971,  0.7183,\n",
      "         1.3057,  0.6465,  0.9722,  0.6880,  1.1348,  0.6001,  0.7207,  1.0732,\n",
      "         0.5103,  1.3369,  0.4329,  0.9360,  0.5371,  0.2286,  1.4092,  0.7139,\n",
      "         1.3350,  0.5366,  0.7446,  0.4353,  0.6196,  1.4961,  1.0957,  1.2734,\n",
      "         0.4927,  1.3203,  1.4609,  0.5127,  1.0410,  0.7690,  0.7759,  1.2686,\n",
      "         0.4932,  1.0615,  1.0234,  1.3672,  1.1230,  0.8506,  0.3733,  1.2920,\n",
      "         0.7749,  1.2617,  0.7310,  1.1963,  1.1973,  0.5854,  0.7578,  0.3572,\n",
      "         1.2539,  0.5142,  1.2246,  0.7812,  0.7979,  0.7769,  0.2128,  0.2649,\n",
      "         1.2764,  0.3413, -0.2532,  0.9736,  0.3755,  0.8569,  0.0519,  0.6597,\n",
      "         0.5972,  0.3875, -0.0285,  1.1201,  0.4695,  0.1633,  0.2068,  1.0273,\n",
      "         0.8672,  0.0656,  0.1048,  0.6846, -0.0648,  0.8999,  0.0137,  0.2659,\n",
      "         0.4155,  0.1339,  0.5747,  0.4563,  1.4443,  0.6226,  1.2393,  0.5698,\n",
      "         1.0996,  0.5439,  0.8696,  0.8403,  0.6748,  0.0441,  0.5791, -0.1021,\n",
      "         1.6221,  0.7744,  1.2852,  0.8193,  1.8115,  1.0596,  0.7871,  0.6045,\n",
      "         0.4646,  1.4043,  0.6450,  1.4541,  0.4360,  1.0361,  0.6641,  1.2441,\n",
      "         0.8467,  0.9331,  0.6167,  0.2720,  1.0576,  1.6377,  0.8452,  0.7085,\n",
      "         0.4751,  0.9043,  0.7891,  1.0859,  0.9663,  0.1542,  0.7344,  0.4751,\n",
      "        -0.0043,  0.5728,  1.1514, -0.1177,  0.8237,  0.1625,  0.8628,  1.0752,\n",
      "         0.9141,  0.2876,  0.4128, -0.1541,  0.6025,  1.1084,  0.8784,  0.7725,\n",
      "         1.0127,  0.0635,  0.5273,  1.1348, -0.0890,  0.7910,  0.0243,  0.9521,\n",
      "        -0.0474,  0.9229,  0.0073,  0.3125, -0.3152,  0.5386,  0.2213, -0.1974,\n",
      "         0.5625,  0.1917, -0.2112,  1.0342,  0.0807,  0.3782,  0.8291,  0.3518,\n",
      "        -0.0209,  0.7759,  0.5557, -0.2235,  1.0293,  0.7842,  0.3540,  1.0107,\n",
      "         0.0707,  0.8027,  0.6343,  0.0870,  0.4688,  0.0146,  0.7119,  0.6909,\n",
      "        -0.0440,  1.0332, -0.1721,  0.4482,  0.5259,  0.1159,  0.4143,  0.6221,\n",
      "         0.9575,  1.0322,  1.4385,  0.6611,  1.0498,  1.4912,  0.7495,  0.6162,\n",
      "         0.6362,  0.6055,  0.8345,  0.2800,  1.6465,  1.1816,  0.6963,  1.2803,\n",
      "         1.4805,  0.5884,  0.5220, -0.1265,  0.8779,  0.7886,  0.4224,  0.5371,\n",
      "         0.6899,  0.5605,  0.3374,  1.1709, -0.2688,  0.3530, -0.4060,  0.6494,\n",
      "         0.7520, -0.2247,  1.3496,  0.5352,  0.6016,  1.6475,  0.9893,  0.6436,\n",
      "         0.6768,  0.4446,  1.1152,  0.8774,  1.0518,  0.8560,  0.2321,  0.6289,\n",
      "         1.1328,  1.3994,  0.5811,  0.8340,  0.7954,  0.5449,  0.2448,  0.9067,\n",
      "         0.0315,  0.6904,  0.9683,  0.2749,  1.1904,  0.5649,  1.0498,  0.7856,\n",
      "         1.0156,  0.5259,  1.1621,  0.4697,  0.6860,  0.6924,  0.1703,  1.5273,\n",
      "         0.8735,  1.4375,  0.7900,  0.8428, -0.2079, -0.5527, -0.1534,  0.3291,\n",
      "        -0.0187,  0.8481,  0.2452,  0.2922,  0.4309,  0.4990,  0.5273,  0.9805,\n",
      "         0.6953,  1.1885,  1.0557,  1.0693,  0.3394,  0.8687,  1.3711,  0.7852,\n",
      "         0.6689,  1.0420,  0.6577,  0.0824,  0.3621,  1.8438,  1.0039,  1.7451,\n",
      "         1.0469,  1.1611, -0.1937,  0.3601,  0.0782,  1.7031,  1.1631,  1.4521,\n",
      "         0.6743,  0.9282,  0.7051, -0.1516, -0.2389,  1.2686,  0.6792,  1.3818,\n",
      "         0.7231,  0.7607,  0.5547,  0.9292,  0.0475,  0.4971,  0.7524,  0.1903,\n",
      "         1.5801,  0.7520,  1.1035,  0.6128,  0.6431,  0.4214,  1.5059,  0.8511,\n",
      "         0.7949,  0.6235,  0.7651,  0.5503,  0.1797,  0.4795, -0.0076,  0.4526,\n",
      "         0.5029, -0.1044,  0.8286,  0.6748,  0.5806,  0.2185,  0.4795,  1.0420,\n",
      "         0.2561,  0.3940,  0.5171,  0.8506,  0.1791,  1.6758,  1.1875,  1.6143,\n",
      "         0.9268,  1.5156,  0.6738,  0.7163,  1.7314,  0.9902,  1.3438,  0.7676,\n",
      "         1.4600,  0.9971,  1.4834,  1.1289,  1.6826,  0.9390,  0.4160,  1.0205,\n",
      "         0.6973,  0.7280,  1.3223,  1.1650,  1.4062,  1.1328,  1.3486,  0.8491,\n",
      "         1.3730,  1.4453,  0.6694,  0.2949,  0.6113,  1.7715,  0.7881,  1.0215,\n",
      "         1.6230,  0.9805,  0.1110,  0.1960,  0.9307,  0.6333,  0.4741,  0.3000,\n",
      "         1.7256,  1.0947,  1.4834,  0.9834,  1.5791,  1.1035,  1.2842,  1.6826,\n",
      "         1.0703,  0.4761], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 3.1006e-01, -3.3569e-01, -3.8843e-01,  1.0864e-01, -8.2910e-01,\n",
      "        -4.2065e-01, -6.1328e-01, -8.7463e-02, -5.5225e-01, -3.3765e-01,\n",
      "        -1.6138e-01,  2.5830e-01,  3.6694e-01,  1.0632e-01, -2.2205e-01,\n",
      "         1.6370e-01, -1.2854e-01,  3.0200e-01,  5.2277e-02,  4.3140e-01,\n",
      "         8.1970e-02,  5.0684e-01,  6.1133e-01,  2.8491e-01,  3.2397e-01,\n",
      "         8.7830e-02,  4.7778e-01, -1.7444e-01,  3.2446e-01,  4.3774e-01,\n",
      "        -1.8958e-01,  4.2627e-01,  2.6416e-01, -3.9453e-01,  2.3010e-02,\n",
      "         3.6621e-01,  3.7622e-01,  2.8052e-01,  4.5312e-01,  2.3730e-01,\n",
      "         5.4639e-01,  7.7002e-01,  4.4653e-01,  3.3496e-01,  4.5923e-01,\n",
      "         3.0933e-01, -3.9429e-01,  3.0884e-01,  1.9836e-01,  1.4722e-01,\n",
      "         1.5894e-01, -4.0503e-01, -1.3293e-01,  2.9004e-01, -3.1250e-01,\n",
      "         3.5449e-01,  6.2207e-01,  2.6099e-01,  6.0010e-01,  4.4482e-01,\n",
      "        -2.2766e-01,  8.9478e-02,  3.2812e-01,  4.8071e-01,  4.1846e-01,\n",
      "        -1.1774e-01,  6.5625e-01,  5.2344e-01,  4.5044e-01, -2.4695e-01,\n",
      "         4.0869e-01, -1.2939e-01,  4.9731e-01,  4.1895e-01,  6.2354e-01,\n",
      "         7.3425e-02,  3.7915e-01,  4.7021e-01, -8.2092e-02,  3.7427e-01,\n",
      "         3.5425e-01,  6.7822e-01, -1.7105e-02, -6.7871e-02,  4.2749e-01,\n",
      "         4.6045e-01,  5.4834e-01, -5.1416e-01,  3.3472e-01,  3.0469e-01,\n",
      "        -1.1407e-01,  2.4670e-01, -3.1174e-02, -5.7129e-01,  3.8721e-01,\n",
      "         3.3911e-01, -8.5938e-02,  1.7493e-01,  1.2634e-01, -2.1561e-02,\n",
      "         2.5098e-01,  5.9418e-02,  5.1367e-01,  5.2148e-01, -1.3870e-02,\n",
      "         4.7778e-01,  1.5222e-01, -1.3684e-01,  3.0786e-01,  3.5303e-01,\n",
      "         3.7305e-01, -3.4277e-01, -6.4355e-01, -2.9221e-02, -1.5112e-01,\n",
      "         1.3354e-01,  1.7908e-01,  7.2998e-02,  4.3359e-01, -6.7566e-02,\n",
      "         3.6736e-03, -1.2854e-01, -3.2324e-01, -8.1738e-01,  1.8494e-01,\n",
      "         6.0364e-02,  3.0542e-01,  4.7925e-01,  3.3057e-01,  8.5840e-01,\n",
      "        -2.8442e-01, -7.4072e-01,  1.8616e-01, -3.1226e-01, -3.9648e-01,\n",
      "        -2.5098e-01, -3.1689e-01, -1.6785e-01,  8.3618e-02,  5.5756e-02,\n",
      "         7.2876e-02,  4.5630e-01,  6.1475e-01,  7.7332e-02,  2.1973e-02,\n",
      "         1.4763e-03,  3.5645e-01,  3.6719e-01,  4.0283e-01,  9.6680e-02,\n",
      "         4.7070e-01,  3.3032e-01,  6.1914e-01, -1.7105e-02,  1.5259e-01,\n",
      "         2.1204e-01,  5.0830e-01,  8.0872e-02, -4.4830e-02, -2.5562e-01,\n",
      "        -2.0459e-01, -4.4746e-03,  2.7246e-01, -3.6987e-01,  2.8198e-01,\n",
      "         2.9834e-01, -6.7383e-01, -6.4502e-01, -2.1387e-01,  1.7346e-01,\n",
      "        -7.5150e-04,  3.7622e-01,  3.8940e-01,  2.7466e-01, -1.9617e-01,\n",
      "         2.9004e-01,  1.1810e-01,  1.2219e-01,  3.5229e-01,  6.3477e-01,\n",
      "        -1.5417e-01,  2.8540e-01,  6.6113e-01,  8.8721e-01, -1.3416e-01,\n",
      "         1.5601e-01, -1.3110e-01,  5.0635e-01,  5.3125e-01,  2.7295e-01,\n",
      "         1.4441e-01,  4.1797e-01,  4.8486e-01,  2.3889e-01,  1.2756e-01,\n",
      "         3.8849e-02,  2.2302e-01, -1.1029e-01,  3.8940e-01, -1.1432e-01,\n",
      "         3.3228e-01, -2.9541e-01, -6.0310e-03,  3.6938e-01,  3.5352e-01,\n",
      "         1.1212e-01, -5.1904e-01,  3.4595e-01,  4.8267e-01,  5.7324e-01,\n",
      "         4.6631e-01, -2.9639e-01,  3.5986e-01, -4.6094e-01, -3.5400e-01,\n",
      "         3.6450e-01,  2.8027e-01,  4.2993e-01,  2.6514e-01, -7.7539e-01,\n",
      "         4.0845e-01,  1.0632e-01,  3.1982e-01,  6.0596e-01,  3.1860e-01,\n",
      "         2.2293e-02, -4.7028e-02,  5.0391e-01,  2.8638e-01, -2.0801e-01,\n",
      "        -1.8051e-02,  3.8672e-01,  4.0356e-01,  2.3889e-01, -2.6172e-01,\n",
      "         5.7568e-01, -6.4648e-01,  1.9946e-01,  4.0039e-01,  2.3145e-01,\n",
      "         4.3140e-01, -1.4111e-01,  2.3560e-01,  2.8003e-01,  5.2441e-01,\n",
      "        -9.3201e-02,  9.2712e-02,  4.2236e-01,  3.8037e-01,  4.1284e-01,\n",
      "        -1.9434e-01,  3.8550e-01,  1.9482e-01,  1.3458e-02, -2.2986e-01,\n",
      "         4.5013e-02, -6.1377e-01,  1.2451e-01,  2.7148e-01,  5.7520e-01,\n",
      "         4.7266e-01, -9.5673e-03, -2.1460e-01,  1.7786e-01,  1.4526e-01,\n",
      "         3.7354e-01,  8.5602e-03, -9.6588e-03,  7.3120e-02,  1.9519e-01,\n",
      "         6.6797e-01,  4.9255e-02,  5.0781e-01,  4.8633e-01,  2.6978e-01,\n",
      "         2.2083e-01,  2.1167e-01,  1.3208e-01,  3.6499e-01, -1.8478e-02,\n",
      "        -2.4377e-01,  1.6003e-01,  3.3032e-01,  1.5881e-01,  6.1816e-01,\n",
      "        -4.6387e-01, -3.2812e-01, -2.0447e-01,  1.5491e-01,  2.4207e-01,\n",
      "        -4.4238e-01,  9.8419e-03,  1.8884e-01,  3.5547e-01, -4.5020e-01,\n",
      "         4.3732e-02,  4.2749e-01,  2.2266e-01, -6.0699e-02, -1.3513e-01,\n",
      "        -8.3130e-02,  5.4346e-01, -3.5083e-01, -6.2225e-02, -4.3433e-01,\n",
      "         4.1748e-01, -4.7998e-01,  9.3323e-02,  4.3262e-01,  9.5749e-04,\n",
      "         4.4019e-01, -4.8730e-01,  3.3350e-01,  1.3855e-01, -5.3125e-01,\n",
      "         4.0259e-01,  8.6853e-02,  4.1968e-01,  4.9927e-01,  4.1699e-01,\n",
      "         2.7075e-01, -6.7444e-02,  4.1724e-01,  2.1655e-01,  5.6689e-01,\n",
      "        -5.8008e-01,  1.1823e-01,  3.9014e-01,  2.4146e-01,  6.5869e-01,\n",
      "         6.4746e-01,  4.3384e-01,  5.1221e-01,  1.8945e-01,  7.1289e-01,\n",
      "         3.6597e-01, -1.8417e-02,  3.7256e-01,  5.6543e-01,  4.6484e-01,\n",
      "         5.9570e-01,  2.0264e-01,  5.4150e-01,  5.6055e-01,  2.3682e-01,\n",
      "         4.6387e-01,  8.6084e-01, -4.2023e-02,  3.4937e-01,  5.2344e-01,\n",
      "         5.3955e-01, -8.3923e-02,  4.1846e-01,  1.5051e-01, -6.3667e-03,\n",
      "         2.6172e-01,  2.4216e-02, -7.1777e-01,  1.6296e-01, -1.0699e-01,\n",
      "        -4.2041e-01, -3.2983e-01,  2.5421e-02, -6.0938e-01, -3.3423e-01,\n",
      "        -3.5400e-01,  5.3101e-02,  2.3706e-01, -3.1952e-02, -6.6943e-01,\n",
      "        -1.7654e-02, -6.2500e-01, -6.7822e-01, -1.6064e-01, -7.1411e-02,\n",
      "        -4.4238e-01, -1.9556e-01, -3.9600e-01,  4.0552e-01,  1.5356e-01,\n",
      "        -4.7412e-01, -1.2469e-01, -5.5084e-02,  2.8223e-01, -5.0098e-01,\n",
      "         3.1311e-02,  1.2177e-02,  1.1993e-02, -9.9365e-01, -1.9702e-01,\n",
      "        -5.1123e-01, -6.4404e-01, -5.5908e-01, -5.5511e-02,  1.3501e-01,\n",
      "         2.8488e-02, -7.3340e-01,  5.5908e-02, -6.6748e-01, -1.9507e-01,\n",
      "         4.1089e-01, -2.4772e-04, -4.2700e-01, -3.2373e-01, -4.1846e-01,\n",
      "        -3.1885e-01, -1.5060e-02, -1.3721e-01, -6.4209e-01, -4.7803e-01,\n",
      "        -1.2030e-01,  1.6492e-01, -2.1472e-01, -3.5547e-01, -9.8206e-02,\n",
      "        -1.3843e-01, -7.4365e-01, -1.0089e-01,  1.1066e-01, -4.5685e-02,\n",
      "         1.1621e-01, -2.0605e-01, -3.6084e-01, -3.8843e-01, -1.3660e-01,\n",
      "         1.6675e-01,  6.0156e-01,  6.3037e-01,  3.7744e-01,  5.7275e-01,\n",
      "         5.5127e-01,  2.9419e-01,  1.3464e-01,  2.3389e-01,  5.1904e-01,\n",
      "        -5.1807e-01,  1.1591e-01,  9.3628e-02,  5.4004e-01,  5.3027e-01,\n",
      "         4.6240e-01,  5.5664e-01,  1.0876e-01, -3.1128e-01,  6.6357e-01,\n",
      "         6.1127e-02, -3.4370e-03,  3.3423e-01,  2.8027e-01,  6.1182e-01,\n",
      "        -5.8105e-01,  6.8970e-02, -5.7666e-01, -5.5428e-03,  1.9910e-01,\n",
      "         2.7881e-01,  2.7539e-01,  1.2115e-01,  1.4001e-01,  5.2979e-01,\n",
      "         1.6357e-01,  4.4531e-01,  1.7627e-01,  6.4453e-01, -2.1851e-01,\n",
      "         3.8110e-01, -1.5063e-01,  6.3477e-01, -2.1753e-01,  7.1533e-01,\n",
      "        -9.1211e-01,  1.5247e-01,  2.2229e-01,  4.3140e-01,  6.6162e-01,\n",
      "         5.5664e-01,  4.8889e-02,  1.7200e-01, -3.9917e-01,  5.5225e-01,\n",
      "        -3.2845e-03,  1.0504e-01,  7.8955e-01, -8.2825e-02,  1.9604e-01,\n",
      "         4.7314e-01,  3.0200e-01,  4.3311e-01,  3.9502e-01,  4.2358e-01,\n",
      "         6.9238e-01,  2.8857e-01, -6.6223e-02,  3.5791e-01, -3.7079e-02,\n",
      "         1.2360e-01,  5.6348e-01,  6.7920e-01,  5.3271e-01,  2.1313e-01,\n",
      "        -5.0195e-01, -9.2285e-02, -5.3662e-01,  8.9172e-02, -1.9922e-01,\n",
      "         3.9551e-01,  4.8340e-02,  6.1328e-01,  8.1299e-02, -3.5596e-01,\n",
      "        -1.8225e-01, -1.8469e-01,  5.4639e-01, -4.0894e-01, -1.6861e-02,\n",
      "         2.6929e-01, -6.7188e-01,  6.3110e-02,  1.5356e-01,  6.2891e-01,\n",
      "        -4.5319e-02,  1.7419e-01, -5.0385e-02,  1.8994e-01, -1.4124e-01,\n",
      "         1.4600e-01,  6.6797e-01,  4.8169e-01,  1.5601e-01,  7.2803e-01,\n",
      "        -5.5273e-01, -5.0751e-02, -3.8940e-01,  1.5759e-01,  9.3457e-01,\n",
      "         3.6963e-01,  4.7192e-01,  6.1377e-01,  8.5840e-01, -1.0187e-01,\n",
      "        -1.4209e-01,  1.7029e-01,  5.2441e-01,  1.6516e-01,  3.8745e-01,\n",
      "         5.8936e-01,  3.9478e-01,  9.5703e-02,  1.0938e-01, -5.0140e-02,\n",
      "         1.6736e-01, -1.6357e-01, -1.1774e-01,  5.1123e-01,  3.0420e-01,\n",
      "         7.8662e-01, -2.5391e-01, -4.3555e-01,  3.1592e-01, -3.1799e-02,\n",
      "         6.8359e-01,  5.5518e-01,  2.4390e-01,  3.2135e-02, -7.5879e-01,\n",
      "         7.6111e-02, -5.7617e-01, -5.9229e-01,  1.7407e-01, -2.7124e-01,\n",
      "         1.2183e-01,  3.0396e-01, -6.9946e-02, -1.1017e-01, -4.4702e-01,\n",
      "         6.0669e-02, -5.8136e-02, -3.3521e-01, -2.5000e-01, -1.9507e-01,\n",
      "        -1.9116e-01,  3.5913e-01,  6.0791e-01,  8.0664e-01,  9.6191e-01,\n",
      "         2.7173e-01,  3.8672e-01,  6.4600e-01,  6.2256e-01,  9.7803e-01,\n",
      "         4.6655e-01,  3.4839e-01, -1.7868e-02,  4.0918e-01,  4.1919e-01,\n",
      "         4.5020e-01,  3.1152e-01,  4.5923e-01, -3.9209e-01,  3.8574e-01,\n",
      "         4.8950e-01,  4.0137e-01,  6.8298e-02,  3.3350e-01,  2.0740e-01,\n",
      "         4.0186e-01,  4.3726e-01,  2.6807e-01,  5.6152e-01,  7.3584e-01,\n",
      "         8.5547e-01, -1.2286e-01, -7.7942e-02,  1.5051e-01,  3.0005e-01,\n",
      "         6.4014e-01,  3.7305e-01,  1.1113e+00, -4.0479e-01, -7.0996e-01,\n",
      "         5.0537e-01,  5.3027e-01,  5.2783e-01, -3.5400e-01,  2.4585e-01,\n",
      "         7.8223e-01,  4.2700e-01,  6.2158e-01,  4.4629e-01,  6.6943e-01,\n",
      "         1.9470e-01,  2.3438e-01,  7.4072e-01,  3.9746e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.5970,  0.3794, -0.1368,  ...,  1.3972,  1.5290, -0.5351],\n",
      "         [ 0.6204,  0.3097, -0.0952,  ...,  0.8941,  1.0560, -0.5420],\n",
      "         [ 1.3701,  0.0881, -0.0524,  ...,  1.1313,  1.5869, -0.6117],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 4.3433e-01,  1.1357e+00,  5.8447e-01,  4.4141e-01, -1.6602e-02,\n",
      "        -2.6123e-01,  4.5605e-01,  5.2539e-01, -8.0078e-02,  4.5128e-03,\n",
      "         5.1819e-02,  3.9575e-01,  5.2637e-01,  2.9419e-01,  6.2402e-01,\n",
      "         2.4451e-01,  3.0975e-02,  6.4502e-01,  7.8662e-01,  1.0869e+00,\n",
      "         8.0371e-01,  1.2920e+00,  4.5898e-01,  7.6416e-01,  9.9609e-01,\n",
      "         1.9214e-01,  3.1079e-01,  7.0850e-01,  7.9590e-01,  3.7280e-01,\n",
      "         4.5020e-01,  1.5234e-01,  4.4824e-01,  3.6084e-01,  8.6670e-02,\n",
      "         4.3066e-01,  3.1494e-01,  1.0322e+00,  1.8127e-01,  1.1973e+00,\n",
      "        -1.4172e-01,  7.9102e-01, -1.7029e-01,  1.0547e+00,  6.0693e-01,\n",
      "         1.9421e-01,  5.7471e-01, -2.5000e-01,  1.4160e+00,  5.5322e-01,\n",
      "         9.9219e-01,  7.0374e-02,  1.2451e+00,  6.5186e-01,  1.3184e+00,\n",
      "         6.9385e-01,  1.8933e-01,  5.7422e-01,  4.2432e-01, -2.1863e-01,\n",
      "        -2.9590e-01, -1.8958e-01,  6.1084e-01, -3.5596e-01,  1.3965e+00,\n",
      "         1.1504e+00,  6.1084e-01,  4.6167e-01,  1.1660e+00,  7.5439e-01,\n",
      "         9.1553e-01,  5.7227e-01,  1.8896e-01,  1.6191e+00,  5.8496e-01,\n",
      "         1.4678e+00,  6.4014e-01,  1.1270e+00,  4.4165e-01,  8.7793e-01,\n",
      "         5.1562e-01,  1.4043e+00,  7.4561e-01,  1.5479e+00,  8.4375e-01,\n",
      "         7.9395e-01,  1.0566e+00,  7.4951e-01,  5.5225e-01,  1.0244e+00,\n",
      "         5.7373e-01,  5.1953e-01,  7.1094e-01, -1.7749e-01, -3.5938e-01,\n",
      "         7.5391e-01,  1.5791e+00,  9.9512e-01,  1.2822e+00,  6.6602e-01,\n",
      "         1.0615e+00,  1.1855e+00,  2.7515e-01,  4.7241e-01,  3.2666e-01,\n",
      "         1.1816e+00,  1.0469e+00,  1.1328e+00,  6.3184e-01,  1.2188e+00,\n",
      "         9.7852e-01,  2.8955e-01,  4.5581e-01,  6.9678e-01,  9.7217e-01,\n",
      "         4.8486e-01,  9.4775e-01,  4.1479e-01,  2.1680e-01,  1.2910e+00,\n",
      "         7.3975e-01,  1.1201e+00,  3.2910e-01,  5.2734e-01,  5.7373e-01,\n",
      "         1.3574e+00,  4.4385e-01,  5.0000e-01,  8.8525e-01,  1.2274e-01,\n",
      "         1.1836e+00,  4.2847e-01,  1.2383e+00,  3.9966e-01,  6.3623e-01,\n",
      "         8.2080e-01,  9.1846e-01,  9.6143e-01,  4.5679e-01,  1.1895e+00,\n",
      "         7.7686e-01,  5.4248e-01,  4.8193e-01,  2.0642e-01,  8.3643e-01,\n",
      "         1.0156e+00,  2.3401e-01,  8.8818e-01, -1.0565e-01,  1.2754e+00,\n",
      "         8.7305e-01,  1.0654e+00,  8.0469e-01,  1.6162e-01,  1.4248e+00,\n",
      "         9.0039e-01,  1.4287e+00,  5.5566e-01,  8.4717e-01,  1.0830e+00,\n",
      "         1.1094e+00,  5.7910e-01,  7.0215e-01,  6.1475e-01,  8.1006e-01,\n",
      "         1.6211e-01,  1.0273e+00,  3.7476e-01,  4.2285e-01,  7.1680e-01,\n",
      "         6.9629e-01,  7.5830e-01, -8.0200e-02,  3.2227e-01, -2.5342e-01,\n",
      "         5.8643e-01,  6.8457e-01,  8.4277e-01,  2.1973e-01,  1.0996e+00,\n",
      "         7.2021e-01,  1.3066e+00,  3.8843e-01,  1.0107e+00,  8.1445e-01,\n",
      "         9.7412e-01,  9.6875e-01, -1.6858e-01,  1.2732e-01,  8.6719e-01,\n",
      "         7.4219e-01,  1.0120e-01,  5.6152e-01,  1.0919e-01,  7.7019e-03,\n",
      "         8.4961e-01,  1.0654e+00,  5.3760e-01,  1.6785e-01,  1.9209e+00,\n",
      "         1.4502e+00,  8.6523e-01,  9.6094e-01,  7.0459e-01, -4.7192e-01,\n",
      "        -3.4576e-02, -3.2178e-01, -1.1896e-01,  4.6094e-01,  4.6362e-01,\n",
      "         2.9565e-01,  2.5879e-01,  8.2080e-01,  5.5322e-01,  7.4756e-01,\n",
      "         5.5322e-01,  4.6167e-01,  1.4990e+00,  7.6270e-01,  1.2217e+00,\n",
      "         1.2080e+00,  9.6338e-01,  9.0674e-01,  1.4238e+00,  8.2959e-01,\n",
      "         4.1064e-01,  1.0175e-01, -2.5684e-01,  3.2715e-01,  5.1904e-01,\n",
      "         9.2676e-01,  6.5820e-01,  8.5449e-01,  8.9111e-01,  5.3418e-01,\n",
      "         5.5420e-01,  8.4839e-02,  8.1885e-01, -6.2866e-02,  1.1182e+00,\n",
      "         8.7305e-01,  1.1807e+00,  9.6777e-01,  5.7373e-01,  1.8428e+00,\n",
      "         9.4238e-01,  1.4736e+00,  7.3633e-01,  2.2168e-01,  9.0820e-01,\n",
      "        -1.5271e-01,  2.9846e-02,  7.6562e-01,  2.5684e-01,  1.4893e+00,\n",
      "         2.6587e-01,  7.0508e-01,  4.8486e-01,  1.5137e+00,  9.3262e-01,\n",
      "         7.0361e-01,  5.3418e-01,  1.7261e-01,  5.1270e-01,  5.6445e-01,\n",
      "        -2.0544e-01, -3.6804e-02, -1.9861e-01,  3.1519e-01,  5.1270e-01,\n",
      "         4.0869e-01,  2.0740e-01,  1.8066e+00,  9.5117e-01,  1.2128e-01,\n",
      "         1.5601e-01,  5.9570e-02,  8.5632e-02,  1.4355e+00,  7.0361e-01,\n",
      "         9.8242e-01,  7.3535e-01,  4.3994e-01,  7.1045e-01,  1.8768e-02,\n",
      "         6.8652e-01,  6.9092e-01,  4.0344e-02,  6.7383e-01,  2.2766e-01,\n",
      "         5.3711e-01,  1.5588e-01,  1.5342e+00,  7.1094e-01,  1.0146e+00,\n",
      "         5.2588e-01,  4.5581e-01,  1.2659e-01,  9.3701e-01,  1.9312e-01,\n",
      "         1.6279e+00,  1.0664e+00,  1.1709e+00,  6.3574e-01,  1.0244e+00,\n",
      "         9.3945e-01,  4.2896e-01,  2.1289e-01,  1.5391e+00,  1.0312e+00,\n",
      "        -6.3599e-02,  1.1367e+00,  7.0654e-01,  4.9951e-01,  3.5352e-01,\n",
      "         1.3789e+00,  7.1143e-01,  1.0391e+00,  4.5752e-01, -5.0977e-01,\n",
      "         2.9199e-01, -2.6880e-01, -3.7012e-01,  5.9937e-02,  7.4854e-01,\n",
      "        -1.2915e-01,  2.6025e-01,  3.7134e-01,  1.2900e+00,  5.4785e-01,\n",
      "         9.6582e-01,  5.1855e-01, -1.7957e-01, -4.8676e-02,  5.0774e-03,\n",
      "         1.4424e+00,  7.5830e-01,  1.2217e+00,  8.2275e-01,  7.5244e-01,\n",
      "         1.0944e-01,  2.6733e-01,  1.0811e+00,  1.3291e+00,  1.0215e+00,\n",
      "         5.9766e-01,  6.8652e-01,  6.8359e-01,  8.1726e-02,  4.6191e-01,\n",
      "         2.7409e-03, -2.7148e-01,  4.1553e-01, -3.2739e-01,  5.3564e-01,\n",
      "         5.6201e-01,  9.1211e-01,  5.2344e-01,  9.0039e-01,  2.8027e-01,\n",
      "         1.5552e-01, -1.9653e-01, -1.4706e-03,  3.6328e-01, -6.4209e-02,\n",
      "         8.1494e-01,  1.1957e-01,  6.4990e-01,  1.3438e+00,  1.2842e+00,\n",
      "         7.2803e-01,  6.1621e-01,  4.1992e-01,  6.7480e-01,  7.0850e-01,\n",
      "         1.2018e-01, -7.2144e-02,  3.4888e-01,  1.4685e-01,  5.2881e-01,\n",
      "         7.3486e-01, -1.8433e-01,  7.5098e-01,  9.1211e-01,  1.0596e+00,\n",
      "         5.3564e-01,  1.2832e+00,  7.0898e-01, -2.0105e-01, -6.3416e-02,\n",
      "         3.1177e-01,  6.6162e-01, -1.9299e-01,  6.9629e-01, -1.8387e-02,\n",
      "         1.2217e+00,  5.8398e-01,  2.6172e-01, -1.3538e-01,  4.7852e-01,\n",
      "         4.3970e-01,  6.7969e-01,  7.9785e-01,  6.6357e-01,  5.3516e-01,\n",
      "         1.1035e+00,  3.3295e-02,  2.7930e-01,  6.5283e-01,  3.0762e-02,\n",
      "         1.2891e+00,  5.4541e-01,  6.0645e-01,  8.7354e-01,  1.4922e+00,\n",
      "         7.7490e-01,  1.2573e-01,  2.8882e-01,  3.8867e-01,  1.1230e+00,\n",
      "         1.2866e-01,  5.6689e-01,  1.0312e+00,  3.3569e-01,  1.2433e-01,\n",
      "         4.5508e-01,  2.2095e-02,  1.1338e+00,  8.4473e-01,  8.3350e-01,\n",
      "         2.1436e-01,  1.4951e+00,  9.9219e-01,  1.1504e+00,  8.9111e-02,\n",
      "         1.5173e-01,  5.2979e-01,  2.8442e-02,  1.1455e+00,  4.8682e-01,\n",
      "         1.0166e+00,  2.4426e-01,  1.6787e+00,  6.3428e-01,  1.0986e+00,\n",
      "         8.5059e-01,  6.4307e-01, -2.0862e-01, -1.7761e-01,  4.1016e-01,\n",
      "         1.1377e+00,  5.6592e-01, -1.5991e-01,  1.0859e+00,  1.4000e-02,\n",
      "         2.9810e-01,  1.3584e+00,  2.9321e-01,  7.6123e-01,  4.9121e-01,\n",
      "         1.4375e+00,  2.7124e-01,  8.5596e-01,  1.4756e+00,  3.1689e-01,\n",
      "         5.5371e-01,  9.0381e-01,  3.6108e-01,  9.5459e-01,  2.4548e-01,\n",
      "         2.7588e-02, -2.1802e-01, -2.5586e-01, -3.4326e-01,  1.5466e-01,\n",
      "         1.1847e-01,  1.3574e+00,  8.7988e-01,  7.4023e-01,  3.7573e-01,\n",
      "         4.0967e-01, -6.7334e-01,  1.0908e+00,  1.9409e-01,  1.0986e+00,\n",
      "         1.9775e-01,  6.2109e-01, -5.3131e-02,  1.9263e-01,  8.8379e-01,\n",
      "         3.9722e-01, -2.3840e-01,  1.2598e+00,  2.1057e-01,  7.7051e-01,\n",
      "        -3.2593e-01,  1.5222e-01, -3.5620e-01,  3.9941e-01,  4.1211e-01,\n",
      "         2.1545e-01,  1.5222e-01,  1.5015e-01,  1.1064e+00,  9.5361e-01,\n",
      "         8.5840e-01, -2.4471e-03,  1.2002e+00,  1.0098e+00, -3.4180e-01,\n",
      "        -4.8706e-01, -1.4014e-01,  3.8208e-01,  6.7871e-02,  4.5703e-01,\n",
      "         4.1333e-01,  3.8916e-01,  4.7510e-01,  3.0518e-01,  5.0781e-01,\n",
      "         3.7915e-01,  2.2095e-02,  4.9609e-01,  9.4788e-02,  2.9346e-01,\n",
      "         1.4380e-01,  8.5791e-01,  6.8848e-01,  8.0322e-01,  1.1309e+00,\n",
      "         7.7197e-01, -3.4351e-01,  2.3499e-01,  1.0371e+00,  8.7354e-01,\n",
      "         4.0137e-01,  9.4922e-01,  3.3862e-01,  2.4768e-01,  3.3228e-01,\n",
      "        -4.7607e-02, -1.7041e-01,  4.2212e-01,  1.3203e+00,  4.8486e-01,\n",
      "         8.1787e-01,  9.5850e-01,  4.8584e-01,  1.2927e-01,  6.0010e-01,\n",
      "         1.8176e-01,  2.1423e-01,  1.0332e+00,  2.7930e-01,  7.3633e-01,\n",
      "         1.3843e-01,  9.5166e-01,  4.3726e-01,  8.5449e-01,  3.3691e-01,\n",
      "         1.8689e-01,  2.5171e-01,  3.1689e-01,  1.3486e+00,  7.3730e-01,\n",
      "         8.4082e-01,  1.9238e-01,  9.9121e-01,  1.8140e-01,  5.5420e-01,\n",
      "        -4.1962e-02,  8.4668e-01,  1.0785e-01,  7.0117e-01,  1.0762e+00,\n",
      "         4.9121e-01,  6.5039e-01, -7.8552e-02,  5.8447e-01,  9.0479e-01,\n",
      "         8.3789e-01,  8.2764e-01,  6.8408e-01,  6.7920e-01,  1.0537e+00,\n",
      "         5.6543e-01,  1.0162e-01,  8.9697e-01,  6.5967e-01,  4.8633e-01,\n",
      "         2.4304e-01, -2.3083e-01, -5.5122e-03, -5.9619e-01,  2.7026e-01,\n",
      "         1.4685e-01,  6.1035e-01,  7.5830e-01,  7.0557e-01,  6.4404e-01,\n",
      "         5.1318e-01,  9.2822e-01,  5.5322e-01,  6.9397e-02,  9.1943e-01,\n",
      "         6.5088e-01,  4.7266e-01,  1.1908e-01,  3.4424e-01,  3.2251e-01,\n",
      "        -1.5234e-01,  2.6050e-01,  7.0457e-03,  1.2480e+00,  1.0479e+00,\n",
      "         8.7354e-01,  4.1211e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.2820, -0.1025,  0.2542, -0.1635, -0.2477, -0.4119, -0.4119, -0.2001,\n",
      "        -0.9092, -0.7510, -0.6523, -0.0605,  0.0614, -0.3025,  0.4226,  0.0129,\n",
      "         0.0990,  0.1605, -0.0534,  0.5811,  0.8911,  0.4819,  0.8188,  0.8281,\n",
      "         0.7739,  0.0898,  0.2974,  0.0874,  0.4988,  0.2349,  0.4138, -0.2949,\n",
      "         0.1383,  0.1239,  0.3789,  0.4795,  0.3445,  0.2551,  0.1059,  0.4983,\n",
      "        -0.3799, -0.3887, -0.1586, -0.1807,  0.3035,  0.0853, -0.3054, -0.2213,\n",
      "         0.2007,  0.4250,  0.6665, -0.1543,  0.0262,  0.2358,  0.4517,  0.2384,\n",
      "         0.7061,  0.3022,  0.1066, -0.4795,  0.0044, -0.0324, -0.2700, -0.3479,\n",
      "         0.5083,  1.0068,  0.8408,  0.2825,  0.1497,  0.4338,  0.7954,  0.7246,\n",
      "         0.3569,  0.2620,  0.6606,  0.6694,  0.8813,  0.6113,  0.6665,  0.4062,\n",
      "        -0.0531,  0.0621,  0.9526,  0.2361,  0.0951,  0.8848,  0.4072,  0.6006,\n",
      "         0.1930, -0.1026,  0.2189, -0.1854,  0.2522,  0.6426,  0.0898, -0.3689,\n",
      "         0.3389,  0.4580,  0.6392,  1.1396,  0.8555,  0.6401, -0.2717, -0.0503,\n",
      "        -0.4517,  0.6328,  0.8545,  0.7007, -0.2369, -0.3489, -0.0304,  0.0102,\n",
      "        -0.1628,  0.2109,  0.0690,  0.2145,  0.2117,  0.3303, -0.0218,  0.4146,\n",
      "         0.5635,  0.4519,  0.9590,  0.3176, -0.2220,  0.0178, -0.2054, -0.2559,\n",
      "         0.0264,  0.3752,  0.0590, -0.1309,  0.4216,  0.5146,  0.1193, -0.1620,\n",
      "         0.2512,  0.3000,  0.2607,  0.2313,  0.6934,  0.3665, -0.1801,  0.1284,\n",
      "        -0.5591,  0.1406,  0.2737, -0.1158,  0.0701,  0.0434,  0.2832,  0.4778,\n",
      "         1.0527, -0.4834,  0.2191,  1.1055,  0.5381,  0.3635,  1.1846,  0.5303,\n",
      "         0.1580,  0.4631,  0.5068,  0.2271,  0.5615, -0.3179,  0.7227,  0.4983,\n",
      "         0.1976,  0.0188, -0.0443,  0.7661, -0.4534, -0.8901, -0.9336,  0.5708,\n",
      "         0.1567,  0.1990, -0.0321,  0.7095,  0.6265,  0.6460,  0.1605,  1.0029,\n",
      "         0.1754, -0.1954, -0.2021, -0.0647, -0.1144,  0.5996,  0.0395, -0.5688,\n",
      "         0.2379, -0.0656, -0.5020,  0.8130,  0.5161, -0.1678,  0.1885,  0.4670,\n",
      "         0.2048,  0.6890,  0.7490,  0.9102, -0.6743, -0.5352, -0.3052, -0.5493,\n",
      "         0.2076,  0.2734,  0.0143, -0.2715,  0.5947,  0.2198,  0.8174, -0.1078,\n",
      "         0.4800, -0.0067,  0.8140,  0.3560,  0.6060,  1.3877,  0.5659,  0.2257,\n",
      "         0.7397,  0.1854,  0.4348,  0.3662,  0.2764,  0.2749,  0.0138,  0.4526,\n",
      "         0.3765,  0.8765,  0.4314, -0.2250, -0.3267, -0.4839, -0.5034,  0.0532,\n",
      "         0.2615,  0.4187,  0.9370, -0.1161,  0.1375,  0.9854,  0.6616,  0.7612,\n",
      "        -0.2732,  0.0521, -0.1791, -0.2627,  0.5752, -0.1260,  0.3801,  0.6865,\n",
      "        -0.4172, -0.1835,  0.1194,  0.4656,  0.2585,  0.1787, -0.4946, -0.1251,\n",
      "         0.5913, -0.3513, -0.0595, -0.1798,  0.0152, -0.2158, -0.5024, -0.4126,\n",
      "        -0.2142,  0.4319, -0.5459, -0.0449, -0.2208, -0.4980,  0.5684,  0.2996,\n",
      "         1.0859, -0.0905,  0.4648, -0.2041,  0.2040,  0.5605, -0.1488,  0.3860,\n",
      "         0.6050, -0.4045,  0.1129,  0.1504,  0.8164,  0.5132,  1.3496,  0.4426,\n",
      "         0.5693, -0.2223,  0.2993,  0.5640,  0.0300,  0.6338,  0.6919,  0.3508,\n",
      "         0.7607,  0.6772,  0.1910,  0.2219,  0.3489,  0.8228, -0.2056,  0.8276,\n",
      "         0.7534, -0.2181,  0.1284,  0.5586,  0.1381,  0.8315, -0.1752, -0.1141,\n",
      "        -0.5020, -0.5493,  0.0374, -0.5493,  0.4875, -0.2086,  0.4563, -0.4326,\n",
      "         0.5537,  0.0648,  0.8145, -0.3000, -0.4202, -0.0137, -0.3838,  0.4407,\n",
      "         0.7803,  0.3621,  0.5752,  0.3743,  0.5854, -0.1509,  0.2847,  0.2888,\n",
      "         0.1041,  0.3992,  0.5010,  0.0088, -0.8843, -0.5625, -0.8486, -0.3135,\n",
      "        -0.4082, -0.5083, -0.1511, -0.1192,  0.5239, -0.5591,  0.0331, -0.5215,\n",
      "        -0.1011,  0.1378, -0.7822, -0.3616, -0.6729, -0.2057, -0.2382, -0.5483,\n",
      "         0.3079,  0.3801,  0.4634,  0.2551,  0.1881, -0.3193, -0.3667,  0.0961,\n",
      "         0.1381,  0.2115,  0.0694, -0.0387, -0.5840, -0.1747,  0.2576,  0.3347,\n",
      "         0.3330,  0.0172,  0.4629, -0.4888, -1.1445, -0.6172, -0.5405,  0.0337,\n",
      "        -0.9126, -0.5596, -0.4370,  0.2169,  0.1186,  0.3213, -0.2163,  0.5708,\n",
      "         0.4490, -0.3252, -0.2362,  0.5176,  0.2118,  0.5103, -0.1971,  0.2007,\n",
      "        -0.4214, -0.2854,  0.3181,  0.3677,  0.4592,  0.5352,  0.2031,  0.5801,\n",
      "         0.1904,  0.4446, -0.2151,  0.4863,  0.2913,  0.5054,  0.4504,  0.5884,\n",
      "         0.1995, -0.3604, -0.3560,  0.4983,  0.6567,  0.9927, -0.0764,  0.3840,\n",
      "         0.9917,  0.5874, -0.3281,  0.2236, -0.3984, -0.4436,  0.6016, -0.0482,\n",
      "         0.8521, -0.0206,  0.4150,  0.1561,  0.6519,  0.4104,  0.3220, -0.5718,\n",
      "        -0.4883, -0.2563,  0.2549,  0.0925, -0.6768,  0.1525,  0.3542, -0.0845,\n",
      "         0.2107,  0.0674,  0.5469, -0.6772, -0.2179,  0.1990,  0.6680, -0.0431,\n",
      "         0.0039,  0.6934, -0.1057,  0.4761,  0.6191, -0.3882, -0.3708, -0.4180,\n",
      "        -0.4834, -0.2067,  0.5156,  0.3220,  0.2776,  0.5493,  0.6450,  0.5874,\n",
      "        -0.2208, -0.3657,  0.2637, -0.0975,  0.4868, -0.2766,  0.6016, -0.6812,\n",
      "        -0.5688,  0.2324,  0.0784, -0.5425,  0.1897,  0.0911,  0.5605, -0.5615,\n",
      "         0.2585,  0.5181, -0.3040, -0.1290,  0.2115,  0.3645,  0.3628,  0.1871,\n",
      "         0.5244,  0.7012, -0.3982,  0.2751,  0.5044, -0.2537, -0.0736, -0.6064,\n",
      "         0.4207, -0.1390,  0.1669, -0.1936,  0.4102, -0.1004,  0.0437,  0.3928,\n",
      "        -0.1224, -0.3535,  0.2644, -0.7539, -0.0648, -0.3455,  0.5020,  0.2832,\n",
      "         0.3303, -0.0255,  0.6377,  0.1870, -0.1064, -0.0302,  0.4893,  0.2993,\n",
      "         0.4194, -0.0471, -0.8306,  0.2115,  0.0253, -0.1231,  0.2920,  0.1572,\n",
      "         0.3035,  0.7979,  0.1329,  0.3704,  0.0950,  0.5493, -0.1229, -0.5996,\n",
      "        -0.0900,  0.2286,  0.5991, -0.4858, -0.1514, -0.1711, -0.0066,  0.0565,\n",
      "         0.2356,  0.3010, -0.4514, -0.1702,  0.6943, -0.2428,  0.1302, -0.2744,\n",
      "         0.2021,  0.6636, -0.4602,  0.3406,  0.3486,  0.7563,  0.0521, -0.1022,\n",
      "        -0.5024, -0.2544,  0.1023,  0.4170,  0.6240,  0.8208, -0.1456,  0.8066,\n",
      "         0.2751,  0.7173, -0.3855,  0.1782,  0.7637,  0.2949,  0.1022, -0.4407,\n",
      "        -0.2469, -0.0583, -0.5083, -0.4663,  0.0754,  0.4368,  0.5854,  0.8315,\n",
      "         0.0268, -0.3354,  0.6113, -0.4932,  0.0100,  0.6533, -0.1042, -0.5361,\n",
      "         0.2725,  0.2339, -0.5791,  0.2773,  0.0287, -0.4185,  0.0306,  0.3381,\n",
      "         0.4128], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.7512,  0.6329, -0.2099,  ...,  1.3532,  1.5397, -0.2985],\n",
      "         [ 0.3910,  0.5167,  0.0620,  ...,  0.9670,  0.8853, -0.1126],\n",
      "         [ 0.5640,  0.2223,  0.1091,  ...,  0.2188,  0.7187,  0.1813],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits:  tensor([ 2.8101e-01,  1.2139e+00,  9.4482e-01,  8.7646e-01, -8.7219e-02,\n",
      "         1.0869e+00,  5.5469e-01,  5.3516e-01,  3.4204e-01,  6.4160e-01,\n",
      "         1.3123e-01,  9.7949e-01,  5.6299e-01,  5.8740e-01,  3.6743e-01,\n",
      "         3.5840e-01,  1.4883e+00,  6.3916e-01,  3.7402e-01,  1.8237e-01,\n",
      "         6.1914e-01,  1.4062e+00,  4.6265e-01,  4.0015e-01,  1.3989e-01,\n",
      "         5.6494e-01,  1.1240e+00,  3.4302e-01,  9.8730e-01,  2.9175e-01,\n",
      "         7.4414e-01,  3.3936e-01,  2.4524e-01,  5.7666e-01, -3.1128e-01,\n",
      "         1.0723e+00,  3.5303e-01,  1.6248e-01, -2.6172e-01,  1.6045e+00,\n",
      "         7.4512e-01,  4.9951e-01,  1.9568e-01, -9.4849e-02,  1.1553e+00,\n",
      "         5.3760e-01,  1.9421e-01,  1.7126e-01, -1.2030e-01,  9.0381e-01,\n",
      "         5.6152e-01,  3.1665e-01,  6.2305e-01, -1.9604e-01,  6.5723e-01,\n",
      "         3.2910e-01,  3.1763e-01, -2.5415e-01,  1.4502e+00,  2.8247e-01,\n",
      "         5.1758e-01,  1.3496e+00,  2.7734e-01,  2.5293e-01,  3.9246e-02,\n",
      "         6.0608e-02,  1.6338e+00,  8.6279e-01,  5.0928e-01,  3.1665e-01,\n",
      "         6.5479e-01,  2.2791e-01,  1.1377e+00,  1.1260e+00,  4.4922e-01,\n",
      "         8.3801e-02,  3.3301e-01,  1.4478e-01,  1.1943e+00,  5.3857e-01,\n",
      "         4.7803e-01,  4.2871e-01,  1.8225e-01,  9.1248e-02,  1.5176e+00,\n",
      "         3.8306e-01,  7.7051e-01, -3.2251e-01, -8.5999e-02,  2.0178e-01,\n",
      "         6.3770e-01,  7.8955e-01, -1.0590e-02,  1.7517e-01, -9.6069e-02,\n",
      "         8.0469e-01,  6.6040e-02,  1.5527e+00,  3.9282e-01,  3.7549e-01,\n",
      "         2.0703e-01,  1.0078e+00,  2.8784e-01,  3.6963e-01,  4.1797e-01,\n",
      "         1.1172e+00,  2.5952e-01,  1.2969e+00,  7.6367e-01,  1.3000e-01,\n",
      "         5.2588e-01,  4.6338e-01,  2.4524e-01,  1.2861e+00,  4.8486e-01,\n",
      "         5.2344e-01,  1.2988e+00,  7.2168e-01,  3.6523e-01,  3.9282e-01,\n",
      "        -5.5450e-02,  1.0371e+00,  6.3672e-01,  3.6646e-01,  8.4717e-01,\n",
      "         1.9910e-01, -1.2024e-01,  3.0566e-01,  7.5244e-01, -8.2031e-02,\n",
      "        -1.7605e-03,  2.2400e-01,  6.6064e-01,  7.9004e-01,  3.0542e-01,\n",
      "         2.4792e-01,  1.4893e+00,  6.4697e-01,  2.5049e-01,  5.3320e-01,\n",
      "         5.3125e-01,  4.9414e-01, -5.9723e-02,  2.4255e-01,  1.0879e+00,\n",
      "         9.2163e-02, -3.7256e-01,  2.3743e-01, -3.0502e-02, -1.4075e-01,\n",
      "         1.3057e+00,  6.5527e-01,  3.0029e-01,  3.5596e-01, -3.3356e-02,\n",
      "         1.1396e+00,  6.3574e-01,  3.6475e-01,  7.7197e-01,  5.3662e-01,\n",
      "         6.8652e-01, -1.6028e-01,  8.8574e-01,  1.6797e-01,  6.5234e-01,\n",
      "         1.5527e-01,  3.7036e-01,  2.7734e-01,  3.7402e-01, -3.2422e-01,\n",
      "         1.6514e+00,  4.8584e-01,  9.4434e-01,  8.7451e-01,  1.3281e+00,\n",
      "         4.6167e-01,  1.5100e-01,  4.6777e-01,  1.0859e+00,  1.0127e+00,\n",
      "         3.5083e-01,  3.5352e-01,  1.4990e-01,  1.2412e+00,  2.9956e-01,\n",
      "         6.9287e-01,  3.1641e-01, -1.6064e-01,  3.9526e-01, -1.6357e-01,\n",
      "         1.4062e-01,  6.7993e-02,  1.0850e+00, -3.7201e-02,  6.0010e-01,\n",
      "         6.8555e-01,  3.1934e-01,  1.3223e+00,  9.6484e-01,  5.6641e-01,\n",
      "         7.5500e-02,  6.9971e-01,  4.6045e-01,  1.7566e-01,  1.1005e-01,\n",
      "         4.3921e-01,  3.6255e-02,  9.2285e-01,  1.6614e-01,  7.2266e-01,\n",
      "         2.7466e-01,  5.5371e-01,  9.0576e-02,  7.8003e-02,  7.0508e-01,\n",
      "        -2.8735e-01,  6.8018e-01,  8.3740e-01,  1.3000e-01,  9.4727e-02,\n",
      "        -2.6398e-02,  1.4026e-01,  8.1494e-01,  4.4434e-01,  1.8811e-01,\n",
      "         1.1711e-02,  1.9641e-01, -6.7444e-02,  5.4443e-01,  4.6655e-01,\n",
      "         3.4155e-01,  3.7598e-01,  3.7183e-01,  1.6592e+00,  9.0283e-01,\n",
      "         5.5566e-01,  2.9126e-01,  4.1602e-01,  1.4160e+00,  6.0693e-01,\n",
      "         5.1074e-01,  2.4255e-01,  2.9956e-01,  9.2041e-01,  4.1382e-01,\n",
      "         8.0322e-02, -6.1432e-02,  1.5918e+00,  7.6514e-01,  5.1611e-01,\n",
      "         2.1008e-01,  1.5564e-01,  1.4004e+00,  4.9414e-01,  4.0698e-01,\n",
      "         5.1318e-01,  1.3076e+00,  3.2935e-01,  4.1016e-01,  2.4231e-02,\n",
      "         1.1467e-02,  1.6260e+00,  9.5898e-01,  4.8486e-01,  3.4204e-01,\n",
      "         5.5908e-01, -2.7237e-02,  1.4219e+00,  8.3105e-01,  3.5815e-01,\n",
      "         1.2139e+00,  6.4844e-01,  5.1416e-01, -2.2339e-01,  1.8994e-01,\n",
      "        -7.9285e-02,  7.3792e-02,  1.6699e+00,  4.0430e-01,  3.4229e-01,\n",
      "         2.0630e-01,  1.0273e+00,  3.5107e-01,  2.7271e-01, -5.2185e-02,\n",
      "         1.5791e+00,  5.2637e-01,  5.0391e-01,  6.1426e-01,  2.8149e-01,\n",
      "         4.4531e-01,  4.3628e-01,  1.0107e+00,  1.2363e+00,  5.3906e-01,\n",
      "         1.9238e-01,  1.1426e+00,  4.9902e-01,  2.2498e-01,  9.2236e-01,\n",
      "         1.2783e+00,  6.1230e-01,  5.6006e-01,  9.9219e-01, -1.0419e-01,\n",
      "         1.1777e+00,  2.5439e-01,  1.3340e+00,  4.9707e-01,  2.2522e-01,\n",
      "        -2.7124e-01, -2.0190e-01,  9.5520e-02,  5.2344e-01,  3.0713e-01,\n",
      "         2.4158e-01, -2.4854e-01, -9.9731e-02, -4.9878e-01,  1.6553e+00,\n",
      "         8.0371e-01,  2.6147e-01,  2.2058e-01,  3.5376e-01,  3.9917e-01,\n",
      "         1.5107e+00,  5.3125e-01,  1.5137e+00,  5.8301e-01, -1.1835e-01,\n",
      "         9.9414e-01, -9.0408e-03,  1.4600e+00,  2.0349e-01,  5.5359e-02,\n",
      "         1.5381e+00,  7.3584e-01,  1.4971e+00,  8.5596e-01,  2.9761e-01,\n",
      "         1.1924e+00,  1.8555e-01,  1.1273e-01, -1.9641e-01,  1.1533e+00,\n",
      "         4.0552e-01,  9.9609e-02, -1.3965e-01,  1.2559e+00,  5.9668e-01,\n",
      "         4.4409e-01,  3.8037e-01,  3.8647e-01,  1.5225e+00,  7.2803e-01,\n",
      "         5.1074e-01,  2.4719e-01,  6.2354e-01,  1.3467e+00,  5.6885e-01,\n",
      "         4.9512e-01,  2.2839e-01,  5.0049e-01,  8.3691e-01,  4.3945e-01,\n",
      "         2.9980e-01, -1.5100e-01,  1.6309e+00,  7.8369e-01,  5.4980e-01,\n",
      "         2.5269e-01,  4.9121e-01,  1.8811e-01,  2.1912e-01, -4.6844e-02,\n",
      "         1.1357e+00,  1.2878e-01,  1.6729e+00,  3.3496e-01,  6.2158e-01,\n",
      "         5.2686e-01,  2.3285e-02,  1.4561e+00,  7.2461e-01,  6.2891e-01,\n",
      "         4.5728e-01, -1.7300e-03,  1.2617e+00,  5.2783e-01,  4.8779e-01,\n",
      "         5.9766e-01,  4.3396e-02,  1.0801e+00,  2.5415e-01,  8.6035e-01,\n",
      "         1.3420e-02,  1.6074e+00,  8.9746e-01,  4.7388e-01,  3.1177e-01,\n",
      "         6.1670e-01, -1.7993e-01,  1.0898e+00,  1.4722e-01,  1.6484e+00,\n",
      "         2.9224e-01, -2.9755e-02,  1.4756e+00,  4.2041e-01,  5.1172e-01,\n",
      "         3.3496e-01,  2.8418e-01, -4.2786e-02,  1.3799e+00,  6.0059e-01,\n",
      "         4.3652e-01,  5.3467e-01,  7.7087e-02,  1.1016e+00,  7.5439e-02,\n",
      "        -1.0950e-01,  8.7695e-01,  3.1299e-01,  9.8145e-02, -4.1412e-02,\n",
      "        -5.1270e-01, -3.3203e-01, -6.1768e-02,  2.9688e-01,  9.5703e-02,\n",
      "         1.8018e-01, -3.4253e-01, -2.5220e-01, -5.7080e-01,  1.5879e+00,\n",
      "         7.6855e-01,  2.5830e-01,  1.5820e-01,  2.3181e-01,  3.9746e-01,\n",
      "         1.3887e+00,  5.5420e-01,  5.4688e-01,  8.3350e-01,  3.1323e-01,\n",
      "         5.6738e-01,  2.3157e-01,  9.9512e-01,  4.9170e-01,  4.3213e-01,\n",
      "         7.0410e-01,  2.5635e-01,  5.2100e-01,  1.2213e-01,  1.0342e+00,\n",
      "         3.9551e-01,  8.0127e-01,  4.5654e-01,  4.0186e-01,  7.0166e-01,\n",
      "         3.7134e-01,  7.8711e-01,  5.8740e-01,  5.1074e-01, -1.8152e-01,\n",
      "         5.5664e-01,  4.2139e-01,  4.2627e-01,  2.3059e-01,  5.0586e-01,\n",
      "         3.3887e-01,  3.0054e-01,  1.0400e+00, -7.5134e-02,  1.0752e+00,\n",
      "         4.7559e-01,  4.2920e-01,  7.1436e-01,  2.6953e-01,  1.0469e+00,\n",
      "         7.1973e-01,  5.0098e-01, -2.4231e-02,  1.0547e+00,  5.4346e-01,\n",
      "         1.1224e-01,  6.6797e-01,  4.7217e-01, -9.8724e-03,  8.6035e-01,\n",
      "         5.3673e-03,  1.2578e+00,  3.3179e-01, -6.3232e-02,  1.2588e+00,\n",
      "         5.4443e-01,  2.0215e-01,  4.3457e-01,  1.1553e+00,  1.4246e-01,\n",
      "         1.1572e+00,  5.8057e-01,  4.5654e-01,  7.3633e-01,  4.4849e-01,\n",
      "         7.8760e-01,  3.4131e-01, -2.9980e-01,  2.6318e-01, -6.3281e-01,\n",
      "         1.3877e+00,  6.7578e-01,  1.9678e-01,  2.9736e-01,  9.7363e-01,\n",
      "         8.4473e-01,  7.8491e-02,  1.2715e+00,  7.9980e-01,  4.6045e-01,\n",
      "         5.3223e-01,  2.4011e-01, -2.5342e-01,  1.7432e-01, -6.4307e-01,\n",
      "         1.1875e+00,  4.2798e-01,  4.7852e-01,  1.1115e-01,  6.0498e-01,\n",
      "         1.2225e-01,  4.1260e-01,  1.4688e+00,  6.8262e-01,  1.3867e+00,\n",
      "         7.9346e-01, -1.6711e-01,  9.2090e-01, -6.1829e-02,  1.3955e+00,\n",
      "         2.1179e-01,  8.0139e-02,  1.4980e+00,  9.7559e-01,  4.9951e-01,\n",
      "        -3.2007e-01,  1.3887e+00,  5.0293e-01,  4.7998e-01,  7.4072e-01,\n",
      "        -1.9458e-01,  1.4355e+00,  4.1455e-01,  8.0176e-01,  1.3604e+00,\n",
      "         8.8574e-01,  3.4363e-02,  5.1611e-01,  7.2461e-01, -5.9814e-02,\n",
      "         1.3652e+00,  3.4912e-01, -1.3696e-01,  1.2323e-01,  2.5391e-01,\n",
      "         3.7622e-01,  1.4443e+00,  7.4316e-01,  4.8730e-01,  1.5234e-01,\n",
      "         7.7441e-01, -1.1196e-03,  1.3760e+00,  5.7812e-01,  5.3369e-01,\n",
      "         1.5564e-01,  5.9180e-01, -2.9800e-02,  1.1885e+00,  4.5776e-01,\n",
      "         1.1514e+00,  4.5239e-01,  9.0869e-01,  5.7959e-01,  2.7222e-01,\n",
      "         6.6992e-01,  1.7480e-01, -7.9224e-02,  8.1250e-01,  8.0994e-02,\n",
      "        -6.0616e-03,  5.6592e-01,  1.6394e-01,  2.8046e-02,  8.3691e-01,\n",
      "        -3.7231e-02,  1.3496e+00,  3.8330e-01, -1.1884e-01,  1.7275e+00,\n",
      "         5.4346e-01,  4.6802e-01,  6.7822e-01,  1.3953e-01,  4.3945e-01,\n",
      "         4.9658e-01,  5.4199e-01,  1.2715e+00,  3.8892e-01,  3.3862e-01,\n",
      "         1.8860e-01, -4.5972e-01,  2.7588e-01,  1.6475e+00,  5.3613e-01,\n",
      "         6.7871e-01,  3.4180e-01,  8.4717e-01,  1.4539e-01,  1.7346e-01,\n",
      "         1.3193e+00,  5.0049e-01, -2.2620e-01,  2.3645e-01,  2.2766e-01,\n",
      "         1.9653e-01,  2.4573e-01,  8.7598e-01, -2.7393e-01,  8.9502e-01,\n",
      "         3.9282e-01,  1.8140e-01,  1.3740e+00,  4.7461e-01,  1.1436e+00,\n",
      "         2.5977e-01, -4.2920e-01,  1.4331e-01,  1.5293e+00,  5.7422e-01,\n",
      "         1.1807e+00,  9.4434e-01,  8.8428e-01,  6.5674e-01,  9.2090e-01,\n",
      "         8.4229e-02,  2.5195e-01,  1.2383e+00,  1.0645e+00,  7.4365e-01,\n",
      "         3.5278e-01,  3.3276e-01,  1.2100e+00,  8.6572e-01,  3.9526e-01,\n",
      "         3.8135e-01,  1.1201e+00,  2.1973e-01,  1.4902e+00,  3.3179e-01,\n",
      "         5.2246e-01,  7.9834e-01,  4.6143e-01,  1.9531e-01,  8.4814e-01,\n",
      "         2.5806e-01,  2.9572e-02, -2.7100e-02,  1.6045e+00,  7.0020e-01,\n",
      "         5.1465e-01,  1.7810e-01,  6.4355e-01,  3.3203e-01,  4.5459e-01,\n",
      "         1.4736e+00,  6.9580e-01,  4.9170e-01,  1.6516e-01,  6.8359e-01,\n",
      "         1.3477e+00,  5.9961e-01,  5.7910e-01,  2.6611e-01,  6.1426e-01,\n",
      "         7.5928e-01,  5.9277e-01,  3.9380e-01, -8.3313e-03,  1.5752e+00,\n",
      "         7.7197e-01,  5.2979e-01,  2.6733e-01,  6.3574e-01,  9.8145e-02,\n",
      "         5.4932e-01,  1.0713e+00,  4.7363e-02,  1.3408e+00,  7.5488e-01,\n",
      "         7.4463e-01,  3.4985e-01,  5.5664e-01, -1.6724e-02,  1.3242e+00,\n",
      "         5.3320e-01,  5.2197e-01, -6.8359e-02,  1.1660e+00,  7.2412e-01,\n",
      "         6.2451e-01,  1.0858e-01,  1.3525e+00,  6.6846e-01,  4.5776e-01,\n",
      "        -2.8854e-02,  1.3008e+00,  5.8984e-01,  5.1367e-01,  6.4600e-01,\n",
      "         1.2979e+00,  4.0356e-01,  7.0947e-01,  1.6223e-01,  1.1700e-01,\n",
      "        -2.9175e-02, -2.0996e-01,  6.4014e-01,  2.9102e-01,  1.1660e+00,\n",
      "         1.2539e+00,  7.4023e-01,  4.9902e-01, -3.6469e-02,  6.4648e-01,\n",
      "         6.0181e-02,  7.0801e-01,  4.3408e-01,  2.2717e-01,  1.1152e+00,\n",
      "         1.2383e+00,  7.3145e-01,  1.0773e-01,  5.7471e-01,  8.3447e-01,\n",
      "         5.6787e-01, -1.8042e-01,  3.4766e-01, -1.0870e-01,  2.0496e-01,\n",
      "         1.6748e+00,  9.6387e-01,  6.4502e-01,  2.8882e-01,  7.0410e-01,\n",
      "         4.9805e-01,  1.5051e-01,  9.3701e-01,  3.6426e-01,  2.7856e-01,\n",
      "         1.2764e+00,  7.3340e-01,  6.2207e-01,  5.0879e-01,  1.2051e+00,\n",
      "         3.7134e-01,  3.8354e-01,  1.0801e+00,  1.1504e+00,  3.9331e-01,\n",
      "         9.8828e-01,  9.8340e-01,  2.9492e-01,  1.2266e+00,  5.2441e-01,\n",
      "         8.9648e-01,  4.5972e-01,  5.4053e-01,  1.1904e+00,  5.1758e-01,\n",
      "         2.8595e-02,  1.5127e+00,  9.4043e-01,  4.5386e-01,  5.6348e-01,\n",
      "         9.2334e-01,  3.6469e-02,  9.2236e-01,  1.1484e+00,  5.3467e-01,\n",
      "         8.8428e-01,  2.1252e-01,  5.2344e-01,  3.4033e-01, -2.7710e-01,\n",
      "         2.3352e-01,  2.9370e-01, -1.7749e-01,  1.4912e+00,  6.4453e-01,\n",
      "        -2.4109e-01,  6.2305e-01,  5.2148e-01,  5.0732e-01,  8.1885e-01,\n",
      "         1.1082e-03, -1.1804e-01,  4.9707e-01,  3.1006e-01, -8.4290e-02,\n",
      "         8.0811e-01,  9.3457e-01,  5.8740e-01,  7.2461e-01,  2.3157e-01,\n",
      "         6.8799e-01,  9.0332e-01,  4.9902e-01, -1.8860e-02,  1.2006e-01,\n",
      "         3.1982e-01,  7.4658e-01, -1.2732e-01,  6.2646e-01,  3.9551e-02,\n",
      "         4.0063e-01,  1.3147e-01, -1.4807e-01,  1.5361e+00,  5.5273e-01,\n",
      "        -1.1981e-01,  4.0186e-01, -2.9590e-01,  4.5433e-03,  6.4209e-01,\n",
      "         1.1064e+00,  5.9863e-01,  1.0078e+00,  1.0068e+00,  6.3330e-01,\n",
      "         3.5950e-02,  5.8154e-01, -1.1023e-01,  2.4524e-01,  4.5605e-01,\n",
      "         6.1963e-01,  6.0205e-01,  7.0117e-01,  7.4805e-01,  7.9102e-01,\n",
      "         8.7451e-01,  7.5317e-02,  7.9639e-01,  2.0349e-01,  3.8086e-01,\n",
      "         1.4883e+00,  7.5684e-01,  4.9316e-01,  1.1700e-01,  8.8086e-01,\n",
      "         1.2725e+00,  5.9082e-01,  4.9414e-01,  8.7524e-02,  7.2021e-01,\n",
      "         5.1465e-01,  2.8589e-01,  4.9512e-01, -2.3621e-01,  1.4307e+00,\n",
      "         7.3584e-01,  5.4492e-01,  9.1125e-02,  1.9104e-02,  1.4512e+00,\n",
      "         7.7637e-01,  6.4453e-01,  3.1689e-01, -5.8563e-02,  1.0801e+00,\n",
      "         4.5679e-01,  3.5669e-01,  7.4268e-01,  1.0879e+00,  2.9736e-01,\n",
      "         3.0786e-01, -3.0444e-01, -6.3110e-02,  1.3838e+00,  8.0420e-01,\n",
      "         6.9531e-01,  1.9714e-01,  5.5127e-01, -1.4893e-01,  1.1621e+00,\n",
      "         3.2031e-01,  4.4263e-01, -5.0323e-02,  1.6943e-01, -2.4841e-01,\n",
      "        -1.6125e-01,  1.4287e+00,  7.9053e-01,  5.2930e-01,  1.7041e-01,\n",
      "         6.9922e-01, -5.6519e-02,  6.3721e-01,  5.5176e-01,  3.6768e-01,\n",
      "        -2.2205e-01,  1.4844e+00,  8.7061e-01,  5.6348e-01,  5.3711e-01,\n",
      "         4.6997e-01, -1.2329e-01,  2.1021e-01,  4.3042e-01,  4.4653e-01,\n",
      "         1.4941e-01, -1.8555e-01,  1.3604e+00,  7.3096e-01,  4.7217e-01,\n",
      "         1.1504e+00,  9.3408e-01,  6.7725e-01,  3.6548e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 2.9028e-01, -3.7109e-01,  6.6895e-02,  3.8257e-01, -6.3428e-01,\n",
      "         2.3193e-01,  1.4355e-01,  1.7188e-01,  2.1167e-01, -2.0825e-01,\n",
      "        -6.1230e-01, -3.4717e-01, -5.2582e-02, -1.2177e-01,  6.1127e-02,\n",
      "         3.2031e-01, -2.4353e-01,  3.1250e-01,  2.8882e-01,  3.1494e-01,\n",
      "         6.2451e-01, -2.0898e-01,  3.1421e-01,  2.4561e-01,  3.2617e-01,\n",
      "         5.0879e-01, -2.8613e-01,  4.3140e-01,  3.9771e-01,  2.4231e-01,\n",
      "         1.7236e-01,  6.3660e-02,  3.1812e-01,  4.9194e-01, -2.0337e-01,\n",
      "         4.0356e-01,  3.4180e-01,  4.9121e-01, -6.0693e-01, -3.8422e-02,\n",
      "         2.9297e-01,  2.2852e-01,  3.5229e-01, -4.3481e-01,  4.2969e-01,\n",
      "         4.5679e-01,  5.1953e-01,  1.9910e-01,  2.8174e-01,  5.2197e-01,\n",
      "         2.7808e-01,  5.5371e-01,  3.5522e-01, -4.4019e-01, -1.4966e-01,\n",
      "         1.3525e-01, -2.3132e-01, -6.1230e-01,  5.4980e-01,  4.3359e-01,\n",
      "         3.4351e-01, -2.0776e-01,  4.8926e-01, -5.7892e-02,  1.1523e-01,\n",
      "        -2.7393e-01, -7.9651e-02,  2.4231e-01,  3.9453e-01,  3.3228e-01,\n",
      "         3.1372e-01,  1.4221e-01, -8.7402e-02,  3.1519e-01,  2.6733e-01,\n",
      "         3.8391e-02,  5.1660e-01, -2.6978e-01,  2.2754e-01,  1.9360e-01,\n",
      "        -1.0370e-01,  1.4465e-01, -8.4686e-03, -3.3691e-01,  5.7812e-01,\n",
      "         3.6646e-01,  4.0039e-01, -4.5752e-01, -3.2568e-01,  3.8965e-01,\n",
      "        -7.1350e-02, -7.3926e-01, -3.2544e-01, -7.6782e-02, -3.6353e-01,\n",
      "        -1.1908e-01, -2.0801e-01, -1.0567e-03,  1.0596e-01,  6.7480e-01,\n",
      "         1.5564e-01, -1.2482e-01,  7.1240e-01, -8.4381e-03,  6.9824e-02,\n",
      "         4.2450e-02,  1.3947e-02,  9.5276e-02,  4.6600e-02,  2.0410e-01,\n",
      "        -4.0030e-04,  3.2178e-01, -4.1553e-01,  2.6855e-01,  1.8848e-01,\n",
      "         1.5479e-01,  1.8054e-01,  4.9780e-01,  6.0303e-01,  1.9067e-01,\n",
      "         2.9224e-01,  5.8398e-01,  2.7637e-01,  6.4551e-01,  3.9819e-01,\n",
      "         3.5858e-02, -1.6357e-01, -3.0298e-01,  9.8267e-02,  3.0444e-01,\n",
      "         1.5259e-01, -6.3721e-01, -3.7207e-01,  1.2012e-01,  3.1030e-01,\n",
      "        -2.2961e-01,  4.3994e-01,  1.1639e-01,  2.4988e-01,  1.2549e-01,\n",
      "         2.9541e-01,  4.2505e-01,  1.1823e-01,  6.7480e-01,  4.3602e-03,\n",
      "         7.9468e-02, -3.8843e-01,  2.6099e-01, -8.9844e-01, -6.4355e-01,\n",
      "         3.4204e-01,  4.1846e-01,  5.2246e-01,  1.1499e-01,  2.0569e-01,\n",
      "         5.2148e-01,  2.4670e-01,  5.9863e-01,  3.6572e-01, -2.7466e-01,\n",
      "        -3.2617e-01, -1.5735e-01, -2.9321e-01,  2.5757e-01, -4.8682e-01,\n",
      "        -4.9561e-01,  1.2955e-02,  3.7567e-02,  1.7358e-01, -2.2961e-01,\n",
      "         5.2783e-01,  5.5371e-01,  3.3276e-01, -7.1387e-01,  1.9165e-01,\n",
      "         1.8323e-01, -7.8430e-02,  4.6094e-01, -7.5781e-01,  1.3196e-01,\n",
      "         1.8994e-01,  3.2446e-01, -5.8838e-01,  3.6353e-01,  4.2920e-01,\n",
      "         3.9648e-01,  3.9856e-02, -4.7168e-01,  2.3450e-01, -9.3555e-01,\n",
      "        -1.2323e-01, -2.6709e-01, -2.2766e-01, -7.1338e-01,  3.9233e-01,\n",
      "        -2.5830e-01, -4.8682e-01,  8.6182e-02,  5.0842e-02, -4.0863e-02,\n",
      "        -3.5571e-01, -2.3792e-01,  1.9885e-01, -2.2961e-01, -1.2482e-01,\n",
      "         1.4539e-01, -2.9688e-01, -5.2051e-01, -5.5029e-01,  3.3325e-02,\n",
      "         3.4473e-01, -8.1970e-02, -3.7183e-01, -4.7754e-01,  9.4757e-03,\n",
      "        -4.0918e-01, -3.8306e-01, -3.3496e-01, -8.3301e-01, -3.6548e-01,\n",
      "        -2.7759e-01, -4.7119e-01,  6.7253e-03,  6.8604e-02,  8.2703e-02,\n",
      "        -1.6931e-01,  2.3102e-02, -8.0811e-01, -7.8278e-03, -9.0881e-02,\n",
      "        -6.5674e-02, -7.0117e-01,  3.4351e-01, -2.8711e-01,  3.2983e-01,\n",
      "         2.4231e-01,  3.5889e-01,  6.2549e-01, -2.3914e-01,  3.2593e-01,\n",
      "         2.5903e-01,  3.6890e-01,  4.7729e-01,  2.2485e-01,  2.9053e-01,\n",
      "         7.4316e-01, -7.4902e-01, -1.6394e-01,  2.2046e-01,  2.5391e-01,\n",
      "         3.0151e-01, -3.5229e-01,  3.8184e-01,  3.3960e-01,  8.9258e-01,\n",
      "         4.9512e-01, -9.9426e-02,  4.7510e-01,  1.7505e-01,  6.0913e-02,\n",
      "        -6.3623e-01, -6.3660e-02,  1.8591e-01,  3.3057e-01,  1.9958e-01,\n",
      "         4.6729e-01, -4.4482e-01,  5.6885e-01,  3.7451e-01,  5.6836e-01,\n",
      "         1.2091e-01,  4.9854e-01,  4.5728e-01, -2.8442e-01, -3.4058e-01,\n",
      "         1.1591e-01, -6.3916e-01,  5.1880e-02,  1.4026e-01,  7.5635e-01,\n",
      "         2.3462e-01, -5.1361e-02,  4.8364e-01,  6.5674e-02, -4.9707e-01,\n",
      "         3.0322e-01,  3.8330e-01,  5.1025e-01,  2.3181e-01, -1.4359e-02,\n",
      "        -2.6904e-01, -6.0883e-02,  5.7404e-02, -8.4045e-02,  4.6313e-01,\n",
      "        -3.7451e-01,  3.9233e-01,  4.0967e-01, -2.1582e-01,  1.9006e-01,\n",
      "         4.0796e-01,  6.5186e-01, -6.1249e-02, -1.4490e-01, -2.6147e-01,\n",
      "         8.2764e-02,  2.2107e-01,  3.9209e-01,  1.9116e-01,  4.8511e-01,\n",
      "        -4.3262e-01, -1.1395e-01,  3.7988e-01, -5.0507e-02,  4.5020e-01,\n",
      "         3.0908e-01, -3.8672e-01, -1.6760e-01, -5.5566e-01, -7.0381e-03,\n",
      "         2.7100e-01,  3.6621e-01,  2.9395e-01, -6.3965e-01, -3.1421e-01,\n",
      "         2.6660e-01,  4.6826e-01,  2.5952e-01,  2.7417e-01, -5.7178e-01,\n",
      "        -1.5259e-01, -5.6836e-01,  9.4910e-02,  4.0625e-01, -5.1971e-02,\n",
      "         1.7920e-01,  8.4570e-01, -1.0175e-01,  2.4365e-01,  8.2031e-01,\n",
      "         3.5669e-01,  2.7881e-01,  2.7563e-01,  2.3132e-01,  3.9177e-03,\n",
      "         3.3661e-02,  3.7720e-01, -5.4840e-02,  4.1187e-01,  2.8052e-01,\n",
      "         7.3633e-01, -2.3596e-01,  3.5669e-01, -2.8809e-01,  3.7012e-01,\n",
      "         1.5149e-01,  4.3140e-01,  4.9463e-01, -1.5466e-01,  3.9526e-01,\n",
      "         2.5586e-01,  4.5679e-01,  4.5752e-01,  2.9517e-01,  3.6450e-01,\n",
      "         2.8125e-01, -7.5635e-01, -7.1045e-02,  3.0957e-01,  2.6318e-01,\n",
      "         3.5669e-01, -6.7041e-01,  8.4656e-02, -6.6650e-01, -2.1741e-01,\n",
      "         1.7480e-01,  1.7102e-01,  1.9971e-01,  1.4722e-01,  1.7444e-01,\n",
      "         4.5142e-01, -1.7480e-01,  2.1680e-01,  1.2805e-01,  7.1289e-01,\n",
      "         1.1188e-01, -1.9922e-01,  3.6963e-01,  2.8833e-01,  7.5342e-01,\n",
      "         3.6304e-01, -2.7588e-01,  3.9380e-01, -4.0625e-01,  8.4167e-02,\n",
      "        -1.5552e-01, -7.7820e-02,  1.8042e-01,  4.1504e-01,  3.7915e-01,\n",
      "         6.3037e-01, -2.4817e-01,  2.7661e-01,  2.7246e-01,  1.6125e-01,\n",
      "         4.4556e-01,  4.5074e-02,  2.4866e-01,  2.6962e-02,  4.5605e-01,\n",
      "         5.4541e-01,  2.6514e-01, -1.7639e-01,  4.5093e-01,  3.6279e-01,\n",
      "         8.4668e-01,  4.0698e-01, -3.0786e-01,  5.1562e-01,  2.6660e-01,\n",
      "        -5.5518e-01, -8.1238e-02,  1.3416e-01,  3.9917e-01,  3.2166e-02,\n",
      "        -4.7266e-01, -1.1334e-01,  3.3521e-01,  1.7929e-02,  4.8877e-01,\n",
      "         4.1455e-01, -3.6328e-01, -9.6008e-02, -5.5908e-01, -1.1835e-01,\n",
      "         2.1680e-01,  3.5815e-01,  3.5815e-01, -6.9238e-01,  3.6377e-01,\n",
      "        -8.3252e-02,  2.8247e-01,  9.3079e-02,  3.8354e-01,  8.4766e-01,\n",
      "         2.0959e-01,  3.4937e-01, -3.3173e-02,  3.0713e-01,  1.1157e-01,\n",
      "         4.0552e-01,  7.3340e-01,  4.0283e-01,  3.6475e-01, -7.4829e-02,\n",
      "         4.3799e-01,  1.5295e-01,  2.9272e-01,  1.0986e-01,  3.6230e-01,\n",
      "         4.3286e-01, -4.9469e-02,  2.3730e-01,  8.5205e-01, -6.7285e-01,\n",
      "        -9.0454e-02, -6.6956e-02,  1.9188e-03, -6.1084e-01, -5.1172e-01,\n",
      "         8.4961e-02,  6.9084e-03,  1.1731e-01, -2.3666e-02,  2.2644e-01,\n",
      "         2.2375e-01,  8.0017e-02,  3.9966e-01,  6.3623e-01,  2.6514e-01,\n",
      "         2.4316e-01,  5.8691e-01, -2.7832e-01,  2.0496e-01,  2.9932e-01,\n",
      "         3.5522e-01,  3.9307e-01,  7.3340e-01, -1.6931e-01,  2.8906e-01,\n",
      "         3.2983e-01,  4.7729e-01,  6.4160e-01,  2.6779e-02,  5.7080e-01,\n",
      "         4.7217e-01,  7.5146e-01,  4.2407e-01,  1.4197e-01,  6.1328e-01,\n",
      "         2.2049e-02,  2.6660e-01,  1.1481e-01,  4.6045e-01,  7.6221e-01,\n",
      "         3.1958e-01, -3.0200e-01, -5.5371e-01, -3.2593e-01, -2.2424e-01,\n",
      "        -1.8091e-01,  6.3049e-02,  5.6299e-01,  1.0687e-01,  1.6541e-01,\n",
      "         4.8315e-01, -4.9512e-01,  2.9102e-01,  1.6797e-01,  7.5098e-01,\n",
      "         4.1089e-01, -3.6938e-01, -6.3574e-01, -1.8970e-01, -2.3523e-01,\n",
      "        -1.4160e-01,  4.7485e-01,  4.2725e-01,  5.1953e-01,  8.5303e-01,\n",
      "        -5.7031e-01,  3.7500e-01,  2.4268e-01,  9.4678e-01,  1.5930e-01,\n",
      "         6.8457e-01, -7.2852e-01, -3.7567e-02, -6.2891e-01,  9.2102e-02,\n",
      "         3.1348e-01, -1.4905e-01, -8.5083e-02,  1.3770e-01,  6.9336e-01,\n",
      "        -2.1545e-01,  4.4141e-01,  3.0249e-01, -5.1611e-01,  2.7618e-02,\n",
      "         6.0234e-03,  2.2424e-01,  4.2212e-01,  1.1768e-01,  9.8938e-02,\n",
      "         6.1426e-01, -8.0908e-01,  1.4270e-01, -1.5588e-01, -7.9248e-01,\n",
      "        -1.2436e-02,  3.2544e-01, -1.7175e-01, -3.6157e-01, -7.3145e-01,\n",
      "        -3.2837e-01, -3.5522e-01,  3.3179e-01,  2.2058e-01,  4.7144e-01,\n",
      "         1.2830e-01,  6.6602e-01, -2.8125e-01,  3.5425e-01,  2.8662e-01,\n",
      "         4.5337e-01,  9.3994e-02,  5.1611e-01, -1.0052e-01,  4.0161e-01,\n",
      "         4.4385e-01,  1.2000e-01,  7.8857e-02, -1.7586e-03,  4.2896e-01,\n",
      "         1.2067e-01,  6.0303e-01, -7.8027e-01,  1.2085e-02,  4.8657e-01,\n",
      "         2.0825e-01,  3.3539e-02,  3.3203e-01, -4.0454e-01,  1.5295e-01,\n",
      "         2.3438e-01,  2.2778e-01,  4.2236e-01, -1.5088e-01,  4.8804e-01,\n",
      "         4.5715e-02,  2.1997e-01,  3.9453e-01,  5.1221e-01,  2.3328e-01,\n",
      "         3.5742e-01,  4.0332e-01, -2.0126e-02,  5.6201e-01, -5.6934e-01,\n",
      "        -3.5309e-02, -5.8807e-02, -5.8643e-01, -1.8811e-01, -6.0944e-02,\n",
      "         5.2051e-01,  1.4355e-01, -5.5817e-02,  4.4629e-01, -5.5811e-01,\n",
      "        -1.4514e-01,  1.5637e-01, -5.7471e-01,  2.5146e-01, -6.3965e-01,\n",
      "        -4.9658e-01, -4.4238e-01, -7.7095e-03, -4.3042e-01,  1.6150e-01,\n",
      "         3.0225e-01,  3.9734e-02,  1.6138e-01,  4.2285e-01,  2.8625e-02,\n",
      "         2.3779e-01, -1.2708e-01, -8.0811e-01, -1.7532e-02,  2.6562e-01,\n",
      "        -5.4688e-01,  2.6562e-01, -5.9723e-02,  4.6997e-01,  1.9495e-01,\n",
      "        -1.4819e-01, -4.8633e-01,  2.2229e-01,  1.6284e-01,  8.0383e-02,\n",
      "         3.7549e-01, -3.2837e-01,  2.2437e-01, -1.3013e-01,  4.0112e-01,\n",
      "         1.2964e-01,  8.5754e-02,  2.4329e-01,  5.2734e-01, -3.2684e-02,\n",
      "         2.8906e-01,  3.5889e-01,  6.5967e-01, -1.8384e-01,  3.8794e-01,\n",
      "         3.0200e-01,  4.3848e-01, -5.0391e-01,  3.6011e-02,  4.0820e-01,\n",
      "         3.7988e-01,  4.4336e-01,  5.5469e-01, -6.1865e-01, -3.5742e-01,\n",
      "        -4.0674e-01,  3.5571e-01,  2.5562e-01,  3.9893e-01,  3.6133e-01,\n",
      "        -1.3135e-01,  3.7793e-01,  3.5034e-01,  4.3628e-01,  1.7603e-01,\n",
      "         2.5244e-01,  2.0654e-01,  4.2572e-02, -8.0859e-01, -1.0300e-02,\n",
      "         3.5791e-01,  2.9004e-01,  3.3838e-01, -7.0264e-01, -1.2360e-01,\n",
      "        -5.1562e-01,  1.4417e-01, -5.1460e-03,  1.2390e-01,  1.7114e-01,\n",
      "        -3.7384e-02,  1.5051e-01,  3.4473e-01, -3.4204e-01, -5.4901e-02,\n",
      "         2.3026e-02,  2.8516e-01, -1.1810e-01,  2.6343e-01,  2.2766e-01,\n",
      "        -1.5308e-01,  1.4111e-01,  2.7832e-01,  6.7676e-01,  1.7627e-01,\n",
      "        -1.3147e-01,  5.4541e-01,  1.9141e-01,  9.3359e-01,  6.5088e-01,\n",
      "         1.6138e-01,  7.5830e-01,  4.4800e-02,  4.6997e-01,  8.7769e-02,\n",
      "         1.0236e-01, -8.0420e-01,  9.8816e-02, -1.9531e-01,  9.5825e-02,\n",
      "        -1.8665e-01,  2.6221e-01, -4.6411e-01, -8.0078e-01, -4.2017e-01,\n",
      "        -3.4985e-01, -3.5864e-01,  2.1265e-01, -3.0127e-01,  4.1870e-02,\n",
      "        -2.6147e-01,  9.0088e-02, -1.6553e-01,  2.7441e-01, -1.3901e-02,\n",
      "         2.9810e-01, -3.0298e-01,  3.7939e-01, -6.6467e-02, -5.9863e-01,\n",
      "         2.8107e-02,  1.9031e-01,  3.5913e-01,  4.0088e-01,  3.6377e-01,\n",
      "        -6.0547e-01, -1.5808e-01, -1.9397e-01, -1.9067e-01, -5.3760e-01,\n",
      "         2.3596e-01,  2.5879e-01,  5.5908e-02,  8.3301e-01,  3.6157e-01,\n",
      "        -4.3262e-01,  3.4204e-01, -2.7319e-01,  1.2488e-01,  5.2344e-01,\n",
      "        -4.3677e-01,  1.1407e-01,  4.6045e-01, -5.0018e-02,  4.1821e-01,\n",
      "         3.3179e-01, -3.8666e-02,  2.4365e-01,  2.7368e-01,  3.6499e-01,\n",
      "        -8.9160e-01, -1.1115e-01,  7.3303e-02, -6.2793e-01, -6.7090e-01,\n",
      "        -3.1299e-01, -2.2351e-01, -1.8631e-02,  1.6467e-01,  2.6831e-01,\n",
      "        -5.3711e-01,  5.9906e-02, -1.0724e-01, -5.0850e-03, -7.6562e-01,\n",
      "        -4.1479e-01, -7.2461e-01, -4.8340e-01,  1.1487e-01,  1.0406e-01,\n",
      "        -5.5469e-01, -5.1807e-01, -4.0112e-01, -1.9336e-01, -2.6831e-01,\n",
      "         5.5786e-02, -5.8398e-01, -2.4772e-04, -1.7139e-01, -4.3799e-01,\n",
      "        -2.8931e-01,  8.9722e-03,  2.9663e-01, -1.9067e-01, -7.6782e-02,\n",
      "        -2.9541e-01,  4.9858e-03,  2.2180e-01,  1.7310e-01, -5.9717e-01,\n",
      "        -2.2131e-01, -3.9722e-01,  1.2573e-01,  1.7810e-01,  2.6782e-01,\n",
      "         9.9670e-02, -2.3401e-01, -3.4277e-01,  2.0496e-01,  3.3740e-01,\n",
      "        -5.6738e-01, -2.2058e-01, -8.3008e-01, -8.2715e-01,  1.1621e-01,\n",
      "         3.0054e-01,  5.2295e-01, -1.9617e-01,  1.1377e-01,  3.6475e-01,\n",
      "        -9.4238e-02, -2.5806e-01, -3.4766e-01,  9.4543e-02, -1.0394e-01,\n",
      "        -9.6252e-02, -6.0059e-01, -1.0980e-01,  2.7661e-01, -1.6003e-01,\n",
      "        -1.4966e-01, -5.5811e-01,  8.5266e-02, -4.2969e-01,  3.4497e-01,\n",
      "        -3.0371e-01,  1.9067e-01,  3.3154e-01,  4.9780e-01,  3.5669e-01,\n",
      "        -7.5569e-03,  2.9150e-01,  4.0723e-01,  5.8350e-01,  3.1104e-01,\n",
      "         4.2725e-01,  3.0176e-01,  2.6294e-01, -7.7588e-01,  3.2196e-02,\n",
      "         2.6221e-01,  2.9053e-01,  4.3872e-01, -3.3813e-01,  3.2764e-01,\n",
      "         1.6699e-01,  8.6816e-01,  2.8271e-01, -1.2854e-01,  5.2979e-01,\n",
      "         1.4832e-01,  8.1787e-01,  4.4287e-01,  1.7725e-01,  3.3325e-01,\n",
      "         3.8940e-01,  3.5571e-01, -5.1123e-01, -9.1370e-02,  8.4045e-02,\n",
      "         2.9150e-01,  3.8184e-01,  4.7437e-01, -6.3330e-01,  2.6367e-01,\n",
      "         1.2384e-01,  2.1106e-01, -2.1179e-01, -3.3643e-01,  3.0640e-01,\n",
      "        -4.3506e-01, -4.3182e-02,  1.5076e-01,  4.1772e-01,  3.4399e-01,\n",
      "         4.7290e-01, -4.3701e-01, -5.3787e-04,  5.6519e-02,  3.6279e-01,\n",
      "        -5.1123e-01,  4.8438e-01,  6.7017e-02,  2.6611e-01,  6.6406e-01,\n",
      "         3.6157e-01, -1.0724e-01, -4.8975e-01,  4.4995e-01,  1.6724e-01,\n",
      "        -4.2773e-01, -5.4395e-01,  3.5327e-01,  1.5723e-01,  9.2285e-01,\n",
      "         1.7346e-01,  2.3364e-01,  4.7241e-01,  3.2397e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.1658,  0.1652, -0.0669,  ...,  1.2405,  1.5148, -0.2990],\n",
      "         [ 0.9270,  0.4181,  0.0232,  ...,  1.1493,  1.0614, -0.2856],\n",
      "         [ 1.2219,  0.1448, -0.0749,  ...,  0.8304,  1.2399, -0.3825],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 5.2246e-01,  9.3604e-01,  7.7588e-01,  8.2910e-01,  4.6191e-01,\n",
      "        -1.1597e-02, -1.5503e-01,  8.0078e-01,  3.2617e-01,  2.0581e-01,\n",
      "        -1.0742e-01,  1.2299e-01,  3.9331e-01, -2.3376e-01,  1.0020e+00,\n",
      "        -1.0089e-01,  8.2910e-01,  4.7998e-01,  4.6313e-01,  3.4595e-01,\n",
      "        -3.6426e-01,  3.3667e-01, -1.0120e-01,  4.2847e-02,  6.4893e-01,\n",
      "         1.4355e-01,  1.3086e+00,  1.1582e+00,  6.4893e-01,  8.0664e-01,\n",
      "         4.5239e-01,  6.7090e-01,  3.8989e-01,  8.7305e-01,  5.4248e-01,\n",
      "         1.0052e-01,  4.7949e-01,  8.0273e-01,  4.1040e-01,  5.5420e-02,\n",
      "         5.9717e-01, -5.5664e-01,  4.7266e-01, -2.5708e-01,  7.4414e-01,\n",
      "        -4.4849e-01,  1.1224e-01,  4.4287e-01,  5.1514e-01, -1.9495e-01,\n",
      "         5.4639e-01,  5.5859e-01, -7.5745e-02,  4.9805e-01,  1.3062e-01,\n",
      "         1.3892e-01, -2.7979e-01,  2.7924e-02,  3.8892e-01, -3.7476e-01,\n",
      "         9.2773e-01,  8.2324e-01,  5.4541e-01,  3.7402e-01,  6.6162e-02,\n",
      "         5.6689e-01, -2.2620e-01,  3.0444e-01,  7.7393e-01, -1.8127e-01,\n",
      "         9.2188e-01,  3.8086e-01, -4.1064e-01,  8.0908e-01, -1.9580e-01,\n",
      "         3.7231e-01, -4.5288e-01,  1.1191e+00,  1.7242e-02,  9.5605e-01,\n",
      "         1.2634e-01,  3.4009e-01, -1.2952e-01,  8.3545e-01, -1.1243e-01,\n",
      "         3.6963e-01,  3.2178e-01,  4.3530e-01, -2.4097e-01,  9.0527e-01,\n",
      "         5.8301e-01,  3.5132e-01,  8.2422e-01,  9.3918e-03,  1.5049e+00,\n",
      "         7.9199e-01,  1.2480e+00,  9.6533e-01,  9.0271e-02,  3.5797e-02,\n",
      "         6.7432e-01,  4.5972e-01,  6.9214e-02,  5.0635e-01,  5.4492e-01,\n",
      "         4.0186e-01, -4.4702e-01,  3.4717e-01,  3.9624e-01,  4.0820e-01,\n",
      "        -3.6084e-01,  9.5752e-01,  5.1416e-01,  2.4377e-01,  7.9688e-01,\n",
      "        -1.0248e-01,  8.5889e-01,  8.6853e-02,  6.0645e-01,  4.8486e-01,\n",
      "         7.4316e-01,  6.3086e-01,  4.3945e-01,  6.2073e-02,  6.7578e-01,\n",
      "         8.9502e-01,  5.7178e-01,  6.4648e-01,  6.6211e-01, -2.7808e-01,\n",
      "         9.2188e-01,  3.6328e-01,  3.4937e-01,  3.8794e-01,  1.0664e+00,\n",
      "         6.0596e-01,  2.6318e-01,  7.1582e-01, -3.6914e-01,  5.1904e-01,\n",
      "        -6.8506e-01,  4.8584e-01,  9.8730e-01,  1.7796e-03,  5.1453e-02,\n",
      "        -1.6003e-01,  1.6089e-01, -1.3477e-01,  3.4229e-01,  5.9277e-01,\n",
      "         6.3867e-01,  4.0259e-01,  9.1260e-01, -3.2300e-01,  1.0742e+00,\n",
      "         7.0605e-01,  1.5137e+00,  7.8711e-01,  2.7417e-01,  5.4102e-01,\n",
      "        -5.4230e-02,  3.0371e-01,  1.8921e-01, -5.0342e-01,  2.2253e-01,\n",
      "         1.4763e-02,  3.6377e-01,  8.8721e-01, -7.7881e-02,  1.0244e+00,\n",
      "         6.1230e-01,  1.4883e+00,  6.6553e-01,  1.0352e+00, -1.3489e-01,\n",
      "        -3.3496e-01,  1.0410e+00, -2.4622e-01,  3.7085e-01, -1.8994e-01,\n",
      "         4.7803e-01,  3.9697e-01,  1.9409e-01,  5.9961e-01,  2.7856e-01,\n",
      "        -3.7915e-01,  1.7344e+00,  7.3438e-01,  9.4385e-01,  1.0830e+00,\n",
      "         5.1758e-01,  7.7393e-01,  7.0020e-01,  9.3750e-02, -6.7566e-02,\n",
      "         9.3652e-01,  1.4709e-01,  8.1934e-01, -1.8884e-01,  3.2300e-01,\n",
      "         4.9609e-01, -2.2107e-01,  5.9033e-01, -1.9409e-02, -6.4636e-02,\n",
      "         4.9316e-01,  5.0468e-03,  1.1064e+00,  8.1543e-01,  1.0947e+00,\n",
      "         7.4854e-01,  2.0972e-01,  7.3535e-01,  9.1992e-01,  1.5244e+00,\n",
      "         1.3066e+00,  9.6045e-01,  9.2627e-01,  7.2217e-01,  8.8330e-01,\n",
      "         8.0225e-01,  1.2432e+00,  7.0752e-01,  1.4932e+00,  1.1475e+00,\n",
      "         9.4824e-01,  9.4287e-01,  6.7041e-01,  9.2725e-01,  7.4170e-01,\n",
      "         1.2988e+00,  7.9590e-01,  7.4646e-02,  6.0254e-01,  1.4473e+00,\n",
      "         7.3096e-01,  4.3921e-01,  5.5542e-02,  1.5889e+00,  7.5879e-01,\n",
      "         1.0488e+00,  1.5029e+00,  1.3789e+00,  8.4619e-01,  1.0283e+00,\n",
      "        -1.5491e-01,  6.3843e-02,  7.8735e-02,  6.5869e-01, -4.4067e-01,\n",
      "         1.0029e+00, -3.8403e-01,  8.4375e-01,  4.2969e-01,  2.1643e-01,\n",
      "         4.2236e-01, -5.1172e-01,  3.4204e-01,  3.0308e-03, -6.2988e-02,\n",
      "         7.8467e-01,  7.5134e-02, -1.9702e-01,  9.8730e-01,  6.4453e-01,\n",
      "         8.5645e-01,  9.1113e-01,  8.7305e-01,  1.1543e+00,  1.2812e+00,\n",
      "         1.0674e+00,  4.9756e-01,  7.2119e-01,  3.6841e-01,  5.5322e-01,\n",
      "         1.4275e-02,  2.1753e-01,  4.6411e-01,  4.1431e-01,  3.0957e-01,\n",
      "        -2.4658e-01,  9.6631e-01,  2.8320e-01,  5.1904e-01,  7.1582e-01,\n",
      "         6.5771e-01, -2.2632e-01,  5.0391e-01,  2.1179e-01,  4.8633e-01,\n",
      "         1.0273e+00,  4.1077e-02,  6.9922e-01,  2.5122e-01,  6.9824e-02,\n",
      "         5.7422e-01, -1.7029e-01,  9.5898e-01,  5.8105e-01, -1.2604e-02,\n",
      "         8.8574e-01,  2.4866e-01,  2.3987e-01, -5.1465e-01,  2.4866e-01,\n",
      "         3.9160e-01,  6.4551e-01,  7.9492e-01,  1.1230e+00,  7.1191e-01,\n",
      "         1.5723e+00,  1.1543e+00,  6.2061e-01,  6.6504e-01,  8.9648e-01,\n",
      "         9.7314e-01,  1.7883e-01,  9.4238e-01,  5.1465e-01, -3.3112e-02,\n",
      "        -8.3252e-02,  1.4092e+00,  3.5693e-01,  1.4600e+00,  9.1211e-01,\n",
      "         7.4707e-01,  1.3604e+00,  8.2764e-01,  1.1113e+00,  8.0078e-01,\n",
      "         7.2217e-01,  3.4277e-01,  1.0664e+00,  6.9678e-01,  5.5127e-01,\n",
      "         4.4580e-01,  1.4551e+00,  1.0215e+00,  1.6562e+00,  5.7568e-01,\n",
      "         1.0811e+00,  1.0684e+00,  4.1675e-01,  4.7461e-01,  7.3242e-01,\n",
      "         1.1807e+00,  1.5020e+00,  1.1660e+00,  1.6318e+00,  7.2705e-01,\n",
      "         7.0947e-01,  9.9902e-01,  1.3125e+00,  5.3760e-01,  9.4629e-01,\n",
      "         9.7949e-01,  7.8223e-01, -7.1960e-02,  4.3457e-01,  9.0881e-02,\n",
      "         4.7424e-02, -4.6631e-01,  4.0698e-01, -5.9547e-03,  2.5120e-03,\n",
      "         9.7314e-01,  1.6953e+00,  9.4727e-01,  9.2236e-01,  1.0498e+00,\n",
      "         1.4287e+00,  5.0391e-01,  8.8672e-01,  1.4766e+00,  6.3574e-01,\n",
      "         9.5312e-01,  8.3936e-01,  3.0640e-01,  1.1836e+00, -5.1880e-02,\n",
      "         1.1191e+00,  2.8613e-01,  3.4814e-01,  5.2393e-01,  4.4312e-01,\n",
      "         1.0615e+00,  4.0991e-01,  1.4043e+00,  1.1094e+00,  4.5020e-01,\n",
      "         1.5791e+00,  1.2878e-01,  2.2717e-01,  4.2419e-02,  8.5059e-01,\n",
      "         8.0225e-01,  1.6250e+00,  7.7783e-01,  2.3108e-01, -1.5454e-01,\n",
      "         1.6240e+00,  1.2021e+00,  7.4316e-01,  5.8861e-03,  4.1284e-01,\n",
      "         2.8149e-01, -4.1847e-03,  9.1016e-01,  1.1676e-01,  1.5000e+00,\n",
      "         1.1816e+00,  9.9219e-01,  8.5254e-01,  7.2070e-01,  6.9531e-01,\n",
      "         6.7822e-01,  1.2676e+00,  6.4453e-01,  6.1621e-01,  4.6216e-01,\n",
      "         1.3281e+00,  1.1152e+00,  7.0020e-01,  1.4023e+00,  1.0928e+00,\n",
      "         7.7051e-01,  6.9678e-01,  1.5518e+00,  4.5068e-01,  4.7754e-01,\n",
      "         4.9854e-01,  8.5205e-01,  1.6736e-01,  1.5000e+00,  7.1533e-01,\n",
      "         9.8145e-01, -2.4460e-02,  8.6816e-01,  1.9580e-01, -2.9932e-01,\n",
      "         2.4622e-01,  1.0342e+00,  1.6387e+00,  1.2119e+00,  8.9893e-01,\n",
      "         8.5156e-01,  7.2314e-01,  7.0459e-01,  6.0986e-01,  1.2402e+00,\n",
      "         6.4502e-01,  5.8301e-01,  4.6484e-01,  1.6465e+00,  8.5449e-01,\n",
      "         5.5176e-01,  9.7168e-01,  5.4541e-01,  5.9375e-01,  9.4727e-01,\n",
      "         6.4404e-01,  1.4434e+00,  5.3857e-01,  8.9307e-01,  8.6182e-02,\n",
      "         1.2598e-01,  1.8884e-01,  7.9297e-01,  9.4482e-02,  7.1289e-01,\n",
      "         3.0859e-01,  3.7573e-01,  4.1577e-01,  6.6260e-01,  1.1865e+00,\n",
      "         6.8018e-01,  1.4121e+00,  4.7803e-01,  3.0371e-01,  1.5195e+00,\n",
      "         8.2764e-01,  5.2832e-01,  6.2256e-01,  1.8477e+00,  5.9619e-01,\n",
      "         1.2822e+00,  2.5244e-01,  1.7715e+00,  1.0537e+00,  8.8721e-01,\n",
      "         7.8516e-01,  8.7061e-01,  1.0479e+00,  1.1387e+00,  1.2725e+00,\n",
      "         6.6406e-01,  8.3936e-01,  1.0361e+00,  1.8646e-02,  1.2988e+00,\n",
      "         3.6523e-01,  9.3018e-01,  2.2461e-01,  6.6309e-01,  1.5537e+00,\n",
      "         6.6650e-01,  8.8086e-01,  1.6787e+00,  8.1006e-01,  3.4009e-01,\n",
      "         8.1543e-01,  6.8262e-01,  5.5762e-01,  3.5205e-01,  8.0566e-01,\n",
      "         2.1899e-01,  3.6768e-01, -1.4001e-01,  9.5825e-02, -2.4231e-01,\n",
      "        -1.5222e-01,  3.8647e-01,  5.2344e-01, -8.7500e-04,  3.0908e-01,\n",
      "        -6.7940e-03,  9.2334e-01,  7.7026e-02,  5.6738e-01,  4.8535e-01,\n",
      "         3.9087e-01,  3.9819e-01,  8.8135e-02, -1.3562e-01,  8.3069e-02,\n",
      "         2.1912e-02,  2.1802e-01,  6.0352e-01,  1.0127e+00,  1.0068e+00,\n",
      "         9.6191e-01, -1.7529e-01,  4.5801e-01,  2.4011e-01, -1.1255e-01,\n",
      "         5.3076e-01,  6.7969e-01,  5.4736e-01,  9.5703e-01,  3.4253e-01,\n",
      "         8.7061e-01,  4.2090e-01,  2.7222e-01,  1.1078e-01,  4.0894e-01,\n",
      "         1.0527e+00,  3.5913e-01,  6.0107e-01,  1.1957e-01, -2.2925e-01,\n",
      "         5.8545e-01,  3.4546e-02, -2.5195e-01,  1.4734e-01,  2.6660e-01,\n",
      "         2.9358e-02,  5.7764e-01,  1.0986e+00,  1.6670e+00,  8.8086e-01,\n",
      "         1.0088e+00,  1.7686e+00,  9.1943e-01,  2.2791e-01,  8.4033e-01,\n",
      "         8.6133e-01,  1.1543e+00,  8.6230e-01, -2.2241e-01,  1.3635e-01,\n",
      "         2.1948e-01,  1.7891e+00,  1.3535e+00,  1.0830e+00,  9.6631e-01,\n",
      "         9.0967e-01,  8.9893e-01,  7.9541e-01,  1.3389e+00,  7.8320e-01,\n",
      "         7.1167e-02, -7.4219e-02,  1.7539e+00,  1.3848e+00,  9.4385e-01,\n",
      "         1.1924e+00,  1.3708e-01,  3.7842e-01,  2.2937e-01,  3.7174e-03,\n",
      "         8.0664e-01,  1.6083e-02,  7.0996e-01,  1.8184e+00,  7.3975e-01,\n",
      "         1.0342e+00,  4.7754e-01,  1.4287e+00,  6.5381e-01,  7.1191e-01,\n",
      "         1.1725e-01,  1.7539e+00,  1.3311e+00,  1.0332e+00,  9.1357e-01,\n",
      "         7.2949e-01,  7.4756e-01,  1.6084e+00,  1.0332e+00,  1.5247e-01,\n",
      "         1.4707e+00,  1.5869e+00,  8.4766e-01,  1.4502e+00,  1.2715e+00,\n",
      "         9.8535e-01,  4.4873e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 4.1431e-01, -4.5868e-02,  2.0340e-02,  5.9326e-02,  6.8665e-02,\n",
      "         1.2299e-01, -4.0747e-01, -5.5328e-02, -1.7273e-01, -6.8848e-01,\n",
      "        -2.6807e-01, -4.3481e-01, -3.6591e-02, -2.2668e-01,  7.9102e-02,\n",
      "        -4.8877e-01, -5.0586e-01,  3.8037e-01,  2.9492e-01,  7.6355e-02,\n",
      "        -2.3303e-01,  7.4902e-01, -7.1106e-02, -4.1284e-01,  3.3301e-01,\n",
      "        -2.3865e-01,  3.8184e-01,  1.6040e-01,  4.2114e-01, -1.7688e-01,\n",
      "         3.0273e-01, -9.3567e-02,  4.1602e-01, -2.9248e-01,  4.5386e-01,\n",
      "         1.3000e-01,  2.7417e-01, -5.2246e-01,  5.0049e-01,  1.3574e-01,\n",
      "         2.0349e-01, -7.5635e-01,  3.0151e-01, -8.7433e-03,  1.0046e-01,\n",
      "        -3.7061e-01, -7.6538e-02,  9.9365e-02,  2.7368e-01, -4.8193e-01,\n",
      "        -7.9895e-02,  1.7444e-01, -4.4458e-01,  4.7058e-02,  5.8960e-02,\n",
      "        -2.7051e-01, -9.3079e-02,  6.5247e-02, -4.9365e-01, -3.3276e-01,\n",
      "        -1.6272e-01, -5.6299e-01,  3.1738e-01,  6.3049e-02, -8.3887e-01,\n",
      "        -5.1221e-01, -4.1406e-01, -3.9429e-01, -4.0112e-01, -4.8145e-01,\n",
      "        -1.7810e-01, -5.4834e-01, -2.7124e-01, -3.9355e-01, -8.3203e-01,\n",
      "         1.6479e-02, -2.6221e-01,  2.3486e-01, -1.7810e-01, -4.2139e-01,\n",
      "        -5.7220e-02, -2.6343e-01, -4.2847e-01, -6.1914e-01, -7.1436e-01,\n",
      "        -1.8115e-01, -2.8882e-01, -4.7656e-01, -3.0005e-01, -6.2109e-01,\n",
      "         3.2471e-01,  1.8958e-01, -5.1709e-01, -1.9788e-01,  6.2939e-01,\n",
      "         8.8916e-01,  6.4697e-01,  5.6348e-01, -2.8931e-01, -1.8018e-01,\n",
      "        -4.3530e-01,  2.0374e-01, -4.2822e-01, -3.9209e-01, -1.5491e-01,\n",
      "        -2.2595e-01,  1.1261e-02, -2.9419e-01, -1.1542e-01, -1.1884e-01,\n",
      "        -2.3669e-01, -5.6885e-01,  3.6890e-01,  1.7676e-01,  9.9548e-02,\n",
      "        -9.2480e-01,  9.8145e-02, -9.1125e-02,  6.1230e-01,  4.4971e-01,\n",
      "        -6.0272e-02,  4.5166e-01,  1.4478e-01, -3.7646e-01,  4.5972e-01,\n",
      "         1.2817e-01,  2.1558e-01,  2.0630e-01, -1.2375e-02, -8.5205e-01,\n",
      "        -5.2681e-03,  3.2739e-01,  5.9033e-01,  3.7781e-02, -5.8105e-01,\n",
      "         3.2959e-01,  1.2054e-01, -2.0157e-02, -7.9688e-01, -4.6234e-02,\n",
      "        -4.5972e-01, -8.2581e-02, -5.5206e-02, -5.9180e-01, -1.3086e-01,\n",
      "         2.0630e-02, -1.5552e-01, -8.4814e-01, -3.2495e-01, -3.2153e-01,\n",
      "        -6.2646e-01,  4.1431e-01, -2.4988e-01, -1.0333e-01,  4.1260e-01,\n",
      "         5.8154e-01,  3.3228e-01,  4.9048e-01,  2.2302e-01, -3.5132e-01,\n",
      "        -5.2783e-01, -5.9601e-02, -1.5723e-01, -4.5972e-01,  1.6272e-01,\n",
      "        -1.2537e-01, -6.8665e-02,  2.0239e-01, -5.9601e-02,  3.5522e-01,\n",
      "         6.6895e-01,  2.5073e-01,  4.2871e-01,  1.2207e-01, -2.8198e-01,\n",
      "        -7.1436e-01,  3.5498e-01, -6.2402e-01,  5.0732e-01, -5.2686e-01,\n",
      "        -7.2632e-02,  3.4033e-01,  2.4567e-02, -3.7727e-03, -3.7646e-01,\n",
      "        -2.6807e-01,  1.4429e-01,  1.9202e-01,  9.4922e-01,  2.7661e-01,\n",
      "         1.3574e-01,  7.8271e-01,  8.5840e-01,  3.1006e-01, -5.7812e-01,\n",
      "         1.2451e-01, -5.7227e-01,  1.1554e-01, -6.5283e-01,  3.2129e-01,\n",
      "         2.6077e-02, -5.4980e-01, -2.5000e-01,  2.0264e-01, -7.8271e-01,\n",
      "         2.0935e-01, -1.5344e-01,  1.8176e-01,  2.8882e-01, -4.1779e-02,\n",
      "         1.7798e-01,  2.6520e-02, -4.4775e-01, -1.5820e-01,  1.7371e-01,\n",
      "         6.2256e-01,  7.7332e-02,  7.4023e-01,  9.3164e-01,  6.6602e-01,\n",
      "        -1.7590e-01, -6.0638e-02,  6.6504e-01,  1.9763e-01,  5.9619e-01,\n",
      "         1.5039e-01,  7.1582e-01,  8.5986e-01,  7.4365e-01, -2.4133e-01,\n",
      "         8.2245e-03,  5.7080e-01, -7.4902e-01,  3.2788e-01,  2.2766e-01,\n",
      "         3.5327e-01,  2.8223e-01,  1.0321e-01,  5.4395e-01,  9.9854e-01,\n",
      "         3.1372e-01,  5.0098e-01,  3.2617e-01,  7.2754e-01, -1.0223e-01,\n",
      "        -6.9482e-01, -2.0398e-01, -5.5127e-01, -2.8979e-01, -3.5107e-01,\n",
      "        -4.9805e-01, -5.5566e-01, -6.1768e-01,  4.6362e-01,  4.2505e-01,\n",
      "         4.0161e-02, -4.8486e-01,  7.7930e-01,  8.3374e-02, -3.4839e-01,\n",
      "         6.2598e-01,  7.2876e-02, -2.1936e-01,  4.3652e-01, -5.3369e-01,\n",
      "        -3.6133e-01,  1.5540e-01,  4.8853e-01,  3.0127e-01,  2.9761e-01,\n",
      "         2.8540e-01,  5.0049e-01,  1.7114e-01,  4.9292e-01, -1.4229e-03,\n",
      "        -5.5469e-01, -8.5022e-02,  1.9958e-02,  1.0840e-01, -2.2107e-01,\n",
      "        -7.0117e-01, -2.9834e-01,  3.0591e-01,  4.1748e-02, -2.5757e-01,\n",
      "         3.6957e-02, -1.9165e-01,  2.5659e-01,  7.4707e-02,  3.6263e-04,\n",
      "        -2.7759e-01,  3.1641e-01, -3.9453e-01,  1.1639e-01, -1.2549e-01,\n",
      "        -5.4834e-01, -5.6201e-01,  1.9812e-01,  6.6223e-02, -4.5361e-01,\n",
      "         3.3618e-01,  2.3645e-01, -2.7657e-03, -6.2207e-01,  3.9624e-01,\n",
      "         1.5051e-01, -4.0137e-01, -3.0347e-01,  2.2644e-01,  3.8281e-01,\n",
      "         2.6929e-01,  1.6077e-01,  9.5166e-01, -1.4551e-01,  5.6055e-01,\n",
      "         7.8430e-02,  4.9390e-01,  3.5400e-01,  1.0449e+00,  3.3130e-01,\n",
      "        -3.9038e-01,  1.7383e-01,  6.3965e-02,  2.4768e-01,  2.9102e-01,\n",
      "         8.4473e-01,  2.9810e-01,  6.2598e-01, -3.1348e-01,  2.2021e-01,\n",
      "         7.3926e-01,  5.5573e-02,  7.1289e-01,  6.2891e-01, -5.1172e-01,\n",
      "         4.5654e-01, -1.0394e-01,  6.0352e-01,  3.5815e-01,  6.8115e-01,\n",
      "         2.0862e-01,  7.2803e-01, -2.9004e-01,  2.3450e-01,  3.1104e-01,\n",
      "         3.7659e-02,  6.1084e-01,  6.3281e-01,  2.2827e-01,  4.0710e-02,\n",
      "         1.0312e+00,  1.4172e-01,  3.4839e-01,  6.7334e-01,  2.1362e-01,\n",
      "         6.1816e-01, -6.7200e-02,  3.9526e-01,  2.1619e-01, -3.9886e-02,\n",
      "         3.5645e-02, -3.3740e-01,  8.8770e-01,  2.6001e-01, -3.3496e-01,\n",
      "         1.8787e-01,  5.7275e-01,  3.3374e-01,  1.1113e+00,  6.3770e-01,\n",
      "         8.1482e-02,  1.2488e-01,  8.1592e-01,  4.1382e-01,  2.9007e-02,\n",
      "         9.6826e-01,  2.6050e-01, -7.4219e-02,  2.3059e-01, -5.6787e-01,\n",
      "         1.3611e-01,  3.6987e-02,  3.1128e-01, -4.8096e-01,  4.4507e-01,\n",
      "         2.2705e-01,  1.1337e-02,  5.0977e-01,  1.4844e-01, -1.0773e-01,\n",
      "         5.3467e-01, -6.0303e-01,  1.8384e-01,  1.0443e-01,  3.4253e-01,\n",
      "         3.8452e-01,  4.3994e-01,  7.8955e-01,  2.3352e-01,  1.3037e-01,\n",
      "         6.5283e-01,  4.1162e-01,  8.2617e-01, -4.0527e-01,  4.6777e-01,\n",
      "         3.2910e-01, -5.6934e-01,  4.1772e-01,  1.4610e-03,  4.1968e-01,\n",
      "         7.6562e-01,  2.5171e-01,  9.3066e-01,  1.0146e+00,  7.4561e-01,\n",
      "        -9.8572e-02,  1.8140e-01,  9.2383e-01, -2.2986e-01,  4.6606e-01,\n",
      "         4.4067e-01,  3.9136e-01,  8.1641e-01,  6.7725e-01,  4.5435e-01,\n",
      "         8.4668e-01,  4.7974e-01,  4.5483e-01,  2.0923e-01,  3.6646e-01,\n",
      "         4.2627e-01,  8.1396e-01, -3.1714e-01,  4.1553e-01,  1.0391e+00,\n",
      "         5.1611e-01, -1.9006e-01,  5.6006e-01,  2.6270e-01,  5.0488e-01,\n",
      "         3.0762e-01,  4.6167e-01,  3.4131e-01,  7.7295e-01,  1.5466e-01,\n",
      "         9.1260e-01,  1.0605e+00,  8.6816e-01,  2.4780e-02,  1.5442e-01,\n",
      "         9.9023e-01, -1.3733e-01,  4.5996e-01,  5.0751e-02,  2.1118e-01,\n",
      "        -4.0497e-02,  3.0176e-01,  3.1982e-01,  8.1348e-01,  6.7334e-01,\n",
      "         4.7754e-01,  3.1372e-01,  9.3750e-01, -2.0276e-01, -1.7883e-01,\n",
      "         1.8323e-01, -5.2344e-01,  7.2266e-01, -5.4883e-01,  1.9006e-01,\n",
      "         1.0056e-02,  2.4460e-02,  1.2512e-01,  5.5859e-01,  5.5420e-01,\n",
      "         6.7969e-01,  3.3057e-01,  8.6572e-01,  2.0166e-01,  6.2695e-01,\n",
      "         3.4814e-01,  5.2539e-01,  3.3008e-01,  4.1504e-01,  9.9316e-01,\n",
      "         5.0635e-01,  3.4985e-01, -1.9417e-03,  5.7959e-01,  1.6797e-01,\n",
      "         2.8442e-01,  8.6279e-01,  7.1436e-01,  3.9819e-01,  5.5518e-01,\n",
      "         9.6436e-01,  5.2051e-01,  8.4912e-01, -1.3618e-03,  7.4805e-01,\n",
      "         3.6304e-01,  8.5156e-01, -8.1177e-02,  3.8135e-01, -5.2261e-04,\n",
      "         4.0552e-01,  9.6094e-01,  3.7720e-01, -4.1290e-02,  6.5576e-01,\n",
      "         5.1660e-01, -2.3425e-01,  6.1963e-01, -3.6987e-01,  1.2128e-01,\n",
      "        -3.1104e-01,  1.9727e-01, -7.7393e-01, -2.8174e-01,  3.4607e-02,\n",
      "        -6.8848e-01,  3.5742e-01,  3.8037e-01,  2.5024e-01,  9.8828e-01,\n",
      "         2.5317e-01,  2.9980e-01,  2.7856e-01,  3.7964e-01, -2.5513e-01,\n",
      "        -6.7200e-02, -4.0137e-01,  5.1147e-02, -8.4521e-01,  5.3711e-02,\n",
      "        -9.4043e-01, -5.8105e-01,  1.8237e-01,  6.4502e-01,  8.3691e-01,\n",
      "         3.4253e-01, -1.9604e-01, -2.3572e-01, -2.6099e-01, -7.2510e-01,\n",
      "         2.8870e-02,  4.2627e-01,  5.5969e-02, -2.1936e-01,  4.1675e-01,\n",
      "         2.4854e-01,  3.3276e-01,  8.7524e-02,  4.3677e-01,  1.3672e-01,\n",
      "        -8.5266e-02,  3.9819e-01,  3.0933e-01,  6.2549e-01, -1.9189e-01,\n",
      "         3.8745e-01,  1.8860e-01, -1.5015e-01, -7.2266e-02,  5.7080e-01,\n",
      "         1.7407e-01, -4.0039e-01, -2.8271e-01,  3.0029e-01,  6.4697e-01,\n",
      "         2.2266e-01,  1.6760e-01,  5.3125e-01, -7.4902e-01,  5.5809e-03,\n",
      "        -2.4329e-01,  4.3701e-01,  3.5278e-01, -7.3389e-01,  5.5908e-02,\n",
      "         4.0558e-02,  3.0688e-01,  7.3877e-01,  2.1167e-01,  8.9160e-01,\n",
      "         1.0527e+00,  6.6846e-01, -1.9336e-01,  1.4062e-01,  7.8076e-01,\n",
      "        -1.5723e-01, -2.5220e-01,  4.0552e-01,  3.6743e-01,  6.8018e-01,\n",
      "         1.7871e-01, -7.8174e-01,  3.3989e-03, -1.8152e-01, -7.2571e-02,\n",
      "         3.5718e-01, -5.8496e-01, -5.5420e-01,  1.5369e-01,  4.2076e-03,\n",
      "         5.2979e-01,  4.0942e-01,  7.1729e-01,  8.4668e-01,  3.1006e-01,\n",
      "         1.2317e-01,  1.3147e-01,  7.4512e-01,  1.3440e-01,  9.3799e-01,\n",
      "         1.0430e+00, -1.2030e-01,  2.0898e-01,  3.2153e-01,  1.0822e-01,\n",
      "         7.3730e-01, -1.7102e-01,  4.2627e-01,  2.9126e-01,  2.0544e-01,\n",
      "         4.9487e-01,  4.2334e-01], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.2473,  0.3265, -0.2041,  ...,  1.3224,  1.2499, -0.5061],\n",
      "         [ 0.4637,  0.5302, -0.0741,  ...,  0.6818,  0.5789, -0.3328],\n",
      "         [ 1.1072,  0.3303, -0.2791,  ...,  1.0294,  0.8438, -0.6100],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.6445, 1.3604, 0.6533,  ..., 1.2793, 0.9712, 0.5913], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.6523, 0.1617, 0.2737,  ..., 0.2456, 0.5884, 0.4980], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.9597,  0.0089, -0.4311,  ...,  1.0346,  1.0265, -0.5073],\n",
      "         [ 1.0428,  0.0499,  0.2331,  ...,  1.0188,  0.7449, -0.0427],\n",
      "         [ 0.6090,  0.1479,  0.1384,  ...,  1.0878,  0.6380, -0.0308],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.7427,  0.7993,  0.7920,  0.6455,  0.4707,  0.7666, -0.0721,  1.4316,\n",
      "         0.8125,  1.1396,  0.6621,  0.8447,  0.1997,  0.4753,  0.7920,  0.1116,\n",
      "         0.9185,  0.7393,  0.7583,  0.4607,  1.5010,  0.8198,  0.7168,  1.5586,\n",
      "         0.9097,  0.6045, -0.2321, -0.1225, -0.1180,  1.7148,  1.1533,  0.5898,\n",
      "        -0.0421,  1.2637,  0.2727,  1.3232,  0.1747,  0.3464,  0.3870,  0.6772,\n",
      "         0.3894,  0.3301,  1.7959,  1.1064,  0.3916, -0.3049,  0.4670, -0.1542,\n",
      "         0.0859, -0.2349,  0.2905,  0.2998,  1.5459,  0.9824,  1.1240,  0.8359,\n",
      "         0.9883,  1.3428,  0.4202,  0.5156,  1.0742,  0.8013,  1.2285,  0.5796,\n",
      "        -0.0590,  1.5537,  0.4849,  0.9707,  0.4250, -0.1267,  0.1401, -0.1938,\n",
      "         0.0466, -0.1389,  1.2402,  0.1721,  0.3191,  0.2544,  0.6016,  0.3362,\n",
      "        -0.0868,  0.5605,  0.2952,  0.4612,  1.3447,  0.6343,  1.0498,  0.5801,\n",
      "         1.6973,  0.5039,  1.0498,  0.7129,  0.0916,  1.3945,  0.2627,  0.4075,\n",
      "         0.2527,  0.7935,  0.5034,  0.1512,  0.5552, -0.0961,  0.8267, -0.3047,\n",
      "         1.7734,  0.5742,  1.0664,  0.7627,  0.1470,  0.3630,  0.4443,  0.0920,\n",
      "         1.5352,  1.0703,  1.3906,  0.9053,  1.2832,  1.0527,  1.1777,  0.4790,\n",
      "         1.3311,  0.8354,  1.3848,  0.6558,  0.4038, -0.0130,  1.4463,  0.4033,\n",
      "         0.5972,  0.4341, -0.2800,  0.2625,  0.2512,  0.1385,  1.2764,  0.2140,\n",
      "         0.4202,  0.2218,  0.7256,  0.3240,  0.4395,  1.7500,  0.4038,  0.6875,\n",
      "         0.6968,  1.7295,  0.4951,  0.7017,  0.8252,  0.0590,  1.4854,  0.3879,\n",
      "         0.4922,  0.3418,  1.2754,  0.3896,  0.6865,  1.2285,  1.2646,  0.7520,\n",
      "         0.5747,  0.7324,  1.6895,  0.5864,  0.6035,  0.5898,  1.7520,  1.0068,\n",
      "         0.4666,  1.6562,  1.1689,  0.1971,  0.5537, -0.0095,  0.4189,  0.4819,\n",
      "        -0.0974,  1.5586,  1.0918,  1.3193,  0.8457,  1.2256,  1.0918,  1.2559,\n",
      "         0.6001,  1.4385,  0.8237,  1.3809,  0.6318,  0.4453, -0.0908,  1.6699,\n",
      "         0.6587,  0.8696,  0.5264, -0.2759,  0.2744, -0.1106,  0.2294,  0.0405,\n",
      "         0.2421,  1.3711,  0.2150,  0.4045,  0.2710,  0.6353,  0.5205,  0.7412,\n",
      "         1.3633,  0.7036,  1.2949,  0.6890, -0.0963,  0.0045,  0.0552,  1.9004,\n",
      "         0.6494,  0.7021,  0.7056, -0.0303,  1.8740,  0.6567,  1.0625,  1.0000,\n",
      "         1.1543,  0.7002,  0.0249,  0.7817,  0.4780,  0.4048,  1.6162,  0.4475,\n",
      "         0.5737,  0.5693,  1.2627,  0.5444,  0.3379, -0.1782,  1.2041,  0.4844,\n",
      "         0.1081,  0.8940, -0.0295,  1.8955,  0.6470,  1.0869,  1.0947,  0.9390,\n",
      "         0.7759,  0.0036, -0.1635,  1.2666,  0.2661,  0.3274, -0.3074,  1.5381,\n",
      "         0.8262,  0.5537,  0.7231,  1.1289, -0.0641,  0.6050,  0.1479,  0.1802,\n",
      "         0.5283,  0.7349,  0.7290,  0.0259,  1.6924,  1.1064,  1.0674,  0.9038,\n",
      "         0.9229,  0.0494,  0.5688,  0.3230, -0.1670,  0.5894,  0.5923,  0.2029,\n",
      "         0.1356,  0.5718,  0.5386,  1.0059,  0.7388,  0.2443,  1.2686,  0.6372,\n",
      "         0.3796,  0.2045,  0.2573,  0.9980,  1.0244,  0.2474,  0.8086,  0.4204,\n",
      "         0.5229, -0.0131,  0.4600, -0.1273,  0.8066,  0.1223,  0.3665,  0.9580,\n",
      "         0.8237,  0.3352,  1.1016,  1.4961,  1.2998,  1.0098,  1.2764,  0.4033,\n",
      "         0.2908,  0.7139, -0.3027,  0.6768,  1.0049,  0.5337,  0.0695,  0.4500,\n",
      "         0.2542,  1.7559,  1.1104,  1.3281,  0.9404,  0.9561, -0.1692,  1.5371,\n",
      "         0.7104,  1.4102,  0.5693,  0.7554,  1.1953,  0.5288,  0.1721,  0.5088,\n",
      "         0.9087,  0.0535,  0.6050,  0.2788,  1.4463,  1.6973,  0.8296,  0.9961,\n",
      "         0.9287,  0.1346,  0.9253,  0.3123,  1.2871,  1.3184,  1.1650,  0.2874,\n",
      "         0.3481,  1.0732,  0.6069,  1.0615,  0.6235, -0.0912,  1.2266,  0.8789,\n",
      "         1.0742,  0.8242,  0.5933,  0.4622,  0.9448,  0.8604,  0.4285, -0.0693,\n",
      "         1.4355,  1.1562,  0.9473,  0.8838,  0.5312,  0.4417,  0.7861,  0.7988,\n",
      "         0.4492,  0.3909, -0.0224,  0.6216,  0.8071,  0.3081,  0.7168, -0.0422,\n",
      "         0.1940,  1.1182, -0.0222,  1.6934,  0.6641,  0.8779,  0.9233,  0.5518,\n",
      "         0.2983,  0.7007,  0.4099,  0.3591,  1.3262,  0.5986,  0.5737, -0.2399,\n",
      "         0.2920,  0.2191, -0.0652, -0.2605,  1.7490,  1.0176,  1.1201,  0.7188,\n",
      "        -0.0637,  0.1498,  1.2402,  0.9360,  0.6050,  0.1842,  0.5752,  0.7798,\n",
      "         0.3306,  0.5347,  0.6279,  1.0205,  1.2451,  0.5723,  1.0449,  0.3442,\n",
      "         0.2096,  0.6777,  0.8018, -0.0792,  0.4863,  0.2350,  0.1891,  0.9575,\n",
      "         0.3257,  0.7275,  0.3459,  0.9341,  0.9004,  0.4211,  1.0078,  0.1847,\n",
      "         0.1395,  0.3955,  0.2004,  0.7427,  0.6982, -0.0474,  1.2236,  0.9053,\n",
      "         0.3645,  1.3184,  0.6729,  0.8667,  0.3015,  0.1696, -0.0103,  0.1019,\n",
      "        -0.1087,  1.0576,  0.1379,  1.2432,  0.7295,  0.3635, -0.1904,  0.8696,\n",
      "         0.7456,  0.1589, -0.0635,  0.7695, -0.2050,  0.6616,  0.0724,  1.1035,\n",
      "         0.4902,  0.7944,  0.7915, -0.2363,  0.5771,  0.1444,  0.3899, -0.2057,\n",
      "        -0.0440,  0.6138,  0.4363, -0.0417,  0.1172, -0.5298,  0.0318,  0.7910,\n",
      "         0.8120,  0.9326,  0.6665,  0.4636,  0.4241,  1.6328,  0.8403,  1.1094,\n",
      "         0.7871,  0.7754,  1.4355,  0.8452,  1.0576,  0.7676,  0.8442, -0.0222,\n",
      "         0.6147,  0.7280,  0.4165,  0.3818, -0.2708, -0.2915,  1.7031,  0.8916,\n",
      "         0.7949,  0.7578,  0.5757,  0.7725,  0.7251,  0.7856,  0.5308,  0.0641,\n",
      "         1.3945,  0.7598,  0.6396,  0.6846,  1.2441,  0.5986, -0.1204,  1.0146,\n",
      "         0.3882,  0.0299,  1.5195,  0.8511,  1.1777,  0.8428,  0.9678,  0.1895,\n",
      "         0.8257,  1.0303,  0.2925,  1.3916,  0.7905,  0.9248,  0.1713,  1.5781,\n",
      "         1.0840,  0.1289,  1.1973,  0.8159,  1.1436,  0.6143,  0.6890,  1.4072,\n",
      "         0.7275,  0.6738,  0.3191,  0.9673,  0.3174,  1.0781, -0.2081,  0.5894,\n",
      "        -0.0158, -0.3862,  0.6646,  0.7627, -0.2198,  1.0146,  0.5723,  0.6553,\n",
      "        -0.2308,  0.8477,  0.2593,  0.1078,  0.2520,  0.3472,  0.3000,  0.1780,\n",
      "         0.7988,  0.9766,  0.5508,  0.4607, -0.3552,  1.3193,  0.3379,  1.1611,\n",
      "         0.4026,  1.0479,  0.4277,  0.9111,  0.4463,  0.0538,  0.8047, -0.2274,\n",
      "         0.5200,  0.7207,  0.5728, -0.4836,  1.1787,  0.4133,  0.7666,  0.9751,\n",
      "         0.0457,  0.7524,  0.5596,  0.4070,  1.4854,  0.5859,  0.7437,  1.3799,\n",
      "         0.6274,  0.7705,  1.3750,  0.4810,  0.6172,  1.2764,  0.6665,  0.7578,\n",
      "         0.7827,  1.0068,  0.3025,  0.7822,  0.5088,  0.6587,  0.8428,  0.5610,\n",
      "         1.2100,  1.6514,  0.8403,  0.8232,  0.7100,  0.4241,  0.9150,  0.6460,\n",
      "         0.4087,  1.5088,  0.7466,  0.1726,  0.6719,  0.1038,  0.5845,  0.0114,\n",
      "         0.4751,  0.7891,  0.3037,  0.6265, -0.0177,  0.1284,  0.7217,  0.5938,\n",
      "         0.7993,  0.1114,  0.4932,  0.0737,  0.4224,  0.0560,  0.9688,  0.1379,\n",
      "         0.4644,  0.3789,  0.8130,  0.9272,  0.1866,  0.9814, -0.0484,  1.2568,\n",
      "         0.5195,  0.1992,  0.6387,  0.7588,  0.5186,  0.1902,  0.0128,  0.6216,\n",
      "        -0.0066,  0.9385,  0.4849,  0.4241,  1.4746,  0.7412,  0.8296,  0.6548,\n",
      "         0.7603,  0.0549,  0.3206,  0.4785,  0.1621,  0.3704,  0.8018,  0.5659,\n",
      "         0.5669,  0.8110,  0.8638,  0.6475,  0.6348,  0.4834,  0.6411,  0.9043,\n",
      "         0.7480,  0.3777,  0.9575,  0.5557,  1.0430,  0.9399,  1.3516,  0.8442,\n",
      "         1.2188,  1.0391,  1.0146,  1.4268,  1.2080,  1.0156,  1.0674],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_logits:  tensor([ 0.4604,  0.0546,  0.5435,  0.1421,  0.4888,  0.3542, -0.5693,  0.2754,\n",
      "         0.2357,  0.6890,  0.4260,  0.2600, -0.5908, -0.4182, -0.1713, -0.4685,\n",
      "        -0.2627, -0.0970, -0.0474,  0.4080,  0.3130,  0.8008,  0.5649,  0.2367,\n",
      "         0.6011,  0.5732, -0.7686,  0.0402, -0.4912,  0.8110,  0.5298,  0.5088,\n",
      "        -0.8032,  0.2472,  0.2423,  0.0958,  0.0659,  0.2866,  0.4451,  0.0770,\n",
      "         0.1215, -0.0630,  0.8604,  0.5693, -0.4170, -0.5269, -0.0839,  0.3191,\n",
      "         0.1423, -0.8501, -0.8618, -0.4883,  0.3906,  0.5093,  0.9858,  1.0840,\n",
      "         0.5513,  1.0020,  0.2959,  1.0352,  0.9839,  0.5742,  0.5391,  0.5234,\n",
      "        -0.1196,  0.7817,  0.6211,  0.7070,  0.3960, -0.8179, -0.3083, -0.1663,\n",
      "        -0.4175,  0.4045, -0.0580, -0.0326,  0.2312,  0.3840,  0.2766,  0.3452,\n",
      "        -0.3445, -0.1926, -0.6606,  0.3875,  0.3215,  0.6245,  0.5786,  0.3757,\n",
      "         0.3586,  0.4885,  0.4722,  0.3701, -0.9956, -0.3127, -0.0682,  0.1792,\n",
      "         0.3157, -0.1266,  0.0099, -0.8462,  0.1570, -0.7065, -0.4956, -0.2595,\n",
      "         0.5278,  0.5708,  0.5854,  0.5718, -0.4209,  0.0982, -0.0704, -0.5908,\n",
      "         0.3369,  0.4973,  1.0059,  0.9053,  0.9468,  0.6167,  0.6108,  0.6177,\n",
      "         0.4175,  0.6938,  1.0967,  0.2290,  0.8018,  0.1627,  0.9463,  0.2598,\n",
      "         0.8003, -0.3391, -0.5229, -0.5508,  0.5000, -0.1776, -0.3525, -0.1213,\n",
      "         0.1775,  0.3826,  0.2010, -0.7041,  0.3767,  0.5610,  0.1515,  0.6597,\n",
      "         0.5688,  0.4539,  0.0713,  0.5073,  0.5703, -0.9927, -0.1838,  0.0287,\n",
      "         0.3181,  0.5049,  0.3042,  0.1002,  0.3152,  0.1805,  0.1929,  0.1367,\n",
      "        -0.0897, -0.1993,  0.7793,  0.2227,  0.8789,  0.5762,  0.0661,  0.6333,\n",
      "         0.3838,  0.6982,  0.4824, -0.4141,  0.3596, -0.8052, -0.5127,  0.3816,\n",
      "        -0.1205,  0.5063,  0.5854,  1.0537,  0.9727,  1.0479,  0.7139,  0.7134,\n",
      "         0.6377,  0.3616,  0.7183,  1.0518,  0.3435,  0.9146,  0.1500,  0.7534,\n",
      "         0.6416,  0.8857, -0.4226, -0.5771, -0.4131, -0.2350, -0.4531,  0.4622,\n",
      "         0.0987, -0.1284, -0.0552,  0.2322,  0.4890,  0.3064, -0.4790, -0.0988,\n",
      "         0.2478,  0.6221,  0.0462,  0.5356, -0.9810, -0.2499, -0.7910,  0.6421,\n",
      "         0.2107,  0.6035,  0.5269, -0.7402,  0.4424,  0.1494,  0.8374,  0.9692,\n",
      "         0.3716,  0.4873, -0.8838,  0.1589,  0.4917,  0.2959,  0.0474,  0.1129,\n",
      "         0.4819,  0.6582,  0.1236,  0.1240, -0.0126,  0.0594, -0.0015, -0.0576,\n",
      "        -0.3784,  0.3606, -0.1180,  0.4854,  0.1619,  0.8579,  0.9219,  0.6064,\n",
      "         0.4167, -0.1660, -0.5342,  0.6211,  0.2280,  0.0415, -0.0363,  0.4944,\n",
      "         0.4463,  0.5972, -0.1338,  0.3401, -0.7964, -0.3975, -0.2805, -0.5356,\n",
      "        -0.2839, -0.0206, -0.1245, -0.8770,  0.3613,  0.7114,  0.4060, -0.8872,\n",
      "         0.1113, -0.7446,  0.1189,  0.0095, -0.8223, -0.4946, -0.1345, -0.5039,\n",
      "        -0.2242, -0.2727, -0.2769,  0.3784,  0.0225, -0.2686, -0.6240, -0.6108,\n",
      "         0.1802, -0.2729, -0.7817,  0.0229, -0.4819, -0.6367, -0.2639, -0.5117,\n",
      "        -0.4236, -0.2225,  0.4519, -0.0231,  0.3247, -0.4482,  0.2490,  0.1038,\n",
      "         0.0755, -0.3115, -0.3130, -0.4136,  0.2169,  0.5356,  0.1241, -0.0429,\n",
      "        -0.0509,  0.1945, -0.0667,  0.2849,  0.4670, -0.6172, -0.2971, -0.2839,\n",
      "        -0.7144,  0.5132,  0.5420,  0.9004,  0.9004,  0.9351, -0.4956,  0.6577,\n",
      "         0.9478,  0.7324,  0.6450,  1.1406,  0.6440,  0.6987,  0.1942,  0.4082,\n",
      "         0.1981, -0.6279, -0.1532, -0.3569,  0.5020,  0.3760,  0.6924,  0.7183,\n",
      "         0.6968, -0.1199,  0.1274, -0.0997,  0.5063,  0.5547, -0.3191, -0.0818,\n",
      "         0.1799,  0.4778, -0.5635, -0.2668,  0.0565,  0.0055,  0.0293,  0.2152,\n",
      "         0.1465, -0.0615,  0.1573,  0.0160, -0.0449,  0.1243,  0.0652, -0.6182,\n",
      "         0.4829,  0.0918,  0.3721,  0.1782, -0.4629,  0.3928,  0.3057, -0.1055,\n",
      "         0.0354, -0.1783,  0.1792, -0.0283, -0.0017,  0.2500,  0.0969, -0.8384,\n",
      "        -0.0415, -0.7495, -0.2384,  0.4104,  0.5820,  0.7695,  0.8188,  0.6646,\n",
      "         0.8682,  0.3887,  0.2495,  0.6436,  0.2297,  0.1235,  0.2207,  0.2751,\n",
      "         0.1865,  0.2908, -0.0155, -0.1038,  0.3381,  0.8843,  0.6406,  0.5430,\n",
      "        -0.2238, -0.0514,  0.1989,  0.8203,  0.6494,  0.6782,  0.4971, -0.0341,\n",
      "         0.5073, -0.3296, -0.0155,  0.2413,  0.4702,  0.5977,  0.4382,  0.3245,\n",
      "         0.7002,  0.8477,  0.5024, -0.6714,  0.1537, -0.2959, -0.2876,  0.0103,\n",
      "         0.1393, -0.1210, -0.2048,  0.3354,  0.4421,  0.4514,  0.2419,  0.2639,\n",
      "         0.6357,  0.7246,  0.0497,  0.4387,  0.1560, -0.0803,  0.5903,  0.7085,\n",
      "         0.2871,  0.6084,  0.4246,  1.0752,  0.1893, -0.6465, -0.5654,  0.1708,\n",
      "        -0.3621, -0.2083,  0.0240,  0.4854,  0.0963, -0.3630, -0.4434,  0.2302,\n",
      "         0.2020,  0.1064,  0.2844, -0.0442, -0.6313,  0.0820, -0.3022, -0.1220,\n",
      "        -0.3865,  0.1531, -0.3130, -0.7490,  0.2329, -0.3752, -0.2603, -0.5591,\n",
      "        -0.4563, -0.1525, -0.1731,  0.3118,  0.2208, -0.4639,  0.1627, -0.0500,\n",
      "         0.3091,  0.6646,  0.4187, -0.3884,  0.3726,  0.1763,  0.4932,  0.7388,\n",
      "         0.6812,  0.8687,  0.0939,  0.3679,  0.6694,  0.5708,  0.7925, -0.5767,\n",
      "         0.2100, -0.1321,  0.3230, -0.0266, -0.1667, -0.0058,  0.6167,  0.3774,\n",
      "         0.7144,  0.7900,  0.3430,  0.5493,  0.4956,  0.6660,  0.7070,  0.1639,\n",
      "         0.2827,  0.5254,  0.6470,  0.8662,  0.0510,  0.0660, -0.6992,  0.2080,\n",
      "         0.7339, -0.5796,  0.2036,  0.3865,  0.7798,  0.8149,  1.0410,  0.0055,\n",
      "         0.3506,  0.7739,  0.3049,  0.3142,  0.8320,  0.9409, -0.1174,  0.3491,\n",
      "         0.2017,  0.1089,  0.4797,  0.4556,  0.8271, -0.1306, -0.0660,  0.1318,\n",
      "         0.4431,  0.0732, -0.0933,  0.7891, -0.3054,  0.5508, -0.6909, -0.2974,\n",
      "        -0.3379,  0.0510,  0.0084,  0.1768, -0.4094, -0.1625, -0.1004, -0.1022,\n",
      "        -0.3516,  0.5015,  0.1566, -0.6445, -0.3418,  0.1494, -0.2369, -0.5220,\n",
      "        -0.1061, -0.0292,  0.0414,  0.1730, -0.4170,  0.4792,  0.1422,  0.4888,\n",
      "         0.6382,  0.1301, -0.3413,  0.2101, -0.0066, -0.3438,  0.0810, -0.5630,\n",
      "        -0.0092,  0.2729,  0.3611, -0.6084,  0.6099,  0.5879,  0.5542,  0.4067,\n",
      "        -0.1072,  0.6226, -0.4204,  0.3728, -0.0812,  0.2659,  0.6191,  0.2394,\n",
      "         0.3865,  1.1475, -0.1035,  0.2489,  0.5674, -0.0609,  0.3420,  1.0469,\n",
      "         0.2715,  0.0500,  0.5664,  0.6211,  0.6138,  0.1608,  0.1306,  0.4036,\n",
      "         0.3481, -0.1598, -0.0097,  0.7065,  0.3828,  0.4026,  0.7144,  0.5459,\n",
      "         0.1132,  0.4954, -0.1595, -0.6499, -0.2413, -0.2922,  0.0310, -0.2142,\n",
      "         0.1256,  0.3977, -0.3286,  0.2062,  0.3420, -0.6182,  0.2600,  0.1119,\n",
      "        -0.1934, -0.5244, -0.4346, -0.4263,  0.1075, -0.1757,  0.3926, -0.2469,\n",
      "        -0.0026, -0.0701,  0.2803,  0.1370, -0.2333, -0.2240, -0.2656,  0.5195,\n",
      "         0.5830, -0.5366,  0.3296,  0.3699, -0.3411,  0.2568, -0.1554, -0.1638,\n",
      "        -0.2905,  0.2333,  0.2930, -0.4736, -0.0514,  0.3008,  0.8394,  0.0978,\n",
      "        -0.3450, -0.7354, -0.0020,  0.2279,  0.0211, -0.6357, -0.0227,  0.0797,\n",
      "        -0.4854, -0.0163,  0.1070, -0.0143, -0.5767, -0.3232,  0.1774,  0.5244,\n",
      "         0.1163, -0.2788, -0.0905,  0.1140,  0.7446,  0.0889,  0.3008,  0.2920,\n",
      "         0.5601,  0.5283,  0.3792,  0.3582,  0.4961,  0.6919,  0.0740],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.2234,  0.5415, -0.1428,  ...,  1.3260,  1.4773, -0.3831],\n",
      "         [ 0.8170,  0.6234,  0.0063,  ...,  0.8570,  1.0443, -0.3544],\n",
      "         [ 0.5600,  0.5441,  0.0395,  ...,  0.5861,  0.8590, -0.2830],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.5566, 1.0225, 0.6748,  ..., 0.8096, 0.5859, 0.4165], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.3074, 0.0499, 0.2302,  ..., 0.0750, 0.2070, 0.4102], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0143,  0.1308,  0.0427,  ...,  0.2066,  0.1161, -0.0600],\n",
      "         [ 0.1106,  0.2612, -0.0516,  ...,  0.5453,  0.4683, -0.1591],\n",
      "         [ 0.2846,  0.1547,  0.0888,  ...,  0.0028,  0.2060, -0.0366],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0207,  ..., -0.0773, -0.0273, -0.0988],\n",
      "         [-0.0090,  0.0621, -0.0207,  ..., -0.0773, -0.0273, -0.0988],\n",
      "         [-0.0090,  0.0621, -0.0207,  ..., -0.0773, -0.0273, -0.0988]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2710, 0.6084, 0.4609, 0.2499, 0.4902, 0.2279, 0.4756, 0.3081, 0.1373,\n",
      "        0.5713, 0.6313, 0.6006, 0.6143, 0.5820, 0.6333, 0.2593],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1497, -0.0932, -0.1908,  0.0235, -0.2350, -0.0767, -0.2264,  0.0135,\n",
      "        -0.2284, -0.0740, -0.1224, -0.0397, -0.1875, -0.2825, -0.2367,  0.1783],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8114,  0.3539,  0.0130,  ...,  1.6916,  0.9518, -0.4723],\n",
      "         [ 0.8694,  0.2121,  0.2904,  ...,  1.6544,  0.8573, -0.1325],\n",
      "         [ 0.7449,  0.2224,  0.4452,  ...,  1.5683,  0.9635, -0.4226],\n",
      "         ...,\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989],\n",
      "         [-0.0090,  0.0621, -0.0208,  ..., -0.0774, -0.0274, -0.0989]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 3.6865e-01,  7.6904e-01,  5.5029e-01,  8.5889e-01, -1.0608e-01,\n",
      "         5.7373e-01,  2.4548e-01,  2.9980e-01,  2.6904e-01,  1.1689e+00,\n",
      "         5.6592e-01,  3.2178e-01,  1.1670e+00,  5.0342e-01,  5.1465e-01,\n",
      "         5.4260e-02,  1.3887e+00,  6.3818e-01,  2.4109e-01,  2.6099e-01,\n",
      "         1.1871e-01,  1.4404e+00,  4.1284e-01,  1.7949e+00,  4.7510e-01,\n",
      "         2.6270e-01,  1.0797e-01,  2.0728e-01,  1.7295e+00,  5.0879e-01,\n",
      "         3.2251e-01,  1.8567e-01,  4.2334e-01,  6.4502e-01,  1.6475e+00,\n",
      "         5.6787e-01,  2.5537e-01,  3.3661e-02,  1.3008e+00,  8.5254e-01,\n",
      "        -2.0859e-02,  3.4106e-01, -3.1372e-01,  1.2138e-02,  9.9316e-01,\n",
      "         8.2812e-01,  4.9365e-01,  5.6885e-01,  2.2559e-01,  1.6772e-01,\n",
      "         2.7515e-01,  1.7227e+00,  3.4253e-01,  8.6475e-01,  6.7529e-01,\n",
      "        -1.1743e-01,  1.4600e-01,  2.1576e-02,  9.2676e-01,  6.8262e-01,\n",
      "        -2.2644e-01, -2.2766e-02,  9.3115e-01, -1.9360e-01,  1.0107e+00,\n",
      "         5.1318e-01,  1.1182e+00,  1.5967e+00,  8.7354e-01,  1.9062e+00,\n",
      "         7.6611e-01,  4.8047e-01,  3.5474e-01,  5.8740e-01,  7.0996e-01,\n",
      "         6.0742e-01,  9.3201e-02,  3.6652e-02,  5.3760e-01,  6.3281e-01,\n",
      "         1.4380e-01,  1.1924e+00,  7.9785e-01, -8.0811e-02,  2.3889e-01,\n",
      "         6.1523e-01,  2.5415e-01, -1.6626e-01,  1.7148e+00,  9.8047e-01,\n",
      "         1.2432e+00,  2.9736e-01,  9.8535e-01,  7.6123e-01,  9.8242e-01,\n",
      "         4.8828e-01,  1.7773e-01, -1.8097e-02,  9.5459e-01,  4.8180e-03,\n",
      "        -1.3293e-01,  4.3921e-01,  1.1517e-01, -2.7271e-01,  1.6174e-01,\n",
      "         2.5757e-01,  7.2754e-02,  8.6451e-04, -1.6101e-01,  1.8591e-01,\n",
      "         1.1627e-01,  8.4082e-01,  4.4604e-01,  1.8097e-02, -4.2664e-02,\n",
      "         1.2756e-01,  8.0273e-01,  4.4434e-01,  8.2275e-02,  3.4814e-01,\n",
      "         3.6792e-01,  1.7158e+00,  8.5840e-01,  5.8350e-01,  6.7285e-01,\n",
      "         7.7002e-01,  2.3376e-01,  1.0586e+00,  4.6289e-01,  3.8965e-01,\n",
      "         3.9453e-01,  2.5391e-01,  1.6104e+00,  3.4082e-01,  5.9961e-01,\n",
      "         5.0977e-01,  4.4165e-01,  9.1992e-01,  6.5918e-01,  6.7773e-01,\n",
      "         4.6802e-01,  7.1191e-01,  3.8940e-01,  9.0723e-01,  6.7432e-01,\n",
      "         5.3564e-01,  2.1948e-01,  1.0967e+00, -1.3440e-01, -1.1633e-01,\n",
      "        -6.8176e-02,  1.9268e+00,  5.3174e-01,  3.4814e-01,  9.5605e-01,\n",
      "         5.9082e-01,  7.1826e-01, -2.3926e-02,  6.7090e-01, -2.7368e-01,\n",
      "         1.3145e+00,  4.0454e-01,  6.0498e-01,  7.4658e-01,  6.7969e-01,\n",
      "         1.9688e+00,  1.0635e+00,  7.9199e-01,  1.0088e+00,  9.3604e-01,\n",
      "         6.5088e-01,  5.9668e-01,  1.9307e+00,  9.8633e-01,  6.8164e-01,\n",
      "         8.5889e-01,  8.3008e-01,  6.4893e-01,  5.4541e-01, -1.2183e-01,\n",
      "         1.7490e+00,  5.1953e-01,  3.6816e-01,  3.9355e-01,  5.5078e-01,\n",
      "         3.3478e-02,  7.8760e-01,  2.2253e-01,  8.7769e-02,  3.0005e-01,\n",
      "         1.1803e-02,  1.3154e+00,  7.3633e-01,  5.6152e-01,  1.2139e+00,\n",
      "         6.9824e-01,  4.2725e-01,  1.8047e+00,  9.6143e-01,  5.4297e-01,\n",
      "        -2.1338e-01,  1.2783e+00,  3.1860e-01,  4.5142e-01,  6.9629e-01,\n",
      "         3.8965e-01, -1.7664e-01,  9.0283e-01,  1.4326e+00,  6.5820e-01,\n",
      "         5.8936e-01,  6.2305e-01, -2.6169e-02,  1.7588e+00,  8.8623e-01,\n",
      "         5.4785e-01, -2.1057e-01,  2.6074e-01,  3.3276e-01,  3.5132e-01,\n",
      "         1.8301e+00,  6.3525e-01,  3.1396e-01,  7.3340e-01,  7.6660e-02,\n",
      "         1.3213e+00,  4.0234e-01,  5.4053e-01,  5.1660e-01,  7.7881e-01,\n",
      "         2.3206e-01,  3.6475e-01,  1.5027e-01,  1.6621e+00,  6.5186e-01,\n",
      "         2.4109e-01,  1.2744e+00,  2.2668e-01, -5.2429e-02,  1.8506e+00,\n",
      "         5.0781e-01,  2.7051e-01,  1.2480e+00,  4.0137e-01,  6.6162e-01,\n",
      "         9.2676e-01,  1.2769e-01,  6.4404e-01,  4.1943e-01,  1.2305e+00,\n",
      "         2.4304e-01,  8.5889e-01,  2.1191e-01,  1.1102e-01,  6.8994e-01,\n",
      "         5.3833e-02,  1.0869e+00,  1.6675e-01,  8.3057e-01,  1.8250e-01,\n",
      "         1.2500e-01,  8.0762e-01,  6.3416e-02, -2.7054e-02,  1.3701e+00,\n",
      "         3.8239e-02,  2.5269e-01,  3.1982e-01,  4.9561e-01,  6.4209e-01,\n",
      "         2.4036e-01,  2.5806e-01,  5.1074e-01, -4.7638e-02, -3.1860e-01,\n",
      "        -2.7481e-02,  8.9209e-01,  1.4136e-01,  7.2021e-02,  5.9375e-01,\n",
      "        -6.2927e-02,  3.8647e-01,  1.6571e-02,  4.1040e-01,  8.3057e-01,\n",
      "         1.4404e-01,  5.9814e-01,  6.9336e-01,  2.9077e-01,  5.3955e-01,\n",
      "         8.7988e-01,  1.5552e-01,  1.5222e-01,  6.6504e-01,  3.8843e-01,\n",
      "         9.7754e-01,  2.2217e-01,  4.1504e-01,  7.0020e-01,  7.8418e-01,\n",
      "         7.7246e-01,  1.2139e+00,  8.7744e-01,  3.8696e-01,  8.6621e-01,\n",
      "         5.2002e-01,  9.1797e-01,  1.0801e+00,  1.4600e+00,  7.3096e-01,\n",
      "         5.9814e-01,  9.1406e-01,  1.0918e+00,  7.2461e-01,  2.1521e-01,\n",
      "         6.7480e-01,  4.1699e-01,  7.6660e-01,  9.9023e-01,  1.3447e+00,\n",
      "         5.9961e-01,  5.7959e-01,  2.1423e-02,  1.7461e+00,  7.5488e-01,\n",
      "         3.1860e-01,  6.3525e-01,  8.9502e-01,  4.8340e-01,  9.1797e-01,\n",
      "         8.0127e-01,  3.4668e-01,  5.6396e-01,  3.2861e-01,  5.2783e-01,\n",
      "         4.1333e-01,  4.3970e-01,  3.6133e-01,  6.6650e-02,  3.1555e-02,\n",
      "         5.5420e-01, -1.7920e-01,  3.8647e-01, -6.8848e-02,  1.3398e+00,\n",
      "         7.0850e-01,  5.6885e-01,  1.1484e+00,  7.3340e-01,  4.2700e-01,\n",
      "         1.3848e+00,  8.2275e-01,  2.9834e-01,  6.3867e-01,  3.7817e-01,\n",
      "         7.0801e-01,  4.8877e-01,  7.4561e-01,  9.3628e-02,  1.3262e+00,\n",
      "         6.1279e-01,  6.7432e-01, -1.4801e-02,  1.3994e+00,  5.1270e-01,\n",
      "         6.0107e-01,  6.3281e-01,  2.7808e-01,  9.9023e-01,  3.0298e-01,\n",
      "         1.8286e-01,  2.0886e-01,  7.3926e-01, -2.3486e-01,  5.0110e-02,\n",
      "        -6.5002e-02, -2.8540e-01,  5.5908e-01,  1.2578e+00,  5.3467e-01,\n",
      "         5.9521e-01,  6.2451e-01,  2.8857e-01,  8.6572e-01, -2.4561e-01,\n",
      "         1.9910e-01,  1.4429e-01,  1.7847e-01,  4.7070e-01,  6.2500e-02,\n",
      "         6.6748e-01,  1.5703e+00,  4.8291e-01,  2.5049e-01,  7.3047e-01,\n",
      "         2.5195e-01,  1.5830e+00,  7.6514e-01,  1.3281e+00,  4.4043e-01,\n",
      "         1.1025e+00,  4.1846e-01,  1.2607e+00,  5.8154e-01,  6.2354e-01,\n",
      "         3.9648e-01,  1.5791e+00,  9.7412e-01,  4.2065e-01,  1.2998e+00,\n",
      "         4.4385e-01,  3.7012e-01,  8.8684e-02,  1.3047e+00,  5.0098e-01,\n",
      "         6.0547e-01,  3.7085e-01,  1.6003e-01,  1.0223e-01,  5.4297e-01,\n",
      "         6.8481e-02,  1.7224e-01,  1.9556e-01,  3.7378e-01,  3.8025e-02,\n",
      "         7.2852e-01,  1.2852e+00,  5.6396e-01,  6.2402e-01,  7.0068e-01,\n",
      "         5.1660e-01, -1.4856e-01, -2.4780e-01, -2.4280e-01,  1.7051e+00,\n",
      "         7.9980e-01,  1.0742e+00,  5.0830e-01,  5.1367e-01,  6.7480e-01,\n",
      "         1.9385e-01,  1.1445e+00, -2.0093e-01,  1.4727e+00,  7.9883e-01,\n",
      "         7.1240e-01,  7.7637e-01,  5.5664e-01,  3.2593e-01,  6.6650e-01,\n",
      "         1.1078e-01,  5.0879e-01,  9.7363e-01,  4.7803e-01,  1.3000e-01,\n",
      "         7.0703e-01, -2.3251e-03,  5.9961e-01,  2.4109e-02,  1.6514e+00,\n",
      "         8.8867e-01,  7.3242e-01,  9.5605e-01,  5.6250e-01, -9.8877e-02,\n",
      "         1.3428e+00,  5.6689e-01,  7.2998e-01,  8.3643e-01,  8.4473e-01,\n",
      "         2.9932e-01,  3.4790e-01,  3.3844e-02,  4.7314e-01, -2.4121e-01,\n",
      "        -4.9744e-02,  9.1895e-01, -1.5381e-01,  8.0371e-01,  4.0747e-01,\n",
      "         3.9209e-01,  1.5908e+00,  6.2109e-01,  6.9434e-01,  7.3730e-01,\n",
      "         6.8169e-03,  7.0557e-01,  7.5635e-01,  1.5693e+00,  6.7236e-01,\n",
      "        -8.4457e-03,  1.7456e-01,  2.8638e-01,  5.1392e-02,  1.1053e-01,\n",
      "         7.3584e-01,  2.8052e-01,  1.4075e-01, -3.3521e-01,  4.3884e-02,\n",
      "        -5.2393e-01,  4.7461e-01, -2.0081e-01,  4.4946e-01, -1.5869e-01,\n",
      "         1.2793e+00,  6.1230e-01,  6.9971e-01,  4.5801e-01,  4.0601e-01,\n",
      "         1.0537e+00,  1.2627e+00,  3.4814e-01,  7.2607e-01,  3.0786e-01,\n",
      "         3.8635e-02, -8.7219e-02,  1.2832e+00,  6.9922e-01,  5.0342e-01,\n",
      "         1.3271e+00,  8.4521e-01,  6.6113e-01,  2.0496e-01,  2.0044e-01,\n",
      "         1.2764e+00,  5.9619e-01,  5.7666e-01,  6.6260e-01,  1.2607e+00,\n",
      "         6.6016e-01,  6.2158e-01,  7.3193e-01,  1.3545e+00,  7.0752e-01,\n",
      "         5.7080e-01,  5.1855e-01,  1.0625e+00,  9.6533e-01,  7.0068e-01,\n",
      "         5.6885e-01,  6.3037e-01,  3.6230e-01,  1.5566e+00,  7.2852e-01,\n",
      "         4.2163e-01,  1.3271e+00,  7.6660e-01,  3.6816e-01,  1.0557e+00,\n",
      "         4.5874e-01,  6.4148e-02,  1.1865e+00,  9.8438e-01,  7.1143e-01,\n",
      "         5.0391e-01,  9.7656e-01,  5.1758e-01,  2.5513e-01,  1.2461e+00,\n",
      "         2.9639e-01,  5.0928e-01,  7.1411e-02,  1.3643e+00,  7.0605e-01,\n",
      "         5.8545e-01,  2.1436e-01,  2.9761e-01, -1.0382e-01, -3.7506e-02,\n",
      "         1.2139e+00,  1.2607e+00,  6.9092e-01,  7.7490e-01, -1.5823e-02,\n",
      "         1.7212e-02, -2.0581e-01,  5.5713e-01,  4.3262e-01,  1.3447e+00,\n",
      "         7.0654e-01,  8.8672e-01,  5.0586e-01,  3.9868e-01,  1.3389e+00,\n",
      "         4.9170e-01,  5.4834e-01,  1.4209e+00,  5.6152e-01,  5.9424e-01,\n",
      "         1.3750e+00,  4.1016e-01,  6.4893e-01, -1.0480e-01,  6.0840e-01,\n",
      "         4.9194e-02,  1.1143e+00,  7.4036e-02,  1.4150e+00,  6.0791e-01,\n",
      "         8.2861e-01,  8.9844e-01,  8.5205e-01,  5.6458e-02,  1.5605e+00,\n",
      "         7.2217e-01,  8.5791e-01,  2.7985e-02,  1.7212e-01, -3.6377e-01,\n",
      "         2.8027e-01, -2.8613e-01,  1.0527e+00,  8.4814e-01,  6.6699e-01,\n",
      "         2.0679e-01,  3.4985e-01,  8.9661e-02, -3.5736e-02, -2.9199e-01,\n",
      "         5.5127e-01,  3.3862e-01,  3.4839e-01,  2.4475e-01, -3.3618e-01,\n",
      "         4.8486e-01, -6.7078e-02, -7.0496e-02,  1.1055e+00,  4.3652e-01,\n",
      "         8.2617e-01,  3.5010e-01,  4.4434e-01,  3.7720e-01,  1.5459e+00,\n",
      "         6.2939e-01,  5.6396e-01,  1.3887e+00,  6.1865e-01,  6.1475e-01,\n",
      "         1.4883e+00,  6.0693e-01,  8.6279e-01,  4.3164e-01,  6.7041e-01,\n",
      "        -7.2510e-02,  3.7402e-01,  2.6782e-01,  4.6191e-01,  3.2764e-01,\n",
      "         1.1346e-01,  1.5881e-01,  3.6987e-01,  9.7900e-02,  6.5039e-01,\n",
      "         1.4629e+00,  5.0635e-01,  1.2734e+00,  7.5537e-01,  2.8589e-01,\n",
      "         4.9414e-01,  1.3789e+00,  6.3916e-01,  6.3281e-01,  1.4512e+00,\n",
      "         5.5713e-01,  8.0322e-01,  3.3569e-01,  4.4360e-01,  1.5613e-01,\n",
      "        -1.3123e-01,  6.8701e-01,  4.8145e-01,  2.5293e-01,  3.5156e-01,\n",
      "         9.6130e-02,  1.6367e+00,  7.8906e-01,  1.8433e-01,  1.0166e+00,\n",
      "         4.3091e-02,  9.0918e-01,  1.5068e+00,  8.8428e-01,  9.4238e-01,\n",
      "        -3.4847e-03,  2.7026e-01,  4.6631e-01,  1.9995e-01,  1.6006e+00,\n",
      "         9.9023e-01,  4.8145e-01,  1.1689e+00,  5.5322e-01, -4.5166e-01,\n",
      "        -3.7061e-01, -1.8542e-01,  1.5918e+00,  8.0078e-01,  1.5342e+00,\n",
      "         8.7598e-01,  1.0166e+00,  5.0293e-02,  5.9912e-01,  3.8379e-01,\n",
      "         1.4248e+00,  6.0889e-01,  7.4365e-01,  1.1221e+00,  7.8174e-01,\n",
      "         1.6328e+00,  7.4268e-01,  8.7598e-01,  7.9541e-01,  1.6230e+00,\n",
      "         8.7744e-01,  7.3584e-01,  6.0254e-01,  6.4551e-01,  1.2529e+00,\n",
      "         6.7334e-01,  8.6816e-01,  6.5674e-01,  1.4541e+00,  9.9902e-01,\n",
      "         5.7617e-01,  1.4600e+00,  9.6191e-01,  6.7920e-01,  1.4246e-01,\n",
      "         1.2627e+00,  6.6260e-01,  1.0986e+00,  1.5566e+00,  9.1211e-01,\n",
      "         3.9941e-01,  3.7671e-01,  1.7432e+00,  1.0098e+00,  5.7031e-01,\n",
      "         6.9238e-01,  3.4106e-01,  1.0410e+00,  6.0449e-01,  1.0957e+00,\n",
      "         5.2881e-01,  1.4727e+00,  8.2959e-01,  7.8955e-01,  7.5732e-01,\n",
      "        -4.5020e-01,  1.5732e+00,  8.7500e-01,  8.0029e-01,  9.8535e-01,\n",
      "         1.2396e-01, -7.3853e-02,  6.6040e-02, -1.5405e-01,  4.6558e-01,\n",
      "         7.5537e-01,  1.2274e-01,  3.5913e-01, -1.9226e-01,  1.5781e+00,\n",
      "         8.8574e-01,  9.3262e-01,  1.7773e-01,  1.2360e-01,  1.4658e+00,\n",
      "         7.4707e-01,  5.6006e-01,  6.3037e-01,  2.6025e-01,  7.6477e-02,\n",
      "         9.0479e-01,  4.8682e-01,  1.5117e+00,  7.7930e-01,  6.8311e-01,\n",
      "         4.0894e-01,  1.4375e+00,  5.6250e-01,  6.1230e-01,  3.3716e-01,\n",
      "         3.3960e-01, -5.6732e-02,  4.0503e-01,  6.9580e-01,  2.0996e-02,\n",
      "         5.8105e-01,  9.6069e-02,  3.8208e-01,  2.9102e-01,  5.4297e-01,\n",
      "         3.7327e-03,  1.4590e+00,  7.7832e-01,  8.8525e-01, -1.5015e-01,\n",
      "         7.6611e-01,  1.5002e-01,  3.2300e-01,  1.4580e+00,  5.8057e-01,\n",
      "         1.1847e-01,  2.7368e-01,  5.6152e-01,  4.2053e-02,  4.2285e-01,\n",
      "         5.5664e-01,  7.3242e-01,  7.2803e-01, -4.1699e-01,  1.5146e+00,\n",
      "         9.2822e-01,  1.1777e+00,  6.4697e-01, -4.0985e-02,  1.9202e-01,\n",
      "         2.0508e-01,  1.2671e-01,  3.2422e-01, -7.1045e-01,  1.2067e-01,\n",
      "         6.0889e-01, -1.3623e-01,  2.5415e-01, -1.8402e-02,  1.3877e+00,\n",
      "         1.1885e+00,  1.0146e+00,  1.0518e+00,  1.3542e-02,  1.4590e+00,\n",
      "         6.7529e-01,  9.8926e-01,  9.0137e-01, -2.7802e-02,  8.4375e-01,\n",
      "         2.1216e-01,  5.0635e-01,  1.6833e-01,  2.7026e-01, -7.7553e-03,\n",
      "         2.9736e-01, -1.6370e-01,  5.7715e-01, -2.0459e-01,  6.5234e-01,\n",
      "         5.5713e-01,  2.8931e-01,  9.0088e-01,  7.4463e-02, -2.0004e-02,\n",
      "         5.8044e-02,  6.8359e-01,  4.3506e-01,  3.9990e-01,  5.5322e-01,\n",
      "         6.9580e-01,  1.3291e+00,  9.9219e-01,  6.9141e-01,  3.6694e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 3.5669e-01, -1.0370e-01, -3.6597e-01, -3.8745e-01, -4.6240e-01,\n",
      "        -2.9224e-01, -7.8955e-01,  2.4246e-02, -2.9028e-01, -2.3962e-01,\n",
      "        -4.2267e-02,  3.0762e-01,  4.9097e-01, -1.7590e-01,  1.8909e-01,\n",
      "        -4.5459e-01,  2.3462e-01,  2.3364e-01,  2.6871e-02,  2.0227e-01,\n",
      "         3.2910e-01, -1.5698e-01,  3.3789e-01,  5.3711e-02,  2.1387e-01,\n",
      "         8.1360e-02,  2.3376e-01,  5.4150e-01,  2.7271e-01,  2.2852e-01,\n",
      "         1.5222e-01,  2.2070e-01,  4.7241e-01, -1.8628e-01,  1.1182e-01,\n",
      "        -9.8694e-02,  4.0210e-01, -2.1509e-01,  8.7097e-02,  2.0312e-01,\n",
      "        -4.1528e-01, -2.1912e-01, -2.9785e-01, -3.5229e-01, -8.4656e-02,\n",
      "         1.1340e-01,  1.2195e-01,  5.1465e-01,  3.4717e-01, -1.4758e-01,\n",
      "        -3.0688e-01,  1.9165e-02, -5.7281e-02,  7.9498e-03, -9.3231e-03,\n",
      "         9.2163e-02,  1.2952e-01, -5.7770e-02,  4.3774e-01,  1.1084e-01,\n",
      "        -5.6689e-01, -4.0356e-01, -1.2018e-01, -3.0225e-01,  3.0859e-01,\n",
      "         1.4246e-01,  3.9856e-02,  3.6597e-01,  4.7217e-01,  2.4731e-01,\n",
      "         2.9321e-01,  9.1614e-02,  3.2251e-01,  6.1572e-01,  2.9297e-01,\n",
      "         3.3252e-01, -6.2988e-01, -1.7004e-01,  2.3962e-01, -1.1273e-01,\n",
      "        -2.7344e-01, -2.3206e-01,  3.0127e-01, -5.4639e-01, -4.7266e-01,\n",
      "        -7.3633e-01, -3.0762e-01, -1.0883e-01,  2.3071e-01,  4.6959e-03,\n",
      "         8.9307e-01,  7.4951e-02,  2.6074e-01,  3.9551e-01,  2.9556e-02,\n",
      "        -4.2969e-01, -8.1836e-01, -1.1816e+00, -7.1960e-02,  1.0541e-01,\n",
      "        -2.0203e-01,  1.4429e-01, -3.7817e-01, -8.2080e-01, -3.0225e-01,\n",
      "         5.2673e-02, -2.2156e-01, -6.4746e-01, -5.2783e-01,  1.5100e-01,\n",
      "        -1.0488e+00, -3.5181e-01, -2.4500e-01, -2.8027e-01, -2.1313e-01,\n",
      "        -7.6904e-01, -3.0420e-01, -2.4402e-01, -1.8213e-01, -1.3586e-01,\n",
      "        -4.6606e-01,  1.8152e-01, -2.0850e-01,  4.3896e-01, -1.2286e-01,\n",
      "        -9.4543e-02, -1.0565e-01, -1.7847e-01, -2.0432e-02, -6.6101e-02,\n",
      "         9.1064e-02, -2.7420e-02,  5.1709e-01, -9.7595e-02, -1.9470e-01,\n",
      "         3.0054e-01, -3.3057e-01,  5.8350e-02, -3.5938e-01, -2.5659e-01,\n",
      "         2.8809e-02,  3.2886e-01,  2.2125e-02,  1.6736e-01,  1.0208e-02,\n",
      "         1.7624e-02, -8.1848e-02, -2.0483e-01, -6.7676e-01, -5.6250e-01,\n",
      "        -4.8486e-01,  3.8062e-01,  5.8708e-03,  3.5889e-01,  2.2363e-01,\n",
      "        -4.3091e-01, -3.0249e-01,  4.1504e-01, -5.1416e-01, -7.0435e-02,\n",
      "        -4.5807e-02, -9.8816e-02,  6.1914e-01, -3.2568e-01, -4.2969e-01,\n",
      "        -5.9891e-04, -2.6880e-01,  3.8940e-01, -5.2295e-01,  7.9163e-02,\n",
      "         5.2979e-01,  7.2205e-02, -3.9215e-02, -3.4619e-01,  1.9849e-01,\n",
      "        -6.7822e-01,  6.8848e-02,  5.6836e-01,  5.7739e-02, -3.6133e-01,\n",
      "         1.6577e-01, -1.8298e-01,  3.2300e-01,  3.2373e-01,  8.8745e-02,\n",
      "        -6.6260e-01, -2.5928e-01,  8.7524e-02, -3.6694e-01, -2.4185e-02,\n",
      "        -1.5137e-01,  7.8271e-01,  3.1860e-01,  8.1592e-01,  4.0332e-01,\n",
      "         5.4883e-01, -6.2207e-01,  2.6709e-01, -1.1188e-01,  6.0107e-01,\n",
      "        -5.9131e-01,  2.3766e-03, -1.0638e-01,  6.5918e-01, -3.1055e-01,\n",
      "        -5.5078e-01, -3.9404e-01,  1.2915e-01,  4.5312e-01,  1.0242e-01,\n",
      "         6.2256e-01, -1.2274e-01, -7.0361e-01,  1.2891e-01, -7.4768e-02,\n",
      "         5.2637e-01, -8.3594e-01,  2.3157e-01, -3.6792e-01, -3.3374e-01,\n",
      "         1.6272e-01,  4.4409e-01,  5.3662e-01,  6.2134e-02, -5.4980e-01,\n",
      "        -1.6553e-01, -9.8328e-02,  6.1182e-01,  1.1127e-01,  6.6895e-02,\n",
      "         4.0747e-01, -4.7021e-01, -5.7080e-01,  7.4524e-02, -2.3157e-01,\n",
      "         6.4014e-01, -1.2894e-02,  2.7124e-01, -5.6250e-01,  4.9243e-01,\n",
      "         5.9013e-03,  5.1367e-01,  1.9458e-01,  1.9922e-01, -3.7384e-02,\n",
      "         3.0884e-01,  5.7568e-01, -4.3164e-01,  3.4155e-01, -7.9834e-02,\n",
      "         4.8279e-02, -4.6045e-01, -5.6793e-02,  2.3605e-02, -2.0105e-01,\n",
      "         1.6296e-01,  7.0618e-02,  6.6711e-02, -5.3369e-01, -6.5125e-02,\n",
      "        -2.1732e-04, -1.5942e-01,  2.2583e-01, -4.7461e-01,  2.2656e-01,\n",
      "        -9.0576e-02, -3.5986e-01, -2.9297e-01, -1.2115e-01,  9.3140e-02,\n",
      "        -9.5032e-02, -6.8665e-02,  7.0007e-02, -9.4299e-02, -6.7505e-02,\n",
      "        -4.3848e-01, -3.6035e-01,  3.1067e-02,  9.7900e-02, -1.7078e-01,\n",
      "         1.8909e-01,  2.8882e-01,  2.0264e-01,  2.4438e-01,  3.6572e-01,\n",
      "         4.1797e-01,  1.6284e-01, -3.7445e-02,  3.2422e-01,  3.9941e-01,\n",
      "         9.2712e-02,  8.5205e-02, -6.8237e-02,  6.5576e-01,  3.7842e-01,\n",
      "         2.2095e-01,  2.6318e-01,  2.0813e-01, -4.2456e-01, -3.3545e-01,\n",
      "         2.9932e-01,  1.7651e-01,  2.0557e-01, -8.9172e-02, -6.4148e-02,\n",
      "        -9.5886e-02,  2.5488e-01, -3.9697e-01,  6.4819e-02,  2.2522e-01,\n",
      "         2.8271e-01,  2.5146e-01,  3.1592e-01,  2.4695e-01,  2.6581e-02,\n",
      "         5.0293e-02, -9.8083e-02,  2.4219e-01, -4.1772e-01,  1.4938e-02,\n",
      "         1.9641e-01,  2.9663e-01, -4.1528e-01,  3.7695e-01,  2.1643e-01,\n",
      "         5.1172e-01,  2.6660e-01,  3.3276e-01,  2.7002e-01, -5.8398e-01,\n",
      "         6.4659e-03, -1.1920e-01, -2.9370e-01,  1.5479e-01, -3.3325e-01,\n",
      "         1.4587e-01, -1.2769e-01,  1.5833e-01,  4.6436e-01, -6.7480e-01,\n",
      "        -1.6272e-01, -2.4011e-01,  1.9073e-02, -2.2375e-01,  7.1875e-01,\n",
      "         1.0925e-01,  7.9785e-01,  4.4434e-01,  5.6299e-01, -3.3838e-01,\n",
      "         2.9419e-01,  3.0200e-01,  1.5144e-02,  3.3356e-02, -6.8298e-02,\n",
      "         4.5972e-01,  1.9104e-02,  1.0938e-01,  3.4515e-02,  6.5674e-01,\n",
      "        -1.9608e-02,  5.8008e-01, -3.8477e-01,  6.3428e-01, -5.8136e-02,\n",
      "         2.2046e-01,  1.0931e-01, -7.9346e-01,  1.1658e-01, -6.1310e-02,\n",
      "         3.4790e-01, -6.2988e-01, -4.5923e-01, -8.1641e-01, -2.0789e-01,\n",
      "        -4.3652e-01, -6.5137e-01,  2.4329e-01,  5.5078e-01,  7.7698e-02,\n",
      "         6.8945e-01, -4.4525e-02, -7.8308e-02, -2.4133e-01, -2.6978e-01,\n",
      "         5.6824e-02,  4.1772e-01, -4.0186e-01,  2.5342e-01, -4.2725e-01,\n",
      "         2.7197e-01,  3.7140e-02,  5.0342e-01,  6.0059e-01,  2.1350e-01,\n",
      "        -5.3174e-01, -6.8359e-02,  4.3042e-01,  1.6235e-01,  4.9023e-01,\n",
      "         1.5137e-01,  4.8389e-01,  3.9795e-01, -1.7395e-01, -1.4563e-01,\n",
      "         4.1626e-01,  1.9360e-01,  1.2939e-01,  8.2129e-01,  4.4629e-01,\n",
      "         5.8411e-02,  4.3066e-01,  5.7037e-02,  5.8740e-01, -4.7394e-02,\n",
      "         8.1360e-02,  3.3521e-01, -1.7346e-01, -6.8018e-01,  3.1738e-01,\n",
      "        -2.8467e-01,  1.4648e-01,  2.3425e-01, -1.5417e-01, -9.9548e-02,\n",
      "         2.8003e-01,  5.6787e-01,  8.7158e-02,  7.3633e-01,  1.2598e-01,\n",
      "        -5.5908e-01, -7.8223e-01, -4.7516e-02, -3.6230e-01, -2.3157e-01,\n",
      "         1.5356e-01,  6.1084e-01,  2.8259e-02,  6.2646e-01,  9.1370e-02,\n",
      "        -5.4169e-02, -1.4368e-01, -7.1191e-01,  1.2671e-01,  1.2659e-01,\n",
      "         3.6523e-01,  4.3481e-01,  1.5039e-01,  4.9829e-01,  1.3171e-01,\n",
      "        -1.0260e-01,  2.6123e-01, -3.2422e-01,  2.6196e-01, -2.4490e-02,\n",
      "         1.4786e-02, -6.3770e-01, -1.7981e-01, -1.7773e-01,  9.2224e-02,\n",
      "        -1.7212e-01,  1.9116e-01,  2.9419e-01,  5.9570e-01, -2.4988e-01,\n",
      "         6.1230e-01, -7.9834e-02,  8.5266e-02,  4.6460e-01,  3.2251e-01,\n",
      "         2.2571e-01,  4.4165e-01,  1.6785e-01, -1.5625e-01,  1.4575e-01,\n",
      "        -1.1847e-01,  1.2317e-01, -3.6401e-01,  2.7368e-01, -3.6792e-01,\n",
      "         3.4741e-01,  4.1577e-01, -8.8135e-02,  6.5576e-01,  2.0642e-01,\n",
      "        -1.2457e-01,  2.2986e-01,  5.2246e-01,  5.7587e-02,  5.9473e-01,\n",
      "        -1.4148e-01, -3.8208e-01, -2.8760e-01, -2.7319e-01, -1.8408e-01,\n",
      "         1.5515e-01,  1.3818e-01,  3.7793e-01, -5.1221e-01, -9.3628e-02,\n",
      "        -3.1519e-01,  2.7039e-02, -4.5068e-01,  3.6450e-01, -3.3643e-01,\n",
      "         6.0254e-01, -4.8187e-02,  6.8994e-01, -3.2764e-01,  3.5962e-01,\n",
      "         1.3855e-01,  1.7188e-01,  5.9814e-01,  1.3342e-01, -1.3779e-02,\n",
      "         1.8896e-01, -8.1104e-01, -1.1664e-01,  1.2720e-01,  7.7979e-01,\n",
      "         7.7454e-02,  1.7468e-01,  5.7227e-01,  7.2754e-01,  3.9038e-01,\n",
      "         5.8789e-01,  1.5588e-01,  5.4004e-01,  5.8057e-01,  2.0337e-01,\n",
      "         6.6772e-02,  3.4058e-01, -5.8380e-02,  8.4534e-02, -7.8552e-02,\n",
      "         1.7896e-01,  7.1240e-01, -1.3831e-01,  1.9397e-01,  2.5659e-01,\n",
      "         1.1774e-01,  4.4067e-01,  5.4053e-01, -1.8774e-01, -1.0443e-01,\n",
      "         8.7793e-01,  9.6741e-02,  1.7383e-01,  5.9570e-01, -6.2164e-02,\n",
      "         2.7002e-01,  5.0391e-01,  7.7087e-02,  3.8239e-02, -1.2985e-02,\n",
      "         5.5908e-01,  4.1626e-01,  2.6050e-01,  7.9590e-01,  2.8174e-01,\n",
      "         3.1104e-01,  3.5864e-01,  5.5957e-01, -3.5614e-02,  3.3472e-01,\n",
      "         1.5796e-01,  3.8550e-01,  5.1611e-01,  3.7354e-02,  3.1250e-01,\n",
      "         1.7273e-01, -4.5258e-02,  1.1719e-02,  1.4355e-01, -5.7373e-01,\n",
      "        -5.4395e-01, -4.5166e-01, -1.6919e-01, -2.7661e-01, -6.4331e-02,\n",
      "         3.1082e-02,  1.8628e-01, -3.4204e-01,  3.5303e-01, -2.5024e-01,\n",
      "         8.8074e-02,  8.9746e-01,  7.4365e-01,  1.2024e-01,  7.9834e-01,\n",
      "        -2.4988e-01,  1.6342e-02,  7.0850e-01, -6.2744e-01, -1.0544e-02,\n",
      "        -5.6055e-01,  2.7466e-01, -1.4938e-02,  7.0654e-01,  1.3855e-02,\n",
      "         7.4854e-01,  2.6196e-01,  1.6296e-01,  1.7944e-01,  4.4604e-01,\n",
      "         8.8965e-01,  3.8867e-01,  1.3806e-01, -8.6133e-01, -3.9771e-01,\n",
      "        -7.6025e-01, -1.0381e+00,  9.1003e-02,  8.8501e-03,  4.7217e-01,\n",
      "         1.9943e-02, -2.2742e-01,  3.6835e-02, -2.9565e-01, -4.5020e-01,\n",
      "        -1.7615e-01,  6.7993e-02,  7.6721e-02, -3.1909e-01, -1.0312e+00,\n",
      "        -7.2168e-01, -3.8989e-01, -5.7178e-01,  1.3342e-01,  7.1680e-01,\n",
      "         1.0187e-01,  2.0825e-01, -3.8281e-01,  3.3936e-01,  3.4521e-01,\n",
      "        -6.7261e-02,  6.1621e-01,  5.6445e-01, -5.7587e-02,  7.1680e-01,\n",
      "         5.8167e-02,  6.9214e-02,  4.1943e-01,  5.9912e-01,  1.1139e-01,\n",
      "        -8.1970e-02,  2.9388e-02,  7.7019e-03, -4.2969e-01, -3.9917e-01,\n",
      "         3.9612e-02, -6.5979e-02,  3.0713e-01,  1.8958e-01,  1.7944e-01,\n",
      "        -1.5137e-01,  3.1787e-01,  1.6309e-01,  2.3950e-01,  5.2930e-01,\n",
      "        -2.5040e-02,  4.7339e-01, -9.0698e-02,  6.7480e-01, -1.3168e-02,\n",
      "         4.9011e-02,  3.8354e-01,  6.3477e-01, -1.0626e-01, -7.2693e-02,\n",
      "        -8.0859e-01, -1.3049e-01, -2.1655e-01, -4.0308e-01, -4.7314e-01,\n",
      "        -4.8535e-01,  3.1470e-01,  6.2061e-01, -6.0547e-01,  3.9282e-01,\n",
      "        -1.4514e-01,  2.8467e-01,  4.8511e-01, -1.4075e-01,  5.3125e-01,\n",
      "        -2.3010e-01, -1.7322e-01, -1.5210e-01, -3.7646e-01, -3.6499e-01,\n",
      "         2.4207e-01,  6.8896e-01,  3.0762e-01, -4.8877e-01, -9.7510e-01,\n",
      "        -1.3245e-01, -2.7002e-01,  3.9575e-01,  6.7969e-01,  5.6592e-01,\n",
      "        -1.1945e-01,  5.3662e-01, -5.7564e-03,  2.1576e-02, -2.3010e-01,\n",
      "         3.3838e-01,  4.9048e-01,  5.8447e-01,  2.3340e-01,  3.7939e-01,\n",
      "         4.7668e-02,  4.1779e-02,  1.1957e-01,  6.8262e-01,  3.0981e-01,\n",
      "         1.5526e-03,  1.9336e-01,  4.6240e-01,  6.2598e-01,  2.1179e-01,\n",
      "         3.1299e-01,  1.2244e-01,  6.4600e-01, -6.7505e-02,  3.9404e-01,\n",
      "         7.2803e-01,  1.3745e-01,  1.3196e-01,  5.2246e-01, -3.0273e-01,\n",
      "         1.5674e-01,  6.2305e-01,  1.0651e-01, -6.7627e-02,  3.2990e-02,\n",
      "         3.1082e-02,  5.0684e-01,  1.9348e-01,  2.4948e-02,  1.7761e-01,\n",
      "         6.0889e-01, -4.4263e-01,  3.0103e-01,  3.0859e-01,  4.7534e-01,\n",
      "         3.4668e-01, -3.4619e-01, -2.8467e-01, -8.8135e-01, -1.9739e-01,\n",
      "        -5.3760e-01,  5.9326e-01, -3.3374e-01,  4.6417e-02,  4.5380e-02,\n",
      "        -4.6539e-02, -9.7021e-01, -4.9707e-01, -7.3535e-01, -5.5078e-01,\n",
      "        -8.9478e-02, -6.6260e-01, -6.3477e-02, -2.7246e-01,  4.6069e-01,\n",
      "        -9.1370e-02,  6.0889e-01, -8.8574e-01, -2.0374e-01,  2.9883e-01,\n",
      "         4.3604e-01,  7.6416e-01, -2.0093e-01, -4.7900e-01, -9.6826e-01,\n",
      "        -6.8298e-02, -1.9678e-01,  7.6782e-02,  1.9012e-02,  4.7754e-01,\n",
      "        -4.7314e-01,  6.0498e-01, -6.6101e-02,  4.2529e-01, -8.9160e-01,\n",
      "         1.5396e-02,  3.1860e-01, -2.5928e-01, -3.1519e-01, -5.4443e-01,\n",
      "        -2.8198e-01,  2.1863e-01, -5.1514e-01, -5.1318e-01, -1.7517e-01,\n",
      "        -3.0322e-01,  5.8740e-01, -5.1788e-02,  6.1377e-01, -5.6348e-01,\n",
      "         7.7087e-02, -5.8887e-01, -5.1221e-01,  8.3191e-02,  6.1859e-02,\n",
      "        -7.9102e-02, -4.9414e-01, -1.4636e-01,  4.3793e-02, -3.3643e-01,\n",
      "        -4.0625e-01, -1.3708e-01, -3.7646e-01, -1.4099e-01,  8.4290e-02,\n",
      "         2.8247e-01,  7.4609e-01, -2.1790e-01, -4.7949e-01, -1.7468e-01,\n",
      "        -2.5317e-01, -6.7432e-01, -9.1003e-02, -4.2554e-01, -2.3230e-01,\n",
      "        -3.3521e-01, -5.9668e-01, -1.4722e-01, -4.3286e-01,  3.6987e-02,\n",
      "         2.2302e-01,  7.1484e-01,  1.8359e-01,  1.3000e-01,  6.4453e-02,\n",
      "         3.1494e-01,  8.4961e-01, -2.9175e-01, -1.4359e-02, -5.5939e-02,\n",
      "        -3.3765e-01, -7.4768e-02, -1.4893e-01,  5.8868e-02, -4.0558e-02,\n",
      "        -6.9458e-02, -5.9766e-01, -3.4546e-01, -3.6255e-01, -1.1676e-01,\n",
      "        -3.5083e-01, -7.5195e-01, -5.1416e-01, -7.9736e-01, -6.6455e-01,\n",
      "        -4.3066e-01, -1.2769e-01, -4.2847e-01,  4.3602e-03, -5.3418e-01,\n",
      "        -1.3306e-01, -2.2302e-01,  1.9238e-01,  3.0835e-01,  3.5986e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.03157894611358643 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00002: avg_val_f1 reached 0.03158 (best 0.12353), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_2.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(5.9951, device='cuda:0')\tavg_answer_loss:  tensor(5.4378, device='cuda:0')\tavg_type_loss:  tensor(0.1115, device='cuda:0')\tavg_val_f1:  0.03157894611358643\tavg_val_em:  0.0\tavg_val_prec:  0.01875\tavg_val_recall:  0.1\n",
      "sequence_output: tensor([[[ 1.1868e+00,  4.9570e-01,  7.9585e-02,  ...,  6.7020e-01,\n",
      "           1.2148e+00, -4.6993e-01],\n",
      "         [ 6.4094e-01,  9.1624e-02,  1.4776e-01,  ...,  2.8801e-01,\n",
      "           8.9195e-01, -2.0868e-01],\n",
      "         [ 1.0871e+00,  7.0238e-02,  4.7752e-02,  ...,  6.8136e-01,\n",
      "           7.5481e-01,  1.8409e-02],\n",
      "         ...,\n",
      "         [ 1.3288e-01,  5.4471e-02,  4.0295e-02,  ..., -5.3040e-02,\n",
      "           1.5816e-01, -5.3554e-02],\n",
      "         [ 1.8836e-04,  5.1411e-02, -1.9635e-02,  ..., -7.1479e-02,\n",
      "          -2.6287e-02, -9.1044e-02],\n",
      "         [ 8.9321e-02,  9.9620e-02, -1.1032e-01,  ...,  2.0535e-01,\n",
      "           1.2119e-01, -1.5005e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.6288e-01,  5.7398e-02, -2.0029e-01,  ...,  5.6444e-01,\n",
      "           1.5359e+00, -5.5062e-01],\n",
      "         [ 7.3769e-01,  6.0442e-01, -1.5397e-01,  ...,  1.3510e+00,\n",
      "           1.0335e+00, -3.5876e-01],\n",
      "         [ 1.1355e+00,  3.3258e-01, -3.1852e-01,  ...,  1.3696e+00,\n",
      "           9.9157e-01, -4.2696e-01],\n",
      "         ...,\n",
      "         [-5.1405e-03,  4.5840e-02,  8.0033e-04,  ..., -7.9928e-02,\n",
      "          -2.3443e-02, -7.5229e-02],\n",
      "         [-3.6496e-04,  4.0490e-02, -2.1041e-02,  ..., -8.4460e-02,\n",
      "          -3.4924e-02, -8.5801e-02],\n",
      "         [ 1.5910e-01,  2.2347e-01, -3.4995e-01,  ...,  2.7292e-01,\n",
      "           2.3601e-01, -5.5639e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2344,  0.3514,  0.0681,  ...,  1.6919,  1.3317, -0.2747],\n",
      "         [ 1.2378,  0.4550,  0.3785,  ...,  1.4066,  0.9875, -0.2661],\n",
      "         [ 1.0789, -0.0152,  0.1664,  ...,  1.1473,  0.9856, -0.1188],\n",
      "         ...,\n",
      "         [ 0.0047,  0.0959, -0.0512,  ..., -0.0762, -0.0726, -0.2351],\n",
      "         [ 0.1743,  0.3020, -0.0449,  ..., -0.0869,  0.1435, -0.1787],\n",
      "         [-0.0166,  0.0610, -0.0192,  ..., -0.0726, -0.0118, -0.0898]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.2875e-01,  7.0696e-01, -3.0643e-01,  ...,  1.2160e+00,\n",
      "           1.2638e+00, -7.8390e-01],\n",
      "         [ 7.0087e-01,  5.9479e-02, -1.3367e-01,  ...,  6.2862e-01,\n",
      "           5.4297e-01, -3.7888e-01],\n",
      "         [ 6.0071e-01,  8.3391e-01, -2.7402e-01,  ...,  5.7812e-01,\n",
      "           1.0444e+00, -6.3224e-01],\n",
      "         ...,\n",
      "         [ 1.3834e-03,  7.2210e-02, -3.7073e-02,  ..., -1.0172e-01,\n",
      "          -1.9500e-02, -1.0761e-01],\n",
      "         [-1.8454e-02,  6.6149e-02, -2.1269e-02,  ..., -5.9216e-02,\n",
      "          -3.3530e-02, -7.7849e-02],\n",
      "         [ 6.8161e-04,  4.3594e-02, -1.9093e-02,  ..., -8.5446e-02,\n",
      "          -1.6632e-02, -9.1823e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0859,  0.4110,  0.0106,  ...,  1.3433,  1.4842, -0.2865],\n",
      "         [ 0.7537,  0.5030,  0.1800,  ...,  1.0747,  0.7785,  0.4042],\n",
      "         [ 0.7037,  0.3814, -0.0090,  ...,  0.6803,  0.7746, -0.1992],\n",
      "         ...,\n",
      "         [ 0.0143,  0.0545, -0.0276,  ..., -0.0676, -0.0358, -0.0775],\n",
      "         [ 0.3256,  0.5480,  0.0289,  ..., -0.3339,  0.0431, -0.2392],\n",
      "         [ 0.0045,  0.0673, -0.1030,  ..., -0.0535, -0.0029, -0.1951]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1429,  0.5198, -0.1048,  ...,  1.6680,  1.3268, -0.3960],\n",
      "         [ 0.7862,  0.4791,  0.1477,  ...,  0.3778,  0.8728,  0.2323],\n",
      "         [ 1.0954,  0.3872,  0.0857,  ...,  0.3713,  1.0292, -0.1991],\n",
      "         ...,\n",
      "         [ 0.0984,  0.2523,  0.0644,  ..., -0.0683,  0.0042, -0.2609],\n",
      "         [-0.0100,  0.0540, -0.0067,  ..., -0.0831, -0.0209,  0.0358],\n",
      "         [-0.0038,  0.0787, -0.0228,  ..., -0.1184, -0.0286, -0.0932]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2784,  0.2732, -0.0515,  ...,  1.7374,  1.2377, -0.5273],\n",
      "         [ 0.3451,  0.3292,  0.0111,  ...,  0.8002,  1.0179,  0.2286],\n",
      "         [ 0.7547, -0.1342,  0.3709,  ...,  2.0916,  1.2505, -0.7235],\n",
      "         ...,\n",
      "         [-0.0690,  0.0646, -0.0196,  ..., -0.0396, -0.0164, -0.2036],\n",
      "         [ 0.0184,  0.0716, -0.0079,  ..., -0.1073, -0.0256, -0.0761],\n",
      "         [ 0.0439,  0.3475,  0.2154,  ...,  0.1382, -0.0167, -0.4791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3127,  0.3307, -0.2473,  ...,  0.9686,  0.7792, -0.5038],\n",
      "         [ 0.7854,  0.6538,  0.1661,  ...,  0.9232,  1.0370, -0.4127],\n",
      "         [ 1.2406,  0.0674,  0.2111,  ...,  0.3980,  1.2887, -0.5967],\n",
      "         ...,\n",
      "         [-0.0077,  0.0372,  0.0083,  ..., -0.1052, -0.0306, -0.0832],\n",
      "         [-0.0213,  0.0538, -0.0146,  ..., -0.0739, -0.0206, -0.0858],\n",
      "         [ 0.0101,  0.1869,  0.0163,  ..., -0.0699, -0.0553, -0.2728]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8408,  0.7002,  0.1612,  ...,  1.2523,  1.2350, -0.3424],\n",
      "         [ 0.5760,  0.1938,  0.1058,  ...,  1.4056,  0.6819, -0.3551],\n",
      "         [ 1.1916,  0.5560, -0.1531,  ...,  1.1999,  1.5502, -0.1902],\n",
      "         ...,\n",
      "         [-0.0900,  0.0786,  0.0746,  ..., -0.1702,  0.1654, -0.2024],\n",
      "         [ 0.0443,  0.5473,  0.0065,  ...,  0.0736,  0.1000, -0.1361],\n",
      "         [-0.1367,  0.1967, -0.3433,  ..., -0.3439,  0.3232, -0.7978]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 3.0704e-01,  5.0644e-01, -3.2776e-01,  ...,  1.4069e+00,\n",
      "           1.3714e+00, -4.6469e-01],\n",
      "         [ 6.9514e-01,  6.5024e-01,  4.1891e-01,  ...,  1.6205e+00,\n",
      "           1.0757e+00, -2.3203e-01],\n",
      "         [ 8.5773e-01,  6.0161e-01, -1.6683e-01,  ..., -6.6360e-02,\n",
      "           9.6790e-01, -4.9324e-01],\n",
      "         ...,\n",
      "         [-9.1750e-03,  5.7483e-02, -1.4937e-02,  ..., -7.7271e-02,\n",
      "          -3.1159e-02, -8.8684e-02],\n",
      "         [ 1.6360e-01,  1.8739e-01, -1.1063e-01,  ..., -1.8403e-01,\n",
      "          -1.0616e-01, -4.5019e-01],\n",
      "         [ 6.7381e-04,  5.8555e-02, -1.3617e-02,  ..., -7.2353e-02,\n",
      "          -2.7345e-02, -6.8126e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0206,  0.5055, -0.2304,  ...,  1.3963,  1.4805, -0.5961],\n",
      "         [ 0.9636,  0.8409,  0.2654,  ...,  1.0444,  1.6126,  0.3417],\n",
      "         [ 0.5555,  0.5147,  0.1850,  ...,  0.8837,  1.4522, -0.6204],\n",
      "         ...,\n",
      "         [ 0.0097,  0.0592, -0.0105,  ..., -0.0775, -0.0237, -0.0944],\n",
      "         [ 0.0261,  0.0619, -0.0257,  ..., -0.0822, -0.0298, -0.0928],\n",
      "         [ 0.0040,  0.0613, -0.0156,  ..., -0.0913, -0.0234, -0.0684]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2658,  0.1955, -0.1452,  ...,  1.5831,  0.9657, -0.0367],\n",
      "         [ 0.8372,  0.2273,  0.0222,  ...,  0.6601,  1.0589, -0.2822],\n",
      "         [ 1.1326,  0.0150, -0.1304,  ...,  1.4182,  0.9089, -0.1122],\n",
      "         ...,\n",
      "         [-0.0265,  0.1890, -0.1895,  ...,  0.5216,  0.1462, -0.0289],\n",
      "         [-0.0159,  0.0616, -0.0188,  ..., -0.0761, -0.0275, -0.0874],\n",
      "         [ 0.0327,  0.4946,  0.1349,  ...,  0.0446,  0.0765, -0.2368]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8754,  0.4800, -0.0380,  ...,  1.7514,  1.2882, -0.4421],\n",
      "         [ 1.0262,  0.5921,  0.1377,  ...,  1.4175,  0.4922, -0.5926],\n",
      "         [ 0.4963,  0.5016,  0.1197,  ...,  0.2062,  0.5757, -0.1295],\n",
      "         ...,\n",
      "         [-0.0095,  0.0535, -0.0256,  ..., -0.0805, -0.0230, -0.0754],\n",
      "         [-0.0081,  0.0437, -0.0172,  ..., -0.0926, -0.0351, -0.0900],\n",
      "         [-0.0124,  0.0478, -0.0204,  ..., -0.0830, -0.0145, -0.0991]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8377,  0.1765, -0.0223,  ...,  1.5973,  1.1771, -0.3555],\n",
      "         [ 0.9794,  0.1266,  0.2066,  ...,  0.7944,  0.6852, -0.4634],\n",
      "         [ 1.2659,  0.1908, -0.1840,  ...,  0.6595,  1.1093, -0.6719],\n",
      "         ...,\n",
      "         [-0.0034,  0.0572, -0.0167,  ..., -0.0396, -0.0287, -0.0803],\n",
      "         [ 0.0073,  0.0832, -0.0577,  ..., -0.1298, -0.0471, -0.5065],\n",
      "         [-0.0101,  0.0544, -0.0245,  ..., -0.0902, -0.0345, -0.0933]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.4257,  0.3366,  0.1575,  ...,  1.6149,  0.9699, -0.2660],\n",
      "         [ 1.0922,  0.5609,  0.3606,  ...,  1.1643,  0.7674,  0.3677],\n",
      "         [ 1.0917,  0.1174,  0.0239,  ...,  0.6283,  1.0448, -0.4585],\n",
      "         ...,\n",
      "         [-0.0117,  0.0864, -0.0298,  ..., -0.0817, -0.0406, -0.0927],\n",
      "         [-0.0069,  0.0753, -0.0271,  ..., -0.0875, -0.0487, -0.0101],\n",
      "         [ 0.0144,  0.0544, -0.0216,  ..., -0.0830, -0.0242, -0.0834]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1454,  0.3744,  0.0901,  ...,  1.4280,  1.2924, -0.1776],\n",
      "         [ 0.3359,  0.6584, -0.0526,  ...,  0.9405,  0.8025, -0.1396],\n",
      "         [ 1.3412,  0.5917, -0.0221,  ...,  0.7162,  1.0674, -0.2394],\n",
      "         ...,\n",
      "         [ 0.3189,  0.3983, -0.0196,  ..., -0.2337,  0.0823, -0.3397],\n",
      "         [-0.0134,  0.0462, -0.0195,  ..., -0.0805, -0.0289, -0.1108],\n",
      "         [ 0.0081,  0.1742,  0.0317,  ..., -0.0345, -0.0043, -0.2281]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9664,  0.6398, -0.2985,  ...,  1.1941,  1.4529,  0.0484],\n",
      "         [ 0.4986,  0.4653,  0.0230,  ...,  0.2143,  0.8239, -0.1638],\n",
      "         [ 0.3058,  0.0418,  0.1450,  ...,  1.2356,  0.9400, -0.3326],\n",
      "         ...,\n",
      "         [-0.0108,  0.0560, -0.0208,  ..., -0.0733, -0.0236, -0.0892],\n",
      "         [-0.0210,  0.0565, -0.0172,  ..., -0.0815, -0.0304, -0.0923],\n",
      "         [-0.0073,  0.0516, -0.0185,  ..., -0.0908, -0.0315, -0.0804]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1851,  0.2015, -0.1487,  ...,  1.1878,  1.2215, -0.4339],\n",
      "         [ 1.3742,  0.3032,  0.1451,  ...,  0.5511,  0.6597, -0.4785],\n",
      "         [ 0.3710,  0.3173, -0.1122,  ...,  0.6646,  1.2662, -0.2350],\n",
      "         ...,\n",
      "         [ 0.0060,  0.0643, -0.0202,  ..., -0.0829, -0.0253, -0.0821],\n",
      "         [ 0.0626,  0.2350,  0.0433,  ...,  0.1596,  0.0080, -0.1003],\n",
      "         [ 0.0260,  0.0387, -0.0351,  ..., -0.0826, -0.0257, -0.0809]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.1051e-01,  3.5532e-01, -9.0539e-02,  ...,  1.2931e+00,\n",
      "           1.1317e+00, -3.3221e-01],\n",
      "         [ 5.1791e-01,  5.2001e-02,  2.2551e-01,  ...,  1.7493e+00,\n",
      "           1.4631e-01, -1.0080e-01],\n",
      "         [-1.6351e-02,  1.7726e-01, -7.9736e-02,  ...,  1.1107e+00,\n",
      "           6.6371e-01, -6.4660e-01],\n",
      "         ...,\n",
      "         [-5.7768e-02,  3.3121e-01, -4.7618e-02,  ...,  4.7832e-01,\n",
      "          -3.3666e-03, -2.5398e-01],\n",
      "         [ 1.8549e-02,  5.3676e-02, -1.5988e-02,  ..., -8.1698e-02,\n",
      "          -1.3990e-02, -8.9088e-02],\n",
      "         [-1.5251e-02,  5.2780e-02,  1.4517e-03,  ..., -8.2355e-02,\n",
      "          -2.0668e-02, -8.1304e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9269,  0.5355, -0.1893,  ...,  1.4043,  0.9874, -0.1094],\n",
      "         [ 0.4309,  0.9126, -0.1237,  ...,  1.1116,  0.3242,  0.1678],\n",
      "         [ 0.9777,  0.6662, -0.2342,  ...,  0.7988,  1.2403, -0.5485],\n",
      "         ...,\n",
      "         [ 0.0113,  0.0538, -0.0203,  ..., -0.0767, -0.0224, -0.0860],\n",
      "         [-0.0097,  0.0514, -0.0359,  ..., -0.0835, -0.0330, -0.0834],\n",
      "         [-0.0154,  0.0738, -0.0340,  ..., -0.0803, -0.0172, -0.0951]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3622, -0.0284, -0.0713,  ...,  1.1954,  1.2864, -0.4010],\n",
      "         [ 0.9377,  0.4623,  0.3169,  ...,  1.0496,  0.9646, -0.5419],\n",
      "         [ 1.1733,  0.0374, -0.0712,  ...,  1.0815,  0.9367, -0.0555],\n",
      "         ...,\n",
      "         [ 0.1248,  0.0200, -0.1297,  ..., -0.1228, -0.0182, -0.2423],\n",
      "         [-0.0052,  0.0654, -0.0170,  ..., -0.0638, -0.0390, -0.0802],\n",
      "         [ 0.0038,  0.0468, -0.0158,  ..., -0.0752, -0.0306, -0.0971]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0842,  0.3960,  0.0029,  ...,  1.1174,  1.0339, -0.1279],\n",
      "         [ 0.8984,  0.3473,  0.3210,  ...,  1.1315,  0.9415,  0.3165],\n",
      "         [ 0.8191,  0.3388, -0.0251,  ...,  1.0400,  0.4350,  0.0354],\n",
      "         ...,\n",
      "         [-0.0105,  0.0498, -0.0233,  ..., -0.0814, -0.0302, -0.0744],\n",
      "         [-0.0117,  0.0576, -0.0074,  ..., -0.1142, -0.0236, -0.0781],\n",
      "         [-0.0127,  0.0845,  0.0049,  ..., -0.0808, -0.0209, -0.0797]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0314e+00,  2.2913e-01, -3.5266e-01,  ...,  1.4961e+00,\n",
      "           1.5948e+00, -4.2230e-01],\n",
      "         [ 6.4595e-01,  2.3152e-01,  2.3014e-01,  ...,  1.0168e+00,\n",
      "           3.9492e-01, -1.5388e-01],\n",
      "         [ 7.1233e-01,  4.6479e-01, -3.4113e-01,  ...,  9.4396e-01,\n",
      "           1.1485e+00,  2.7474e-02],\n",
      "         ...,\n",
      "         [-9.9281e-03,  5.7768e-02, -2.4584e-02,  ..., -7.2487e-02,\n",
      "          -1.0923e-02, -7.8705e-02],\n",
      "         [-1.4318e-02,  4.9291e-02, -3.1769e-02,  ..., -6.9144e-02,\n",
      "          -3.7182e-02, -8.2095e-02],\n",
      "         [ 1.4148e-04,  4.1470e-02, -2.9342e-02,  ..., -7.8853e-02,\n",
      "          -3.1317e-02, -7.9151e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.4934,  0.4627, -0.2544,  ...,  1.6836,  1.7642, -0.1241],\n",
      "         [ 1.0210,  0.5844,  0.3061,  ...,  1.7931,  0.7807, -0.4295],\n",
      "         [ 0.8356,  0.5976,  0.2031,  ...,  0.9347,  1.1002,  0.0413],\n",
      "         ...,\n",
      "         [ 0.0070,  0.0436, -0.0099,  ..., -0.0694, -0.0233, -0.0853],\n",
      "         [-0.0262,  0.1591,  0.0300,  ..., -0.0863, -0.0737, -0.2364],\n",
      "         [-0.0145,  0.0612, -0.0190,  ..., -0.0900, -0.0249, -0.1023]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7493,  0.1612, -0.1108,  ...,  1.5140,  1.1622, -0.1606],\n",
      "         [ 1.0569,  0.3974,  0.2515,  ...,  1.4785,  1.2116, -0.2593],\n",
      "         [ 0.9294,  0.1812, -0.1979,  ...,  0.1902,  1.1303, -0.4427],\n",
      "         ...,\n",
      "         [-0.0057,  0.0533, -0.0257,  ..., -0.0760, -0.0244, -0.0829],\n",
      "         [ 0.0035,  0.0633, -0.0054,  ..., -0.1123, -0.0156, -0.0588],\n",
      "         [ 0.0177,  0.0476, -0.0204,  ..., -0.0756, -0.0264, -0.0844]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.5021e-01,  3.9719e-01,  7.3493e-02,  ...,  1.7992e+00,\n",
      "           6.9801e-01, -2.6169e-01],\n",
      "         [ 4.6051e-01,  1.2414e-01,  3.2901e-01,  ...,  1.4628e+00,\n",
      "           6.9874e-01, -1.0413e-01],\n",
      "         [ 7.9533e-01,  2.9819e-01,  1.9166e-01,  ...,  1.1373e+00,\n",
      "           5.4221e-01, -1.2973e-01],\n",
      "         ...,\n",
      "         [-1.8308e-02,  6.9456e-02, -2.4124e-02,  ..., -7.9185e-02,\n",
      "          -3.4117e-02, -8.7498e-02],\n",
      "         [ 1.6136e-01,  3.6945e-01, -2.9434e-01,  ...,  2.1302e-01,\n",
      "           7.5992e-02, -9.8588e-02],\n",
      "         [-1.7416e-03,  5.6772e-02, -1.9342e-02,  ..., -8.1674e-02,\n",
      "          -1.8433e-02, -7.2314e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0753,  0.1094,  0.3738,  ...,  1.8732,  1.0328, -0.3776],\n",
      "         [ 1.3849,  0.3359,  0.1405,  ...,  1.1076,  0.9953,  0.4025],\n",
      "         [ 0.4917,  0.5699, -0.4873,  ...,  0.1817,  0.7902, -0.3023],\n",
      "         ...,\n",
      "         [-0.0047,  0.0521, -0.0033,  ..., -0.0878, -0.0154, -0.0915],\n",
      "         [-0.0035,  0.0794, -0.0067,  ..., -0.0761, -0.0250, -0.0837],\n",
      "         [-0.0049,  0.0503, -0.0172,  ..., -0.0815, -0.0237, -0.0844]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.5011,  0.3869, -0.1173,  ...,  1.3975,  1.5728,  0.0375],\n",
      "         [ 1.0125, -0.0243,  0.1793,  ...,  0.6259,  0.8760, -0.0444],\n",
      "         [ 1.1692,  0.0579,  0.0300,  ...,  1.2935,  0.6015,  0.0141],\n",
      "         ...,\n",
      "         [ 0.0521,  0.2957, -0.0181,  ...,  0.3364,  0.2763, -0.2419],\n",
      "         [ 0.0124,  0.0492, -0.0233,  ..., -0.0740, -0.0117, -0.0857],\n",
      "         [-0.0061,  0.0379, -0.0103,  ..., -0.0860, -0.0233, -0.0822]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.8886e-01,  2.4726e-01, -8.6373e-02,  ...,  1.0414e+00,\n",
      "           1.2970e+00,  6.1743e-02],\n",
      "         [ 7.0801e-01,  4.1622e-01,  1.5184e-01,  ...,  1.4703e+00,\n",
      "           6.9172e-01,  8.1964e-02],\n",
      "         [ 6.2441e-01,  1.3426e-01,  6.7032e-02,  ...,  1.6814e+00,\n",
      "           7.1599e-01, -4.4222e-01],\n",
      "         ...,\n",
      "         [-1.3622e-03,  4.2313e-02, -1.1264e-02,  ..., -8.4754e-02,\n",
      "          -2.5398e-02, -8.8046e-02],\n",
      "         [-9.2638e-03,  3.4221e-02, -1.3604e-02,  ..., -1.1399e-01,\n",
      "          -2.1054e-02, -1.0651e-01],\n",
      "         [ 1.1816e-02,  1.5093e-01, -2.3848e-02,  ..., -1.3289e-01,\n",
      "           2.6135e-02, -3.0796e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 9.8717e-01,  2.4922e-01, -1.6340e-01,  ...,  1.5135e+00,\n",
      "           1.3886e+00, -3.5810e-02],\n",
      "         [ 1.0534e+00,  4.3445e-01,  3.9653e-01,  ...,  1.2566e+00,\n",
      "           6.0253e-01,  7.5191e-02],\n",
      "         [ 1.2849e+00,  5.0203e-01, -4.9230e-02,  ...,  4.0723e-01,\n",
      "           3.1242e-01, -4.3933e-01],\n",
      "         ...,\n",
      "         [ 6.2008e-03,  5.4043e-02, -8.0971e-03,  ..., -7.7356e-02,\n",
      "          -1.5894e-02,  2.5575e-02],\n",
      "         [ 7.3451e-03,  5.1119e-02, -2.3316e-02,  ..., -8.0715e-02,\n",
      "          -9.4525e-03, -7.7721e-02],\n",
      "         [-1.9579e-02,  1.2221e-01, -2.1757e-02,  ..., -9.8781e-04,\n",
      "          -2.0826e-02, -2.1723e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0631e+00,  1.5249e-01, -5.8582e-02,  ...,  1.1890e+00,\n",
      "           2.0660e-01,  4.2265e-02],\n",
      "         [ 1.1427e+00, -1.2060e-01,  5.0587e-01,  ...,  8.3988e-01,\n",
      "           8.4444e-01,  4.0884e-01],\n",
      "         [ 1.1260e+00,  3.8527e-01,  1.1013e-01,  ...,  3.8572e-01,\n",
      "           1.2656e+00, -4.8290e-01],\n",
      "         ...,\n",
      "         [-1.1102e-02,  5.7788e-02, -3.0765e-02,  ..., -8.2329e-02,\n",
      "          -2.2909e-02, -6.2887e-02],\n",
      "         [-4.7185e-04,  5.5571e-02, -2.1850e-02,  ..., -1.1548e-01,\n",
      "          -2.6709e-02, -8.4346e-02],\n",
      "         [-1.3649e-02,  5.2225e-02, -1.2243e-02,  ..., -8.2568e-02,\n",
      "          -2.9088e-02,  2.1338e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2096e+00,  7.1184e-01, -3.9602e-04,  ...,  7.4284e-01,\n",
      "           1.5489e+00, -2.1744e-01],\n",
      "         [ 1.1197e-01,  4.0994e-01, -2.6457e-01,  ...,  5.0289e-01,\n",
      "           4.4083e-01,  3.0178e-01],\n",
      "         [ 5.0261e-01,  2.8223e-01, -8.3079e-02,  ...,  6.9424e-01,\n",
      "           8.4927e-01, -5.9329e-01],\n",
      "         ...,\n",
      "         [ 8.9852e-05,  5.2432e-02, -2.4781e-02,  ..., -1.0563e-01,\n",
      "          -1.5843e-02, -8.5092e-02],\n",
      "         [-6.6066e-03,  5.4821e-02, -1.7679e-02,  ..., -8.0540e-02,\n",
      "          -1.3971e-02,  2.1194e-02],\n",
      "         [ 3.7986e-02,  7.3694e-02, -5.6999e-02,  ..., -6.7042e-02,\n",
      "          -1.7003e-02, -1.5702e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1678e+00,  3.0407e-01, -4.0742e-02,  ...,  1.7182e+00,\n",
      "           1.2661e+00,  1.0316e-01],\n",
      "         [ 1.7261e-01,  4.4639e-01,  2.2348e-01,  ...,  1.1269e+00,\n",
      "           3.3183e-01,  1.5438e-01],\n",
      "         [ 1.0852e+00, -1.8386e-01,  1.8360e-01,  ...,  1.2962e+00,\n",
      "           1.1969e+00,  8.2708e-03],\n",
      "         ...,\n",
      "         [-8.5544e-03,  5.7266e-01,  1.1373e-01,  ..., -3.7964e-01,\n",
      "          -1.5390e-01, -2.8094e-01],\n",
      "         [-1.0133e-04,  1.9571e-01,  1.9711e-02,  ..., -9.2360e-02,\n",
      "          -6.4066e-02, -2.3691e-01],\n",
      "         [-1.1635e-02,  6.8337e-02, -3.1484e-02,  ..., -1.0704e-01,\n",
      "          -3.2018e-02, -9.5106e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 2.7821e-01,  4.1537e-01,  1.6610e-01,  ...,  1.9932e+00,\n",
      "           9.8284e-01, -1.4753e-01],\n",
      "         [ 1.8385e-01,  6.7231e-01, -7.3245e-02,  ...,  9.6949e-01,\n",
      "           4.9026e-01, -3.0655e-01],\n",
      "         [ 1.0553e+00,  2.5225e-01,  2.3147e-03,  ...,  1.2416e+00,\n",
      "           8.4722e-01, -2.9058e-01],\n",
      "         ...,\n",
      "         [ 1.5795e-02,  5.3585e-02, -2.4897e-02,  ..., -1.0212e-01,\n",
      "          -2.6191e-02, -1.0317e-01],\n",
      "         [-8.5756e-04,  6.7485e-02, -1.5990e-02,  ..., -9.6934e-02,\n",
      "          -2.0893e-02,  2.9625e-02],\n",
      "         [ 3.1227e-02, -1.5249e-02,  1.1124e-01,  ..., -7.5447e-02,\n",
      "          -9.5079e-02, -1.9375e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0257,  0.5121,  0.0754,  ...,  1.5076,  0.7193,  0.0713],\n",
      "         [ 0.5782,  0.3025,  0.4687,  ...,  0.7524,  0.6765, -0.1085],\n",
      "         [ 0.8009,  0.2645, -0.1781,  ...,  1.1660,  1.1813, -0.2404],\n",
      "         ...,\n",
      "         [-0.0018,  0.0445, -0.0127,  ..., -0.0860, -0.0190, -0.0638],\n",
      "         [ 0.0082,  0.0825, -0.0046,  ..., -0.0103, -0.0043, -0.2212],\n",
      "         [-0.0129,  0.0606, -0.0100,  ..., -0.0786, -0.0342, -0.0817]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6569,  0.2340, -0.1697,  ...,  1.5411,  0.9230, -0.3566],\n",
      "         [ 0.6370,  0.3076,  0.0549,  ...,  1.2155,  1.0226,  0.2692],\n",
      "         [ 0.6761,  0.2116, -0.1076,  ..., -0.2284,  1.0061, -0.2695],\n",
      "         ...,\n",
      "         [ 0.0021,  0.0572, -0.0168,  ..., -0.0820, -0.0290, -0.0855],\n",
      "         [-0.0166,  0.0564, -0.0326,  ..., -0.0810, -0.0348, -0.0797],\n",
      "         [-0.0102,  0.0576, -0.0210,  ..., -0.0660, -0.0244, -0.0959]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2266,  0.6010,  0.1694,  ...,  2.0643,  0.7618, -0.2034],\n",
      "         [ 0.6831,  0.4915, -0.2404,  ...,  1.4097,  1.0389, -0.1845],\n",
      "         [ 0.8567,  0.3339, -0.2623,  ...,  1.2424,  0.8195, -0.0880],\n",
      "         ...,\n",
      "         [-0.0129,  0.0618, -0.0238,  ..., -0.0827, -0.0301, -0.1037],\n",
      "         [ 0.0801,  0.0859, -0.0474,  ...,  0.0496, -0.0401, -0.1761],\n",
      "         [-0.0116,  0.0603, -0.0145,  ..., -0.0854, -0.0207, -0.0765]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0606,  0.4922, -0.0091,  ...,  1.3801,  1.6208,  0.4896],\n",
      "         [ 1.2144,  0.1976,  0.0433,  ...,  1.7808,  0.9126,  0.5053],\n",
      "         [ 0.9078,  0.4484,  0.0371,  ...,  1.3613,  1.0612, -0.1151],\n",
      "         ...,\n",
      "         [-0.0159,  0.0569, -0.0125,  ..., -0.0678, -0.0121, -0.0935],\n",
      "         [-0.0062,  0.0489, -0.0258,  ..., -0.0790, -0.0313, -0.0758],\n",
      "         [-0.0139,  0.0405, -0.0089,  ..., -0.0708, -0.0198, -0.0779]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9812,  0.4214, -0.0814,  ...,  1.4457,  1.0093, -0.5451],\n",
      "         [ 0.4033,  0.3762,  0.3228,  ...,  1.3236,  0.9121,  0.0653],\n",
      "         [ 1.0665, -0.1181,  0.2412,  ...,  1.4110,  0.6663, -0.2697],\n",
      "         ...,\n",
      "         [-0.0157,  0.0465, -0.0208,  ..., -0.0715, -0.0312, -0.0916],\n",
      "         [ 0.0060,  0.0566, -0.0129,  ..., -0.0888, -0.0255, -0.0933],\n",
      "         [-0.0086,  0.0604, -0.0163,  ..., -0.0703, -0.0240, -0.0929]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0914e+00,  5.4047e-01, -1.3868e-01,  ...,  1.6043e+00,\n",
      "           1.1923e+00, -1.5665e-01],\n",
      "         [ 1.4332e+00,  3.8224e-01, -9.1362e-02,  ...,  1.8151e+00,\n",
      "           4.7204e-01,  3.8396e-01],\n",
      "         [ 9.6748e-01,  1.4250e-01,  3.4557e-02,  ...,  1.0377e+00,\n",
      "           9.0647e-01, -1.5253e-01],\n",
      "         ...,\n",
      "         [ 5.2224e-03,  5.6653e-02, -2.1541e-02,  ..., -8.1932e-02,\n",
      "          -2.0699e-02, -8.4466e-02],\n",
      "         [-1.0542e-03,  4.0670e-02, -1.6395e-02,  ..., -8.9170e-02,\n",
      "          -2.2832e-02, -6.9120e-02],\n",
      "         [ 6.0990e-02,  8.9308e-02, -3.2610e-02,  ..., -1.0878e-01,\n",
      "          -1.2376e-02, -2.0972e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.7083,  0.3606, -0.0650,  ...,  0.6629,  1.5342,  0.5451],\n",
      "         [ 1.2190,  0.1853,  0.3749,  ...,  1.9281,  1.0654,  0.1016],\n",
      "         [ 1.1518,  0.0083,  0.0332,  ...,  1.5171,  0.8468,  0.2052],\n",
      "         ...,\n",
      "         [-0.0082,  0.0398, -0.0443,  ..., -0.0869, -0.0340,  0.0196],\n",
      "         [-0.0189,  0.0735, -0.0337,  ..., -0.0236, -0.0146, -0.1987],\n",
      "         [-0.0131,  0.0590, -0.0201,  ..., -0.0702, -0.0287, -0.0930]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1083,  0.3541,  0.0353,  ...,  1.6441,  1.3913, -0.3121],\n",
      "         [ 0.4986,  0.6940,  0.1609,  ...,  1.0389,  0.7141,  0.2294],\n",
      "         [ 0.8072,  0.5063, -0.2674,  ...,  1.5571,  1.4451, -0.0468],\n",
      "         ...,\n",
      "         [ 0.2319,  0.1636, -0.0356,  ..., -0.2294,  0.0260, -0.1886],\n",
      "         [ 0.0496,  0.0504, -0.0522,  ..., -0.0697, -0.0259, -0.1519],\n",
      "         [ 0.0931,  0.0662, -0.0984,  ..., -0.0165, -0.0079, -0.2368]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0765e+00,  4.9172e-01,  5.2044e-02,  ...,  1.4066e+00,\n",
      "           1.3888e+00, -5.1054e-02],\n",
      "         [ 7.8169e-01,  6.8628e-01,  1.9838e-01,  ...,  1.2197e+00,\n",
      "           9.1965e-01, -2.0230e-01],\n",
      "         [ 1.0295e+00,  3.9597e-01,  2.2952e-01,  ...,  4.7252e-01,\n",
      "           9.4404e-01, -1.2820e-01],\n",
      "         ...,\n",
      "         [ 6.6421e-02,  4.0160e-01,  7.4251e-02,  ..., -2.2722e-01,\n",
      "          -9.5400e-03, -2.5673e-01],\n",
      "         [ 1.3732e-03,  5.2854e-02, -1.2211e-02,  ..., -7.7272e-02,\n",
      "          -1.8403e-02, -8.5972e-02],\n",
      "         [-1.7344e-02,  6.3332e-02, -1.0210e-02,  ..., -7.5922e-02,\n",
      "          -2.5682e-02, -8.3243e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.3456,  0.4891, -0.3323,  ...,  1.2964,  0.8572,  0.3018],\n",
      "         [ 0.5111, -0.0180, -0.2247,  ...,  0.9455,  0.6756,  0.2066],\n",
      "         [ 1.2287, -0.0131, -0.0171,  ...,  1.6474,  1.0231, -0.2014],\n",
      "         ...,\n",
      "         [-0.1836,  0.0522, -0.3413,  ...,  0.6379,  0.1319, -0.4818],\n",
      "         [-0.0149,  0.0863, -0.0356,  ..., -0.1163, -0.0436, -0.0775],\n",
      "         [-0.0159,  0.0531, -0.0319,  ..., -0.0818, -0.0251, -0.0850]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2040e+00,  5.4509e-01,  1.6370e-01,  ...,  2.1298e+00,\n",
      "           1.2945e+00, -4.1140e-01],\n",
      "         [ 8.4556e-01,  5.2167e-01,  1.2619e-01,  ...,  1.4774e+00,\n",
      "           7.7910e-01, -6.2670e-02],\n",
      "         [ 4.1606e-01,  3.4943e-01,  5.2325e-02,  ...,  1.1651e+00,\n",
      "           1.1105e+00, -4.0156e-01],\n",
      "         ...,\n",
      "         [ 1.3040e-01,  2.7075e-01, -8.5620e-02,  ..., -1.5817e-02,\n",
      "          -3.5004e-02, -1.4099e-03],\n",
      "         [-1.7365e-02,  5.6405e-02, -1.9926e-02,  ..., -7.9853e-02,\n",
      "          -3.0075e-02, -8.5366e-02],\n",
      "         [ 3.0591e-01,  2.6121e-01, -2.3055e-01,  ..., -7.6792e-02,\n",
      "           1.3397e-01, -1.8651e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.4087,  0.4166, -0.2050,  ...,  1.6767,  1.3233, -0.8170],\n",
      "         [ 0.8190,  0.1948,  0.0157,  ...,  1.5150,  1.3609, -0.2475],\n",
      "         [ 1.2366,  0.7589, -0.0770,  ...,  0.9682,  1.2374,  0.4332],\n",
      "         ...,\n",
      "         [-0.0111,  0.0621,  0.0045,  ..., -0.0790, -0.0319, -0.0802],\n",
      "         [-0.0020,  0.0513, -0.0167,  ..., -0.0683,  0.0255, -0.1715],\n",
      "         [ 0.0063,  0.0266, -0.0148,  ..., -0.1034, -0.0047, -0.0646]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3829,  0.5938, -0.2454,  ...,  1.8268,  1.2159, -0.2335],\n",
      "         [ 0.0163,  0.6330,  0.0683,  ...,  1.1658,  0.3922, -0.3157],\n",
      "         [ 0.8995,  0.4240,  0.0795,  ...,  1.8180,  1.1940, -0.5656],\n",
      "         ...,\n",
      "         [-0.0141,  0.0580, -0.0306,  ..., -0.0697, -0.0314, -0.0750],\n",
      "         [-0.0104,  0.0883, -0.0302,  ..., -0.0835, -0.0252, -0.0877],\n",
      "         [-0.0406,  0.2368, -0.1284,  ..., -0.3105, -0.0519, -0.2448]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9763,  0.4978, -0.1070,  ...,  2.2275,  1.3059, -0.0724],\n",
      "         [ 0.4641,  0.5205,  0.1176,  ...,  1.1447,  0.6920, -0.1298],\n",
      "         [ 0.7743,  0.5232,  0.0395,  ...,  1.6384,  0.9934, -0.4347],\n",
      "         ...,\n",
      "         [-0.0248,  0.1422,  0.0135,  ..., -0.0636, -0.0826, -0.2140],\n",
      "         [-0.0134,  0.0543,  0.0119,  ..., -0.0722, -0.0229, -0.0868],\n",
      "         [-0.0342,  0.2365, -0.0726,  ..., -0.1632, -0.1321, -0.0314]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3217,  0.2755,  0.3222,  ...,  1.7178,  1.3341,  0.1581],\n",
      "         [ 0.4890,  0.4441, -0.0838,  ...,  1.6067,  1.1128,  0.2482],\n",
      "         [ 0.9005,  0.2249, -0.5013,  ...,  0.4352,  0.9375, -0.1576],\n",
      "         ...,\n",
      "         [ 0.0096,  0.0475, -0.0224,  ..., -0.0743, -0.0251, -0.0905],\n",
      "         [ 0.0228,  0.6567,  0.0784,  ..., -0.3731,  0.0279, -0.0074],\n",
      "         [ 0.0093,  0.2457, -0.0037,  ..., -0.0213,  0.0464, -0.3256]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2004,  0.2839,  0.0484,  ...,  2.2792,  1.2206,  0.0545],\n",
      "         [ 0.2432,  0.5255,  0.1959,  ...,  1.4910,  0.7560,  0.1296],\n",
      "         [ 0.5303,  0.7121,  0.3872,  ...,  0.8035,  0.4190,  0.0214],\n",
      "         ...,\n",
      "         [ 0.1846,  0.0946,  0.0483,  ...,  0.1881,  0.0400, -0.6380],\n",
      "         [-0.0137,  0.0577, -0.0294,  ..., -0.0808, -0.0328,  0.0200],\n",
      "         [ 0.4448,  0.7524, -0.1109,  ...,  0.3330,  0.7691, -0.3212]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0816e+00,  1.6700e-01,  4.8658e-02,  ...,  2.0760e+00,\n",
      "           1.5351e+00, -4.9691e-01],\n",
      "         [ 5.2985e-01,  3.5448e-01, -4.9728e-01,  ...,  1.2821e+00,\n",
      "           1.1349e+00, -3.4845e-01],\n",
      "         [ 9.7108e-01,  4.1379e-01,  1.4518e-01,  ...,  9.0413e-01,\n",
      "           1.6359e+00, -5.4431e-01],\n",
      "         ...,\n",
      "         [-1.8492e-03,  3.9559e-02, -2.9755e-02,  ..., -8.0662e-02,\n",
      "          -2.6500e-02,  1.3535e-02],\n",
      "         [ 5.0817e-03,  5.7214e-02, -2.2074e-02,  ..., -6.1503e-02,\n",
      "          -1.8703e-02, -8.0538e-02],\n",
      "         [ 1.0450e-02,  7.2606e-02, -4.0136e-02,  ..., -8.7330e-02,\n",
      "          -1.7735e-02, -1.4185e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1173,  0.0595,  0.1395,  ...,  1.7980,  1.5267, -0.2706],\n",
      "         [ 0.5358,  0.7498,  0.3169,  ...,  0.5254,  1.0895, -0.1913],\n",
      "         [ 1.0184,  0.1932,  0.2018,  ...,  1.5945,  1.2845, -0.3643],\n",
      "         ...,\n",
      "         [-0.0620,  0.1040, -0.0390,  ..., -0.0361, -0.0824, -0.0320],\n",
      "         [-0.0161,  0.0481, -0.0239,  ..., -0.0748, -0.0246, -0.0794],\n",
      "         [-0.0156,  0.0837, -0.0215,  ..., -0.0731, -0.0190, -0.0651]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3939,  0.3369,  0.0459,  ...,  0.2747,  1.0882, -0.2398],\n",
      "         [ 0.2051,  0.4022, -0.0690,  ...,  1.3085,  0.3791,  0.1381],\n",
      "         [ 0.6686,  0.2601, -0.1071,  ...,  1.6863,  0.9592, -0.0309],\n",
      "         ...,\n",
      "         [ 0.0069,  0.1559,  0.0203,  ..., -0.0352, -0.0247, -0.2247],\n",
      "         [ 0.1694,  0.1104, -0.0627,  ..., -0.0392,  0.1629, -0.0093],\n",
      "         [-0.0070,  0.0651, -0.0194,  ..., -0.0760, -0.0448, -0.0942]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.3891e-01,  5.1561e-01, -1.5316e-03,  ...,  9.1523e-01,\n",
      "           1.1422e+00, -1.6294e-01],\n",
      "         [ 1.4467e-01,  9.8537e-01,  7.7052e-02,  ...,  3.1457e-01,\n",
      "           8.2666e-01, -2.1555e-02],\n",
      "         [ 7.7594e-01,  4.9353e-01,  2.3235e-01,  ...,  2.1901e+00,\n",
      "           8.5135e-01,  8.6039e-02],\n",
      "         ...,\n",
      "         [-2.0006e-01, -6.2488e-02, -2.2293e-03,  ...,  2.3796e-01,\n",
      "           3.3404e-02, -7.0329e-01],\n",
      "         [-6.2128e-02,  1.6609e-01, -5.5624e-03,  ..., -7.2475e-02,\n",
      "          -8.8331e-02, -2.1961e-01],\n",
      "         [-5.7191e-03,  5.8529e-02, -2.5209e-02,  ..., -7.7151e-02,\n",
      "          -2.7710e-02, -9.4668e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0113e+00,  6.2208e-01,  8.2360e-02,  ...,  1.7497e+00,\n",
      "           1.3050e+00, -3.6355e-01],\n",
      "         [ 1.0096e+00,  2.1687e-01,  1.6751e-02,  ...,  1.2991e+00,\n",
      "           7.0273e-01, -2.0785e-02],\n",
      "         [ 1.3436e+00,  2.9954e-01, -1.4649e-01,  ...,  1.4816e+00,\n",
      "           1.1271e+00, -3.5198e-01],\n",
      "         ...,\n",
      "         [-5.8377e-02,  7.9307e-02, -1.6092e-02,  ..., -2.7147e-02,\n",
      "          -5.2010e-02, -2.2025e-01],\n",
      "         [-1.2171e-02,  5.4782e-02, -2.5213e-02,  ..., -8.4692e-02,\n",
      "          -1.8070e-02, -9.0092e-02],\n",
      "         [ 4.6304e-05,  4.7708e-02, -2.7732e-02,  ..., -8.8787e-02,\n",
      "          -3.4978e-02, -8.9576e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2219,  0.2281, -0.3937,  ...,  1.7494,  1.3130, -0.4794],\n",
      "         [ 0.3016,  0.4997,  0.0719,  ...,  1.7689,  0.4055, -0.5276],\n",
      "         [ 0.7679,  0.3610,  0.2431,  ...,  0.1458,  0.4411,  0.1421],\n",
      "         ...,\n",
      "         [-0.0151,  0.1700,  0.0631,  ..., -0.1051, -0.0591, -0.2214],\n",
      "         [ 0.0070,  0.1277,  0.0425,  ..., -0.0625, -0.0454, -0.1904],\n",
      "         [-0.0055,  0.0534, -0.0174,  ..., -0.0859, -0.0301, -0.0853]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0342e+00,  3.9939e-01, -7.8099e-02,  ...,  1.0966e+00,\n",
      "           1.0614e+00, -1.3700e-01],\n",
      "         [ 9.5820e-01,  7.1993e-01,  2.6233e-01,  ...,  1.3679e+00,\n",
      "           7.8311e-01,  5.9258e-01],\n",
      "         [ 4.5508e-01,  5.1990e-01, -1.4810e-01,  ...,  8.9696e-01,\n",
      "           9.5789e-01,  3.2978e-02],\n",
      "         ...,\n",
      "         [-7.4902e-03,  6.1963e-02, -2.1043e-02,  ..., -1.1190e-01,\n",
      "          -2.7152e-02, -8.1568e-02],\n",
      "         [-1.9245e-02,  6.2449e-02,  1.0705e-03,  ..., -8.8334e-02,\n",
      "          -2.3431e-02, -6.6668e-02],\n",
      "         [-2.3014e-02,  4.0484e-01, -3.0051e-02,  ..., -2.5790e-01,\n",
      "           1.0496e-01, -4.7477e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 8.2926e-01,  7.4103e-01,  1.8816e-01,  ...,  1.4459e+00,\n",
      "           1.3012e+00, -3.1154e-01],\n",
      "         [ 3.2491e-01,  1.1779e-01, -2.0099e-01,  ...,  1.7008e-01,\n",
      "           7.7813e-01, -1.7590e-01],\n",
      "         [ 6.0970e-01,  5.3081e-01,  1.4870e-01,  ..., -6.1973e-01,\n",
      "           7.6085e-01, -1.7144e-01],\n",
      "         ...,\n",
      "         [-1.0647e-02,  9.8659e-02, -2.2898e-02,  ..., -7.4642e-02,\n",
      "          -2.9777e-02, -9.7918e-02],\n",
      "         [-3.2750e-03,  6.4952e-02,  3.0417e-03,  ..., -7.2610e-02,\n",
      "          -1.8893e-02, -8.6864e-02],\n",
      "         [-7.6614e-04,  8.9456e-02, -3.1121e-02,  ..., -8.9495e-02,\n",
      "          -4.7856e-02,  1.4601e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3815,  0.4566,  0.1409,  ...,  0.9363,  1.0735, -0.2050],\n",
      "         [ 0.3887,  0.4466, -0.5126,  ...,  0.8019,  0.8750,  0.0832],\n",
      "         [ 0.0847, -0.4033, -0.3966,  ...,  0.4918,  1.0235,  0.3799],\n",
      "         ...,\n",
      "         [-0.0169,  0.0643, -0.0255,  ..., -0.0691, -0.0383, -0.0932],\n",
      "         [-0.0129,  0.1703, -0.4084,  ...,  0.0522,  0.1737, -0.2656],\n",
      "         [ 0.5680,  0.6352, -0.3302,  ...,  0.3069,  0.2738, -0.1685]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.6095e-01,  5.4300e-01,  4.5166e-02,  ...,  1.5694e+00,\n",
      "           1.3586e+00, -5.9145e-02],\n",
      "         [ 8.3211e-01,  3.8522e-01,  2.5500e-01,  ...,  2.0846e+00,\n",
      "           8.0270e-01, -1.0372e-01],\n",
      "         [ 1.8306e-01, -1.0686e-01,  2.8772e-01,  ...,  1.4140e+00,\n",
      "           5.3020e-01,  1.1368e-01],\n",
      "         ...,\n",
      "         [-6.6874e-03,  5.7956e-02, -2.4193e-02,  ..., -6.9722e-02,\n",
      "          -2.1637e-02, -1.1713e-01],\n",
      "         [-1.2661e-03,  4.1412e-02, -1.8717e-02,  ..., -7.6608e-02,\n",
      "          -2.8845e-02, -8.2241e-02],\n",
      "         [-2.5316e-02,  1.6413e-01, -6.3007e-03,  ..., -3.7053e-02,\n",
      "          -1.6326e-02, -2.2566e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0797e+00,  2.3035e-01, -8.3345e-02,  ...,  1.0026e+00,\n",
      "           1.5387e+00, -2.4602e-01],\n",
      "         [ 2.8110e-01, -2.7629e-01,  4.8425e-01,  ...,  1.6561e+00,\n",
      "           8.9203e-01,  1.2266e-01],\n",
      "         [-6.3970e-02,  1.6899e-01, -8.6363e-02,  ...,  1.0223e+00,\n",
      "           1.3893e+00, -2.5880e-01],\n",
      "         ...,\n",
      "         [-1.6399e-03,  4.1529e-02, -1.3369e-02,  ..., -8.8984e-02,\n",
      "          -1.5745e-02, -1.0447e-01],\n",
      "         [ 1.6938e-03,  5.2791e-02, -2.8177e-02,  ..., -1.0881e-01,\n",
      "          -3.0493e-02, -8.7400e-02],\n",
      "         [ 2.1818e-01,  7.0077e-02, -1.4442e-02,  ..., -2.7497e-01,\n",
      "           3.3217e-01, -1.1895e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.6236,  0.6921, -0.0654,  ...,  1.7661,  1.4781, -0.4381],\n",
      "         [ 0.8330,  0.5292,  0.0363,  ...,  1.9615,  0.8088, -0.5192],\n",
      "         [ 0.8689,  0.5514, -0.2560,  ...,  1.3108,  1.1022, -0.3276],\n",
      "         ...,\n",
      "         [-0.0030,  0.0589, -0.0161,  ..., -0.0655, -0.0228, -0.0726],\n",
      "         [-0.0152,  0.0546, -0.0325,  ..., -0.0836, -0.0288, -0.0962],\n",
      "         [ 0.0124,  0.0571, -0.0261,  ..., -0.0839, -0.0321, -0.0904]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2729,  0.1881, -0.3646,  ...,  1.4291,  1.6712, -0.5659],\n",
      "         [ 0.8750,  0.2176,  0.3594,  ...,  2.1592,  1.2000,  0.0818],\n",
      "         [ 0.6833, -0.3965, -0.2145,  ...,  0.5627,  0.2745, -0.4037],\n",
      "         ...,\n",
      "         [-0.0118,  0.0548, -0.0275,  ..., -0.0773, -0.0297, -0.0711],\n",
      "         [ 0.0189,  0.0959, -0.0101,  ...,  0.1061, -0.0955, -0.1480],\n",
      "         [-0.2021,  0.4173, -0.3961,  ...,  0.3783,  0.6071, -0.2086]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5865,  0.1415,  0.2325,  ...,  1.3093,  1.2777, -0.3100],\n",
      "         [ 1.1729,  0.4074, -0.3660,  ...,  1.4868,  0.7750, -0.3100],\n",
      "         [ 0.5597,  0.2623, -0.1006,  ...,  1.2148,  0.9153, -0.1398],\n",
      "         ...,\n",
      "         [ 0.2586,  0.0543, -0.0381,  ...,  0.6854,  0.4138, -0.2611],\n",
      "         [-0.0107,  0.0527, -0.0241,  ..., -0.0741, -0.0380, -0.0611],\n",
      "         [-0.0087,  0.0641, -0.0304,  ..., -0.0843, -0.0332,  0.0326]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0751,  0.4638, -0.0629,  ...,  1.6996,  1.4000, -0.5430],\n",
      "         [ 0.2304,  0.9391, -0.0089,  ...,  1.2287,  0.9650, -0.7028],\n",
      "         [ 1.1673,  0.5330, -0.1321,  ...,  1.4689,  0.8788, -1.1833],\n",
      "         ...,\n",
      "         [ 0.0699,  0.2550,  0.2263,  ..., -0.2845,  0.1834, -0.3297],\n",
      "         [-0.0066,  0.0577, -0.0386,  ..., -0.0740, -0.0413,  0.0056],\n",
      "         [-0.0752,  0.1069, -0.0271,  ...,  0.0326, -0.0424, -0.3418]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2962,  0.0586,  0.1095,  ...,  1.7354,  1.0915,  0.2612],\n",
      "         [ 0.1750,  0.1714,  0.1360,  ...,  0.7422,  1.3273, -0.2218],\n",
      "         [ 0.9878,  0.2504, -0.4434,  ...,  1.1250,  0.9064,  0.0367],\n",
      "         ...,\n",
      "         [ 0.2163,  0.2107, -0.0756,  ...,  0.1252,  0.0448, -0.2877],\n",
      "         [ 0.5911, -0.4832, -0.0088,  ..., -0.1725,  0.1439, -0.1786],\n",
      "         [ 0.1587,  0.3389, -0.0843,  ...,  0.1029, -0.1112, -0.3134]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2441,  0.5928, -0.2248,  ...,  1.7061,  0.8879, -0.5836],\n",
      "         [ 0.7651,  0.8374, -0.0362,  ...,  1.0791,  0.6683, -0.1668],\n",
      "         [ 0.0480,  0.5598,  0.1624,  ...,  0.2630,  0.7360, -0.3638],\n",
      "         ...,\n",
      "         [ 0.0487,  0.0459, -0.0356,  ..., -0.0100, -0.0168, -0.1812],\n",
      "         [-0.2005,  0.4051, -0.0740,  ..., -0.3997,  0.0285, -0.1489],\n",
      "         [-0.0094,  0.0799, -0.0246,  ..., -0.0876, -0.0233, -0.0765]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3178,  0.4453,  0.0418,  ...,  1.8901,  1.5671,  0.2421],\n",
      "         [ 0.7573, -0.0381,  0.4033,  ...,  2.3100,  0.7675,  0.7246],\n",
      "         [-0.1692,  0.3315,  0.4224,  ...,  1.7216,  1.2678, -0.1714],\n",
      "         ...,\n",
      "         [ 0.1647,  0.3126, -0.2799,  ...,  0.0608,  0.0804, -0.4998],\n",
      "         [-0.0115,  0.0422, -0.0187,  ..., -0.0766, -0.0346, -0.0837],\n",
      "         [-0.0236,  0.0764, -0.0311,  ..., -0.0572,  0.0153, -0.1991]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9555,  0.4632, -0.3987,  ...,  1.7401,  1.1424, -0.0091],\n",
      "         [ 1.2325,  0.0215,  0.2221,  ...,  1.9964,  0.9019,  0.1325],\n",
      "         [ 0.8115,  0.7834, -0.3464,  ...,  2.1741,  0.9606, -0.6903],\n",
      "         ...,\n",
      "         [-0.0321,  0.0729, -0.0327,  ..., -0.0285,  0.0150, -0.1889],\n",
      "         [-0.0244,  0.0886, -0.0444,  ..., -0.0450, -0.0231, -0.2434],\n",
      "         [-0.0045,  0.4074, -0.2717,  ...,  0.2126, -0.0118, -0.0827]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0049,  0.5048,  0.1385,  ...,  1.0959,  1.2743, -0.3631],\n",
      "         [ 1.0608,  0.3762, -0.1972,  ...,  1.6930,  0.3248, -0.1678],\n",
      "         [ 0.9433, -0.0900,  0.0693,  ...,  1.5392,  0.5343, -0.3799],\n",
      "         ...,\n",
      "         [-0.0168,  0.0538, -0.0139,  ..., -0.0754, -0.0283, -0.0872],\n",
      "         [ 0.0105,  0.0654, -0.0253,  ..., -0.0830, -0.0352, -0.1044],\n",
      "         [ 0.2373,  0.4825, -0.0389,  ...,  0.2871,  0.0692, -0.4771]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7521,  0.2749, -0.3072,  ...,  1.0308,  1.0950,  0.2027],\n",
      "         [ 0.2914,  0.6691, -0.1997,  ...,  0.6127,  1.1295, -0.2901],\n",
      "         [ 0.4681, -0.3634,  0.1409,  ...,  0.8709,  0.7632, -0.2565],\n",
      "         ...,\n",
      "         [ 0.0055,  0.1775, -0.0220,  ..., -0.0422, -0.1083, -0.2459],\n",
      "         [-0.0153,  0.0521, -0.0207,  ..., -0.0745, -0.0235,  0.0237],\n",
      "         [ 0.0288,  0.3296, -0.2334,  ..., -0.2485, -0.3303, -0.2246]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7794,  0.1918,  0.1158,  ...,  1.5967,  1.1678, -0.4388],\n",
      "         [ 0.5608,  0.6175,  0.7135,  ...,  1.2582,  0.7199,  0.1233],\n",
      "         [ 0.7068,  0.5431,  0.0840,  ...,  0.7812,  1.1112, -0.3835],\n",
      "         ...,\n",
      "         [ 0.0221,  0.0687, -0.0442,  ..., -0.0865, -0.0245, -0.1238],\n",
      "         [-0.0046,  0.0841, -0.0233,  ..., -0.0807, -0.0245, -0.0853],\n",
      "         [-0.0310,  0.1500,  0.0275,  ..., -0.0650, -0.0779, -0.2462]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.7731,  0.3791,  0.0278,  ...,  1.1276,  0.8636,  0.0998],\n",
      "         [-0.1818,  0.2199,  0.0085,  ...,  0.4808,  0.8408, -0.1649],\n",
      "         [-0.2278,  0.2555,  0.0141,  ..., -0.0200,  0.5282, -0.4443],\n",
      "         ...,\n",
      "         [-0.0053,  0.0519, -0.0157,  ..., -0.0785, -0.0272, -0.0857],\n",
      "         [-0.0069,  0.0562, -0.0194,  ..., -0.1160, -0.0221, -0.0939],\n",
      "         [ 0.1490,  0.5587, -0.0872,  ...,  0.0960,  0.3735, -0.4025]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4321,  0.3393,  0.3552,  ...,  0.4507,  0.7372, -0.4435],\n",
      "         [ 0.7348,  0.3019, -0.1143,  ...,  0.7614,  0.7831,  0.4917],\n",
      "         [ 0.8751, -0.0229,  0.0957,  ..., -0.0161,  0.6894, -0.2128],\n",
      "         ...,\n",
      "         [-0.0090,  0.0725, -0.0246,  ..., -0.0936, -0.0295, -0.0912],\n",
      "         [ 0.0091,  0.0876, -0.0269,  ..., -0.0726, -0.0303, -0.0875],\n",
      "         [ 0.3317,  0.4407, -0.0763,  ..., -0.3111,  0.1555, -0.3071]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6155,  0.1413, -0.1632,  ...,  1.4749,  1.5908, -0.1149],\n",
      "         [ 0.1298, -0.2570, -0.1076,  ...,  1.0912,  0.9073, -0.1676],\n",
      "         [ 1.4074,  0.5033, -0.1874,  ...,  0.8120,  1.6109, -0.1463],\n",
      "         ...,\n",
      "         [-0.0186,  0.0584, -0.0229,  ..., -0.0797, -0.0357, -0.0837],\n",
      "         [-0.0020,  0.0618, -0.0226,  ..., -0.1060, -0.0246, -0.0926],\n",
      "         [-0.0062,  0.0514, -0.0102,  ..., -0.0871, -0.0175, -0.0819]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.6914e-01,  2.6203e-01,  2.7941e-01,  ...,  1.3296e+00,\n",
      "           1.3473e+00, -2.1579e-01],\n",
      "         [ 1.3281e-01,  4.2147e-01,  1.3648e-03,  ...,  1.9458e+00,\n",
      "           8.0732e-01, -6.0873e-03],\n",
      "         [ 9.9659e-01,  4.3841e-01, -1.3270e-01,  ...,  1.6674e+00,\n",
      "           1.2442e+00, -4.4863e-01],\n",
      "         ...,\n",
      "         [-1.3988e-02,  5.7924e-02, -2.5571e-02,  ..., -9.9302e-02,\n",
      "          -4.2781e-02, -8.1905e-02],\n",
      "         [ 3.0486e-02,  2.5065e-01, -1.3031e-02,  ..., -1.9796e-01,\n",
      "          -2.6407e-02, -5.1451e-02],\n",
      "         [-1.1640e-02,  5.6563e-02, -3.0204e-02,  ..., -1.0645e-01,\n",
      "          -3.3736e-02,  1.2196e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3226,  0.4699, -0.3915,  ..., -0.0117,  1.5867, -0.3009],\n",
      "         [ 0.6782,  0.4291,  0.5760,  ...,  1.3457,  0.7825, -0.2492],\n",
      "         [ 0.7331, -0.0433, -0.1617,  ...,  0.8038,  1.2448, -0.3026],\n",
      "         ...,\n",
      "         [-0.0465,  0.0393,  0.0386,  ..., -0.0536, -0.0611, -0.2278],\n",
      "         [-0.1569,  0.4609, -0.1464,  ..., -0.2127,  0.1301, -0.0296],\n",
      "         [-0.0052,  0.0567,  0.0041,  ..., -0.0703, -0.0146, -0.0736]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1694,  0.1872,  0.1019,  ...,  1.8712,  1.1554, -0.3371],\n",
      "         [ 0.1432,  0.8554,  0.2391,  ...,  0.9987,  0.4830,  0.1029],\n",
      "         [ 1.0283,  0.4383,  0.3198,  ...,  1.2720,  0.6506, -0.0338],\n",
      "         ...,\n",
      "         [-0.0063,  0.0549, -0.0140,  ..., -0.0740, -0.0277, -0.0819],\n",
      "         [-0.0109,  0.0869, -0.0372,  ..., -0.0930, -0.0192, -0.0871],\n",
      "         [-0.0178,  0.0604, -0.0261,  ..., -0.0701, -0.0298, -0.0999]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7275,  0.2833, -0.2268,  ...,  1.8783,  1.3486,  0.0101],\n",
      "         [ 0.1687,  0.1958,  0.4099,  ...,  0.9003,  0.3868, -0.2128],\n",
      "         [ 0.7294, -0.2810, -0.3921,  ...,  0.7494,  0.9784, -0.1775],\n",
      "         ...,\n",
      "         [-0.0118,  0.0603, -0.0282,  ..., -0.1064, -0.0126, -0.0742],\n",
      "         [ 0.0293, -0.0998, -0.1420,  ...,  0.0539,  0.2243, -0.5846],\n",
      "         [-0.0404,  0.0995, -0.0519,  ..., -0.0410, -0.0135, -0.2164]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.2998,  0.3719, -0.0041,  ...,  0.9287,  1.0190, -0.2611],\n",
      "         [ 0.5588,  0.2135,  0.1862,  ...,  0.4907,  0.5855,  0.0763],\n",
      "         [ 0.4793,  0.3192, -0.3609,  ...,  0.2784,  0.6963, -0.0987],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.5547, 1.0176, 0.8618,  ..., 1.5459, 0.7661, 0.4158], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.8232, 0.1429, 0.8174,  ..., 0.5498, 1.3643, 0.5566], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.9567,  0.1627, -0.0792,  ...,  1.2870,  1.0010, -0.1217],\n",
      "         [ 0.5421,  0.2741,  0.1765,  ...,  1.0291,  0.0187, -0.0512],\n",
      "         [ 0.6788, -0.4572,  0.0279,  ...,  0.0015,  1.2331,  0.0751],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.6963,  1.0771,  0.7881,  1.1230, -0.3518,  0.0522, -0.4082, -0.0810,\n",
      "        -0.3274,  0.5645,  2.1211,  0.7842,  1.9072,  0.7651,  0.0226,  0.5669,\n",
      "         0.9346,  0.2242,  0.6245,  0.4028,  2.4512,  0.7354,  1.5449,  0.2742,\n",
      "         0.5034,  2.3613,  0.9014, -0.1693, -0.0495, -0.6221, -0.0865,  0.8706,\n",
      "        -0.5273, -0.0941,  1.2920,  0.5137,  1.0742,  0.4631,  0.4441,  0.1512,\n",
      "         0.3542,  1.0420,  0.6748,  1.1318,  0.7339,  0.5122, -0.3992,  0.6499,\n",
      "         0.5151,  0.3943,  0.6782, -0.4534,  0.4556, -0.3220, -0.0252,  1.5918,\n",
      "         0.3989,  1.3682,  0.7861,  0.4954, -0.3057,  0.7251,  0.3013,  0.9136,\n",
      "         0.4841,  1.4258,  0.6631,  0.4185,  0.5356,  0.3250,  0.3762,  2.4824,\n",
      "         1.0098,  1.4229,  0.4263,  0.1052,  0.3542,  0.4080,  2.2773,  0.9673,\n",
      "         1.3350,  0.5010,  0.7490,  0.1738,  0.4736,  0.5811,  0.2659, -0.1805,\n",
      "         0.8564,  0.7603,  0.4006,  0.3416,  0.3972,  0.0199,  1.1377, -0.4622,\n",
      "         0.2861,  0.1692,  0.6665,  1.9658,  0.8652,  0.8345,  1.5967,  0.4233,\n",
      "         0.9175,  0.5176,  0.3711,  0.0379,  1.0820,  0.9980,  0.6475,  0.0775,\n",
      "        -0.4829, -0.0396, -0.2173,  1.5088,  0.7236,  1.5293,  0.8662,  0.3730,\n",
      "        -0.0208,  0.3137,  0.3176, -0.1448,  1.1875, -0.3430,  1.4229,  0.7080,\n",
      "         1.7646,  0.4646, -0.1229,  0.6309,  1.1191, -0.2534,  0.3606, -0.0236,\n",
      "         0.2549,  0.8726,  0.4622,  0.2219,  1.7529,  0.8081,  0.3384,  0.9814,\n",
      "         0.6396,  1.3672,  0.6567,  0.5269,  1.1162,  2.1953,  0.9644,  1.4893,\n",
      "         0.2117, -0.2969,  1.2969,  0.5117,  1.3008,  1.1279, -0.1520,  0.0070,\n",
      "        -0.2148,  1.8281,  0.8276, -0.1653,  1.6367,  0.4636, -0.2194,  0.5557,\n",
      "         2.2930,  0.9268,  0.8589,  1.8008,  0.5967, -0.1923,  0.7510,  0.5332,\n",
      "         0.7944, -0.2203,  1.3574,  0.6519,  0.0998,  2.0801,  0.9883,  0.1742,\n",
      "         0.5845,  0.3699,  2.7109,  0.9390,  1.7207,  0.8428,  2.4199,  0.8960,\n",
      "         1.6484,  0.6846,  0.7573,  1.6055,  0.6978,  0.7510,  0.6904,  0.0659,\n",
      "        -0.6553, -0.1395,  1.6201,  0.7471,  1.8711,  0.7671, -0.2852,  1.9531,\n",
      "         1.8115,  0.4177,  0.9692, -0.3440,  0.3130,  0.4812,  0.5864,  1.0537,\n",
      "         0.6885,  0.9697,  0.8735, -0.1252,  0.9062,  1.7090,  1.8379,  0.3323,\n",
      "         0.9238,  0.7808, -0.0667,  0.0971,  0.3970,  0.2925,  1.5195,  0.8843,\n",
      "         1.4883,  0.5635,  1.0967,  0.4944,  1.2783,  0.5479,  0.4585,  1.4277,\n",
      "         0.4954,  1.7871,  0.4216,  0.8882,  0.3318, -0.0884,  2.1035,  0.7080,\n",
      "         1.5615,  0.2659,  0.3625,  0.3689,  0.6245,  1.4854,  0.9072,  1.3223,\n",
      "        -0.1716,  1.9756,  1.7783,  0.2905,  0.9072,  0.5449,  0.5957,  1.6465,\n",
      "         0.4175,  0.8267,  0.7002,  1.4873,  1.2256,  0.8276,  0.1106,  1.8652,\n",
      "         0.7803,  1.4893,  0.6030,  1.2812,  1.3389,  0.4812,  0.6807,  0.0939,\n",
      "         1.5664,  0.4277,  1.4033,  0.7598,  0.4016,  0.2438, -0.0447,  0.1322,\n",
      "         1.6016,  0.1023, -0.4236,  1.4775,  0.5342,  0.9551, -0.1098,  1.0420,\n",
      "         0.6445,  0.3320, -0.2181,  1.6611,  0.5181, -0.1549, -0.0361,  1.5244,\n",
      "         1.0469, -0.2542, -0.0741,  0.8232, -0.1295,  0.9575, -0.1488,  0.0914,\n",
      "         0.4473, -0.1171,  0.4243,  0.3740,  1.9756,  0.7056,  1.4443,  0.2700,\n",
      "         0.7939,  0.2563,  0.6646,  0.6230,  0.5059, -0.4546,  0.4482, -0.6860,\n",
      "         2.1582,  0.4119,  1.2812,  0.8115,  2.5508,  1.1816,  0.4141,  0.1725,\n",
      "         0.2469,  1.6855,  0.7261,  1.6455,  0.3074,  1.1113,  0.6646,  1.3750,\n",
      "         0.8047,  0.7744,  0.3850, -0.1659,  1.3281,  2.0645,  0.6265,  0.3362,\n",
      "         0.3879,  1.2334,  0.7021,  1.2822,  0.8442, -0.4868,  0.6660,  0.3652,\n",
      "        -0.4436,  0.4363,  1.2178, -0.3142,  0.7729, -0.1949,  0.9014,  1.1387,\n",
      "         0.6978, -0.3560,  0.2703, -0.3455,  0.3562,  1.1113,  0.5435,  0.5806,\n",
      "         0.8613, -0.1720,  0.2561,  1.0361, -0.3035,  0.6841, -0.2051,  0.8945,\n",
      "        -0.2627,  0.8442, -0.2769,  0.1522, -0.6953,  0.4963,  0.0566, -0.5137,\n",
      "         0.3613,  0.1465, -0.3989,  1.0059, -0.0184,  0.3806,  0.8418,  0.2593,\n",
      "        -0.1992,  0.7075,  0.5410, -0.3735,  0.9805,  0.7129,  0.0963,  0.9482,\n",
      "        -0.3623,  0.6538,  0.4714,  0.0702,  0.4121, -0.2094,  0.4580,  0.4082,\n",
      "        -0.3735,  0.9648, -0.3584,  0.4058,  0.4722,  0.0263,  0.3835,  0.4297,\n",
      "         1.0625,  1.6377,  1.9541,  0.5708,  1.3379,  1.8564,  0.6182,  0.6880,\n",
      "         1.1689,  0.6660,  0.7725, -0.1415,  2.2656,  1.1182,  0.4211,  1.7061,\n",
      "         1.7969,  0.4155,  0.2930, -0.4302,  0.7769,  0.3550,  0.1545,  0.3101,\n",
      "         0.5181,  0.3342,  0.2112,  1.3096, -0.6309,  0.2751, -0.8364,  0.5791,\n",
      "         0.6299, -0.4209,  1.6992,  0.2971,  0.5674,  2.2559,  1.0635,  0.3857,\n",
      "         0.3508,  0.3726,  1.5352,  0.9404,  1.2842,  0.8652, -0.2283,  0.9370,\n",
      "         1.6465,  1.7549,  0.3689,  0.6953,  0.6274,  0.2749, -0.0057,  0.8154,\n",
      "        -0.5186,  0.5469,  0.9224,  0.0976,  1.6641,  0.5850,  1.1602,  0.6118,\n",
      "         1.3154,  0.5439,  1.4609,  0.3799,  0.5400,  0.5620, -0.1499,  2.2930,\n",
      "         0.8276,  1.8945,  0.6274,  0.3994, -0.7559, -0.8984, -0.3323,  0.1947,\n",
      "        -0.1521,  1.2559,  0.3950,  0.4155,  0.4248,  0.1619,  0.3726,  1.5625,\n",
      "         0.7227,  1.0400,  1.4551,  1.1533, -0.4893,  1.0674,  2.0703,  0.5322,\n",
      "         0.4272,  0.9844,  0.5728, -0.2269, -0.0152,  2.7207,  1.1533,  2.2637,\n",
      "         0.9351,  0.8862, -0.9312,  0.0735, -0.2147,  2.3574,  0.9795,  1.8525,\n",
      "         0.6313,  0.6670,  0.3181, -0.4387, -0.5718,  1.6045,  0.7554,  1.2695,\n",
      "         0.5264,  0.4973,  0.2756,  0.8608, -0.1533,  0.3865,  0.5903,  0.0540,\n",
      "         2.1641,  0.8130,  1.2188,  0.4514,  0.0692, -0.0349,  2.0566,  0.8438,\n",
      "         0.6406,  0.1354,  0.6660,  0.2859, -0.0376,  0.3513, -0.5581,  0.0741,\n",
      "         0.3354, -0.2998,  0.6904,  0.6489,  0.1746,  0.0792,  0.2311,  1.0459,\n",
      "        -0.1020,  0.2253,  0.2932,  0.2186, -0.2576,  2.5039,  1.3662,  2.1211,\n",
      "         0.7759,  1.9463,  0.6309,  0.5142,  2.4434,  1.0000,  1.5381,  0.6338,\n",
      "         1.9902,  1.1758,  1.9326,  1.2305,  1.9785,  0.8633, -0.0524,  0.9873,\n",
      "         0.5259,  0.5649,  1.7734,  1.3359,  1.8516,  1.0742,  1.5234,  0.8130,\n",
      "         1.5928,  1.5791,  0.2712, -0.7153,  0.2496,  2.7363,  0.7236,  0.8013,\n",
      "         1.9922,  0.6934, -0.3782,  0.0115,  0.9331,  0.4167,  0.2971,  0.0726,\n",
      "         2.4492,  1.0566,  2.0410,  0.8330,  2.2305,  1.0518,  1.5234,  2.1504,\n",
      "         0.8438,  0.3916], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 8.2031e-01, -4.5093e-01, -1.2445e-01,  3.9258e-01, -1.1348e+00,\n",
      "        -5.6885e-01, -7.9980e-01,  1.4832e-01, -7.1191e-01, -4.7217e-01,\n",
      "        -3.3569e-01,  4.7461e-01,  4.5801e-01,  5.7471e-01, -3.2129e-01,\n",
      "         1.1481e-01, -6.6772e-02,  4.7412e-01,  1.6687e-01,  5.0049e-01,\n",
      "        -2.1265e-01,  7.2705e-01,  7.3340e-01,  1.0850e+00,  3.3374e-01,\n",
      "        -5.3711e-02,  9.8828e-01, -3.0444e-01,  1.4526e-01,  3.7061e-01,\n",
      "        -2.9810e-01,  5.8008e-01,  2.5977e-01, -5.5371e-01, -2.4280e-01,\n",
      "         5.2979e-01,  3.0493e-01,  3.3179e-01,  8.2080e-01, -7.2998e-02,\n",
      "         4.2407e-01,  6.1377e-01,  3.2568e-01,  4.9341e-01,  6.0107e-01,\n",
      "         5.3711e-01, -5.8252e-01,  7.1655e-02,  4.8889e-02,  1.0797e-01,\n",
      "         2.6831e-01, -6.4307e-01, -2.7612e-01,  2.4817e-01, -5.0781e-01,\n",
      "         5.9204e-02,  8.6475e-01,  2.7563e-01,  6.5918e-01,  8.5107e-01,\n",
      "        -5.4395e-01, -2.5049e-01,  1.4404e-01,  3.9160e-01,  4.7144e-01,\n",
      "        -3.4082e-01,  6.7920e-01,  5.5811e-01,  7.6123e-01, -2.4988e-01,\n",
      "         4.7339e-01, -3.7524e-01,  9.5264e-01,  5.0439e-01,  1.5156e+00,\n",
      "        -7.2083e-02,  3.3252e-01,  4.3896e-01, -3.0298e-01,  8.4570e-01,\n",
      "         4.5752e-01,  1.5381e+00, -3.7415e-02, -1.9666e-01,  4.0479e-01,\n",
      "         3.9502e-01,  6.4844e-01, -7.9639e-01,  3.3716e-01,  6.2256e-01,\n",
      "        -2.3352e-01,  4.7180e-02, -9.2224e-02, -9.1553e-01,  4.3213e-01,\n",
      "         5.5225e-01, -2.5244e-01,  6.2927e-02,  1.0077e-01, -2.5098e-01,\n",
      "         2.5195e-01,  4.4946e-01,  5.1758e-01,  9.9365e-01, -1.2842e-01,\n",
      "         7.3389e-01,  2.5586e-01, -4.2017e-01,  2.3975e-01,  4.0649e-01,\n",
      "         3.8989e-01, -4.2212e-01, -7.1143e-01, -9.9182e-02, -3.1177e-01,\n",
      "        -3.8330e-02,  2.6929e-01,  5.2155e-02,  1.3682e+00, -2.5903e-01,\n",
      "        -9.0576e-02, -8.4473e-02, -4.4995e-01, -1.0762e+00,  2.8101e-01,\n",
      "         2.5122e-01,  1.2598e-01,  6.8799e-01,  5.4883e-01,  1.9766e+00,\n",
      "        -5.0195e-01, -8.3105e-01,  2.4707e-01, -4.3335e-01, -4.2480e-01,\n",
      "        -2.8223e-01, -4.4946e-01, -1.7542e-01,  8.7769e-02,  1.2952e-01,\n",
      "        -1.9031e-01,  6.0693e-01,  1.3799e+00, -2.9984e-02,  1.1273e-01,\n",
      "        -1.4221e-01,  5.2539e-01,  6.1621e-01,  6.0498e-01,  1.8213e-01,\n",
      "         1.1240e+00,  5.0977e-01,  1.1299e+00, -1.7578e-01, -1.1285e-01,\n",
      "         2.8613e-01,  9.2188e-01,  3.4448e-01,  2.2852e-01, -4.5972e-01,\n",
      "        -3.8721e-01, -1.4551e-01,  8.1885e-01, -6.7627e-01,  4.2578e-01,\n",
      "         7.1826e-01, -1.0938e+00, -9.7119e-01, -4.6777e-01,  1.8018e-01,\n",
      "         4.0112e-01,  3.7012e-01,  9.1016e-01,  1.6809e-01, -5.8447e-01,\n",
      "         3.3057e-01, -6.6681e-03, -9.5398e-02,  2.5464e-01,  8.4570e-01,\n",
      "        -4.8926e-01,  2.4796e-02,  7.0996e-01,  1.5977e+00, -3.3447e-01,\n",
      "         4.6826e-01, -4.7559e-01,  8.4131e-01,  6.1719e-01,  1.2764e+00,\n",
      "        -1.1115e-01,  6.9971e-01,  5.9766e-01,  1.0869e+00,  2.0081e-01,\n",
      "        -4.6143e-02,  5.8154e-01,  3.7231e-01,  1.2598e+00, -1.7505e-01,\n",
      "         3.6597e-01, -3.4644e-01, -2.0679e-01,  4.6387e-01,  3.7720e-01,\n",
      "         7.2461e-01, -7.1191e-01,  4.6997e-01,  7.0361e-01,  1.0820e+00,\n",
      "         6.3477e-01, -4.4556e-01,  5.6689e-01, -6.8262e-01, -4.2236e-01,\n",
      "         3.2617e-01,  4.5190e-01,  3.9355e-01,  4.8193e-01, -1.1025e+00,\n",
      "         6.1475e-01,  1.0693e-01,  5.1611e-01,  1.1074e+00,  3.5645e-01,\n",
      "        -1.7029e-01, -9.2163e-02,  7.1240e-01,  3.5718e-01, -2.9785e-01,\n",
      "        -1.4661e-01,  7.6221e-01,  4.2017e-01,  6.9336e-01, -1.2854e-01,\n",
      "         9.7705e-01, -8.3838e-01,  2.1240e-01,  7.5293e-01, -1.5427e-02,\n",
      "         7.1973e-01, -3.3813e-01,  3.0249e-01,  4.5337e-01,  1.2275e+00,\n",
      "        -4.2285e-01, -1.4038e-01,  6.3525e-01,  4.8242e-01,  1.2314e+00,\n",
      "        -3.0737e-01,  4.6680e-01,  4.6509e-01,  1.1902e-01, -2.5269e-01,\n",
      "         1.5784e-01, -1.1602e+00,  1.2711e-02,  4.1455e-01,  1.0605e+00,\n",
      "         5.8350e-01,  1.2427e-01, -4.6387e-01,  3.6743e-02,  4.9243e-01,\n",
      "         6.9629e-01, -1.4172e-03, -1.8408e-01, -6.6772e-02,  2.2351e-01,\n",
      "         1.3613e+00, -1.7847e-01,  6.6455e-01,  5.5469e-01,  9.9072e-01,\n",
      "         3.1714e-01,  3.9062e-01,  1.4734e-01,  7.8857e-01, -4.2578e-01,\n",
      "        -4.1333e-01,  2.8589e-01,  1.6174e-01,  2.0776e-01,  1.3379e+00,\n",
      "        -9.1846e-01, -3.5840e-01, -3.4082e-01, -4.5288e-02,  4.2847e-01,\n",
      "        -5.3760e-01, -6.3110e-02,  4.8022e-01,  6.7773e-01, -5.6152e-01,\n",
      "         8.7280e-02,  8.3838e-01,  7.4854e-01, -1.1847e-01, -3.3472e-01,\n",
      "        -1.1823e-01,  8.7695e-01, -5.9521e-01, -2.2449e-01, -4.2578e-01,\n",
      "         6.3672e-01, -6.6650e-01, -1.5515e-01,  4.1064e-01, -2.3453e-02,\n",
      "         5.1318e-01, -5.2783e-01,  2.2693e-01,  1.7346e-01, -6.7627e-01,\n",
      "         4.7217e-01, -1.1737e-01,  7.0068e-01,  5.9766e-01,  1.1035e+00,\n",
      "         2.5293e-01, -2.5415e-01,  2.6367e-01,  9.1248e-02,  6.3379e-01,\n",
      "        -8.5156e-01, -1.4600e-01, -2.4063e-02,  1.6785e-02,  9.6631e-01,\n",
      "         7.8711e-01,  6.6064e-01,  3.9795e-01,  3.6841e-01,  1.5225e+00,\n",
      "        -1.2396e-01, -2.9492e-01,  6.9641e-02,  7.1143e-01,  3.6694e-01,\n",
      "         1.0244e+00,  4.3427e-02,  6.8066e-01,  5.6104e-01,  2.8369e-01,\n",
      "         5.4736e-01,  1.5938e+00, -4.9951e-01,  3.4863e-01,  5.4346e-01,\n",
      "         1.2227e+00, -3.4717e-01,  4.9048e-01,  4.3579e-01,  1.2927e-01,\n",
      "         3.1787e-01,  1.8542e-01, -1.0430e+00,  1.7639e-01, -9.5886e-02,\n",
      "        -8.7842e-01, -5.6006e-01,  3.4912e-02, -5.7715e-01, -5.0635e-01,\n",
      "        -5.2979e-01, -1.4450e-02,  2.2815e-01,  1.3428e-01, -1.0518e+00,\n",
      "        -6.1890e-02, -8.2129e-01, -7.5830e-01, -3.0566e-01, -2.3645e-01,\n",
      "        -7.9980e-01, -4.2334e-01, -5.6641e-01,  2.1472e-01,  2.1399e-01,\n",
      "        -6.5576e-01, -2.3450e-01, -8.7097e-02,  3.3203e-01, -6.5381e-01,\n",
      "         3.0640e-02, -8.2092e-02,  7.1259e-03, -1.2051e+00, -1.7102e-01,\n",
      "        -5.4834e-01, -9.2871e-01, -7.7393e-01, -1.7126e-01,  2.5742e-02,\n",
      "         1.9028e-02, -8.8818e-01, -4.0771e-02, -7.6660e-01, -2.9785e-01,\n",
      "         2.3657e-01, -1.0391e-02, -5.0391e-01, -4.7314e-01, -4.7217e-01,\n",
      "        -3.6108e-01, -2.0557e-01, -1.9800e-01, -9.3262e-01, -5.5664e-01,\n",
      "        -2.1887e-01,  1.1285e-01, -2.0837e-01, -4.5850e-01, -3.2251e-01,\n",
      "        -7.5012e-02, -1.0186e+00, -1.7468e-01,  4.4067e-02, -1.3330e-01,\n",
      "         7.1838e-02, -1.6345e-01, -3.4253e-01, -5.7080e-01, -2.2791e-01,\n",
      "         2.8101e-01,  6.7920e-01,  1.4893e+00,  5.1904e-01,  6.4746e-01,\n",
      "         1.3359e+00,  3.6499e-01,  2.2876e-01,  6.6357e-01,  1.2451e+00,\n",
      "        -7.2852e-01, -1.6602e-01,  2.4170e-01,  1.2051e+00,  5.6543e-01,\n",
      "         6.7676e-01,  9.8340e-01,  1.5234e-01, -4.7461e-01,  8.5059e-01,\n",
      "        -2.3621e-01, -2.1387e-01,  1.1792e-01,  1.9543e-01,  6.6455e-01,\n",
      "        -7.0557e-01,  4.3872e-01, -7.4756e-01, -2.0239e-01,  9.0637e-03,\n",
      "         2.6535e-02,  1.7493e-01,  1.4380e-01,  2.1042e-02,  8.0029e-01,\n",
      "         2.8052e-01,  2.6807e-01,  3.1177e-01,  1.4385e+00, -4.2310e-01,\n",
      "         4.5728e-01, -4.4580e-01,  1.1367e+00, -4.4897e-01,  1.0518e+00,\n",
      "        -1.0420e+00,  3.8770e-01,  2.1338e-01,  6.1572e-01,  1.1533e+00,\n",
      "         8.5352e-01,  3.3154e-01, -3.6499e-02, -5.2539e-01,  3.7769e-01,\n",
      "        -3.0664e-01, -9.0332e-02,  1.3633e+00, -1.7505e-01, -5.5237e-02,\n",
      "         7.4414e-01,  2.5024e-01,  9.6045e-01,  2.2742e-01,  6.0889e-01,\n",
      "         7.2461e-01,  4.7144e-01,  1.2396e-01,  9.9121e-01, -4.0845e-01,\n",
      "        -1.0162e-01,  7.8027e-01,  7.6953e-01,  1.3096e+00,  5.5127e-01,\n",
      "        -7.1631e-01, -1.2537e-01, -6.2305e-01,  3.9459e-02, -2.5903e-01,\n",
      "         3.9624e-01,  3.1836e-01,  1.0273e+00,  6.1816e-01, -4.8242e-01,\n",
      "         4.5508e-01, -2.3157e-01,  1.2734e+00, -5.3516e-01, -1.5234e-01,\n",
      "         9.7705e-01, -1.0244e+00,  4.8999e-01,  2.5903e-01,  1.5117e+00,\n",
      "         4.0869e-01,  2.0239e-01, -2.2595e-01,  1.8884e-01, -4.8486e-01,\n",
      "         1.9897e-02,  1.2080e+00,  4.1406e-01,  3.6011e-01,  1.6934e+00,\n",
      "        -9.1748e-01, -1.6406e-01, -6.7871e-01,  1.6052e-01,  1.6484e+00,\n",
      "         3.1396e-01,  4.9976e-01,  7.8418e-01,  2.0352e+00, -2.2937e-01,\n",
      "        -3.2764e-01,  1.6077e-01,  1.2881e+00,  2.9297e-02,  2.7441e-01,\n",
      "         5.9814e-01,  5.2490e-01,  1.5942e-01,  5.1300e-02, -1.8640e-01,\n",
      "         1.5381e-01, -3.0347e-01, -2.4451e-01,  7.4170e-01,  3.9600e-01,\n",
      "         1.5879e+00, -7.3291e-01, -6.8896e-01,  2.4097e-01,  1.6370e-01,\n",
      "         1.1973e+00,  7.2217e-01,  1.6284e-01, -2.0157e-02, -8.6572e-01,\n",
      "         6.5369e-02, -9.7607e-01, -8.2031e-01,  3.8757e-02, -3.1372e-01,\n",
      "         2.1326e-01,  1.6882e-01, -2.8183e-02, -1.9641e-01, -5.9814e-01,\n",
      "         9.2773e-02, -2.0825e-01, -4.4263e-01, -4.4434e-01, -4.7607e-01,\n",
      "        -4.4629e-01,  8.7097e-02,  9.0137e-01,  9.8877e-01,  1.9170e+00,\n",
      "         3.1006e-01,  6.3818e-01,  1.5449e+00,  8.2031e-01,  1.9883e+00,\n",
      "         4.8462e-01,  9.0527e-01, -2.2327e-01,  8.1494e-01,  4.3188e-01,\n",
      "         1.1904e+00,  3.1519e-01,  1.4824e+00, -7.6758e-01,  2.6318e-01,\n",
      "         5.3467e-01,  4.5020e-01, -2.5830e-01,  7.8906e-01,  2.4316e-01,\n",
      "         1.3076e+00,  3.3179e-01,  5.5859e-01,  9.5312e-01,  9.6680e-01,\n",
      "         1.4980e+00, -5.5713e-01, -4.0259e-01, -2.7393e-01,  3.3179e-01,\n",
      "         1.3770e+00,  5.1758e-01,  2.3008e+00, -8.4961e-01, -8.7695e-01,\n",
      "         4.8682e-01,  5.7373e-01,  6.4746e-01, -5.1123e-01,  1.1346e-01,\n",
      "         1.5039e+00,  5.1855e-01,  1.3271e+00,  9.1699e-01,  1.6729e+00,\n",
      "         2.1667e-01,  4.5239e-01,  1.7480e+00,  4.7290e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.3203,  0.3253,  0.0116,  ...,  1.0309,  1.2115, -0.1952],\n",
      "         [ 0.0569,  0.1760, -0.2564,  ...,  0.9825,  0.7639, -0.3234],\n",
      "         [ 1.2297, -0.0715, -0.0095,  ...,  1.1236,  1.5660, -0.1878],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.4890,  1.5215,  0.5986,  0.0788, -0.1191, -0.4158,  0.5962,  0.4023,\n",
      "        -0.4236,  0.0538, -0.2455,  0.5454,  0.6641,  0.1288,  0.7100,  0.1737,\n",
      "        -0.5806,  0.9028,  0.4038,  2.2969,  1.0098,  2.2676,  0.4150,  0.7217,\n",
      "         1.2744,  0.2487,  0.1735,  0.7427,  0.7046,  0.2134,  0.2122, -0.2004,\n",
      "         0.3621,  0.2554, -0.2700,  0.3989,  0.2261,  1.4678,  0.1392,  1.3193,\n",
      "        -0.5396,  0.7119, -0.4551,  0.8413,  0.3271, -0.0685,  0.4434, -0.3621,\n",
      "         1.9736,  0.4424,  0.9409, -0.1753,  2.0566,  0.6426,  1.4863,  0.6675,\n",
      "        -0.0618,  0.3157,  0.1918, -0.3162, -0.2971, -0.2124,  0.5317, -0.5210,\n",
      "         2.1875,  1.2148,  0.2896,  0.3069,  2.2109,  0.7798,  0.9595,  0.4690,\n",
      "         0.0219,  2.3281,  0.7466,  1.6230,  0.6221,  1.5576,  0.2377,  0.7905,\n",
      "         0.2469,  2.0723,  0.7275,  1.8047,  0.7490,  0.4167,  0.9312,  0.4038,\n",
      "         0.3579,  1.2930,  0.4404,  0.3767,  0.6460, -0.2634, -0.7207,  0.7207,\n",
      "         2.3574,  1.1104,  1.6094,  0.4126,  0.8398,  1.1641,  0.0202,  0.3801,\n",
      "         0.1447,  1.7393,  1.0068,  1.1963,  0.4749,  1.3135,  1.0469,  0.3235,\n",
      "         0.3613,  0.5835,  0.8540,  0.5229,  0.8818,  0.3484, -0.0241,  1.8184,\n",
      "         0.7334,  1.1504, -0.0726,  0.3789,  0.4055,  1.5264,  0.4573,  0.4270,\n",
      "         0.7104,  0.1340,  1.0273,  0.4355,  1.2510,  0.2345,  0.3713,  0.6826,\n",
      "         1.6104,  1.3340, -0.0288,  1.4092,  0.6812,  0.1591,  0.2839, -0.1766,\n",
      "         0.5635,  1.0527,  0.0766,  0.8369, -0.1873,  2.0957,  0.9961,  1.4277,\n",
      "         0.5811,  0.0298,  1.9121,  1.1328,  1.3965,  0.5015,  0.5581,  1.1006,\n",
      "         1.1133,  0.6523,  0.6602,  0.8032,  0.6270, -0.3403,  1.3018, -0.0801,\n",
      "        -0.0427,  0.6094,  0.7783,  0.7510, -0.7085,  0.1560, -0.4519,  0.6606,\n",
      "         0.6162,  0.6377, -0.5278,  1.4072,  0.3101,  2.0352,  0.0918,  1.0527,\n",
      "         0.6367,  0.7583,  1.2070, -0.1992, -0.0139,  1.0703,  0.3538, -0.3191,\n",
      "         0.5215, -0.1157, -0.3181,  0.8799,  1.0947,  0.4885, -0.1324,  3.0703,\n",
      "         1.8281,  0.6675,  0.9653,  0.5303, -0.9097, -0.4021, -0.6299, -0.3713,\n",
      "         0.1920,  0.4622,  0.1409,  0.1311,  1.1396,  0.6304,  0.7324,  0.3198,\n",
      "         0.3870,  2.6387,  0.7881,  1.9102,  1.3916,  0.9653,  0.9844,  2.4023,\n",
      "         0.7939, -0.3716, -0.1080, -0.4167, -0.0191,  0.4561,  1.1416,  0.7490,\n",
      "         0.6494,  1.0166,  0.4014,  0.0275, -0.0235,  0.7446, -0.2477,  1.6650,\n",
      "         1.0078,  1.3027,  1.0703,  0.4763,  2.8926,  1.1650,  1.8350,  0.5903,\n",
      "        -0.1852,  1.0039, -0.1854, -0.0756,  0.9854,  0.2620,  1.9922,  0.0793,\n",
      "         0.2489,  0.2759,  2.2676,  0.6812,  0.4294,  0.3276, -0.0633,  0.5439,\n",
      "         0.6660, -0.4180, -0.0481, -0.2717,  0.2717,  0.5806,  0.3267, -0.1504,\n",
      "         2.2480,  0.7300, -0.0699,  0.1321, -0.0881, -0.2491,  1.9844,  0.5400,\n",
      "         0.8857,  0.5381,  0.3740,  1.0264, -0.2271,  0.6372,  0.9380, -0.1464,\n",
      "         0.7290, -0.2825,  0.5693, -0.1186,  2.4395,  0.5063,  0.9731,  0.4722,\n",
      "         0.2642, -0.2299,  0.9854, -0.0231,  2.4160,  1.0732,  1.3477,  0.1689,\n",
      "         1.2539,  0.8687,  0.2484, -0.0153,  2.3164,  1.0059, -0.4890,  1.3604,\n",
      "         0.6382,  0.5142,  0.0286,  2.4316,  0.5044,  1.1826,  0.4062, -0.7188,\n",
      "        -0.0227, -0.7505, -0.7549, -0.0926,  0.8726, -0.3257,  0.0132, -0.2612,\n",
      "         2.1582,  0.3074,  1.0957, -0.0380, -0.6333, -0.1155, -0.1100,  2.4844,\n",
      "         0.5381,  1.1934,  0.7305,  0.7734, -0.0533, -0.0155,  1.6680,  1.8896,\n",
      "         1.2275,  0.3928,  0.4949,  0.4033, -0.5811,  0.3794, -0.1619, -0.3025,\n",
      "         0.4629, -0.5449,  0.2712,  0.2435,  0.6543,  0.4421,  0.5444,  0.1232,\n",
      "         0.0237, -0.5493, -0.3198,  0.2654, -0.2793,  0.7197, -0.0352,  0.5913,\n",
      "         1.8203,  1.2217,  0.1199,  0.5464,  0.1149,  0.4556,  0.5498,  0.0139,\n",
      "        -0.3191,  0.2886, -0.1813,  0.2479,  0.6396, -0.3120,  0.8633,  1.1670,\n",
      "         1.5693,  0.3630,  1.6338,  0.5269, -0.6528, -0.1482,  0.2903,  0.4609,\n",
      "        -0.5356,  0.5532, -0.2268,  1.8818,  0.6230, -0.0137, -0.4688,  0.3037,\n",
      "         0.2216,  0.5015,  0.7144,  0.8760,  0.1973,  1.3408, -0.4434,  0.1694,\n",
      "         0.5654, -0.1425,  1.4375,  0.5996,  0.4658,  0.7681,  1.4805,  0.8540,\n",
      "         0.0384,  0.0673,  0.1661,  1.8857,  0.1724,  0.4089,  0.8823, -0.1416,\n",
      "         0.0135,  0.3445, -0.2358,  1.2295,  0.8887,  0.7886,  0.0643,  1.9805,\n",
      "         0.9497,  1.1289, -0.3713,  0.0335,  0.4019, -0.1945,  1.6123,  0.2515,\n",
      "         1.0605,  0.0351,  2.3047,  0.6338,  1.0156,  0.6421,  0.3250, -0.6650,\n",
      "        -0.3833,  0.2445,  1.1240,  0.1736, -0.4639,  0.8394, -0.2313, -0.0272,\n",
      "         1.9512,  0.1506,  0.8516,  0.3965,  2.0195,  0.2644,  0.7378,  1.7354,\n",
      "         0.3340,  0.3828,  1.0596,  0.3740,  0.9761,  0.0180, -0.0363, -0.4055,\n",
      "        -0.5098, -0.3867, -0.0295, -0.0588,  2.3105,  0.7344,  0.5503, -0.2957,\n",
      "         0.1888, -1.3076,  1.3213, -0.0317,  1.3877, -0.2343,  0.8218, -0.1173,\n",
      "         0.1637,  0.9346,  0.1963, -0.4106,  1.7422,  0.0948,  0.8364, -0.5649,\n",
      "         0.1198, -0.7124,  0.2396,  0.3301,  0.1984, -0.0431, -0.0937,  1.7627,\n",
      "         1.2461,  0.7451, -0.1514,  1.5811,  1.0742, -0.7944, -0.9512, -0.4231,\n",
      "         0.4692, -0.3425,  0.1658,  0.1873,  0.3218,  0.8213,  0.4939,  0.2710,\n",
      "         0.6431,  0.1053,  0.3411, -0.4543,  0.3533, -0.0375,  1.2227,  0.9580,\n",
      "         1.0049,  1.7363,  0.5142, -1.0059,  0.1111,  1.3242,  1.1025,  0.3877,\n",
      "         1.1875,  0.0524, -0.2448,  0.2588, -0.2644, -0.3538,  0.3064,  2.1250,\n",
      "         0.4653,  0.8481,  0.7856,  0.7832, -0.0274,  0.3113, -0.0585,  0.1349,\n",
      "         1.2490,  0.1819,  0.6470, -0.1254,  0.8320,  0.1117,  0.8286,  0.0925,\n",
      "         0.1534,  0.0211, -0.0703,  1.8301,  0.3657,  0.7178, -0.0170,  1.4463,\n",
      "         0.1077,  0.5425, -0.3552,  1.0752,  0.0582,  0.6426,  1.1113,  0.3799,\n",
      "         0.4993,  0.0168,  0.9746,  1.2090,  0.8989,  0.6699,  0.3511,  0.5176,\n",
      "         1.5547,  0.3936, -0.1104,  1.2861,  0.4067,  0.1642,  0.0556, -0.7900,\n",
      "        -0.0254, -0.9868,  0.1071,  0.1807,  0.9609,  0.9785,  0.7334,  0.2491,\n",
      "         0.0933,  1.3740,  0.2871, -0.0765,  1.3691,  0.3567,  0.1506, -0.0684,\n",
      "         0.1638,  0.1382, -0.5098,  0.2489, -0.3420,  1.5654,  1.2754,  0.7822,\n",
      "         0.3354], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 4.1504e-01, -3.5132e-01,  8.9941e-01, -5.1416e-01, -5.7812e-01,\n",
      "        -7.0508e-01, -5.7275e-01, -4.4092e-01, -1.1182e+00, -8.7451e-01,\n",
      "        -7.6318e-01, -1.8188e-01, -1.0272e-01, -5.7959e-01,  7.2461e-01,\n",
      "         4.5410e-02,  2.3633e-01,  8.5205e-02,  5.8350e-01,  9.9365e-01,\n",
      "         2.0547e+00,  5.2783e-01,  1.9150e+00,  2.0117e+00,  8.8574e-01,\n",
      "         3.2495e-01,  5.6494e-01,  4.1528e-01,  6.8408e-01,  1.1029e-01,\n",
      "         4.3604e-01, -5.0146e-01,  4.7699e-02,  5.0110e-02,  4.9707e-01,\n",
      "         2.9004e-01,  3.5352e-01,  3.6865e-01,  7.4524e-02,  7.3779e-01,\n",
      "        -5.7910e-01, -5.3711e-01, -2.7295e-01, -3.3008e-01,  2.0227e-01,\n",
      "        -1.1029e-01, -2.7759e-01, -1.7725e-01,  6.3477e-02,  5.4590e-01,\n",
      "         1.1855e+00, -5.5029e-01, -2.7856e-01,  2.1436e-01,  4.0552e-01,\n",
      "         3.3813e-01,  1.1816e+00,  2.7490e-01,  1.2115e-01, -6.1816e-01,\n",
      "        -6.3171e-02, -2.2400e-01, -3.4326e-01, -4.6387e-01,  4.4019e-01,\n",
      "         1.2891e+00,  1.3330e+00, -2.7725e-02, -1.5027e-01,  4.1919e-01,\n",
      "         1.8506e+00,  7.2314e-01,  2.9541e-01, -8.4991e-03,  9.0234e-01,\n",
      "         7.0215e-01,  1.5361e+00,  8.9746e-01,  1.3340e+00,  1.0625e+00,\n",
      "        -2.8491e-01, -2.6392e-01,  1.5029e+00,  1.1359e-01, -1.1215e-02,\n",
      "         1.7715e+00,  2.8931e-01,  8.2129e-01, -1.3721e-01, -4.0259e-01,\n",
      "         4.4769e-02, -2.9028e-01,  2.3645e-01,  3.8672e-01,  1.2695e-01,\n",
      "        -5.7861e-01,  1.1548e-01,  7.3730e-01,  8.5352e-01,  1.7959e+00,\n",
      "         1.0908e+00,  8.3496e-01, -4.4092e-01, -1.1871e-01, -6.4209e-01,\n",
      "         5.6445e-01,  1.2129e+00,  1.5010e+00, -4.2773e-01, -4.6680e-01,\n",
      "        -1.6956e-01, -9.5764e-02, -1.3831e-01,  3.6108e-01, -1.4172e-01,\n",
      "         1.1151e-01,  1.0315e-01,  3.1567e-01, -4.6704e-01,  1.3904e-01,\n",
      "         6.4160e-01,  4.2651e-01,  1.5869e+00,  1.2988e-01, -5.0830e-01,\n",
      "        -3.2129e-01, -3.0664e-01, -2.5928e-01,  6.4453e-02,  3.7451e-01,\n",
      "        -1.0529e-01, -1.2238e-01,  4.2505e-01,  7.0312e-01,  8.6487e-02,\n",
      "        -2.7783e-01,  4.7266e-01,  6.7969e-01,  1.0537e+00,  1.5247e-01,\n",
      "         6.0352e-01,  3.0273e-01, -4.0942e-01,  1.7175e-01, -7.6953e-01,\n",
      "        -1.4050e-01,  2.7148e-01, -1.6943e-01,  1.5149e-01, -2.3499e-01,\n",
      "         4.1895e-01,  4.8071e-01,  2.0234e+00, -6.9287e-01,  4.3213e-02,\n",
      "         1.2490e+00,  5.5957e-01,  5.9277e-01,  2.1953e+00,  5.9814e-01,\n",
      "        -6.1096e-02,  3.4375e-01,  3.4253e-01,  1.7249e-01,  1.4229e+00,\n",
      "        -1.0381e+00,  5.7666e-01,  5.2783e-01,  2.4124e-02, -3.3618e-01,\n",
      "        -3.3105e-01,  1.7920e+00, -8.5645e-01, -9.6631e-01, -1.1709e+00,\n",
      "         3.3545e-01,  1.4624e-01,  9.9548e-02, -4.7119e-01,  9.6680e-01,\n",
      "         9.0674e-01,  7.7881e-01,  1.8677e-01,  1.8467e+00,  9.6863e-02,\n",
      "        -1.7908e-01, -6.7236e-01, -3.1128e-01, -2.1619e-01,  1.1357e+00,\n",
      "         1.4496e-02, -7.9248e-01,  1.0260e-01, -3.8147e-02, -7.4951e-01,\n",
      "         1.0938e+00,  5.6250e-01, -2.3792e-01,  2.7856e-01,  9.4055e-02,\n",
      "         1.8518e-01,  1.4336e+00,  1.6260e+00,  1.9521e+00, -8.9648e-01,\n",
      "        -7.7246e-01, -3.5132e-01, -6.6211e-01,  1.0388e-01,  9.7351e-02,\n",
      "         5.0598e-02, -4.0356e-01,  5.5518e-01,  3.5400e-01,  1.8340e+00,\n",
      "        -2.8833e-01,  5.8252e-01, -2.6489e-01,  1.9082e+00,  3.0664e-01,\n",
      "         6.7139e-01,  2.8164e+00,  1.1270e+00,  4.9652e-02,  1.6885e+00,\n",
      "         2.5537e-01,  1.6675e-01,  1.7957e-01,  8.9050e-02, -2.5284e-02,\n",
      "        -2.9810e-01,  2.3474e-01,  2.0129e-01,  1.5000e+00,  7.7441e-01,\n",
      "        -7.3145e-01, -4.5361e-01, -5.6445e-01, -6.8457e-01, -3.8599e-01,\n",
      "        -9.4788e-02,  3.6597e-01,  1.0029e+00, -4.7754e-01, -2.7539e-01,\n",
      "         1.3965e+00,  6.7236e-01,  1.8184e+00, -9.9170e-01, -3.7793e-01,\n",
      "        -4.1626e-01, -2.9834e-01,  1.0078e+00, -4.3994e-01,  2.0801e-01,\n",
      "         1.0225e+00, -1.0117e+00, -5.1953e-01, -2.4133e-01,  8.2520e-01,\n",
      "         1.7639e-01,  1.2659e-01, -7.6270e-01, -2.1545e-01,  1.0371e+00,\n",
      "        -5.6787e-01, -1.9568e-01, -3.2959e-01, -2.4890e-01, -3.2471e-01,\n",
      "        -5.1904e-01, -6.8750e-01, -3.9136e-01,  8.5303e-01, -8.3447e-01,\n",
      "        -2.0605e-01, -1.7493e-01, -6.5869e-01,  7.2559e-01,  5.6055e-01,\n",
      "         2.4590e+00, -2.2571e-01,  5.6055e-01, -4.3848e-01,  3.7964e-02,\n",
      "         1.3457e+00, -6.0400e-01,  2.2876e-01,  1.4922e+00, -8.8867e-01,\n",
      "        -1.9238e-01, -2.0337e-01,  8.5254e-01,  8.0811e-01,  3.0820e+00,\n",
      "         5.1953e-01,  8.3643e-01, -6.7041e-01,  2.1301e-01,  9.1602e-01,\n",
      "        -2.9980e-01,  7.6660e-01,  1.6338e+00,  2.9724e-02,  9.5801e-01,\n",
      "         5.6299e-01,  1.1703e-02,  6.4880e-02,  1.4954e-01,  1.7461e+00,\n",
      "        -7.2705e-01,  7.4756e-01,  7.0996e-01, -3.2275e-01,  5.8197e-02,\n",
      "         6.9824e-01,  1.8219e-02,  1.7109e+00, -5.7031e-01, -2.4597e-01,\n",
      "        -6.8115e-01, -8.4473e-01, -8.7402e-02, -7.4023e-01,  9.5264e-01,\n",
      "        -7.1582e-01,  2.9395e-01, -8.4277e-01,  6.8359e-01, -1.3110e-01,\n",
      "         1.6484e+00, -8.9014e-01, -6.9873e-01, -9.0332e-02, -6.0547e-01,\n",
      "         4.3311e-01,  1.5732e+00,  1.8298e-01,  7.2510e-01,  3.6230e-01,\n",
      "         5.8398e-01, -4.4434e-01,  2.8613e-01,  1.9995e-01,  2.1960e-01,\n",
      "         9.7754e-01,  1.3164e+00, -1.2433e-01, -1.2529e+00, -7.4658e-01,\n",
      "        -1.0205e+00, -3.9209e-01, -5.6836e-01, -6.1475e-01, -3.7109e-01,\n",
      "        -3.2300e-01,  3.3081e-01, -7.7832e-01, -4.3164e-01, -8.2031e-01,\n",
      "        -2.0837e-01,  8.7585e-02, -9.4141e-01, -5.4248e-01, -8.0029e-01,\n",
      "        -3.5742e-01, -3.3081e-01, -6.0791e-01,  3.9966e-01,  6.7578e-01,\n",
      "         5.0879e-01,  9.1553e-02,  3.0029e-01, -5.6494e-01, -5.6299e-01,\n",
      "        -1.2128e-01,  2.9541e-02,  1.0217e-01,  6.6528e-02, -4.9927e-02,\n",
      "        -7.2119e-01, -3.3838e-01, -1.0883e-01,  5.0293e-01,  5.4053e-01,\n",
      "        -4.9988e-02,  8.9551e-01, -6.1133e-01, -1.3252e+00, -6.7871e-01,\n",
      "        -7.3193e-01,  7.9102e-02, -1.2393e+00, -7.1484e-01, -6.2207e-01,\n",
      "        -1.6220e-02,  1.3403e-01,  1.0928e+00, -3.4277e-01,  4.1162e-01,\n",
      "         5.3076e-01, -5.4688e-01, -4.2993e-01,  7.5684e-01,  2.1729e-01,\n",
      "         5.7031e-01, -3.6133e-01,  1.7651e-01, -4.9756e-01, -4.2358e-01,\n",
      "         9.0759e-02,  2.8003e-01,  6.8945e-01,  2.7808e-01,  1.5076e-01,\n",
      "         5.1562e-01,  2.8003e-01,  8.2422e-01, -4.7754e-01,  1.9043e-01,\n",
      "         3.2056e-01,  9.7754e-01,  3.7158e-01,  7.7197e-01,  1.4575e-01,\n",
      "        -3.6572e-01, -4.6851e-01,  4.5532e-01,  4.6362e-01,  1.1533e+00,\n",
      "        -1.9971e-01,  2.3889e-01,  1.4658e+00,  1.2246e+00, -6.8994e-01,\n",
      "         1.7847e-01, -4.2554e-01, -5.3076e-01,  6.9092e-01, -2.1301e-01,\n",
      "         1.2578e+00, -2.5708e-01,  1.4380e-01,  2.6855e-01,  1.3369e+00,\n",
      "         9.9512e-01,  2.9004e-01, -1.0723e+00, -5.8008e-01, -2.4646e-01,\n",
      "         2.6416e-01, -1.2115e-01, -9.2627e-01, -5.7312e-02,  3.9819e-01,\n",
      "        -3.2861e-01,  1.5332e-01,  1.5149e-01,  6.9482e-01, -7.8760e-01,\n",
      "        -4.0527e-01,  2.7173e-01,  1.2754e+00, -2.5757e-01, -4.8523e-02,\n",
      "         1.3984e+00, -3.2471e-01,  6.5039e-01,  1.1807e+00, -6.8604e-01,\n",
      "        -5.0049e-01, -5.4590e-01, -5.7959e-01, -3.0884e-01,  3.7915e-01,\n",
      "         2.8906e-01,  2.1655e-01,  6.1670e-01,  1.1631e+00,  8.0859e-01,\n",
      "        -5.4346e-01, -5.3564e-01,  1.2317e-01, -8.9836e-04,  1.0371e+00,\n",
      "        -3.9282e-01,  9.4189e-01, -6.9727e-01, -8.9697e-01,  2.3401e-01,\n",
      "        -7.1869e-03, -8.7061e-01,  1.4954e-01,  1.5027e-01,  8.5254e-01,\n",
      "        -6.6064e-01,  1.5320e-01,  5.2197e-01, -6.2695e-01,  4.9951e-01,\n",
      "         6.0010e-01,  2.8809e-01,  5.5518e-01, -1.5271e-01,  7.5098e-01,\n",
      "         1.3145e+00, -7.4512e-01,  2.1191e-01,  9.3262e-01, -5.3418e-01,\n",
      "        -1.6833e-01, -8.1787e-01,  6.6553e-01, -4.4165e-01,  2.6074e-01,\n",
      "        -4.4678e-01,  4.9097e-01, -3.8232e-01, -4.5593e-02,  1.0283e+00,\n",
      "        -2.8857e-01, -3.3569e-01,  6.7578e-01, -1.2100e+00, -2.9956e-01,\n",
      "        -6.6113e-01,  8.6084e-01,  1.2024e-01,  7.6514e-01, -3.6255e-01,\n",
      "         1.1582e+00,  1.6064e-01, -2.0813e-01,  1.6663e-02,  4.4507e-01,\n",
      "         7.0752e-01,  1.1953e+00, -1.2756e-01, -1.1660e+00,  1.0602e-01,\n",
      "        -4.5166e-02, -3.0322e-01,  1.8164e-01, -2.4500e-01,  3.6597e-01,\n",
      "         1.5801e+00,  1.3623e-01,  2.5098e-01,  2.5586e-01,  1.3301e+00,\n",
      "        -4.3628e-01, -7.7539e-01, -2.7954e-01,  1.5002e-01,  9.4336e-01,\n",
      "        -6.9189e-01, -3.2788e-01, -3.0884e-01, -1.8665e-01, -3.2177e-03,\n",
      "         2.2058e-01,  1.9482e-01, -5.4834e-01, -5.9961e-01,  1.0430e+00,\n",
      "        -3.7769e-01, -3.0045e-02, -7.0508e-01,  1.7175e-01,  1.0713e+00,\n",
      "        -6.9629e-01,  1.4746e-01,  3.1909e-01,  1.1797e+00, -5.7831e-03,\n",
      "        -3.5254e-01, -7.9443e-01, -5.1904e-01, -5.1941e-02,  2.3425e-01,\n",
      "         8.4521e-01,  1.9170e+00, -2.9419e-01,  8.8379e-01,  1.5918e-01,\n",
      "         1.2598e+00, -7.2900e-01,  4.3549e-02,  1.4834e+00,  7.5977e-01,\n",
      "        -8.0383e-02, -7.3193e-01, -4.7607e-01, -1.5161e-01, -5.6787e-01,\n",
      "        -6.2598e-01, -9.5276e-02,  3.3984e-01,  7.8564e-01,  1.8994e+00,\n",
      "         2.5073e-01, -6.1182e-01,  1.0918e+00, -8.7939e-01, -1.9531e-01,\n",
      "         1.2900e+00, -3.1860e-01, -5.5176e-01,  9.0454e-02,  1.5637e-01,\n",
      "        -7.3975e-01,  1.7761e-01,  1.1005e-03, -8.5742e-01,  3.8818e-02,\n",
      "         8.8623e-01,  4.9609e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.4366,  0.6706, -0.1126,  ...,  1.0545,  1.3095, -0.1559],\n",
      "         [ 0.2738,  0.4784, -0.0041,  ...,  1.0979,  0.6396,  0.1983],\n",
      "         [ 0.4228,  0.2325,  0.0770,  ...,  0.0081,  0.4401,  0.1801],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.4319e-01,  1.7695e+00,  1.1416e+00,  7.5586e-01, -2.2229e-01,\n",
      "         1.4297e+00,  5.1758e-01,  4.0723e-01,  1.3953e-01,  5.6396e-01,\n",
      "         1.2317e-01,  1.3154e+00,  6.0645e-01,  7.9346e-01,  4.5996e-01,\n",
      "         3.1689e-01,  2.1562e+00,  7.0605e-01,  4.0942e-01,  8.5388e-02,\n",
      "         9.4385e-01,  2.0312e+00,  5.4248e-01,  3.8599e-01,  3.3112e-02,\n",
      "         8.4131e-01,  1.5840e+00,  4.9243e-01,  1.2373e+00,  2.0569e-01,\n",
      "         7.6904e-01,  3.7988e-01,  1.8970e-01,  7.6172e-01, -3.3740e-01,\n",
      "         1.5566e+00,  3.8672e-01,  1.8408e-01, -6.6309e-01,  2.2246e+00,\n",
      "         6.7139e-01,  4.1895e-01,  3.5034e-02, -2.6807e-01,  1.9385e+00,\n",
      "         6.2402e-01,  1.3611e-01,  1.3428e-01, -1.1926e-01,  1.4941e+00,\n",
      "         5.5518e-01,  1.9543e-01,  4.5093e-01, -5.3857e-01,  6.6895e-01,\n",
      "         2.4158e-01,  2.2278e-01, -4.7852e-01,  2.0039e+00,  1.5808e-01,\n",
      "         2.4231e-01,  1.5537e+00,  7.0374e-02, -3.1763e-01, -2.2961e-01,\n",
      "        -7.0129e-02,  2.2148e+00,  7.7588e-01,  4.0234e-01,  1.9214e-01,\n",
      "         7.6758e-01,  1.9226e-01,  1.3164e+00,  1.3740e+00,  5.0635e-01,\n",
      "         4.7607e-02,  2.6245e-01, -7.5867e-02,  1.5215e+00,  5.9717e-01,\n",
      "         4.5435e-01,  3.4741e-01, -8.6060e-02, -3.1174e-02,  2.1016e+00,\n",
      "         2.4426e-01,  6.0010e-01, -5.6445e-01, -2.6807e-01,  6.1584e-02,\n",
      "         5.5127e-01,  6.3721e-01, -2.3389e-01, -1.5869e-01, -2.1960e-01,\n",
      "         7.6514e-01, -1.0736e-01,  1.9121e+00,  3.6182e-01,  9.6802e-02,\n",
      "         4.6936e-02,  1.2021e+00,  2.7319e-01,  1.4783e-01,  4.1797e-01,\n",
      "         1.1250e+00,  7.0740e-02,  1.5654e+00,  8.1982e-01,  1.6870e-01,\n",
      "         4.3164e-01,  3.3301e-01,  1.0779e-01,  1.7773e+00,  4.0210e-01,\n",
      "         3.1226e-01,  2.0293e+00,  8.1641e-01,  2.7856e-01,  3.3521e-01,\n",
      "        -9.0271e-02,  1.7148e+00,  6.0449e-01,  1.6516e-01,  7.3145e-01,\n",
      "        -1.7310e-01, -3.4351e-01,  2.5928e-01,  9.6143e-01, -1.5405e-01,\n",
      "        -2.9761e-01,  1.4551e-01,  6.7139e-01,  6.7627e-01,  2.4768e-01,\n",
      "         1.8542e-01,  1.9424e+00,  7.0117e-01,  2.8101e-01,  4.0430e-01,\n",
      "         4.5361e-01,  5.2051e-01, -1.1908e-01, -3.8971e-02,  1.2070e+00,\n",
      "         2.7405e-02, -7.2021e-01,  5.4077e-02, -3.8940e-01, -2.0056e-01,\n",
      "         2.1523e+00,  7.5049e-01,  2.2278e-01,  2.9932e-01, -3.9337e-02,\n",
      "         1.8379e+00,  6.0742e-01,  1.6785e-01,  5.2539e-01,  4.5679e-01,\n",
      "         6.1230e-01, -3.6572e-01,  9.3945e-01, -1.0254e-01,  4.6899e-01,\n",
      "        -3.2867e-02,  4.1675e-01,  3.2324e-01,  2.9810e-01, -6.6357e-01,\n",
      "         2.3203e+00,  2.9272e-01,  6.8848e-01,  1.0322e+00,  1.6885e+00,\n",
      "         5.2295e-01,  9.4421e-02,  3.5229e-01,  1.0840e+00,  1.1094e+00,\n",
      "         3.9941e-01,  2.1167e-01,  6.3721e-02,  1.5322e+00,  2.9785e-01,\n",
      "         5.2490e-01,  2.1216e-01, -4.5898e-01,  1.9824e-01, -5.5273e-01,\n",
      "        -2.7637e-03, -2.5055e-02,  1.2646e+00, -2.3096e-01,  6.0205e-01,\n",
      "         6.3672e-01,  2.9907e-01,  1.6768e+00,  9.9902e-01,  2.9907e-01,\n",
      "        -4.7546e-02,  6.7383e-01,  2.7051e-01,  9.9365e-02,  5.1270e-03,\n",
      "         3.7939e-01, -4.6234e-03,  8.7256e-01,  4.0039e-02,  7.1924e-01,\n",
      "        -8.5266e-02,  2.7051e-01, -1.5491e-01,  7.6103e-04,  6.6602e-01,\n",
      "        -6.2109e-01,  6.9189e-01,  7.5732e-01, -1.0162e-01, -6.2927e-02,\n",
      "        -7.8003e-02, -1.7578e-02,  8.7354e-01,  3.9697e-01,  5.6274e-02,\n",
      "        -1.3513e-01,  1.0071e-01, -2.7832e-01,  5.6689e-01,  3.2520e-01,\n",
      "         7.3486e-02,  4.4800e-01,  3.4888e-01,  2.4629e+00,  8.5400e-01,\n",
      "         5.7520e-01,  1.7883e-01,  7.9004e-01,  2.0254e+00,  6.5479e-01,\n",
      "         4.7974e-01,  1.0779e-01,  6.2158e-01,  1.1328e+00,  4.5386e-01,\n",
      "         6.6284e-02, -4.2041e-01,  2.2324e+00,  6.7676e-01,  4.5264e-01,\n",
      "         3.1952e-02, -6.9580e-02,  2.1074e+00,  5.1562e-01,  1.6003e-01,\n",
      "         2.9663e-01,  1.6045e+00,  1.2659e-01,  1.5051e-01, -1.9470e-01,\n",
      "        -8.4961e-02,  2.2266e+00,  8.4229e-01,  4.2700e-01,  2.2888e-01,\n",
      "         7.2559e-01, -7.9041e-02,  2.0254e+00,  9.2773e-01,  1.4941e-01,\n",
      "         1.6768e+00,  6.5430e-01,  3.8843e-01, -4.8242e-01,  1.9989e-02,\n",
      "        -2.6660e-01, -8.2336e-02,  2.1113e+00,  3.5889e-01,  2.0538e-02,\n",
      "         3.0762e-02,  1.3008e+00,  3.5498e-01,  1.1096e-01, -1.1975e-01,\n",
      "         2.3828e+00,  4.8608e-01,  3.0493e-01,  3.2471e-01, -7.2327e-03,\n",
      "         2.8223e-01,  3.5547e-01,  9.9609e-01,  1.4512e+00,  4.2676e-01,\n",
      "         6.1890e-02,  1.7334e+00,  3.9160e-01, -9.6375e-02,  1.0107e+00,\n",
      "         1.6416e+00,  4.5801e-01,  5.1514e-01,  1.0020e+00, -2.6685e-01,\n",
      "         1.2490e+00,  2.7130e-02,  2.1250e+00,  5.3320e-01,  5.5969e-02,\n",
      "        -2.4084e-01, -2.5537e-01,  8.3557e-02,  3.7939e-01,  1.6492e-01,\n",
      "        -5.4169e-02, -5.8691e-01, -1.3562e-01, -6.9678e-01,  2.2676e+00,\n",
      "         6.8848e-01,  1.4832e-01,  6.8817e-03,  3.5303e-01,  6.4404e-01,\n",
      "         2.3945e+00,  6.0059e-01,  2.1172e+00,  5.4248e-01, -6.9824e-01,\n",
      "         1.0420e+00, -2.1924e-01,  1.9111e+00, -3.5950e-02, -1.9608e-02,\n",
      "         2.2832e+00,  6.7383e-01,  1.9248e+00,  8.8525e-01,  1.0522e-01,\n",
      "         1.6318e+00,  1.9604e-01,  7.2083e-02, -4.2920e-01,  1.6758e+00,\n",
      "         4.4922e-01, -1.0327e-01, -2.4829e-01,  1.7891e+00,  6.0840e-01,\n",
      "         2.1069e-01,  3.1201e-01,  3.3936e-01,  2.2852e+00,  7.2021e-01,\n",
      "         5.6396e-01,  1.6760e-01,  9.9414e-01,  1.9746e+00,  5.5322e-01,\n",
      "         5.0635e-01,  1.0693e-01,  8.7451e-01,  1.0752e+00,  4.4165e-01,\n",
      "         3.2446e-01, -5.8350e-01,  2.2344e+00,  6.4502e-01,  5.4980e-01,\n",
      "         9.0576e-02,  1.0126e-01,  1.6235e-02,  1.4893e-01, -4.9988e-02,\n",
      "         1.2510e+00, -2.6321e-02,  2.4707e+00,  4.1968e-01,  4.9585e-01,\n",
      "         3.6353e-01, -2.0825e-01,  2.1562e+00,  7.5000e-01,  3.7256e-01,\n",
      "         1.4978e-01, -1.0962e-01,  1.9014e+00,  5.8398e-01,  2.6831e-01,\n",
      "         4.6191e-01, -2.2607e-01,  1.1465e+00,  8.7952e-02,  8.3252e-01,\n",
      "        -1.5649e-01,  2.2148e+00,  7.9346e-01,  4.0869e-01,  1.7346e-01,\n",
      "         6.2500e-01, -3.3521e-01,  1.1914e+00, -2.8458e-02,  2.1816e+00,\n",
      "         1.9214e-01, -2.2815e-01,  2.0684e+00,  4.6484e-01,  5.1221e-01,\n",
      "         9.4910e-02, -4.0863e-02, -2.1448e-01,  2.1465e+00,  6.5039e-01,\n",
      "         2.6123e-01,  4.1284e-01, -9.9976e-02,  1.1836e+00, -1.9446e-01,\n",
      "        -4.3237e-01,  1.1025e+00,  3.3276e-01, -4.2877e-03, -2.3181e-01,\n",
      "        -4.4995e-01, -3.9648e-01, -7.8142e-05,  1.4233e-01, -3.2288e-02,\n",
      "        -1.1548e-01, -7.1240e-01, -2.8809e-01, -7.8711e-01,  2.1250e+00,\n",
      "         6.6992e-01,  1.2439e-01, -6.8420e-02,  2.1997e-01,  3.4985e-01,\n",
      "         1.9502e+00,  6.3818e-01,  5.7959e-01,  8.3203e-01,  1.4026e-01,\n",
      "         5.2686e-01,  2.1545e-01,  1.4883e+00,  6.0938e-01,  4.7437e-01,\n",
      "         7.4072e-01,  8.4961e-02,  4.4849e-01,  1.0327e-01,  1.4287e+00,\n",
      "         4.5532e-01,  1.1494e+00,  5.6738e-01,  4.5312e-01,  7.4268e-01,\n",
      "         2.8149e-01,  7.4414e-01,  6.6211e-01,  3.6792e-01, -7.5098e-01,\n",
      "         5.2979e-01,  2.9614e-01,  2.7905e-01, -1.2054e-01,  3.2324e-01,\n",
      "         2.0435e-01,  3.1909e-01,  9.1846e-01, -3.6230e-01,  1.3936e+00,\n",
      "         5.8350e-01,  4.5801e-01,  7.0996e-01,  8.7769e-02,  1.2129e+00,\n",
      "         6.6260e-01,  2.6953e-01, -1.0077e-01,  1.4150e+00,  5.3369e-01,\n",
      "         7.1533e-02,  6.0742e-01,  2.8174e-01, -3.1201e-01,  8.1592e-01,\n",
      "        -2.1313e-01,  1.7998e+00,  2.1960e-01, -3.8818e-01,  1.9531e+00,\n",
      "         5.3076e-01, -8.0139e-02,  2.7588e-01,  1.3770e+00, -1.5491e-01,\n",
      "         1.5137e+00,  7.2949e-01,  4.9780e-01,  7.3682e-01,  2.4084e-01,\n",
      "         7.3438e-01,  1.1658e-01, -4.6338e-01,  1.6504e-01, -9.2871e-01,\n",
      "         1.9785e+00,  5.8643e-01,  2.6886e-02,  2.3132e-01,  1.3535e+00,\n",
      "         1.0684e+00, -1.3191e-02,  1.6953e+00,  7.5928e-01,  1.4954e-01,\n",
      "         2.8589e-01,  1.1365e-01, -4.3066e-01,  1.0034e-01, -9.0234e-01,\n",
      "         1.5527e+00,  3.0371e-01,  4.6899e-01, -7.4036e-02,  7.7979e-01,\n",
      "         7.9102e-02,  3.6694e-01,  2.2832e+00,  6.4746e-01,  1.9922e+00,\n",
      "         6.6797e-01, -7.1631e-01,  9.7852e-01, -2.8540e-01,  1.8408e+00,\n",
      "        -4.5959e-02,  2.2869e-03,  2.1250e+00,  9.7754e-01,  3.4326e-01,\n",
      "        -3.9722e-01,  1.9287e+00,  3.5425e-01,  2.8979e-01,  6.2939e-01,\n",
      "        -3.4375e-01,  2.0605e+00,  2.7075e-01,  8.3398e-01,  1.9395e+00,\n",
      "         7.1338e-01, -4.9878e-01,  5.7422e-01,  6.8994e-01, -3.0151e-01,\n",
      "         1.8340e+00,  4.8767e-02, -1.4319e-01,  1.5228e-02,  1.6821e-01,\n",
      "         3.5425e-01,  2.2520e+00,  6.8848e-01,  5.1465e-01,  5.6152e-02,\n",
      "         1.2139e+00,  2.6611e-02,  1.9678e+00,  5.1855e-01,  5.4395e-01,\n",
      "         3.5004e-02,  8.8965e-01, -7.8247e-02,  1.6904e+00,  4.7192e-01,\n",
      "         1.4473e+00,  3.8281e-01,  9.1943e-01,  5.5322e-01,  1.6919e-01,\n",
      "         9.7510e-01,  5.1880e-02, -5.5615e-01,  1.0225e+00, -9.1003e-02,\n",
      "        -2.1533e-01,  5.9814e-01, -1.5427e-02, -2.7295e-01,  8.1934e-01,\n",
      "        -2.6172e-01,  1.9150e+00,  2.3938e-01, -3.7769e-01,  2.4941e+00,\n",
      "         5.6494e-01,  5.3711e-01,  7.4561e-01,  1.1035e-01,  4.4482e-01,\n",
      "         4.1113e-01,  4.4971e-01,  1.5518e+00,  5.0781e-02, -1.1456e-01,\n",
      "         1.1462e-01, -7.3145e-01,  1.0034e-01,  1.9355e+00,  4.2798e-01,\n",
      "         4.0674e-01,  2.3401e-01,  1.0469e+00,  6.4331e-02, -1.3763e-02,\n",
      "         1.7715e+00,  3.9551e-01, -5.7129e-01,  1.4990e-01,  1.0718e-01,\n",
      "         4.8950e-02,  1.3782e-01,  7.9980e-01, -5.9082e-01,  9.0771e-01,\n",
      "         2.4695e-01, -8.3084e-03,  1.9307e+00,  2.8320e-01,  1.0977e+00,\n",
      "         9.0393e-02, -5.6250e-01, -1.1604e-02,  2.0762e+00,  4.1968e-01,\n",
      "         1.1748e+00,  1.4014e+00,  9.8047e-01,  4.3018e-01,  7.4268e-01,\n",
      "        -1.3557e-02,  1.1902e-01,  1.6357e+00,  1.3311e+00,  7.8320e-01,\n",
      "         2.1338e-01,  1.9641e-01,  1.9043e+00,  9.6533e-01,  2.8247e-01,\n",
      "         2.9517e-01,  1.2822e+00,  2.2919e-02,  2.2422e+00,  3.9771e-01,\n",
      "         4.9731e-01,  7.3096e-01,  2.9028e-01,  4.3518e-02,  8.0469e-01,\n",
      "         3.1219e-02, -1.0089e-01, -1.9238e-01,  2.2969e+00,  6.0107e-01,\n",
      "         5.1562e-01,  8.3923e-02,  8.3789e-01,  2.5171e-01,  5.3662e-01,\n",
      "         2.2734e+00,  6.5039e-01,  5.3564e-01,  6.8359e-02,  1.0225e+00,\n",
      "         1.9424e+00,  5.5713e-01,  6.1865e-01,  1.5112e-01,  9.1602e-01,\n",
      "         9.1357e-01,  6.5771e-01,  4.4922e-01, -4.4922e-01,  2.2070e+00,\n",
      "         6.3623e-01,  5.3564e-01,  1.1829e-01,  2.2668e-01, -2.8366e-02,\n",
      "         4.8999e-01,  1.0576e+00, -1.7834e-01,  1.8740e+00,  8.0908e-01,\n",
      "         7.0898e-01,  3.1152e-01,  4.4580e-01, -1.0535e-01,  1.7422e+00,\n",
      "         5.7324e-01,  3.8159e-01, -2.3047e-01,  1.6348e+00,  7.1240e-01,\n",
      "         6.8408e-01,  1.2451e-01,  1.7637e+00,  5.0244e-01,  2.0703e-01,\n",
      "        -2.3877e-01,  2.0098e+00,  6.7871e-01,  3.3911e-01,  6.2256e-01,\n",
      "         1.5859e+00,  6.4453e-02,  8.9209e-01, -1.5388e-02, -2.6382e-02,\n",
      "        -2.6245e-01, -2.9956e-01,  6.2695e-01,  2.5439e-01,  1.4678e+00,\n",
      "         1.7295e+00,  6.6504e-01,  7.5867e-02, -3.1885e-01,  5.9131e-01,\n",
      "        -2.0691e-01,  6.7334e-01,  3.6084e-01,  1.7139e-01,  1.3691e+00,\n",
      "         1.6016e+00,  6.5820e-01, -2.5342e-01,  4.7925e-01,  8.1348e-01,\n",
      "         5.5371e-01, -3.1982e-01,  2.0703e-01, -2.7295e-01,  1.1389e-01,\n",
      "         2.3828e+00,  8.8770e-01,  5.9424e-01,  1.7078e-01,  6.8799e-01,\n",
      "         2.2583e-01,  7.5806e-02,  9.8340e-01,  2.2461e-01,  1.3855e-01,\n",
      "         1.6650e+00,  6.3428e-01,  4.6973e-01,  2.9248e-01,  1.2275e+00,\n",
      "         3.2153e-01,  3.3740e-01,  1.8350e+00,  1.3545e+00,  2.5879e-01,\n",
      "         1.4814e+00,  1.1152e+00,  2.0679e-01,  1.7383e+00,  5.3906e-01,\n",
      "         1.0752e+00,  5.3027e-01,  5.5518e-01,  1.5107e+00,  4.4531e-01,\n",
      "        -5.3271e-01,  2.0469e+00,  8.2861e-01,  3.0298e-01,  4.9731e-01,\n",
      "         8.7158e-01, -2.6855e-01,  1.0371e+00,  1.1777e+00,  2.8101e-01,\n",
      "         8.3838e-01,  8.5083e-02,  5.2881e-01,  3.2007e-01, -6.2598e-01,\n",
      "         1.4062e-01,  1.2183e-01, -4.3652e-01,  1.9111e+00,  4.6167e-01,\n",
      "        -3.0591e-01,  5.3613e-01,  5.1855e-01,  5.1562e-01,  6.5283e-01,\n",
      "        -1.6028e-01, -3.7134e-01,  4.3921e-01,  2.1057e-01, -3.4448e-01,\n",
      "         1.0049e+00,  9.6094e-01,  4.5044e-01,  6.3965e-01,  7.9102e-02,\n",
      "         8.6963e-01,  9.3896e-01,  3.3667e-01, -1.4905e-01, -1.6699e-01,\n",
      "         3.0151e-01,  7.3193e-01, -2.6587e-01,  6.6504e-01, -4.8218e-03,\n",
      "         3.5742e-01, -6.0303e-02, -3.7329e-01,  1.8271e+00,  3.5791e-01,\n",
      "        -4.4067e-01,  3.4717e-01, -4.3091e-01, -2.3755e-01,  6.4209e-01,\n",
      "         1.1016e+00,  3.1787e-01,  1.4189e+00,  1.0488e+00,  3.1836e-01,\n",
      "        -3.8843e-01,  5.6592e-01, -2.7100e-01,  2.1021e-01,  3.7012e-01,\n",
      "         4.6143e-01,  5.1611e-01,  6.7676e-01,  7.1484e-01,  7.1973e-01,\n",
      "         8.4180e-01, -1.6248e-01,  7.5342e-01,  1.6370e-01,  3.5742e-01,\n",
      "         2.2988e+00,  6.4990e-01,  5.3955e-01,  4.9286e-03,  1.2373e+00,\n",
      "         1.8467e+00,  5.3906e-01,  5.0049e-01, -4.8553e-02,  9.8633e-01,\n",
      "         6.3672e-01,  3.5498e-01,  6.0547e-01, -6.0547e-01,  2.0742e+00,\n",
      "         6.0938e-01,  4.9780e-01, -7.6477e-02, -2.4548e-01,  2.3164e+00,\n",
      "         8.4375e-01,  3.9111e-01,  9.9548e-02, -2.6025e-01,  1.7021e+00,\n",
      "         5.0049e-01,  9.9670e-02,  6.7041e-01,  1.3115e+00,  6.3416e-02,\n",
      "         9.0027e-03, -4.4849e-01, -9.6008e-02,  2.0000e+00,  7.1338e-01,\n",
      "         6.5137e-01,  9.9731e-02,  7.0850e-01, -2.5562e-01,  1.4512e+00,\n",
      "         3.4448e-01,  2.7075e-01, -3.2056e-01, -4.1687e-02, -3.9429e-01,\n",
      "        -2.1814e-01,  2.0625e+00,  7.1191e-01,  4.1138e-01,  7.8308e-02,\n",
      "         8.4766e-01, -1.5051e-01,  4.6680e-01,  4.2944e-01,  2.8442e-01,\n",
      "        -3.2617e-01,  2.2070e+00,  8.8525e-01,  5.2246e-01,  3.0005e-01,\n",
      "         2.0312e-01, -3.4766e-01,  1.1279e-01,  4.4800e-01,  4.4751e-01,\n",
      "        -2.6947e-02, -3.9404e-01,  2.2363e+00,  8.2861e-01,  2.2168e-01,\n",
      "         1.1934e+00,  9.3506e-01,  5.8643e-01,  3.3960e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.4922, -0.7593,  0.0195,  0.8130, -1.0137,  0.0035,  0.3562,  0.0575,\n",
      "         0.2505, -0.4236, -0.7349, -0.6958,  0.0412, -0.2734, -0.0327,  0.3691,\n",
      "        -0.7998,  0.6514,  0.4634,  0.4648,  0.9258, -0.7339,  0.5820,  0.4207,\n",
      "         0.4783,  0.6714, -0.6445,  0.7026,  0.2925,  0.1348,  0.2000,  0.1378,\n",
      "         0.5229,  0.7964, -0.2406,  0.1575,  0.4670,  0.7979, -0.7842, -0.6089,\n",
      "         0.6333,  0.4358,  0.6069, -0.4937,  0.0735,  0.5332,  0.9385,  0.1382,\n",
      "         0.3716,  0.2915,  0.3972,  1.0703,  0.7407, -0.7456, -0.4470,  0.0831,\n",
      "        -0.3162, -0.6865,  0.3042,  0.8516,  0.7607, -0.5308,  0.7476, -0.2781,\n",
      "         0.1121, -0.4795, -0.5542,  0.5225,  0.4653,  0.3750,  0.4949,  0.0467,\n",
      "        -0.2045,  0.1053,  0.2622,  0.2306,  1.0098, -0.4026,  0.0878,  0.2084,\n",
      "         0.0219,  0.4365,  0.1500, -0.4548,  0.3318,  0.7305,  0.8193, -0.6729,\n",
      "        -0.4124,  0.3901, -0.1318, -0.8872, -0.3723, -0.0617, -0.5083, -0.1841,\n",
      "        -0.2482, -0.4187,  0.1096,  0.9004,  0.1276, -0.3601,  0.9575,  0.0963,\n",
      "        -0.0206,  0.0243, -0.0889, -0.0765,  0.0751,  0.2888,  0.1909,  0.7495,\n",
      "        -0.5488,  0.0240,  0.5005,  0.4270, -0.2308,  0.5615,  1.0527,  0.1417,\n",
      "         0.3831,  0.2837,  0.4219,  1.3838,  0.7812,  0.1088, -0.2693, -0.4712,\n",
      "        -0.0916,  0.4646,  0.3892, -0.7114, -0.5312,  0.1531,  0.2959, -0.3171,\n",
      "         0.1873,  0.0940,  0.3091,  0.2996,  0.3518,  0.4875,  0.2422,  1.2051,\n",
      "        -0.3748,  0.0106, -0.6157,  0.2727, -1.1973, -0.8306, -0.0197,  0.4875,\n",
      "         0.9849,  0.0922,  0.3247,  0.1965,  0.4331,  1.2969,  0.7690, -0.5728,\n",
      "        -0.5190, -0.2866, -0.5181,  0.3894, -0.4614, -0.6152, -0.1105,  0.0214,\n",
      "         0.1742, -0.2350,  0.2629,  1.0762,  0.8262, -1.2568, -0.0340,  0.1947,\n",
      "         0.1155,  0.9033, -0.9116, -0.0178,  0.2729,  0.7173, -0.8315,  0.2224,\n",
      "         0.5078,  0.8530,  0.0099, -0.5688,  0.2617, -1.1582, -0.2019, -0.4617,\n",
      "        -0.1785, -0.7886,  0.4893, -0.4507, -0.6914, -0.2362,  0.0754,  0.0794,\n",
      "        -0.4995, -0.3784,  0.1934, -0.4172, -0.2347,  0.0258, -0.3091, -0.7759,\n",
      "        -0.7251, -0.0835,  0.5342, -0.2178, -0.3938, -0.5859, -0.1715, -0.4619,\n",
      "        -0.4810, -0.3372, -0.9258, -0.4573, -0.4368, -0.6880, -0.1528,  0.0747,\n",
      "         0.0141, -0.3496, -0.1469, -1.0352, -0.1692, -0.0472, -0.0435, -0.8110,\n",
      "         0.4182, -0.8560,  0.6030,  0.4023,  0.5381,  1.0938, -0.7773,  0.5791,\n",
      "         0.4712,  0.5547,  0.7500,  0.1345,  0.4448,  1.3369, -0.9741, -0.7739,\n",
      "         0.5278,  0.4663,  0.6074, -0.4373,  0.0731,  0.5986,  1.5889,  0.9624,\n",
      "        -0.5146,  0.7354,  0.3250,  0.0371, -0.8950, -0.5723,  0.4038,  0.4143,\n",
      "         0.1921,  0.7368, -0.6182,  0.2335,  0.4841,  0.9565, -0.1207,  0.6357,\n",
      "         0.9463, -0.6113, -0.4912,  0.1001, -0.8667, -0.4097,  0.1481,  0.9678,\n",
      "         0.1787, -0.3450,  0.8193,  0.1508, -0.7690, -0.0374,  0.4771,  0.9307,\n",
      "         0.3069, -0.0385, -0.3621, -0.3118,  0.0283, -0.2233,  0.8145, -0.6963,\n",
      "         0.1398,  0.9673, -0.4470,  0.1133,  0.3369,  1.2061, -0.2019, -0.2512,\n",
      "        -0.3362,  0.0464,  0.0477,  0.0922,  0.4924,  1.0742, -0.5938, -0.2727,\n",
      "         0.4060, -0.1993,  0.3965,  0.3757, -0.6924, -0.2230, -0.8403, -0.5337,\n",
      "         0.5347,  0.4395,  0.4619, -0.8086, -0.4333, -0.0718,  0.9229, -0.0765,\n",
      "         0.7256, -0.8447, -0.1445, -0.7378, -0.3220,  0.6509, -0.0906, -0.2793,\n",
      "         1.1748, -0.5342,  0.2808,  1.2227,  0.0799,  0.2874,  0.5186,  0.4688,\n",
      "        -0.2340,  0.2463,  0.8306, -0.3479,  0.0953,  0.4954,  1.3057, -0.3962,\n",
      "         0.4036, -0.9214,  0.6089,  0.2859,  0.5967,  0.8369, -0.6963,  0.5879,\n",
      "         0.4041,  0.6104,  0.7305,  0.1587,  0.4285,  0.6548, -0.9517, -0.6597,\n",
      "         0.5840,  0.4343,  0.5391, -0.7935,  0.1038, -0.7515, -0.3494,  0.1289,\n",
      "        -0.0266, -0.0350,  0.2092,  0.3384,  0.9873, -0.3127,  0.0307,  0.3508,\n",
      "         1.2344,  0.2803, -0.3013,  0.0519,  0.5474,  1.3076,  0.6694, -0.3103,\n",
      "         0.3643, -0.5288,  0.0417, -0.2617, -0.5923,  0.3718,  0.4924,  0.3845,\n",
      "         0.7295, -0.2935,  0.2498,  0.0898,  0.0106,  0.7251, -0.2000, -0.0852,\n",
      "         0.0370,  0.6987,  0.9927,  0.4751, -0.3430,  0.0991,  0.6387,  1.4082,\n",
      "         0.6743, -0.4260,  0.5742,  0.2861, -0.8794, -0.3252,  0.1949,  0.4185,\n",
      "         0.0505, -0.5938, -0.2462,  0.3425, -0.1016,  0.4814,  0.5117, -0.6587,\n",
      "        -0.1387, -0.8262, -0.6426,  0.4692,  0.4429,  0.5322, -0.8071,  0.4231,\n",
      "        -0.4670,  0.2539,  0.1632,  0.5151,  1.1611,  0.0372,  0.3848, -0.5454,\n",
      "         0.2366,  0.1818,  0.5063,  0.9927,  0.2827,  0.4104, -0.5581,  0.6172,\n",
      "        -0.2102,  0.2286,  0.1628,  0.4766,  0.5771, -0.2396,  0.1636,  1.2871,\n",
      "        -0.9282, -0.1251, -0.1060,  0.0379, -0.8613, -0.6709,  0.0207, -0.1044,\n",
      "         0.1063, -0.1454, -0.1246,  0.1694,  0.1666,  0.5732,  1.0898,  0.1241,\n",
      "         0.4041,  1.0479, -0.4678, -0.0715,  0.2983,  0.5903,  0.5771,  1.1553,\n",
      "        -0.2542,  0.2590,  0.1724,  0.3113,  1.1953, -0.1318,  0.2607,  0.5220,\n",
      "         1.3027,  0.6890, -0.1357,  1.0332, -0.5132,  0.2014,  0.1860,  0.5698,\n",
      "         1.0293,  0.5581, -0.6934, -0.7339, -0.4980, -0.4937, -0.6714,  0.1257,\n",
      "         0.8555,  0.1123,  0.0752,  0.6382, -0.8682,  0.0889,  0.3809,  1.3018,\n",
      "         0.4712, -0.6396, -0.7549, -0.2725, -0.4602, -0.6255,  0.7163,  0.5669,\n",
      "         0.6851,  1.3154, -0.6328,  0.4387, -0.1102,  1.4902, -0.0878,  1.2266,\n",
      "        -1.0098, -0.0322, -0.7080, -0.2917,  0.6182, -0.1153, -0.3904,  0.2593,\n",
      "         1.1523, -0.5249,  0.1074,  0.7471, -0.5557, -0.1300, -0.0765, -0.0832,\n",
      "         0.7021,  0.2365, -0.0950,  1.1182, -1.0840,  0.1003, -0.1450, -0.8281,\n",
      "        -0.4014,  0.7256, -0.1216, -0.3730, -0.8364,  0.4221, -0.9253,  0.6099,\n",
      "         0.3894,  0.7090,  0.1250,  1.2812, -0.8081,  0.5640,  0.4719,  0.6665,\n",
      "         0.1083,  1.0088, -0.5518,  0.7651,  0.2905,  0.0905,  0.1973,  0.0775,\n",
      "         0.7153,  0.1741,  1.0996, -1.0205, -0.1004,  0.6250,  0.2329,  0.1298,\n",
      "         0.5518, -0.5791,  0.1470,  0.0986,  0.0604,  1.0166, -0.2391,  0.0867,\n",
      "         0.1498,  0.2249,  0.5552,  0.5610,  0.3013,  0.7954,  0.6851, -0.1980,\n",
      "         1.2324, -0.9526, -0.0767, -0.0505, -0.6626, -0.4583,  0.0068,  0.7139,\n",
      "         0.0184, -0.1908,  0.6675, -0.7266, -0.3159,  0.4087, -0.8901,  0.2034,\n",
      "        -0.7090, -0.6172, -0.5088, -0.1349, -0.5732,  0.1175,  0.3701, -0.1530,\n",
      "         0.0109,  0.9360, -0.1785,  0.2488, -0.2352, -0.9780, -0.1815,  0.6826,\n",
      "        -0.8818,  0.0850,  0.0765,  1.0645,  0.3831, -0.1869, -0.7021,  0.1521,\n",
      "         0.0146,  0.1592,  0.8203, -0.7031, -0.0598, -0.0459,  0.9233, -0.0094,\n",
      "         0.0744,  0.0223,  0.2642, -0.0265,  0.5000,  0.5444,  1.3057, -0.6069,\n",
      "         0.3928,  0.4641,  0.3579, -0.7500, -0.4534,  0.6812,  0.5347,  0.6021,\n",
      "         0.9834, -0.6377, -0.2230, -0.9634,  0.6289,  0.3916,  0.6035,  0.7012,\n",
      "        -0.6157,  0.5801,  0.5410,  0.5908,  0.3130,  0.1569,  0.2976,  0.4089,\n",
      "        -1.0576, -0.4902,  0.6895,  0.4766,  0.5430, -0.8315, -0.1497, -0.6528,\n",
      "         0.1102, -0.1544, -0.1017,  0.2361,  0.1065,  0.2153,  0.6807, -0.5630,\n",
      "        -0.2438,  0.0363,  0.5479, -0.3855,  0.0696,  0.4639, -0.2040,  0.1766,\n",
      "         0.1416,  1.1494,  0.4470, -0.2014,  0.2764,  0.3862,  1.6338,  0.9761,\n",
      "        -0.0792,  1.3623, -0.1088,  0.4739,  0.0441,  0.1654, -0.9272,  0.0555,\n",
      "        -0.2651, -0.0696, -0.4629,  0.8301, -0.9058, -1.0098, -0.6387, -0.5342,\n",
      "        -0.4829,  0.2235, -0.4231, -0.1240, -0.4644,  0.5342, -0.4373,  0.2227,\n",
      "        -0.1908,  0.1588, -0.4553,  0.4275, -0.2021, -0.8525, -0.4268,  0.5024,\n",
      "         0.4385,  0.4517,  0.5098, -0.8374, -0.1714, -0.3613, -0.1070, -0.5845,\n",
      "         0.0253,  0.2678,  0.1508,  1.3867,  0.4878, -0.5996,  0.3877, -0.6943,\n",
      "         0.0258,  1.2480, -0.8745,  0.0275,  1.1738, -0.6045,  0.6841,  0.2213,\n",
      "        -0.0495,  0.5142,  0.2869,  0.9014, -1.1279, -0.5830,  0.3931, -0.5703,\n",
      "        -0.7720, -0.4072, -0.3889, -0.1234, -0.0573,  0.7651, -0.8530, -0.1415,\n",
      "        -0.2996, -0.0302, -0.9546, -0.5830, -0.9365, -0.7793, -0.1455,  0.4763,\n",
      "        -0.7686, -0.6958, -0.4897, -0.2629, -0.1234, -0.1708, -0.7422, -0.1686,\n",
      "        -0.3176, -0.6582, -0.5361, -0.1252,  0.7510, -0.4014, -0.2020, -0.5459,\n",
      "        -0.1394,  0.5850,  0.0216, -0.6802, -0.4932, -0.5337,  0.0768,  0.0054,\n",
      "         0.2539,  0.0257, -0.3586, -0.6792,  0.0579,  0.7285, -0.9512, -0.4333,\n",
      "        -0.9702, -0.9819, -0.1143,  0.0756,  1.0752, -0.7319, -0.0267,  0.9585,\n",
      "        -0.1737, -0.4094, -0.4492, -0.0598, -0.2350, -0.1642, -0.7593, -0.3967,\n",
      "         0.1486, -0.3555, -0.2566, -0.7515,  0.0842, -0.6372,  0.4192, -0.8496,\n",
      "         0.4846,  0.4534,  0.6758,  0.7139, -0.4863,  0.5410,  0.5347,  0.7310,\n",
      "         0.5679,  0.3892,  0.3889,  0.6128, -1.0400, -0.4404,  0.5938,  0.4365,\n",
      "         0.6406, -0.4448,  0.1277,  0.2793,  1.3857,  0.6548, -0.1803,  0.3320,\n",
      "         0.2781,  1.3525,  0.6396, -0.1858,  0.4834,  0.4963,  0.2151, -0.8779,\n",
      "        -0.5659,  0.3489,  0.3318,  0.4099,  0.6924, -0.9199,  0.0881,  0.2054,\n",
      "         0.6333, -0.5894, -0.5635,  0.1517, -0.7461, -0.5171,  0.3970,  0.4460,\n",
      "         0.3403,  0.6646, -0.6592, -0.2274, -0.0339,  0.2671, -0.7778,  0.1538,\n",
      "         0.0091,  0.3538,  1.0791,  0.4434, -0.2617, -0.5713,  0.3259,  0.0958,\n",
      "        -0.5234, -0.7573,  0.0981,  0.2357,  1.3877, -0.1857,  0.2469,  1.1504,\n",
      "         0.3950], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8341, -0.0224,  0.1185,  ...,  1.1876,  1.0843,  0.0904],\n",
      "         [ 0.4189,  0.4784,  0.1564,  ...,  1.1481,  0.4921, -0.0880],\n",
      "         [ 0.8769,  0.0650,  0.2238,  ...,  0.6834,  1.0315, -0.1785],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 5.5615e-01,  8.7988e-01,  4.6045e-01,  5.7812e-01, -2.1570e-01,\n",
      "        -5.5389e-03, -3.9648e-01,  7.5195e-01, -1.5747e-01, -3.3081e-01,\n",
      "        -3.5815e-01,  6.4880e-02,  2.1851e-01, -6.0303e-01,  8.9893e-01,\n",
      "        -4.1309e-01,  1.0596e+00,  4.4702e-01,  4.8462e-02,  2.3645e-01,\n",
      "        -5.7666e-01,  2.6318e-01, -3.0933e-01, -2.2510e-01,  5.4150e-01,\n",
      "        -3.6304e-01,  1.8350e+00,  1.5137e+00,  4.8804e-01,  5.2832e-01,\n",
      "         7.1045e-02,  4.1479e-01,  3.3228e-01,  1.1562e+00,  6.4844e-01,\n",
      "        -5.0690e-02,  3.5229e-01,  1.0967e+00,  4.8706e-01, -8.9111e-02,\n",
      "         4.6753e-01, -8.9062e-01,  5.1611e-01, -5.0928e-01,  6.1230e-01,\n",
      "        -6.0156e-01,  6.2439e-02,  4.4824e-01,  5.0098e-01, -3.5400e-01,\n",
      "         5.2197e-01,  4.1504e-01, -1.9495e-01,  3.1226e-01, -7.1716e-03,\n",
      "         7.8857e-02, -3.9355e-01, -1.3391e-01,  2.3352e-01, -6.2891e-01,\n",
      "         8.0713e-01,  9.7119e-01,  5.6055e-01,  1.6235e-01, -3.8208e-01,\n",
      "         4.4849e-01, -4.2310e-01,  1.7834e-01,  7.1191e-01, -3.8916e-01,\n",
      "         8.0225e-01,  1.0425e-01, -6.2109e-01,  6.9971e-01, -2.9810e-01,\n",
      "         3.0078e-01, -7.6221e-01,  1.0684e+00, -5.2002e-01,  9.3018e-01,\n",
      "        -2.2476e-02,  2.1838e-01, -2.9248e-01,  7.6221e-01, -3.2397e-01,\n",
      "         1.4661e-01,  2.3059e-01,  3.0737e-01, -4.8315e-01,  1.1660e+00,\n",
      "         5.7129e-01,  3.9062e-02,  8.6328e-01, -1.5759e-01,  1.9678e+00,\n",
      "         6.4502e-01,  1.3242e+00,  7.3730e-01, -1.3916e-02, -1.4038e-01,\n",
      "         7.3584e-01,  3.7036e-01, -9.6863e-02,  5.0977e-01,  4.3994e-01,\n",
      "         3.3228e-01, -4.8364e-01,  3.4375e-01,  2.5977e-01,  9.9487e-02,\n",
      "        -6.7920e-01,  1.2354e+00,  5.4932e-01, -1.4389e-02,  5.2051e-01,\n",
      "        -3.8989e-01,  7.9736e-01, -2.3999e-01,  4.4336e-01,  9.8206e-02,\n",
      "         8.3008e-01,  3.4766e-01,  4.0137e-01, -1.8127e-01,  4.7217e-01,\n",
      "         7.8760e-01,  2.3645e-01,  4.6167e-01,  3.8428e-01, -4.4849e-01,\n",
      "         9.3408e-01,  3.2471e-01,  2.6172e-01,  2.8973e-03,  1.4307e+00,\n",
      "         7.3535e-01,  1.2097e-01,  5.1855e-01, -6.9824e-01,  4.1382e-01,\n",
      "        -9.2822e-01,  5.0293e-01,  9.1357e-01, -9.3445e-02,  1.0376e-01,\n",
      "        -3.5449e-01, -1.2085e-02, -3.7671e-01,  3.3936e-01,  4.1235e-01,\n",
      "         4.2554e-01,  3.3984e-01,  8.6084e-01, -6.1182e-01,  1.1973e+00,\n",
      "         7.5781e-01,  2.1719e+00,  4.0137e-01, -9.1431e-02,  3.6108e-01,\n",
      "        -2.5562e-01,  2.6538e-01,  1.2817e-01, -7.9980e-01,  4.3030e-02,\n",
      "        -1.6345e-01,  3.5571e-01,  8.7695e-01, -3.7061e-01,  1.2432e+00,\n",
      "         5.6543e-01,  1.8916e+00,  1.3660e-01,  8.5400e-01, -4.0576e-01,\n",
      "        -3.9478e-01,  1.1621e+00, -3.5693e-01,  4.4263e-01, -5.2002e-01,\n",
      "         2.6855e-01,  1.7407e-01, -9.9304e-02,  4.3579e-01, -3.7750e-02,\n",
      "        -6.0156e-01,  2.5723e+00,  7.2510e-01,  7.6270e-01,  1.2686e+00,\n",
      "         4.2041e-01,  2.3450e-01,  4.2407e-01, -2.3059e-01, -3.4497e-01,\n",
      "         9.0771e-01, -1.9028e-02,  9.1797e-01, -3.4009e-01,  2.8760e-01,\n",
      "         3.4912e-02, -3.1519e-01,  5.5029e-01, -4.0161e-01, -4.6021e-01,\n",
      "         3.6646e-01, -3.0005e-01,  1.3408e+00,  8.2861e-01,  9.1504e-01,\n",
      "         5.2686e-01, -3.4729e-02,  5.5371e-01,  7.0996e-01,  2.3457e+00,\n",
      "         1.4932e+00,  1.1602e+00,  7.7734e-01,  2.5269e-01,  1.0205e+00,\n",
      "         6.7725e-01,  1.3213e+00,  4.0283e-01,  2.1660e+00,  1.3213e+00,\n",
      "         1.0674e+00,  6.9873e-01,  1.4136e-01,  9.9121e-01,  5.9033e-01,\n",
      "         1.4092e+00,  5.3467e-01, -5.8008e-01,  4.5386e-01,  1.4521e+00,\n",
      "         4.9146e-01,  2.9028e-01, -3.2837e-01,  2.4160e+00,  4.8999e-01,\n",
      "         9.7754e-01,  2.4727e+00,  1.9004e+00,  4.6216e-01,  9.4434e-01,\n",
      "        -6.8359e-01, -1.1853e-01,  5.8716e-02,  6.7090e-01, -6.4893e-01,\n",
      "         9.0723e-01, -4.6802e-01,  1.0693e+00,  4.2749e-01, -1.3916e-01,\n",
      "         2.5806e-01, -6.4014e-01,  2.9541e-01, -2.8613e-01, -3.7769e-01,\n",
      "         6.0938e-01, -4.4507e-01, -3.7305e-01,  1.0342e+00,  4.1821e-01,\n",
      "         6.8359e-01,  8.6426e-01,  3.1201e-01,  1.3984e+00,  1.2637e+00,\n",
      "         9.8730e-01, -6.1920e-02,  5.5908e-01, -2.4255e-01, -1.0291e-01,\n",
      "        -1.1328e-01,  1.6199e-01,  4.9243e-01,  3.6523e-01,  7.0312e-02,\n",
      "        -3.1006e-01,  9.4678e-01, -2.6947e-02,  2.6440e-01,  6.0645e-01,\n",
      "         4.4238e-01, -4.6655e-01,  4.0771e-01, -1.0474e-01,  2.4719e-01,\n",
      "         9.6045e-01, -2.5977e-01,  6.9238e-01,  1.5869e-01, -1.2000e-01,\n",
      "         3.2568e-01, -5.8252e-01,  9.1162e-01,  4.5874e-01, -2.9395e-01,\n",
      "         6.8311e-01, -4.5319e-03, -2.2705e-02, -7.7734e-01,  1.7615e-01,\n",
      "         5.7190e-02,  4.2310e-01,  7.1191e-01,  1.7217e+00,  5.9131e-01,\n",
      "         2.0000e+00,  1.1416e+00,  1.6895e-01,  2.1960e-01,  1.0723e+00,\n",
      "         5.7227e-01, -1.8323e-01,  5.0928e-01,  2.7319e-01, -3.5938e-01,\n",
      "        -4.7729e-01,  1.5469e+00, -2.3254e-02,  2.0918e+00,  9.6582e-01,\n",
      "         5.7910e-01,  1.1230e+00,  2.2913e-01,  1.2617e+00,  7.2070e-01,\n",
      "         2.7271e-01, -2.5610e-01,  6.4453e-01,  3.0542e-01,  4.1992e-01,\n",
      "         3.7866e-01,  2.0742e+00,  9.6094e-01,  1.9980e+00,  4.5972e-01,\n",
      "         1.2812e+00,  7.4756e-01, -2.5562e-01,  1.8762e-01,  4.9146e-01,\n",
      "         1.4092e+00,  1.7070e+00,  1.1035e+00,  2.1582e+00,  7.4902e-01,\n",
      "         1.9739e-01,  1.0059e+00,  1.5430e+00,  3.7988e-01,  9.9316e-01,\n",
      "         5.8594e-01,  7.4316e-01, -9.9023e-01,  2.9297e-01, -5.1422e-02,\n",
      "        -9.3628e-02, -7.6855e-01,  3.1616e-01, -3.2080e-01, -2.8662e-01,\n",
      "         1.0352e+00,  2.2363e+00,  8.5205e-01,  7.0557e-01,  1.0352e+00,\n",
      "         1.9434e+00,  5.0244e-01,  7.7148e-01,  1.8574e+00,  6.8018e-01,\n",
      "         5.6104e-01,  1.9177e-01,  5.0476e-02,  8.4473e-01, -2.6245e-01,\n",
      "         1.0303e+00,  4.6143e-02,  1.5881e-01,  3.5596e-01,  3.7549e-01,\n",
      "         1.1094e+00,  3.5919e-02,  1.7139e+00,  1.1543e+00,  1.1804e-01,\n",
      "         1.8848e+00, -6.6553e-01,  2.5220e-01, -3.0005e-01,  9.0039e-01,\n",
      "         5.3613e-01,  2.2441e+00,  4.7778e-01, -3.0811e-01, -3.2544e-01,\n",
      "         2.4766e+00,  1.6201e+00,  4.5874e-01, -6.3916e-01,  2.7759e-01,\n",
      "        -4.3945e-02, -2.1680e-01,  8.1787e-01, -4.8657e-01,  2.1895e+00,\n",
      "         1.2705e+00,  1.1123e+00,  5.8691e-01,  2.2046e-01,  7.3438e-01,\n",
      "         5.1562e-01,  1.3184e+00,  3.0444e-01,  3.7988e-01,  3.9209e-01,\n",
      "         2.2793e+00,  1.3477e+00,  3.2495e-01,  2.3379e+00,  1.3369e+00,\n",
      "         4.5752e-01,  6.6992e-01,  2.2852e+00,  4.8364e-01,  4.5264e-01,\n",
      "         3.2935e-01,  9.0479e-01, -4.8242e-01,  2.3203e+00,  4.7607e-01,\n",
      "         9.8340e-01, -3.8257e-01,  8.4180e-01,  1.0498e-01, -4.4116e-01,\n",
      "        -2.0008e-03,  9.2822e-01,  2.0723e+00,  1.2949e+00,  1.0117e+00,\n",
      "         5.6494e-01,  1.8677e-01,  7.0801e-01,  4.1089e-01,  1.2705e+00,\n",
      "         3.2056e-01,  3.1714e-01,  3.9038e-01,  2.3965e+00,  6.5625e-01,\n",
      "         4.2603e-01,  9.4141e-01,  3.4595e-01,  5.1709e-01,  8.5889e-01,\n",
      "         5.4492e-01,  1.7305e+00,  2.5537e-01,  4.9731e-01, -3.1030e-01,\n",
      "        -2.8976e-02, -5.0018e-02,  9.0430e-01, -3.6890e-01,  6.2891e-01,\n",
      "         2.4622e-01,  4.2627e-01,  3.2275e-01,  5.3906e-01,  1.2246e+00,\n",
      "         4.3555e-01,  1.6240e+00,  1.2231e-01, -3.6279e-01,  1.9180e+00,\n",
      "         6.0645e-01,  2.7686e-01,  2.6880e-01,  2.4688e+00,  2.7588e-01,\n",
      "         1.2969e+00, -8.3801e-02,  2.4961e+00,  1.1328e+00,  8.4863e-01,\n",
      "         6.1426e-01,  5.2930e-01,  1.0928e+00,  1.2012e+00,  1.2080e+00,\n",
      "         3.9575e-01,  7.4072e-01,  1.2412e+00, -4.5532e-01,  1.3486e+00,\n",
      "        -2.2324e-02,  9.1455e-01, -9.9304e-02,  4.3628e-01,  2.4355e+00,\n",
      "         6.9287e-01,  7.6807e-01,  2.0488e+00,  8.0957e-01,  8.4778e-02,\n",
      "         9.1602e-01,  5.5225e-01,  5.8838e-01, -2.3120e-01,  6.1426e-01,\n",
      "        -5.4443e-01, -1.1633e-01, -5.8154e-01, -1.1859e-01, -4.6973e-01,\n",
      "        -2.3169e-01,  9.1187e-02,  6.8787e-02, -2.5757e-01,  1.0938e-01,\n",
      "        -3.7988e-01,  6.8994e-01, -6.1670e-01,  2.5921e-03, -1.5540e-01,\n",
      "         3.3691e-01, -2.1118e-01, -1.8689e-01, -4.3042e-01, -2.5098e-01,\n",
      "        -3.2764e-01, -9.3811e-02,  3.2715e-01,  1.3535e+00,  5.5469e-01,\n",
      "         8.1787e-01, -5.0879e-01,  1.7871e-01, -1.0765e-02, -5.9229e-01,\n",
      "         3.1934e-01,  3.0249e-01,  1.3379e-01,  8.8770e-01, -3.2837e-02,\n",
      "         6.8994e-01,  3.9258e-01,  1.9360e-01, -8.5388e-02,  1.9971e-01,\n",
      "         1.0752e+00,  1.1450e-01,  4.9878e-01, -1.2103e-01, -4.4849e-01,\n",
      "         4.6802e-01, -1.2817e-01, -4.9170e-01,  6.7810e-02,  1.6064e-01,\n",
      "        -3.0078e-01,  4.3726e-01,  1.1943e+00,  2.3125e+00,  7.5879e-01,\n",
      "         1.2070e+00,  2.1309e+00,  7.3438e-01, -3.5986e-01,  1.0176e+00,\n",
      "         7.6904e-01,  1.0469e+00,  9.1406e-01, -5.8252e-01,  1.7590e-01,\n",
      "        -7.2205e-02,  2.3516e+00,  1.4180e+00,  1.1943e+00,  6.7920e-01,\n",
      "         3.8086e-01,  9.3408e-01,  6.0938e-01,  1.3867e+00,  5.5273e-01,\n",
      "        -3.8989e-01, -1.1810e-01,  2.5742e+00,  1.6670e+00,  5.3271e-01,\n",
      "         9.5996e-01, -4.0674e-01,  2.7026e-01,  1.0779e-01, -1.5857e-01,\n",
      "         4.9243e-01, -4.6021e-01,  6.3477e-01,  2.5840e+00,  9.0820e-01,\n",
      "         8.8721e-01,  1.8250e-01,  1.3232e+00,  4.3066e-01,  6.7578e-01,\n",
      "        -2.2003e-02,  2.0293e+00,  1.4482e+00,  1.1680e+00,  6.3379e-01,\n",
      "         2.1265e-01,  4.5068e-01,  1.5654e+00,  8.4814e-01, -1.5381e-01,\n",
      "         1.5322e+00,  1.9600e+00,  7.9053e-01,  1.3730e+00,  1.4482e+00,\n",
      "         7.2168e-01,  3.7549e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 6.7822e-01, -1.2213e-01,  8.0566e-02,  1.6821e-01, -1.8967e-02,\n",
      "         6.0425e-02, -5.7031e-01, -2.8052e-01, -3.8770e-01, -8.8574e-01,\n",
      "        -3.1787e-01, -6.4111e-01, -2.9443e-01, -4.5435e-01, -5.2948e-03,\n",
      "        -7.3975e-01, -9.7217e-01,  4.1040e-01,  4.5068e-01,  5.5725e-02,\n",
      "        -4.5459e-01,  5.6787e-01, -7.9895e-02, -6.4600e-01,  2.6025e-01,\n",
      "        -3.7720e-01,  3.3594e-01,  1.4575e-01,  7.0361e-01, -6.0010e-01,\n",
      "         2.4731e-01, -4.5410e-01,  5.0049e-01, -7.4951e-01,  4.7852e-01,\n",
      "         5.2197e-01,  5.8691e-01, -9.2627e-01,  5.2246e-01,  4.7876e-01,\n",
      "         4.7241e-01, -8.8672e-01,  2.1118e-01, -1.8713e-01,  1.5039e-01,\n",
      "        -5.1416e-01, -1.6052e-01, -6.2683e-02,  2.3926e-01, -6.6553e-01,\n",
      "        -2.6025e-01,  5.8441e-02, -5.3125e-01,  7.0618e-02, -3.8513e-02,\n",
      "        -4.4556e-01, -1.4148e-01, -7.9498e-03, -6.8604e-01, -4.6973e-01,\n",
      "        -3.4546e-01, -9.8877e-01,  4.0430e-01,  4.8584e-01, -9.7021e-01,\n",
      "        -7.0020e-01, -4.1577e-01, -4.8193e-01, -3.5767e-01, -5.5566e-01,\n",
      "         9.7656e-03, -7.4805e-01, -4.6973e-01, -5.2734e-01, -8.9160e-01,\n",
      "        -1.0391e-02, -3.3765e-01,  1.6528e-01, -5.4053e-01, -5.1416e-01,\n",
      "        -8.4534e-02, -3.5278e-01, -4.6582e-01, -7.9004e-01, -7.2266e-01,\n",
      "        -1.1652e-01, -3.1152e-01, -5.0244e-01, -5.0098e-01, -1.1289e+00,\n",
      "         4.0161e-01,  4.7095e-01, -3.7769e-01, -4.3066e-01,  6.6504e-01,\n",
      "         1.2568e+00,  1.2402e+00,  9.5459e-01, -3.2690e-01, -3.0908e-01,\n",
      "        -6.1572e-01,  1.5100e-01, -5.4004e-01, -5.5176e-01, -1.0553e-01,\n",
      "        -4.0796e-01, -2.2568e-02, -3.8379e-01, -6.6833e-02, -4.7949e-01,\n",
      "        -4.8535e-01, -1.0098e+00,  4.6680e-01,  6.0449e-01,  2.6758e-01,\n",
      "        -1.0879e+00, -8.2855e-03, -5.4541e-01,  5.2930e-01,  4.3237e-01,\n",
      "        -3.5791e-01,  4.0894e-01,  7.0007e-02, -5.2539e-01,  3.7500e-01,\n",
      "        -3.2959e-02,  1.9666e-01,  9.5947e-02,  3.3203e-02, -1.0225e+00,\n",
      "        -1.5356e-01,  2.2961e-01,  6.5430e-01,  1.7981e-01, -9.6387e-01,\n",
      "         3.8867e-01,  5.0586e-01,  2.2949e-01, -9.7021e-01, -1.7786e-01,\n",
      "        -5.0293e-01, -1.2164e-01, -1.0269e-02, -6.0889e-01, -2.5317e-01,\n",
      "        -7.3853e-02, -2.5391e-01, -9.3164e-01, -3.1470e-01, -2.8784e-01,\n",
      "        -8.2617e-01,  4.8413e-01, -5.2588e-01, -2.6831e-01,  2.6709e-01,\n",
      "         7.9248e-01,  4.2041e-01,  1.3926e+00,  3.0542e-01, -5.5713e-01,\n",
      "        -5.9912e-01, -9.7351e-02, -2.5049e-01, -6.5723e-01,  1.2250e-01,\n",
      "        -2.2302e-01, -1.8335e-01,  1.4233e-01, -2.1460e-01,  2.1338e-01,\n",
      "         8.9648e-01,  1.8115e-01,  1.2041e+00,  2.2021e-01, -5.0098e-01,\n",
      "        -9.0088e-01,  6.9971e-01, -6.6406e-01,  9.2920e-01, -9.0430e-01,\n",
      "        -3.2446e-01,  2.9980e-01, -1.5564e-01,  5.1483e-02, -5.3711e-01,\n",
      "        -5.5518e-01, -1.7737e-01,  2.2449e-01,  1.5820e+00,  2.7441e-01,\n",
      "         3.2275e-01,  1.4961e+00,  7.8418e-01,  3.2568e-01, -9.0918e-01,\n",
      "         1.0944e-01, -7.6953e-01,  3.2690e-01, -7.2607e-01,  6.5625e-01,\n",
      "        -3.5706e-02, -7.6270e-01, -3.0151e-01,  2.4268e-01, -8.9600e-01,\n",
      "         1.3635e-01, -2.8491e-01, -1.5088e-01,  3.1201e-01,  1.5588e-01,\n",
      "         1.8250e-02, -4.0955e-02, -7.4414e-01, -2.7295e-01, -2.0398e-01,\n",
      "         5.7178e-01,  1.4075e-01,  9.0137e-01,  1.9219e+00,  7.7783e-01,\n",
      "        -3.3862e-01, -2.1838e-01,  1.2422e+00, -1.1298e-01,  5.7178e-01,\n",
      "         4.3182e-02,  8.0762e-01,  2.0547e+00,  8.4521e-01, -4.0845e-01,\n",
      "        -1.2152e-01,  1.2061e+00, -1.0986e+00,  5.7739e-02,  4.0063e-01,\n",
      "         4.6631e-01,  5.9375e-01, -8.5571e-02,  5.4590e-01,  2.2031e+00,\n",
      "         5.8838e-01,  5.2490e-01,  3.3374e-01,  1.7148e+00, -2.1985e-01,\n",
      "        -9.3994e-01, -4.2432e-01, -7.0850e-01, -5.1758e-01, -4.7461e-01,\n",
      "        -5.6445e-01, -7.6953e-01, -9.7900e-01,  5.0879e-01,  7.1094e-01,\n",
      "         2.0508e-01, -6.2158e-01,  5.8887e-01,  1.0461e-01, -4.7705e-01,\n",
      "         5.6885e-01, -1.4734e-01, -3.9233e-01,  5.4688e-01, -8.2910e-01,\n",
      "        -4.8438e-01, -3.0838e-02,  4.7046e-01,  4.2676e-01,  5.0000e-01,\n",
      "         3.3179e-01,  5.2637e-01, -1.6418e-01,  4.6191e-01, -2.1484e-01,\n",
      "        -6.6162e-01, -2.1350e-01, -1.1786e-01,  1.0394e-01, -3.8159e-01,\n",
      "        -8.9404e-01, -5.5518e-01,  2.0276e-01, -1.7957e-01, -5.4932e-01,\n",
      "        -1.1334e-01, -5.9131e-01,  5.1361e-02, -8.3496e-02, -1.1920e-01,\n",
      "        -6.0596e-01,  2.8247e-01, -6.3232e-01,  9.6252e-02,  5.7159e-02,\n",
      "        -7.5586e-01, -9.0869e-01,  2.8824e-02,  3.4637e-02, -7.9150e-01,\n",
      "         1.8945e-01,  9.4116e-02,  7.4158e-02, -8.9795e-01,  5.4993e-02,\n",
      "         1.7529e-01, -7.7295e-01, -5.3613e-01,  5.2490e-02,  7.3486e-01,\n",
      "         2.6025e-01,  2.8662e-01,  1.8271e+00, -4.0918e-01,  8.4619e-01,\n",
      "        -1.6418e-01,  3.8037e-01, -4.5776e-02,  8.2959e-01,  3.2886e-01,\n",
      "        -8.2324e-01,  1.1551e-02, -1.0321e-01,  4.1443e-02,  2.9492e-01,\n",
      "         1.6592e+00,  2.7002e-01,  8.2764e-01, -8.4473e-01,  5.9052e-02,\n",
      "         8.3447e-01, -4.6729e-01,  7.0410e-01,  5.8643e-01, -8.0469e-01,\n",
      "         5.2246e-01, -3.0566e-01,  1.0371e+00,  1.6748e-01,  7.7344e-01,\n",
      "         1.4502e-01,  1.1152e+00, -6.4062e-01, -1.1298e-01,  2.7734e-01,\n",
      "        -1.4392e-01,  6.2207e-01,  1.5010e+00,  1.6797e-01,  1.7444e-01,\n",
      "         1.8809e+00,  3.5889e-01,  2.0642e-01,  8.1445e-01,  2.1399e-01,\n",
      "         1.0254e+00, -3.5229e-01,  1.8030e-01, -5.9247e-05, -7.4524e-02,\n",
      "         1.5430e-01, -6.1719e-01,  6.9434e-01,  2.7661e-01, -5.2002e-01,\n",
      "        -2.4646e-01,  3.4204e-01,  3.3838e-01,  1.8027e+00,  8.6182e-01,\n",
      "        -2.0349e-01,  1.3440e-01,  1.3232e+00,  3.7305e-01,  1.6052e-01,\n",
      "         1.9980e+00, -1.6187e-01, -3.0249e-01,  2.0251e-01, -6.7676e-01,\n",
      "         3.0273e-02,  4.6417e-02,  2.6636e-01, -7.6562e-01,  5.0439e-01,\n",
      "         1.2634e-01, -2.7051e-01,  8.8818e-01, -1.7136e-02, -3.1104e-01,\n",
      "         8.8721e-01, -9.7119e-01, -1.1368e-02, -1.8237e-01,  5.2637e-01,\n",
      "         5.9668e-01,  2.0752e-01,  1.3506e+00,  3.1470e-01, -2.2424e-01,\n",
      "         6.2793e-01,  4.0234e-01,  1.6152e+00, -9.9658e-01,  3.7646e-01,\n",
      "         2.2388e-01, -7.6367e-01,  2.2729e-01, -2.5659e-01,  2.2290e-01,\n",
      "         8.3398e-01,  2.9297e-01,  1.1211e+00,  2.2715e+00,  9.8779e-01,\n",
      "        -2.8931e-01,  1.5198e-02,  1.6318e+00, -5.4102e-01,  5.3125e-01,\n",
      "         4.3555e-01,  4.9023e-01,  1.7598e+00,  6.5869e-01,  4.5898e-01,\n",
      "         1.6357e+00,  5.8398e-01,  2.6465e-01,  2.2388e-01,  7.3291e-01,\n",
      "         7.3389e-01,  1.7139e+00, -5.1221e-01,  2.4658e-01,  1.8740e+00,\n",
      "         6.2549e-01, -4.3262e-01,  8.4326e-01,  6.7749e-03,  3.5205e-01,\n",
      "         8.8623e-02,  3.0298e-01, -3.0228e-02,  7.2705e-01,  1.5540e-01,\n",
      "         1.0029e+00,  2.0566e+00,  1.0674e+00, -1.7151e-01, -4.6539e-03,\n",
      "         1.6533e+00, -4.3555e-01,  5.1904e-01, -2.9712e-01,  3.6072e-02,\n",
      "        -7.8430e-02,  3.2446e-01,  3.4888e-01,  9.5264e-01,  5.7715e-01,\n",
      "         3.9844e-01,  1.3599e-01,  1.5771e+00, -5.7031e-01, -4.8828e-01,\n",
      "         9.7961e-02, -6.7822e-01,  1.0449e+00, -8.2129e-01, -9.2712e-02,\n",
      "        -7.5317e-02, -2.5589e-02,  9.4666e-02,  6.4844e-01,  5.0146e-01,\n",
      "         8.1641e-01,  2.1594e-01,  1.4531e+00, -2.1033e-01,  7.5977e-01,\n",
      "         1.1127e-01,  4.3555e-01,  1.1884e-01,  1.8396e-01,  1.5742e+00,\n",
      "         4.2480e-01,  5.3320e-01, -2.0422e-01,  4.5654e-01,  1.0376e-01,\n",
      "         4.0186e-01,  1.3555e+00,  6.8066e-01,  4.4189e-01,  5.6299e-01,\n",
      "         1.4375e+00,  6.1084e-01,  1.4053e+00, -4.6851e-01,  6.1279e-01,\n",
      "         4.4360e-01,  9.7949e-01, -2.4573e-01,  4.4385e-01, -2.6733e-01,\n",
      "         4.0405e-01,  1.2832e+00,  3.6523e-01,  1.3672e-01,  1.1602e+00,\n",
      "         8.1348e-01, -3.8623e-01,  9.9219e-01, -7.3096e-01,  9.8267e-02,\n",
      "        -7.4170e-01, -1.2482e-01, -9.9365e-01, -3.6938e-01, -2.1863e-01,\n",
      "        -6.8359e-01,  1.3513e-01,  2.4316e-01, -1.2878e-01,  8.1299e-01,\n",
      "         3.1445e-01,  2.0374e-01,  4.3286e-01,  2.4490e-02, -7.5146e-01,\n",
      "        -4.5654e-01, -9.3164e-01, -2.7515e-01, -1.1709e+00, -2.4280e-01,\n",
      "        -1.2930e+00, -1.1982e+00, -4.2261e-01,  6.8164e-01,  1.5186e+00,\n",
      "         2.0398e-01, -7.4902e-01, -7.4072e-01, -3.8989e-01, -1.0664e+00,\n",
      "        -1.5955e-01,  3.6694e-01, -2.1011e-02, -4.6704e-01,  3.7988e-01,\n",
      "         2.6230e-02,  3.0054e-01,  6.6589e-02,  4.5996e-01,  9.6283e-03,\n",
      "        -3.2129e-01,  3.1152e-01,  1.6699e-01,  7.7002e-01, -4.3628e-01,\n",
      "         2.2913e-01,  2.1387e-01, -4.6924e-01, -3.3154e-01,  4.9487e-01,\n",
      "         1.8799e-01, -6.1426e-01, -3.1494e-01,  6.6467e-02,  1.4082e+00,\n",
      "         4.7900e-01, -1.3745e-01,  1.0713e+00, -1.0029e+00, -8.5297e-03,\n",
      "        -2.8320e-01,  4.7241e-01,  6.1279e-01, -8.9600e-01, -2.8091e-02,\n",
      "        -3.1738e-01,  1.6388e-02,  7.6855e-01,  2.4255e-01,  1.0205e+00,\n",
      "         2.1113e+00,  8.7744e-01, -3.2129e-01, -3.6377e-02,  1.4170e+00,\n",
      "        -3.3032e-01, -5.1611e-01,  3.3569e-01,  3.9722e-01,  1.4619e+00,\n",
      "         4.8511e-01, -1.1426e+00, -6.2622e-02, -2.5122e-01, -1.0358e-01,\n",
      "         1.3818e-01, -8.6865e-01, -5.1270e-01, -2.7664e-02,  7.8796e-02,\n",
      "         1.0420e+00,  2.0874e-01,  8.6426e-01,  8.7305e-01,  2.6245e-01,\n",
      "        -2.4235e-04, -1.2610e-01,  8.1689e-01,  1.7102e-01,  1.0635e+00,\n",
      "         1.8584e+00, -2.8760e-01, -7.5623e-02,  3.6987e-01, -5.0537e-02,\n",
      "         8.0078e-01, -5.5957e-01,  6.0352e-01,  2.3132e-01,  3.2886e-01,\n",
      "         1.2637e+00,  4.5996e-01], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.6659,  0.2170, -0.0838,  ...,  0.9746,  0.8870, -0.1588],\n",
      "         [ 0.2839,  0.5179, -0.3186,  ...,  0.5254,  0.2797, -0.0199],\n",
      "         [ 0.8640,  0.2600, -0.1219,  ...,  1.6080,  0.6668, -0.3522],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.5327, 1.7285, 0.5708,  ..., 1.5342, 0.6934, 0.3843], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.8555, 0.0117, 0.9795,  ..., 0.2615, 1.4551, 0.5151], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.7847,  0.0075, -0.2257,  ...,  0.8127,  0.6934, -0.2395],\n",
      "         [ 0.7614, -0.0302,  0.4370,  ...,  1.0335,  0.3317,  0.1704],\n",
      "         [ 0.2017,  0.2046,  0.1923,  ...,  0.7811,  0.3167,  0.1419],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 7.1973e-01,  7.8467e-01,  7.4951e-01,  6.6064e-01,  3.3496e-01,\n",
      "         6.6943e-01, -2.9736e-01,  1.9551e+00,  1.0049e+00,  1.3975e+00,\n",
      "         6.0645e-01,  7.7930e-01, -1.3269e-01,  2.9663e-01,  6.1572e-01,\n",
      "        -6.4240e-03,  1.0283e+00,  6.9141e-01,  8.6377e-01,  4.1992e-01,\n",
      "         2.3340e+00,  6.7871e-01,  6.1572e-01,  2.3379e+00,  7.1045e-01,\n",
      "         5.1904e-01, -4.6899e-01, -2.4646e-01, -3.4619e-01,  2.5488e+00,\n",
      "         1.1318e+00,  4.8608e-01, -4.5166e-01,  1.5488e+00,  1.3452e-01,\n",
      "         1.5908e+00,  1.8323e-01,  3.8867e-01,  9.0149e-02,  6.5381e-01,\n",
      "         1.7651e-01,  1.8384e-01,  2.6758e+00,  1.0762e+00, -1.2962e-02,\n",
      "        -5.4590e-01,  4.7217e-01, -2.2107e-01, -7.0496e-02, -4.0698e-01,\n",
      "         2.4915e-01,  2.1936e-01,  2.0625e+00,  1.1328e+00,  1.2539e+00,\n",
      "         6.3428e-01,  9.8340e-01,  2.0352e+00,  5.0732e-01,  4.6484e-01,\n",
      "         1.1582e+00,  6.9141e-01,  1.3340e+00,  4.3018e-01, -1.0547e-01,\n",
      "         2.1113e+00,  5.3369e-01,  6.9629e-01,  1.0803e-01, -4.2065e-01,\n",
      "         7.0068e-02, -4.1382e-01, -1.0162e-01, -2.2656e-01,  1.4961e+00,\n",
      "         1.6553e-01,  3.2788e-01, -8.7524e-02,  4.2798e-01,  7.3853e-02,\n",
      "        -1.7712e-01,  4.9780e-01,  2.2070e-01,  4.1235e-01,  1.9277e+00,\n",
      "         6.7822e-01,  8.6230e-01,  5.2100e-01,  2.2812e+00,  4.9194e-01,\n",
      "         8.1104e-01,  6.1816e-01, -2.9199e-01,  1.7100e+00,  2.4915e-01,\n",
      "         4.0698e-01, -1.1371e-01,  6.7725e-01,  2.5269e-01, -7.4310e-03,\n",
      "         5.1465e-01, -2.4207e-01,  8.0957e-01, -5.0586e-01,  2.3770e+00,\n",
      "         5.6689e-01,  7.1094e-01,  5.5518e-01, -2.3102e-02,  1.8384e-01,\n",
      "         2.4817e-01,  1.4771e-02,  2.0312e+00,  1.2285e+00,  1.6172e+00,\n",
      "         7.4561e-01,  1.3174e+00,  9.8145e-01,  1.2617e+00,  3.9087e-01,\n",
      "         1.7959e+00,  7.5391e-01,  1.7568e+00,  7.1094e-01,  2.1838e-01,\n",
      "        -1.6052e-01,  2.2363e+00,  4.7925e-01,  3.1055e-01,  8.2397e-03,\n",
      "        -6.4355e-01,  1.8762e-01,  2.3645e-01, -1.2683e-01,  1.5684e+00,\n",
      "         2.1985e-01,  4.2896e-01, -1.6321e-01,  6.0352e-01,  2.9004e-01,\n",
      "         3.8989e-01,  2.7715e+00,  5.4932e-01,  5.9180e-01,  6.7578e-01,\n",
      "         2.5820e+00,  6.5723e-01,  5.9180e-01,  6.5869e-01, -3.6255e-01,\n",
      "         1.8438e+00,  3.9331e-01,  4.9390e-01, -3.2684e-02,  1.4678e+00,\n",
      "         4.0405e-01,  5.4199e-01,  1.0273e+00,  1.1787e+00,  6.3867e-01,\n",
      "         3.9233e-01,  8.0322e-01,  2.5117e+00,  7.2607e-01,  4.3091e-01,\n",
      "         4.6045e-01,  2.1328e+00,  9.1699e-01,  2.8003e-01,  2.2031e+00,\n",
      "         1.1025e+00,  2.3926e-02,  5.2344e-01, -1.3452e-01,  2.6660e-01,\n",
      "         2.3938e-01, -3.1348e-01,  2.1211e+00,  1.2373e+00,  1.5801e+00,\n",
      "         7.2754e-01,  1.3320e+00,  1.1074e+00,  1.3867e+00,  4.4141e-01,\n",
      "         1.9141e+00,  7.8418e-01,  1.7852e+00,  6.6455e-01,  2.5391e-01,\n",
      "        -2.5830e-01,  2.2969e+00,  6.7236e-01,  5.7373e-01,  1.4441e-01,\n",
      "        -5.5225e-01,  1.9250e-01, -3.6182e-01,  6.2256e-02, -5.8533e-02,\n",
      "        -1.4702e-02,  1.6387e+00,  2.2595e-01,  4.1479e-01, -1.1102e-01,\n",
      "         4.1943e-01,  4.0137e-01,  6.4404e-01,  1.6709e+00,  6.7334e-01,\n",
      "         1.3516e+00,  6.0352e-01, -3.2715e-01, -9.1370e-02, -2.4500e-01,\n",
      "         2.7617e+00,  7.1387e-01,  5.9863e-01,  6.0693e-01, -4.7455e-02,\n",
      "         2.7148e+00,  7.6807e-01,  9.5459e-01,  9.4873e-01,  9.1992e-01,\n",
      "         5.3662e-01, -4.4482e-01,  8.7988e-01,  4.7778e-01,  2.5562e-01,\n",
      "         1.8574e+00,  4.1138e-01,  5.6348e-01,  1.4771e-01,  1.3350e+00,\n",
      "         4.5166e-01,  2.7856e-01, -4.5117e-01,  1.2393e+00,  2.6758e-01,\n",
      "        -6.7444e-02,  8.9648e-01, -1.1176e-01,  2.8555e+00,  7.6221e-01,\n",
      "         1.0020e+00,  1.0391e+00,  7.8125e-01,  6.6846e-01, -7.6843e-02,\n",
      "        -4.0308e-01,  1.1836e+00,  3.2990e-02,  1.4380e-01, -4.6997e-01,\n",
      "         1.9092e+00,  6.7432e-01,  1.8372e-01,  5.4590e-01,  1.0518e+00,\n",
      "        -4.4995e-01,  5.5811e-01, -7.8064e-02,  1.4477e-03,  2.8271e-01,\n",
      "         6.3818e-01,  6.4258e-01, -1.2524e-01,  2.5312e+00,  9.1602e-01,\n",
      "         9.5361e-01,  8.0322e-01,  7.7100e-01, -2.1887e-01,  5.2979e-01,\n",
      "         2.0593e-01, -3.9429e-01,  2.9688e-01,  4.2163e-01,  4.3274e-02,\n",
      "        -3.4943e-02,  3.8550e-01,  4.3433e-01,  9.3848e-01,  6.1426e-01,\n",
      "         1.2732e-01,  1.0898e+00,  5.3076e-01,  2.7271e-01,  1.2970e-02,\n",
      "         9.8877e-02,  8.8672e-01,  7.6367e-01,  7.3242e-02,  7.9346e-01,\n",
      "         4.1699e-01,  5.0293e-01, -4.4525e-02,  4.6924e-01, -2.5830e-01,\n",
      "         6.7627e-01,  4.0649e-02,  3.5010e-01,  9.1016e-01,  7.6709e-01,\n",
      "         1.9324e-01,  9.4531e-01,  1.5273e+00,  1.4219e+00,  7.3877e-01,\n",
      "         1.1660e+00,  3.9551e-01,  2.3132e-01,  6.8652e-01, -4.8193e-01,\n",
      "         5.9912e-01,  9.2432e-01,  4.2578e-01, -1.1975e-01,  1.5417e-01,\n",
      "        -5.3596e-04,  2.5215e+00,  1.1943e+00,  1.5166e+00,  7.9248e-01,\n",
      "         8.6768e-01, -4.0356e-01,  1.7363e+00,  6.8604e-01,  1.8965e+00,\n",
      "         5.0244e-01,  4.2041e-01,  1.1240e+00,  2.9248e-01, -1.7188e-01,\n",
      "         3.4106e-01,  8.9893e-01, -2.4451e-01,  4.4385e-01, -1.3939e-02,\n",
      "         1.9502e+00,  2.4766e+00,  9.1260e-01,  8.2568e-01,  7.0361e-01,\n",
      "        -1.7896e-01,  7.4121e-01, -1.3142e-03,  1.1592e+00,  1.3203e+00,\n",
      "         8.9404e-01,  2.0752e-02,  1.2695e-01,  8.4521e-01,  2.4011e-01,\n",
      "         1.0039e+00,  5.5859e-01, -2.5220e-01,  1.1533e+00,  6.4990e-01,\n",
      "         1.0225e+00,  7.0117e-01,  4.1650e-01,  3.7793e-01,  9.3164e-01,\n",
      "         8.8379e-01,  2.4622e-01, -3.0542e-01,  1.5645e+00,  1.0293e+00,\n",
      "         8.2861e-01,  7.6562e-01,  4.6094e-01,  3.5791e-01,  8.4277e-01,\n",
      "         8.1055e-01,  5.6494e-01,  3.5938e-01, -1.4343e-01,  4.3188e-01,\n",
      "         6.8262e-01,  1.4209e-01,  4.9927e-01, -3.7622e-01,  1.2061e-01,\n",
      "         1.0967e+00, -2.3340e-01,  2.4082e+00,  6.9238e-01,  6.2793e-01,\n",
      "         5.7373e-01,  4.1895e-01, -2.3669e-01,  5.8447e-01,  3.6548e-01,\n",
      "         3.2153e-01,  1.0781e+00,  3.2520e-01,  5.1416e-01, -4.1650e-01,\n",
      "         2.4036e-01,  9.2163e-02, -2.6538e-01, -3.8379e-01,  2.4727e+00,\n",
      "         8.8574e-01,  9.4092e-01,  6.0938e-01, -2.5464e-01,  1.3196e-01,\n",
      "         1.2148e+00,  6.2354e-01,  4.5142e-01, -2.3035e-01,  5.5957e-01,\n",
      "         7.5781e-01,  2.2534e-01,  4.5947e-01,  3.6230e-01,  1.5332e+00,\n",
      "         1.4824e+00,  3.6401e-01,  1.1309e+00,  3.3789e-01,  1.5125e-01,\n",
      "         4.5166e-01,  6.2109e-01, -6.5381e-01,  3.7891e-01, -1.0078e-02,\n",
      "        -1.1774e-01,  8.2910e-01,  1.6125e-01,  5.6689e-01,  2.8003e-01,\n",
      "         1.1787e+00,  1.0039e+00,  1.8481e-01,  1.0664e+00,  1.8127e-01,\n",
      "         4.6631e-02,  6.2256e-02, -1.2512e-01,  5.8350e-01,  6.6309e-01,\n",
      "        -4.3726e-01,  1.4141e+00,  6.2012e-01, -2.5696e-02,  1.8164e+00,\n",
      "         6.3135e-01,  5.8838e-01, -1.0675e-01,  2.1271e-02, -1.4575e-01,\n",
      "        -3.2990e-02, -3.4521e-01,  1.0977e+00, -5.0842e-02,  1.4189e+00,\n",
      "         3.6255e-01,  3.0396e-01, -5.4053e-01,  6.8652e-01,  8.2617e-01,\n",
      "         1.3647e-01, -2.0398e-01,  7.0996e-01, -4.0649e-01,  5.9473e-01,\n",
      "        -2.0947e-01,  1.0732e+00,  2.6294e-01,  8.5645e-01,  6.5479e-01,\n",
      "        -4.9194e-01,  6.1914e-01,  2.7222e-02,  3.3716e-01, -4.3799e-01,\n",
      "        -1.7383e-01,  5.3809e-01,  5.7129e-01, -7.5867e-02, -4.7272e-02,\n",
      "        -7.0215e-01, -1.8036e-02,  9.7949e-01,  6.4746e-01,  7.5000e-01,\n",
      "         5.2930e-01,  3.1836e-01,  3.5425e-01,  2.6465e+00,  9.6631e-01,\n",
      "         1.2998e+00,  6.9385e-01,  7.2266e-01,  2.1270e+00,  9.7510e-01,\n",
      "         1.1865e+00,  6.7871e-01,  7.2070e-01, -6.3574e-01,  6.6406e-01,\n",
      "         7.5732e-01,  3.2397e-01,  4.4556e-01, -4.0625e-01, -3.7329e-01,\n",
      "         2.3887e+00,  8.8232e-01,  7.9688e-01,  7.0703e-01,  2.3962e-01,\n",
      "         9.3945e-01,  8.9307e-01,  8.9502e-01,  4.8657e-01, -2.2278e-01,\n",
      "         1.6445e+00,  6.9141e-01,  5.8545e-01,  4.7119e-01,  1.2520e+00,\n",
      "         3.9502e-01, -3.6304e-01,  1.1123e+00,  8.6182e-02, -7.6904e-02,\n",
      "         2.2012e+00,  9.9121e-01,  1.3447e+00,  7.6758e-01,  8.4375e-01,\n",
      "        -2.8979e-01,  7.6855e-01,  8.7695e-01,  1.3092e-02,  1.9443e+00,\n",
      "         8.1152e-01,  8.7158e-01, -2.4500e-01,  2.0742e+00,  1.0098e+00,\n",
      "        -1.0016e-01,  1.6221e+00,  7.7637e-01,  1.0479e+00,  4.4189e-01,\n",
      "         3.5010e-01,  1.9785e+00,  6.2598e-01,  6.0303e-01,  1.7395e-01,\n",
      "         9.9609e-01, -3.3894e-03,  1.0303e+00, -6.4062e-01,  4.1528e-01,\n",
      "        -1.2018e-01, -6.1865e-01,  6.0645e-01,  5.4004e-01, -4.4800e-01,\n",
      "         9.7168e-01,  4.5508e-01,  6.2012e-01, -5.9375e-01,  8.4619e-01,\n",
      "         2.8534e-02,  3.0365e-02,  1.7590e-01,  1.6565e-01,  1.2866e-01,\n",
      "         4.8637e-04,  6.5576e-01,  8.1055e-01,  5.2930e-01,  3.8306e-01,\n",
      "        -6.0791e-01,  1.5586e+00,  2.8101e-01,  1.0439e+00,  1.5149e-01,\n",
      "         1.0439e+00,  2.7441e-01,  8.3643e-01,  3.4717e-01, -2.3572e-01,\n",
      "         7.9004e-01, -4.8291e-01,  4.4238e-01,  7.1826e-01,  4.8462e-01,\n",
      "        -7.3730e-01,  1.1719e+00,  3.4424e-01,  4.7021e-01,  9.7754e-01,\n",
      "        -1.6882e-01,  7.3438e-01,  4.8169e-01,  3.3960e-01,  2.5488e+00,\n",
      "         6.8359e-01,  7.4121e-01,  1.8975e+00,  7.0703e-01,  6.5332e-01,\n",
      "         2.4023e+00,  6.1914e-01,  6.3135e-01,  1.6680e+00,  7.4854e-01,\n",
      "         6.2891e-01,  1.4336e+00,  1.3848e+00,  3.0615e-01,  9.3652e-01,\n",
      "         5.3320e-01,  1.0820e+00,  1.0010e+00,  5.6250e-01,  1.2773e+00,\n",
      "         2.4316e+00,  8.8232e-01,  8.0127e-01,  8.5254e-01,  3.4863e-01,\n",
      "         1.0010e+00,  5.7227e-01, -8.0750e-02,  2.0039e+00,  7.3877e-01,\n",
      "        -1.8402e-02,  6.8896e-01, -3.1799e-02,  5.9766e-01, -3.3740e-01,\n",
      "         6.0693e-01,  7.8516e-01,  2.7368e-01,  6.7822e-01, -1.3232e-01,\n",
      "        -7.0435e-02,  7.7881e-01,  5.1172e-01,  5.1709e-01, -1.7822e-01,\n",
      "         4.8706e-01, -5.6427e-02,  4.2798e-01, -2.3132e-01,  9.6436e-01,\n",
      "        -9.8495e-03,  5.3076e-01,  3.0566e-01,  7.9785e-01,  9.5996e-01,\n",
      "         5.8411e-02,  1.0371e+00, -2.1533e-01,  1.4775e+00,  4.2358e-01,\n",
      "         4.1962e-03,  6.2793e-01,  7.2168e-01,  4.1797e-01,  2.0862e-01,\n",
      "        -5.8960e-02,  6.1084e-01, -1.2939e-01,  1.1172e+00,  2.6587e-01,\n",
      "         3.7549e-01,  1.9238e+00,  7.7393e-01,  6.1230e-01,  4.0552e-01,\n",
      "         7.9004e-01, -3.0859e-01,  3.9355e-01,  4.5996e-01,  5.2612e-02,\n",
      "         2.8564e-01,  6.8066e-01,  4.5190e-01,  4.9585e-01,  6.5820e-01,\n",
      "         9.1846e-01,  5.5762e-01,  4.9194e-01,  2.8809e-01,  6.6016e-01,\n",
      "         7.3096e-01,  6.6895e-01,  3.1396e-01,  1.2539e+00,  5.6299e-01,\n",
      "         1.0381e+00,  9.4092e-01,  1.9062e+00,  9.2480e-01,  1.3799e+00,\n",
      "         1.1387e+00,  1.0352e+00,  1.8467e+00,  1.2236e+00,  8.5010e-01,\n",
      "         1.0566e+00], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_logits:  tensor([ 7.4805e-01,  1.8860e-01,  6.2695e-01,  1.0242e-01,  9.5947e-01,\n",
      "         7.6465e-01, -9.2236e-01, -9.8083e-02,  3.5010e-01,  5.6250e-01,\n",
      "         8.1250e-01,  5.5713e-01, -7.0605e-01, -4.0869e-01, -2.1301e-01,\n",
      "        -5.0439e-01, -5.3125e-01, -3.4180e-01, -3.0640e-01,  4.9805e-01,\n",
      "         1.0742e-01,  1.2686e+00,  1.1650e+00, -9.9915e-02,  1.0293e+00,\n",
      "         1.1699e+00, -1.1436e+00, -6.4819e-02, -6.3037e-01,  5.3223e-01,\n",
      "         1.0127e+00,  1.0615e+00, -1.0801e+00,  2.7173e-01,  5.2344e-01,\n",
      "        -1.0553e-01,  5.6946e-02,  3.6255e-01,  7.8174e-01,  4.5923e-01,\n",
      "         8.2245e-03, -1.6870e-01,  6.2402e-01,  1.1143e+00, -7.1289e-01,\n",
      "        -6.1670e-01, -1.5723e-01,  2.1606e-01,  1.5735e-01, -1.0684e+00,\n",
      "        -1.0146e+00, -5.4736e-01, -1.7224e-01,  6.0107e-01,  9.0332e-01,\n",
      "         2.0449e+00,  6.8799e-01,  6.2646e-01,  3.9282e-01,  1.8887e+00,\n",
      "         7.1436e-01,  1.2041e+00,  3.2788e-01,  8.5693e-01, -5.2539e-01,\n",
      "         4.1431e-01,  7.5342e-01,  1.7988e+00,  8.2715e-01, -1.0449e+00,\n",
      "        -5.8154e-01, -3.9355e-01, -6.8604e-01,  3.4937e-01, -2.7002e-01,\n",
      "        -4.9500e-02,  3.2227e-01,  7.0996e-01,  6.6113e-01,  3.6060e-01,\n",
      "        -5.0879e-01, -1.9189e-01, -8.5498e-01,  4.7778e-01,  2.5528e-02,\n",
      "         8.0322e-01,  1.6846e+00,  7.4658e-01, -8.7524e-02,  5.1270e-01,\n",
      "         1.3818e+00,  7.4951e-01, -1.2090e+00, -6.1670e-01, -9.7839e-02,\n",
      "         2.5244e-01,  6.1279e-01,  1.8091e-01, -6.6040e-02, -9.4531e-01,\n",
      "         2.5317e-01, -7.2070e-01, -6.0693e-01, -4.4702e-01,  7.6355e-02,\n",
      "         6.8018e-01,  1.5156e+00,  1.0479e+00, -3.5425e-01, -1.0870e-01,\n",
      "        -2.0154e-01, -8.6475e-01, -1.8054e-01,  5.6982e-01,  9.3018e-01,\n",
      "         1.8311e+00,  7.0703e-01,  1.2891e+00,  3.7085e-01,  1.0342e+00,\n",
      "         2.0801e-01,  1.2510e+00,  9.6777e-01,  3.1299e-01,  1.7930e+00,\n",
      "        -3.2910e-01,  6.6602e-01,  3.4009e-01,  1.8086e+00, -4.9951e-01,\n",
      "        -6.5771e-01, -7.4219e-01,  4.8242e-01, -3.3301e-01, -5.8887e-01,\n",
      "        -1.3879e-01,  2.9004e-01,  7.8906e-01,  6.4941e-01, -8.4424e-01,\n",
      "         4.5898e-01,  2.0459e-01,  3.2446e-01,  1.5918e+00,  9.6240e-01,\n",
      "         3.8940e-02,  1.2708e-01,  1.2188e+00,  1.1143e+00, -1.2139e+00,\n",
      "        -4.3140e-01, -1.3554e-04,  4.2383e-01,  8.2422e-01,  1.1261e-01,\n",
      "         1.2122e-01,  5.1465e-01,  5.6885e-01,  1.3477e-01,  3.7329e-01,\n",
      "        -2.2205e-01, -3.2837e-01,  4.0527e-01,  2.6392e-01,  1.8281e+00,\n",
      "         4.9390e-01, -5.4395e-01,  1.0986e+00,  5.4395e-01,  4.6899e-01,\n",
      "         1.0107e+00, -5.9961e-01,  4.5508e-01, -9.7607e-01, -6.1523e-01,\n",
      "         1.7737e-01, -2.6587e-01, -9.1095e-03,  6.7627e-01,  1.0371e+00,\n",
      "         1.9180e+00,  8.3203e-01,  1.3311e+00,  4.2212e-01,  9.6631e-01,\n",
      "         1.2085e-01,  1.2461e+00,  8.8184e-01,  3.9453e-01,  1.9150e+00,\n",
      "        -4.7461e-01,  3.6646e-01,  8.1348e-01,  2.1484e+00, -5.5078e-01,\n",
      "        -7.6953e-01, -6.3232e-01, -4.7559e-01, -7.3389e-01,  3.9844e-01,\n",
      "        -9.5276e-02, -3.8428e-01, -6.5552e-02,  3.5645e-01,  9.3115e-01,\n",
      "         7.7930e-01, -8.0518e-01, -3.5400e-01,  1.3318e-01,  8.8379e-01,\n",
      "        -1.1761e-01,  7.5732e-01, -1.3135e+00, -3.9111e-01, -1.0352e+00,\n",
      "         2.1570e-01,  2.6709e-01,  1.4297e+00,  8.4814e-01, -9.0234e-01,\n",
      "         1.5747e-01,  1.6528e-01,  9.0283e-01,  1.5156e+00,  1.3750e+00,\n",
      "         1.1006e+00, -1.1504e+00,  3.9673e-02,  5.6836e-01,  6.3867e-01,\n",
      "        -2.8101e-01,  2.3392e-02,  5.2979e-01,  1.0762e+00, -1.0883e-01,\n",
      "         1.5198e-01, -2.7695e-02,  1.4844e-01,  1.4673e-01, -2.2986e-01,\n",
      "        -5.6299e-01,  4.0210e-01, -3.3643e-01,  2.1289e-01,  2.1997e-01,\n",
      "         1.0342e+00,  1.5020e+00,  1.7051e+00,  5.1953e-01, -1.0910e-02,\n",
      "        -9.1992e-01,  6.9287e-01,  5.0446e-02, -7.5867e-02, -2.7100e-01,\n",
      "         2.3987e-01,  4.2847e-01,  1.1328e+00, -7.4512e-01,  6.2402e-01,\n",
      "        -9.8682e-01, -5.4346e-01, -2.4695e-01, -5.9961e-01, -3.5474e-01,\n",
      "        -5.1514e-02, -1.5955e-01, -1.0996e+00,  2.1997e-01,  1.1650e+00,\n",
      "         8.2324e-01, -9.8682e-01,  3.2471e-02, -8.1982e-01, -7.4524e-02,\n",
      "         2.2797e-02, -1.0029e+00, -6.9678e-01, -2.2864e-01, -6.6797e-01,\n",
      "        -3.5645e-01, -3.5132e-01, -4.1382e-01,  4.4678e-01,  5.2826e-02,\n",
      "        -3.1958e-01, -7.1338e-01, -8.6963e-01,  9.8633e-02, -3.0566e-01,\n",
      "        -9.4434e-01,  1.8372e-01, -5.6104e-01, -7.5049e-01, -2.9663e-01,\n",
      "        -6.0400e-01, -4.2554e-01, -2.9517e-01,  2.9858e-01, -1.6003e-01,\n",
      "         3.4863e-01, -5.2344e-01,  1.1902e-01, -4.6570e-02, -2.0889e-02,\n",
      "        -2.2351e-01, -7.8516e-01, -5.9277e-01, -3.0956e-03,  8.3740e-01,\n",
      "         1.0876e-01, -1.0211e-01,  6.7932e-02,  1.2042e-01, -1.5723e-01,\n",
      "         7.3730e-02,  7.6318e-01, -6.5039e-01, -3.1006e-01, -5.6104e-01,\n",
      "        -8.4082e-01,  1.2329e-01,  6.6260e-01,  7.9443e-01,  1.5010e+00,\n",
      "         1.4375e+00, -1.2637e+00,  3.9282e-01,  1.3994e+00,  3.4229e-01,\n",
      "         5.3857e-01,  2.1074e+00,  4.9072e-01,  1.1631e+00, -2.2961e-01,\n",
      "         2.2369e-02,  1.3367e-01, -1.0156e+00, -2.7954e-01, -8.2471e-01,\n",
      "         4.9097e-01, -2.6392e-01,  7.7734e-01,  1.3047e+00,  1.0420e+00,\n",
      "        -2.2815e-01,  1.2042e-01, -3.9282e-01,  2.4097e-01,  5.7275e-01,\n",
      "        -6.8262e-01, -1.1835e-01,  1.4880e-01,  4.7656e-01, -7.2119e-01,\n",
      "        -3.4106e-01, -1.4771e-01, -9.8328e-02, -4.9744e-02, -7.8735e-02,\n",
      "         9.0698e-02, -1.6614e-01,  1.5625e-01, -1.7358e-01, -6.9580e-02,\n",
      "         9.3140e-02,  1.5991e-01, -8.8184e-01,  2.8149e-01, -2.7969e-02,\n",
      "         3.6938e-01,  2.7075e-01, -6.3525e-01,  4.4287e-01,  2.3022e-01,\n",
      "        -2.0728e-01, -6.2134e-02, -1.6870e-01,  1.5076e-01, -4.7058e-02,\n",
      "        -5.8655e-02,  1.9019e-01, -3.3203e-02, -1.0742e+00, -2.4231e-01,\n",
      "        -9.8096e-01, -6.3574e-01,  6.9519e-02,  5.6689e-01,  1.2832e+00,\n",
      "         9.5215e-01,  5.8496e-01,  9.9463e-01,  4.0674e-01,  1.6748e-01,\n",
      "         5.2734e-01,  6.9397e-02,  1.6772e-01,  5.7892e-02,  2.8906e-01,\n",
      "         1.0797e-01,  2.1680e-01, -2.0959e-01, -3.8672e-01,  1.8579e-01,\n",
      "         1.4453e+00,  9.4775e-01,  4.9854e-01, -4.1748e-01, -1.4380e-01,\n",
      "         1.2573e-01,  7.9102e-01,  5.8154e-01,  6.3770e-01,  4.7778e-01,\n",
      "        -9.9304e-02,  5.4199e-01, -5.6250e-01,  4.5557e-01,  3.9502e-01,\n",
      "         1.9006e-01,  6.7773e-01,  2.7319e-01,  2.7222e-01,  9.1357e-01,\n",
      "         1.3936e+00,  7.7295e-01, -8.8623e-01,  1.6248e-01, -4.4604e-01,\n",
      "        -3.5938e-01, -2.1716e-01,  2.7588e-01, -9.2041e-02, -5.2539e-01,\n",
      "         2.9004e-01,  2.4854e-01,  5.5664e-01,  1.7151e-01,  2.5732e-01,\n",
      "         8.3984e-01,  1.2188e+00, -1.9910e-01,  8.6975e-02,  4.0863e-02,\n",
      "        -3.4229e-01,  8.1104e-01,  1.1562e+00,  7.8186e-02,  3.7012e-01,\n",
      "         7.7246e-01,  1.8516e+00,  2.6611e-01, -8.1006e-01, -7.2412e-01,\n",
      "         1.8997e-02, -4.8828e-01, -4.2480e-01, -3.2764e-01,  3.9771e-01,\n",
      "        -1.2695e-01, -6.2402e-01, -7.2217e-01, -7.2083e-02,  1.9363e-02,\n",
      "         6.7871e-02,  4.3677e-01, -9.5032e-02, -8.1055e-01,  2.9602e-02,\n",
      "        -4.5215e-01, -3.7476e-01, -6.0107e-01,  1.6968e-01, -4.0698e-01,\n",
      "        -1.0234e+00, -1.2253e-02, -3.2739e-01, -3.7427e-01, -8.0566e-01,\n",
      "        -6.1182e-01, -2.7295e-01, -3.1909e-01,  2.5488e-01,  3.4668e-01,\n",
      "        -6.3379e-01, -1.2140e-01, -2.7466e-01,  3.7085e-01,  8.4961e-01,\n",
      "         5.1318e-01, -6.6650e-01,  4.3213e-01, -1.5393e-01,  7.0703e-01,\n",
      "         6.4014e-01,  1.2207e+00,  1.4551e+00, -3.2666e-01,  5.5957e-01,\n",
      "         6.2891e-01,  1.0840e+00,  1.3027e+00, -8.7695e-01,  2.5439e-01,\n",
      "        -1.3000e-01,  4.3677e-01, -1.7468e-01, -1.3635e-01, -2.3694e-01,\n",
      "         4.2285e-01,  6.6846e-01,  1.1133e+00,  1.4346e+00,  1.0724e-01,\n",
      "         5.0781e-01,  6.8945e-01,  8.9014e-01,  1.3232e+00, -2.1069e-01,\n",
      "         4.3854e-02,  6.6113e-01,  6.5771e-01,  1.6309e+00, -1.0919e-01,\n",
      "         1.4380e-01, -8.7012e-01,  1.3321e-02,  7.8955e-01, -9.5117e-01,\n",
      "        -2.8003e-01,  5.8252e-01,  7.6855e-01,  1.4668e+00,  1.6670e+00,\n",
      "        -2.2571e-01,  2.8076e-01,  9.1455e-01,  2.3218e-01,  1.3196e-01,\n",
      "         8.2471e-01,  1.9238e+00, -2.6904e-01,  5.4248e-01,  3.5449e-01,\n",
      "         8.9111e-02,  3.7817e-01,  6.7236e-01,  1.6904e+00, -4.0381e-01,\n",
      "         4.2529e-01,  2.9419e-01,  4.7144e-01,  1.2207e-02, -3.1177e-01,\n",
      "         8.5791e-01, -5.0586e-01,  7.3340e-01, -9.3896e-01, -5.4736e-01,\n",
      "        -5.0293e-01, -1.0309e-01, -8.9050e-02,  9.7229e-02, -6.0693e-01,\n",
      "        -4.1577e-01, -3.0737e-01, -2.7637e-01, -5.8350e-01,  5.7031e-01,\n",
      "        -4.2358e-02, -8.3105e-01, -6.4111e-01,  1.4465e-01, -3.4131e-01,\n",
      "        -6.6162e-01, -2.1045e-01,  1.7151e-02, -1.9791e-02,  2.5366e-01,\n",
      "        -7.8125e-01,  2.0728e-01,  6.1432e-02,  4.2456e-01,  8.4033e-01,\n",
      "         1.9128e-01, -5.2148e-01, -9.8991e-04, -2.0007e-01, -5.2295e-01,\n",
      "         1.1060e-01, -8.6377e-01, -6.5430e-02,  3.0737e-01,  4.1235e-01,\n",
      "        -9.3604e-01,  3.3643e-01,  5.4199e-01,  9.1211e-01,  5.2637e-01,\n",
      "        -2.5806e-01,  7.8906e-01, -5.6348e-01,  4.2139e-01, -4.0015e-01,\n",
      "         3.2080e-01,  9.9268e-01,  2.5879e-01,  6.2793e-01,  2.1777e+00,\n",
      "        -3.3398e-01,  3.3252e-01,  9.2480e-01,  4.4373e-02,  5.3271e-01,\n",
      "         1.8262e+00,  4.2114e-01, -1.1002e-02,  1.0332e+00,  1.0244e+00,\n",
      "         1.2998e+00,  2.4304e-01,  1.2610e-01,  9.1016e-01,  1.2090e+00,\n",
      "        -4.2212e-01,  8.7830e-02,  1.1396e+00,  4.5776e-01,  7.8613e-01,\n",
      "         9.6436e-01,  1.2529e+00,  5.2307e-02,  6.6846e-01, -7.7454e-02,\n",
      "        -7.7637e-01, -2.3962e-01, -4.8730e-01, -8.9233e-02, -4.2578e-01,\n",
      "         3.7048e-02,  4.2773e-01, -3.3789e-01,  1.5613e-01,  4.3359e-01,\n",
      "        -7.4365e-01,  1.9922e-01,  2.4512e-01, -2.7515e-01, -7.2998e-01,\n",
      "        -4.2603e-01, -5.3467e-01,  3.4393e-02, -3.2837e-01,  4.7681e-01,\n",
      "        -4.7754e-01, -1.3013e-01, -4.1382e-02,  3.8550e-01,  1.7578e-01,\n",
      "        -3.7207e-01, -2.1399e-01, -4.4458e-01,  5.7764e-01,  1.3799e+00,\n",
      "        -7.4707e-01,  2.3633e-01,  4.3823e-01, -5.0537e-01,  1.0681e-01,\n",
      "        -1.9824e-01, -2.7295e-01, -4.8096e-01,  2.4609e-01,  8.6328e-01,\n",
      "        -1.0127e+00, -1.3489e-01,  4.7119e-01,  1.7148e+00,  1.1215e-02,\n",
      "        -4.1870e-01, -9.8389e-01, -1.5610e-02,  2.2290e-01,  1.4099e-02,\n",
      "        -8.6963e-01, -2.4329e-01,  3.5461e-02, -6.5479e-01, -2.5488e-01,\n",
      "         1.8402e-02,  1.6953e-02, -7.6172e-01, -5.4297e-01,  9.5825e-03,\n",
      "         5.6641e-01,  1.5381e-01, -4.6167e-01, -4.1016e-01,  1.1157e-01,\n",
      "         8.3057e-01,  4.8950e-02,  9.6863e-02,  5.4297e-01,  6.6699e-01,\n",
      "         4.0088e-01,  4.6191e-01,  2.8516e-01,  8.1494e-01,  1.5938e+00,\n",
      "         2.4365e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.0602,  0.3914, -0.0508,  ...,  1.2280,  1.2624, -0.1209],\n",
      "         [ 0.4316,  0.6239,  0.0614,  ...,  0.7822,  0.6388,  0.0024],\n",
      "         [ 0.0974,  0.5105,  0.1141,  ...,  0.2822,  0.4242, -0.0669],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.6841, 1.3281, 0.6162,  ..., 0.6948, 0.4460, 0.3701], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([0.5249, 0.0225, 0.3989,  ..., 0.0947, 0.6558, 0.5356], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0017,  0.1217,  0.0244,  ...,  0.1457,  0.0877, -0.0264],\n",
      "         [ 0.0411,  0.2474, -0.0776,  ...,  0.4126,  0.3875, -0.0984],\n",
      "         [ 0.2174,  0.1703,  0.0397,  ..., -0.1275,  0.1644, -0.0223],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0240,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0240,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0240,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([0.2517, 0.6377, 0.4141, 0.1947, 0.4065, 0.1143, 0.3911, 0.2474, 0.0516,\n",
      "        0.4409, 0.5044, 0.5107, 0.5391, 0.4919, 0.5239, 0.2426],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1689, -0.1344, -0.1505,  0.0241, -0.1996, -0.0919, -0.2325,  0.0216,\n",
      "        -0.2192,  0.0126, -0.1367, -0.0750, -0.1876, -0.2438, -0.1388,  0.2126],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.5200,  0.2299,  0.1061,  ...,  1.1731,  0.4412, -0.3534],\n",
      "         [ 0.5487,  0.0665,  0.4222,  ...,  1.6476,  0.4423,  0.1204],\n",
      "         [ 0.4281,  0.0023,  0.5958,  ...,  1.0608,  0.6954, -0.3659],\n",
      "         ...,\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979],\n",
      "         [-0.0092,  0.0612, -0.0239,  ..., -0.0771, -0.0277, -0.0979]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 2.2717e-01,  4.8853e-01,  6.0425e-02,  6.1377e-01, -1.5552e-01,\n",
      "         4.6777e-01, -9.2712e-02,  8.7708e-02, -4.7729e-02,  1.7930e+00,\n",
      "         5.0830e-01,  4.2480e-02,  1.6328e+00,  5.5518e-01,  2.9053e-01,\n",
      "        -2.2656e-01,  1.9805e+00,  8.0322e-01,  2.7344e-01,  2.5586e-01,\n",
      "        -6.4026e-02,  2.0723e+00,  3.9062e-01,  2.8633e+00,  7.1729e-01,\n",
      "         2.5146e-01,  5.4810e-02, -7.8918e-02,  2.6543e+00,  7.2852e-01,\n",
      "         3.2739e-01,  1.6797e-01,  1.7969e-01,  5.8252e-01,  2.3340e+00,\n",
      "         7.1826e-01,  4.6021e-02, -1.5076e-01,  1.5020e+00,  7.7539e-01,\n",
      "        -6.6064e-01,  2.5146e-01, -5.1660e-01, -1.1182e-01,  9.7070e-01,\n",
      "         8.8965e-01,  5.3027e-01,  2.2217e-01, -1.3477e-01,  9.5581e-02,\n",
      "         1.3318e-01,  2.2207e+00,  4.3066e-01,  7.7881e-01,  4.4873e-01,\n",
      "        -3.9453e-01, -9.3201e-02, -3.5693e-01,  1.3076e+00,  5.5127e-01,\n",
      "        -5.2637e-01, -1.3892e-01,  1.0430e+00, -4.8267e-01,  1.3975e+00,\n",
      "         4.5557e-01,  1.1992e+00,  2.1758e+00,  7.9004e-01,  2.9297e+00,\n",
      "         1.0020e+00,  4.5728e-01,  3.0884e-01,  2.3987e-01,  7.2266e-01,\n",
      "         4.7192e-01, -5.0928e-01, -9.0881e-02,  5.3369e-01,  6.1279e-01,\n",
      "        -1.7395e-01,  1.5449e+00,  6.3916e-01, -5.5176e-01, -3.6957e-02,\n",
      "         5.8740e-01,  1.8652e-01, -2.5684e-01,  2.3574e+00,  1.0439e+00,\n",
      "         1.1514e+00,  2.1008e-01,  1.1162e+00,  7.0020e-01,  9.2285e-01,\n",
      "        -2.0466e-03,  4.5410e-02, -2.8394e-01,  1.3145e+00,  5.7739e-02,\n",
      "        -2.2681e-01,  5.1416e-01, -2.5192e-02, -5.1221e-01, -9.2285e-02,\n",
      "         2.0776e-01, -1.8250e-01, -8.3740e-02, -2.3132e-01,  2.1240e-01,\n",
      "        -1.4954e-01,  1.0781e+00,  5.1562e-01, -8.9722e-02, -1.6980e-01,\n",
      "         2.6978e-02,  1.1230e+00,  5.0049e-01, -9.5947e-02, -4.7089e-02,\n",
      "         4.5349e-02,  2.4473e+00,  9.5996e-01,  2.6855e-01,  3.9966e-01,\n",
      "         2.2412e-01, -2.5562e-01,  1.2051e+00,  3.3032e-01,  7.5012e-02,\n",
      "         1.5320e-01,  1.9043e-02,  2.4570e+00,  4.1162e-01,  6.1963e-01,\n",
      "         3.2275e-01,  1.7554e-01,  9.4824e-01,  4.5410e-01,  5.9131e-01,\n",
      "         5.0098e-01,  8.3203e-01,  3.7085e-01,  7.7295e-01,  6.6406e-01,\n",
      "         5.5811e-01,  5.1758e-02,  9.2920e-01, -7.7246e-01, -2.9272e-01,\n",
      "        -2.9785e-01,  2.8379e+00,  6.5723e-01,  1.7493e-01,  8.7109e-01,\n",
      "         4.6826e-01,  8.2275e-01, -1.7310e-01,  7.2559e-01, -6.4795e-01,\n",
      "         1.9736e+00,  4.6582e-01,  2.2388e-01,  5.5566e-01,  5.7959e-01,\n",
      "         2.9023e+00,  1.2012e+00,  6.4014e-01,  1.0322e+00,  1.1055e+00,\n",
      "         4.6973e-01,  3.1934e-01,  2.8203e+00,  1.1523e+00,  5.5322e-01,\n",
      "         8.3936e-01,  9.9609e-01,  4.4312e-01,  4.8022e-01, -2.0471e-01,\n",
      "         2.5566e+00,  5.7080e-01,  3.8354e-01,  3.5938e-01,  3.7329e-01,\n",
      "        -4.4580e-01,  8.9014e-01,  2.4841e-01, -1.1743e-01,  7.5256e-02,\n",
      "        -3.7573e-01,  2.2031e+00,  7.5732e-01,  3.8208e-01,  1.5049e+00,\n",
      "         3.7402e-01,  2.1777e-01,  2.6191e+00,  1.0195e+00,  2.3206e-01,\n",
      "        -5.3027e-01,  1.8867e+00,  3.5840e-01,  1.5173e-01,  3.7109e-01,\n",
      "         2.3206e-01, -2.2668e-01,  1.1035e+00,  2.0742e+00,  5.8740e-01,\n",
      "         2.4402e-01,  4.3677e-01, -1.2244e-01,  2.5039e+00,  9.5166e-01,\n",
      "         2.8564e-01, -5.2295e-01,  1.9714e-01,  3.3789e-01,  3.8135e-01,\n",
      "         2.5723e+00,  4.4800e-01, -1.6260e-01,  4.9390e-01, -4.5685e-02,\n",
      "         1.8594e+00,  4.0967e-01,  2.4011e-01,  5.0098e-01,  8.7012e-01,\n",
      "        -5.8899e-03,  2.8784e-01,  1.2708e-01,  2.1934e+00,  5.6299e-01,\n",
      "         1.4526e-01,  1.3174e+00, -9.1400e-03, -1.7480e-01,  2.6348e+00,\n",
      "         5.8057e-01,  3.4454e-02,  1.5127e+00,  3.2080e-01,  5.8447e-01,\n",
      "         9.5605e-01, -2.5342e-01,  5.0781e-01,  3.9038e-01,  1.5400e+00,\n",
      "         1.1768e-01,  1.0469e+00,  1.8445e-01,  1.1139e-02,  6.3232e-01,\n",
      "        -1.0767e-01,  1.2432e+00,  8.8989e-02,  1.0107e+00,  1.2476e-01,\n",
      "        -1.5594e-02,  7.1631e-01, -2.1838e-01, -5.6299e-01,  1.7061e+00,\n",
      "        -1.3416e-01,  1.6089e-01,  2.2510e-01,  4.0869e-01,  5.1807e-01,\n",
      "         7.2876e-02,  1.9446e-01,  3.7183e-01, -1.6431e-01, -5.2344e-01,\n",
      "        -2.2424e-01,  1.3525e+00,  1.7480e-01, -3.2074e-02,  5.9131e-01,\n",
      "        -2.1899e-01,  3.0884e-01, -1.6077e-01,  3.7720e-01,  9.0674e-01,\n",
      "         1.1145e-01,  5.4150e-01,  7.2607e-01,  2.3230e-01,  5.1318e-01,\n",
      "         9.4287e-01,  1.2177e-02,  6.6956e-02,  8.3496e-01,  1.3977e-01,\n",
      "         1.2109e+00,  7.6599e-02,  1.5564e-01,  5.4688e-01,  6.5430e-01,\n",
      "         1.1416e+00,  1.6064e+00,  1.1631e+00,  4.1040e-01,  8.2275e-01,\n",
      "         4.6411e-01,  8.1494e-01,  1.0703e+00,  1.7822e+00,  6.3281e-01,\n",
      "         3.2104e-01,  1.2920e+00,  1.4766e+00,  1.0020e+00,  2.4695e-01,\n",
      "         6.5479e-01,  3.8184e-01,  6.3574e-01,  9.3604e-01,  1.5859e+00,\n",
      "         4.7388e-01,  4.1064e-01, -1.0352e-01,  2.5605e+00,  8.3594e-01,\n",
      "         1.3782e-01,  4.4775e-01,  8.3008e-01,  4.0234e-01,  8.9160e-01,\n",
      "         8.8770e-01,  3.3960e-01,  7.2412e-01,  2.9419e-01,  4.8853e-01,\n",
      "         3.5498e-01,  5.3125e-01,  3.4180e-01, -1.4771e-01, -5.0879e-01,\n",
      "         5.7520e-01, -3.2056e-01,  1.4612e-01, -4.1357e-01,  2.2461e+00,\n",
      "         7.2070e-01,  3.0420e-01,  1.3613e+00,  4.2871e-01,  2.3865e-01,\n",
      "         1.9072e+00,  1.1064e+00,  3.5327e-01,  6.3232e-01,  3.4937e-01,\n",
      "         5.1416e-01,  3.2031e-01,  7.3584e-01, -4.8035e-02,  2.0996e+00,\n",
      "         5.6104e-01,  4.0039e-01, -2.8735e-01,  2.2227e+00,  5.6885e-01,\n",
      "         3.9917e-01,  2.4280e-01, -2.8122e-02,  1.2080e+00,  2.2717e-01,\n",
      "        -5.0140e-02,  4.5532e-02,  6.0547e-01, -5.6006e-01, -1.1981e-01,\n",
      "        -1.2622e-01, -3.6426e-01,  8.1445e-01,  1.7422e+00,  4.9097e-01,\n",
      "         3.0200e-01,  4.2944e-01,  2.1216e-01,  7.7441e-01, -4.6680e-01,\n",
      "         1.9421e-01,  5.8167e-02,  6.0364e-02,  4.4043e-01, -9.8755e-02,\n",
      "         9.0723e-01,  1.9541e+00,  3.0884e-01, -1.8555e-01,  5.1465e-01,\n",
      "         1.1218e-01,  2.0156e+00,  7.3389e-01,  1.5771e+00,  3.1372e-01,\n",
      "         1.4043e+00,  5.7129e-02,  1.6133e+00,  4.6436e-01,  6.1279e-01,\n",
      "         1.2463e-01,  2.1641e+00,  1.0752e+00,  2.0557e-01,  1.6406e+00,\n",
      "         4.0356e-01,  8.5938e-02, -2.7393e-01,  1.9277e+00,  5.6934e-01,\n",
      "         4.2969e-01,  8.8501e-02, -3.1311e-02, -7.5806e-02,  4.8706e-01,\n",
      "        -3.4882e-02,  1.7542e-01,  1.5137e-01,  3.1055e-01,  9.8572e-03,\n",
      "         9.6436e-01,  1.7783e+00,  5.3906e-01,  3.7329e-01,  5.5127e-01,\n",
      "         1.9470e-01, -3.6450e-01, -3.5840e-01, -3.6328e-01,  2.1133e+00,\n",
      "         8.7451e-01,  1.2803e+00,  3.9429e-01,  1.9421e-01,  5.7617e-01,\n",
      "         1.1688e-02,  1.1807e+00, -3.3984e-01,  1.8418e+00,  7.9053e-01,\n",
      "         5.8057e-01,  6.6211e-01,  3.3911e-01,  1.4990e-01,  5.4639e-01,\n",
      "        -2.6001e-02,  5.2686e-01,  1.0518e+00,  3.3618e-01, -5.9021e-02,\n",
      "         6.1572e-01, -1.5405e-01,  5.5518e-01, -9.8877e-02,  2.2676e+00,\n",
      "         1.0332e+00,  6.7188e-01,  8.8574e-01,  3.7524e-01, -2.7710e-01,\n",
      "         2.0137e+00,  6.4648e-01,  7.1338e-01,  7.8711e-01,  9.2480e-01,\n",
      "         1.4014e-01,  5.9814e-02, -1.9421e-01,  4.5435e-01, -3.0054e-01,\n",
      "        -1.3831e-01,  9.0088e-01, -2.5488e-01,  7.7979e-01,  2.7930e-01,\n",
      "         3.4985e-01,  2.4785e+00,  5.6885e-01,  5.2686e-01,  6.6797e-01,\n",
      "        -2.7588e-01,  5.2344e-01,  6.6211e-01,  1.8672e+00,  2.8320e-01,\n",
      "        -7.0654e-01,  7.0618e-02,  2.1265e-01, -8.2458e-02,  2.1729e-02,\n",
      "         7.7344e-01,  7.1106e-02, -9.0485e-03, -4.3896e-01, -3.6560e-02,\n",
      "        -7.0654e-01,  4.3042e-01, -3.5718e-01,  3.9697e-01, -4.0723e-01,\n",
      "         2.0566e+00,  5.7812e-01,  4.7412e-01,  3.1152e-01,  3.6792e-01,\n",
      "         1.7041e+00,  1.7285e+00,  2.3157e-01,  4.1699e-01, -1.6589e-01,\n",
      "        -5.7178e-01, -2.8809e-01,  1.5068e+00,  6.3379e-01,  2.5415e-01,\n",
      "         1.8350e+00,  9.8730e-01,  6.2109e-01, -2.9492e-01, -1.0315e-01,\n",
      "         1.7842e+00,  6.5723e-01,  3.9771e-01,  7.0215e-01,  1.9023e+00,\n",
      "         7.0605e-01,  6.1572e-01,  7.9834e-01,  2.0273e+00,  7.7393e-01,\n",
      "         5.5469e-01,  2.9639e-01,  1.2197e+00,  1.0879e+00,  7.2217e-01,\n",
      "         5.8398e-01,  6.2744e-01,  2.8149e-01,  2.0723e+00,  8.3203e-01,\n",
      "         2.3572e-01,  1.8525e+00,  7.6123e-01,  2.1094e-01,  1.2842e+00,\n",
      "         4.2505e-01, -1.3635e-01,  1.6738e+00,  1.1162e+00,  6.9971e-01,\n",
      "         3.7866e-01,  1.0957e+00,  4.7778e-01,  4.4739e-02,  1.7559e+00,\n",
      "         3.2886e-01,  4.4385e-01, -1.0162e-02,  1.9580e+00,  7.7686e-01,\n",
      "         6.7725e-01,  7.7393e-02,  4.8340e-02, -3.1079e-01, -1.7419e-01,\n",
      "         1.7480e+00,  1.6826e+00,  7.4854e-01,  5.8740e-01, -6.3525e-01,\n",
      "        -3.9001e-02, -2.4683e-01,  5.3516e-01,  3.6304e-01,  1.9883e+00,\n",
      "         7.9492e-01,  6.8945e-01,  3.5449e-01,  3.6426e-01,  2.1211e+00,\n",
      "         4.7729e-01,  1.3293e-01,  2.2773e+00,  4.9780e-01,  3.5596e-01,\n",
      "         1.8838e+00,  4.6826e-01,  2.6221e-01, -6.8945e-01,  6.9189e-01,\n",
      "        -7.1167e-02,  1.4287e+00, -4.4458e-01,  2.2891e+00,  6.9336e-01,\n",
      "         5.2197e-01,  6.2598e-01,  7.2217e-01, -3.5669e-01,  2.1348e+00,\n",
      "         4.4360e-01,  8.0420e-01, -2.1606e-01,  1.7419e-01, -5.5029e-01,\n",
      "        -3.8239e-02, -5.4297e-01,  1.0693e+00,  7.1436e-01,  4.7827e-01,\n",
      "         9.8572e-03,  2.0239e-01,  6.3965e-02, -2.4731e-01, -5.4639e-01,\n",
      "         5.7129e-01,  1.8091e-01,  2.0984e-01, -5.9418e-02, -6.1475e-01,\n",
      "         4.0356e-01, -1.6040e-01, -2.9419e-01,  1.4629e+00,  2.4866e-01,\n",
      "         8.7402e-01,  1.8237e-01,  3.6743e-01,  3.3569e-01,  2.5273e+00,\n",
      "         5.8154e-01,  4.6875e-01,  2.2344e+00,  6.3916e-01,  4.6680e-01,\n",
      "         2.0039e+00,  6.8652e-01,  8.4375e-01,  1.9604e-01,  8.2178e-01,\n",
      "         9.8267e-03,  4.1699e-01,  2.6709e-01,  5.3906e-01,  4.3848e-01,\n",
      "         8.0444e-02,  1.5405e-01,  3.6499e-01,  1.9470e-02,  6.3574e-01,\n",
      "         1.9863e+00,  4.8755e-01,  1.5645e+00,  7.2559e-01, -5.3192e-02,\n",
      "         3.8623e-01,  2.1973e+00,  6.5088e-01,  5.2295e-01,  1.9111e+00,\n",
      "         6.5332e-01,  7.7490e-01,  6.8176e-02,  3.2983e-01, -1.0699e-01,\n",
      "        -7.0605e-01,  6.1328e-01,  1.6321e-01,  2.3352e-01,  1.9812e-01,\n",
      "        -3.8422e-02,  2.1973e+00,  5.7617e-01, -2.0605e-01,  1.2285e+00,\n",
      "        -3.9136e-01,  9.3408e-01,  2.3750e+00,  8.2129e-01,  6.7236e-01,\n",
      "        -2.9028e-01,  2.1155e-01,  2.8955e-01, -2.3117e-02,  2.3770e+00,\n",
      "         1.1182e+00,  2.0886e-01,  1.3535e+00,  1.6821e-01, -8.0127e-01,\n",
      "        -5.5029e-01, -2.6221e-01,  2.1641e+00,  5.2930e-01,  2.3633e+00,\n",
      "         8.1250e-01,  6.6260e-01, -2.3901e-01,  4.1602e-01,  5.8838e-02,\n",
      "         2.2539e+00,  6.4209e-01,  5.8301e-01,  1.3984e+00,  6.4941e-01,\n",
      "         2.3008e+00,  8.2373e-01,  9.1846e-01,  4.9146e-01,  2.2578e+00,\n",
      "         9.4580e-01,  7.8857e-01,  4.8511e-01,  3.5889e-01,  1.8242e+00,\n",
      "         6.9482e-01,  7.7588e-01,  2.8662e-01,  1.8828e+00,  1.0693e+00,\n",
      "         1.3123e-01,  1.8779e+00,  1.0166e+00,  3.5498e-01,  4.5532e-02,\n",
      "         1.5195e+00,  4.8218e-01,  1.1338e+00,  2.2578e+00,  1.0098e+00,\n",
      "         2.6978e-01,  3.6377e-02,  2.4629e+00,  1.0781e+00,  5.7227e-01,\n",
      "         3.7598e-01,  2.5586e-01,  1.3955e+00,  5.8008e-01,  1.0811e+00,\n",
      "         2.9785e-01,  1.3398e+00,  5.0049e-01,  6.7285e-01,  6.2354e-01,\n",
      "        -6.7041e-01,  2.4785e+00,  9.5996e-01,  8.1152e-01,  7.8418e-01,\n",
      "        -6.2195e-02, -2.8149e-01,  1.8341e-02, -2.5195e-01,  1.4990e-01,\n",
      "         8.8574e-01, -2.4414e-02,  4.2383e-01, -3.0640e-01,  2.5176e+00,\n",
      "         8.4766e-01,  6.4502e-01,  5.6458e-03,  9.3201e-02,  2.1777e+00,\n",
      "         7.0801e-01,  2.8271e-01,  5.3418e-01,  1.6638e-01, -7.3853e-02,\n",
      "         8.7500e-01,  1.5308e-01,  2.1973e+00,  7.9785e-01,  4.9683e-01,\n",
      "         3.0078e-01,  2.0742e+00,  6.1914e-01,  4.0405e-01,  2.4109e-01,\n",
      "         1.9763e-01, -1.7847e-01,  4.5996e-01,  6.1963e-01, -1.2158e-01,\n",
      "         5.5371e-01, -5.7556e-02,  1.3806e-01,  1.2964e-01,  4.3433e-01,\n",
      "        -2.4731e-01,  2.3691e+00,  7.8906e-01,  6.7529e-01, -4.1406e-01,\n",
      "         6.4307e-01, -2.4573e-01,  2.5977e-01,  1.7070e+00,  2.4695e-01,\n",
      "         5.9204e-02,  3.1958e-01,  5.6250e-01,  3.7994e-02,  2.0532e-01,\n",
      "         4.5483e-01,  7.7051e-01,  8.0469e-01, -7.5000e-01,  2.2012e+00,\n",
      "         7.7344e-01,  8.8574e-01,  6.2842e-01, -2.1484e-01,  5.8411e-02,\n",
      "         5.5084e-03, -2.1863e-01,  4.0552e-01, -9.9609e-01,  6.7261e-02,\n",
      "         5.7764e-01, -2.9639e-01,  1.8237e-01, -2.9077e-01,  1.8936e+00,\n",
      "         1.2002e+00,  5.8789e-01,  1.0908e+00, -3.0200e-01,  2.1094e+00,\n",
      "         6.0205e-01,  6.7676e-01,  8.9746e-01, -2.9321e-01,  6.0596e-01,\n",
      "        -3.0807e-02,  3.5938e-01,  5.0964e-02,  2.0728e-01, -9.1858e-02,\n",
      "         2.0618e-01, -4.2554e-01,  5.7178e-01, -4.2334e-01,  5.7715e-01,\n",
      "         4.0576e-01, -1.7993e-01,  7.6416e-01, -3.9673e-01, -5.8411e-02,\n",
      "         1.5503e-02,  6.2988e-01,  3.4644e-01,  3.4009e-01,  4.7290e-01,\n",
      "         6.6602e-01,  1.6514e+00,  1.0469e+00,  5.1270e-01,  3.3252e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 4.8047e-01,  9.8083e-02, -1.5710e-01, -5.9277e-01, -5.3857e-01,\n",
      "        -3.2031e-01, -1.2451e+00,  5.6244e-02, -4.4434e-01, -4.9365e-01,\n",
      "        -1.1859e-01,  9.1602e-01,  4.1138e-01, -1.2756e-01,  7.2119e-01,\n",
      "        -8.7256e-01,  8.4473e-02,  3.0835e-01,  1.8848e-01,  3.8452e-01,\n",
      "         9.7461e-01, -3.7451e-01,  4.5654e-01, -3.0444e-01,  2.7295e-01,\n",
      "         2.7612e-01,  4.4702e-01,  1.5391e+00, -1.2802e-02,  3.0127e-01,\n",
      "         3.1616e-01,  4.3384e-01,  1.2432e+00, -2.0667e-01, -1.4172e-01,\n",
      "        -4.3182e-03,  1.1494e+00, -4.4092e-01,  1.2891e-01,  5.1904e-01,\n",
      "        -5.3711e-01, -4.1626e-01, -4.8340e-01, -4.3823e-01, -1.1957e-01,\n",
      "        -6.7627e-02, -4.9133e-02,  6.8604e-01,  5.6982e-01, -2.8955e-01,\n",
      "        -4.8242e-01, -2.4304e-01, -1.1917e-02,  1.9934e-01,  5.9766e-01,\n",
      "        -1.9727e-01,  6.3049e-02, -2.4182e-01,  3.2397e-01,  5.1904e-01,\n",
      "        -6.4990e-01, -4.7314e-01, -1.6797e-01, -3.7378e-01,  2.3096e-01,\n",
      "         3.5693e-01,  3.7549e-01,  3.9917e-01,  1.1445e+00, -2.1835e-02,\n",
      "         4.0527e-01,  2.9248e-01,  5.3564e-01,  1.6973e+00,  2.9224e-01,\n",
      "         4.2993e-01, -8.5693e-01, -2.0361e-01,  2.2717e-01, -6.9763e-02,\n",
      "        -4.2529e-01, -4.2114e-01,  8.1738e-01, -7.8906e-01, -7.1582e-01,\n",
      "        -9.0137e-01, -3.7695e-01, -2.3547e-01,  1.3232e-01,  1.5979e-01,\n",
      "         1.5859e+00, -1.1810e-01,  2.7271e-01,  6.5430e-01,  3.7231e-01,\n",
      "        -6.1230e-01, -8.7842e-01, -1.3359e+00, -1.7151e-01,  2.0630e-01,\n",
      "        -5.9082e-02,  1.2109e-01, -2.5903e-01, -9.7705e-01, -3.6304e-01,\n",
      "         9.2102e-02, -1.4502e-01, -8.0322e-01, -6.3867e-01,  3.3691e-01,\n",
      "        -1.1807e+00, -4.6899e-01, -1.7896e-01, -1.4542e-02, -3.6426e-01,\n",
      "        -9.0967e-01, -5.2441e-01, -1.6736e-01,  1.9226e-01, -2.1008e-01,\n",
      "        -7.5195e-01, -1.5942e-01, -1.7676e-01,  1.1006e+00,  8.9478e-02,\n",
      "         2.3880e-02, -1.0211e-01, -5.1172e-01,  4.6539e-03, -6.7902e-03,\n",
      "         3.2153e-01, -1.2250e-01,  3.3521e-01, -8.0017e-02, -1.8692e-02,\n",
      "         8.9111e-01, -3.9600e-01,  1.8054e-01, -4.9902e-01, -4.1333e-01,\n",
      "         2.8589e-01,  3.3374e-01,  4.9805e-01,  4.3530e-01, -6.4270e-02,\n",
      "        -6.5765e-03,  3.0289e-02,  3.3508e-02, -7.7930e-01, -6.4697e-01,\n",
      "        -6.3232e-01,  2.3975e-01,  6.8481e-02,  1.0557e+00,  7.3633e-01,\n",
      "        -5.2832e-01, -3.2959e-01,  6.0254e-01, -5.2783e-01, -4.6570e-02,\n",
      "        -3.1006e-01, -1.1554e-01,  1.7666e+00, -4.5361e-01, -4.2822e-01,\n",
      "        -2.9712e-01, -1.6187e-01,  1.2607e+00, -9.6680e-01,  4.4098e-03,\n",
      "         1.2432e+00,  6.9727e-01, -3.7524e-01, -3.4668e-01,  8.3057e-01,\n",
      "        -1.0166e+00, -8.4991e-03,  1.3359e+00,  3.2153e-01, -5.2539e-01,\n",
      "        -1.7786e-01, -2.4866e-01,  4.0894e-01,  7.6953e-01,  5.3711e-01,\n",
      "        -9.8682e-01, -4.2969e-01,  6.8604e-02, -2.4890e-01,  9.5886e-02,\n",
      "        -2.1021e-01,  8.6670e-01,  5.3223e-01,  1.9414e+00,  8.5010e-01,\n",
      "         1.4346e+00, -1.0996e+00, -7.9651e-02, -9.0332e-02,  1.6514e+00,\n",
      "        -7.9199e-01, -3.3667e-01, -1.2988e-01,  1.5918e+00, -8.7524e-02,\n",
      "        -6.2549e-01, -5.5371e-01, -2.0035e-02,  3.8135e-01,  2.2852e-01,\n",
      "         1.6113e+00,  1.2415e-01, -8.6963e-01, -2.5391e-01, -7.5500e-02,\n",
      "         1.3789e+00, -1.1143e+00,  1.4392e-01, -4.4067e-01, -4.4995e-01,\n",
      "         4.7469e-04,  4.6191e-01,  1.1816e+00,  5.7520e-01, -7.6074e-01,\n",
      "        -6.2891e-01, -1.6443e-01,  1.4854e+00,  1.0849e-02, -1.0297e-01,\n",
      "         8.4082e-01, -4.4629e-01, -6.5869e-01, -2.4109e-01, -2.4158e-01,\n",
      "         1.1055e+00, -1.7834e-01,  5.8594e-01, -7.9834e-01,  2.3438e-01,\n",
      "         1.8463e-02,  1.0771e+00,  1.4740e-02,  2.0850e-01,  1.0913e-01,\n",
      "         3.8940e-01,  1.4707e+00, -5.1611e-01,  4.5679e-01, -2.5195e-01,\n",
      "         1.0663e-01, -6.5234e-01, -1.1603e-01,  5.6244e-02, -1.9934e-01,\n",
      "         3.6646e-01, -1.0590e-01,  1.2433e-01, -7.1582e-01, -7.4280e-02,\n",
      "         4.2206e-02, -1.2018e-01,  6.0352e-01, -8.2471e-01,  4.4678e-01,\n",
      "        -5.4565e-02, -3.2397e-01, -4.3506e-01, -1.1981e-01,  9.9609e-02,\n",
      "        -2.8760e-01, -1.8909e-01,  1.0681e-01, -1.7065e-01, -2.2717e-01,\n",
      "        -6.8164e-01, -6.1963e-01, -1.8021e-02,  1.0388e-01, -1.4001e-01,\n",
      "         3.3740e-01,  2.7271e-01,  2.9712e-01,  4.5654e-02,  2.4463e-01,\n",
      "         4.1162e-01,  2.3547e-01, -2.0068e-01,  3.0225e-01,  5.4443e-01,\n",
      "        -8.5754e-02,  1.9971e-01, -3.4058e-01,  5.6396e-01,  4.4092e-01,\n",
      "         8.5510e-02,  1.8726e-01,  4.7070e-01, -5.9131e-01, -3.8086e-01,\n",
      "         4.4824e-01, -9.1919e-02,  4.4678e-02, -1.1334e-01,  7.0618e-02,\n",
      "         9.8877e-03,  6.8994e-01, -8.5645e-01, -9.9182e-02,  7.5439e-01,\n",
      "         1.0156e+00,  7.8369e-02,  4.8706e-02,  8.4229e-02, -3.8818e-02,\n",
      "         1.1816e-01, -8.8348e-03,  5.9033e-01, -8.9111e-01, -1.6565e-01,\n",
      "         6.8604e-01,  7.9053e-01, -7.6562e-01,  7.4890e-02,  1.2268e-01,\n",
      "         9.2334e-01,  5.8984e-01,  1.9543e-01,  3.3032e-01, -9.1797e-01,\n",
      "        -3.1921e-02,  4.0649e-02, -3.9258e-01,  3.7964e-01, -4.6729e-01,\n",
      "         3.0322e-01, -1.3330e-01,  2.2424e-01,  9.5068e-01, -1.0059e+00,\n",
      "        -2.8394e-01, -2.0605e-01,  1.4026e-01, -4.5386e-01,  6.7041e-01,\n",
      "         1.4612e-01,  1.9736e+00,  7.3633e-01,  1.4590e+00, -9.1504e-01,\n",
      "         4.2023e-02,  2.1753e-01, -2.5772e-02,  1.8127e-01,  9.2773e-02,\n",
      "         1.0811e+00, -8.0811e-02,  9.5581e-02, -1.8042e-01,  5.8545e-01,\n",
      "         5.3802e-02,  1.4707e+00, -9.7705e-01,  2.8516e-01, -6.3782e-02,\n",
      "         8.1934e-01,  6.1279e-01, -1.1123e+00, -2.4063e-02,  5.9906e-02,\n",
      "         6.0059e-01, -6.9189e-01, -4.5239e-01, -1.0410e+00, -2.5000e-01,\n",
      "        -4.7363e-01, -8.0908e-01,  1.5808e-01,  4.4873e-01,  1.5588e-01,\n",
      "         1.6865e+00,  3.9136e-01, -1.2018e-01, -4.1504e-01, -3.8916e-01,\n",
      "        -9.8816e-02,  4.9805e-01, -5.1758e-01,  3.3350e-01, -5.0000e-01,\n",
      "         2.5537e-01, -1.3599e-01,  5.1367e-01,  1.2549e+00,  7.0801e-01,\n",
      "        -7.7051e-01, -1.3501e-01,  4.4165e-01,  3.3997e-02,  7.8857e-01,\n",
      "         9.1858e-03,  7.2412e-01,  2.1033e-01, -2.0813e-01, -4.1199e-02,\n",
      "         7.5781e-01, -2.2498e-01,  1.0278e-01,  1.5322e+00,  1.7188e-01,\n",
      "         1.0297e-01,  9.5459e-01, -5.7178e-01,  2.6343e-01, -3.7354e-02,\n",
      "         5.2686e-01,  5.6787e-01, -2.7173e-01, -7.5391e-01,  5.0732e-01,\n",
      "        -3.8354e-01,  1.3086e-01,  1.7029e-01, -1.8933e-01, -2.6782e-01,\n",
      "         1.9592e-01,  4.4043e-01,  1.6235e-01,  1.7168e+00,  5.3271e-01,\n",
      "        -7.0947e-01, -8.2129e-01, -5.5359e-02, -5.6885e-01, -4.6826e-01,\n",
      "         2.3486e-01,  5.8301e-01,  1.5503e-01,  1.3779e+00,  2.5757e-01,\n",
      "        -2.2839e-01, -2.0459e-01, -8.9307e-01, -9.1003e-02,  5.8411e-02,\n",
      "         2.3071e-01,  4.2725e-01,  2.5000e-01,  1.0322e+00,  1.3794e-01,\n",
      "        -1.8677e-01,  1.5662e-01, -4.1821e-01,  5.3955e-01, -1.3037e-01,\n",
      "        -2.9434e-02, -7.8760e-01, -3.2715e-01, -2.4182e-01, -2.9053e-01,\n",
      "        -2.1155e-01,  4.2163e-01,  2.5024e-01,  1.2520e+00, -4.6875e-01,\n",
      "         3.1494e-01, -1.0345e-01,  1.4062e-01,  3.2642e-01,  3.7354e-01,\n",
      "         3.5181e-01,  8.2861e-01,  2.2888e-01, -2.3523e-01,  4.1962e-02,\n",
      "        -2.5830e-01,  5.7343e-02, -4.4800e-01,  2.8687e-01, -5.5664e-01,\n",
      "         4.3433e-01,  1.8213e-01, -8.0139e-02,  1.3838e+00,  2.3572e-01,\n",
      "        -3.3105e-01,  1.0181e-01,  1.0527e+00, -5.2490e-02,  1.0088e+00,\n",
      "        -1.8994e-01, -3.8940e-01, -2.6440e-01, -3.2056e-01, -3.6670e-01,\n",
      "         4.0771e-02,  4.0833e-02,  5.1709e-01, -6.4307e-01, -1.8567e-01,\n",
      "        -4.0381e-01,  1.6525e-02, -4.9902e-01,  4.0259e-01, -4.9463e-01,\n",
      "         5.0049e-01,  3.4576e-02,  1.6152e+00, -5.5273e-01,  4.6069e-01,\n",
      "         6.2622e-02, -7.2876e-02,  6.8652e-01,  5.9668e-01, -7.3242e-02,\n",
      "         5.2930e-01, -1.1533e+00, -6.1719e-01,  8.9294e-02,  1.3564e+00,\n",
      "        -2.1643e-01,  1.0004e-01,  7.0312e-01,  1.3438e+00,  5.9668e-01,\n",
      "         5.0098e-01,  2.6929e-01,  1.2930e+00,  7.0654e-01, -4.3945e-02,\n",
      "         1.4099e-01,  5.0635e-01, -8.4229e-02, -1.9580e-01, -5.2917e-02,\n",
      "         2.8809e-01,  1.6055e+00, -6.2207e-01,  1.9385e-01,  1.6296e-01,\n",
      "         1.2830e-01,  6.6553e-01,  1.0566e+00, -4.1260e-01, -1.0089e-01,\n",
      "         1.4150e+00, -2.5635e-01,  3.6523e-01,  1.0938e+00, -2.9321e-01,\n",
      "         3.3789e-01,  8.1348e-01, -3.1812e-01,  2.4063e-02,  5.0842e-02,\n",
      "         1.0244e+00,  2.2827e-01,  4.7607e-01,  1.3525e+00,  1.5039e-01,\n",
      "         3.7573e-01,  4.4580e-01,  1.1094e+00, -3.2861e-01,  2.5293e-01,\n",
      "         1.8823e-01,  5.4492e-01,  1.2051e+00, -3.1885e-01,  2.0447e-01,\n",
      "        -2.9739e-02, -4.1309e-01, -2.6073e-03,  9.3115e-01, -7.6514e-01,\n",
      "        -6.7578e-01, -5.0781e-01, -9.1431e-02, -5.4199e-01, -4.8633e-01,\n",
      "         1.1002e-02,  9.4092e-01, -5.9326e-01,  4.5215e-01, -5.7080e-01,\n",
      "         6.3110e-02,  2.2559e+00,  6.9287e-01,  1.8408e-01,  2.0039e+00,\n",
      "        -6.4844e-01,  4.0161e-02,  1.7822e+00, -8.6475e-01, -4.6570e-02,\n",
      "        -6.5137e-01,  2.3486e-01, -1.7700e-01,  6.5137e-01,  1.7114e-01,\n",
      "         1.8926e+00,  6.4795e-01,  3.2837e-01,  2.0251e-01,  4.1772e-01,\n",
      "         1.7695e+00,  2.7856e-01,  3.3496e-01, -1.0889e+00, -2.3645e-01,\n",
      "        -9.5557e-01, -1.1602e+00, -1.3049e-01, -1.5601e-01,  7.4658e-01,\n",
      "         2.2034e-01, -2.7051e-01,  6.8665e-03, -2.1631e-01, -4.3335e-01,\n",
      "        -1.7603e-01,  1.4319e-01, -3.1433e-02, -4.5728e-01, -1.1484e+00,\n",
      "        -7.5684e-01, -4.0796e-01, -6.8213e-01, -7.8552e-02,  1.2900e+00,\n",
      "         8.4534e-02,  4.9146e-01, -5.8838e-01,  4.1968e-01,  1.3037e-01,\n",
      "        -7.6447e-03,  1.7148e+00,  4.8218e-01,  5.6732e-02,  1.8555e+00,\n",
      "        -1.5344e-01,  1.1786e-01,  5.9326e-01,  1.4883e+00,  9.7168e-02,\n",
      "        -5.0293e-02,  7.3608e-02,  8.5266e-02, -3.7085e-01, -4.1064e-01,\n",
      "         1.7175e-01,  2.9068e-02,  4.0283e-01,  4.2334e-01,  2.3401e-01,\n",
      "        -4.4165e-01,  4.2944e-01,  2.3975e-01,  3.6646e-01,  1.4775e+00,\n",
      "         1.7981e-01,  2.2083e-01, -6.3599e-02,  1.6123e+00, -2.5049e-01,\n",
      "         1.0156e-01,  5.3662e-01,  1.4648e+00, -6.9824e-02,  2.3853e-01,\n",
      "        -1.0654e+00, -2.3914e-01, -1.7834e-01, -4.8438e-01, -3.6646e-01,\n",
      "        -6.4893e-01,  2.0959e-01,  1.4170e+00, -7.1533e-01,  4.1162e-01,\n",
      "        -1.4465e-01,  5.0879e-01,  2.0215e-01, -6.9824e-02,  1.5674e+00,\n",
      "        -5.7471e-01, -2.5977e-01, -1.8933e-01, -7.7148e-01, -6.6699e-01,\n",
      "         2.8931e-01,  1.7266e+00,  6.5820e-01, -7.9736e-01, -1.1670e+00,\n",
      "        -2.4915e-01, -5.3564e-01,  3.0151e-01,  1.4512e+00,  3.6792e-01,\n",
      "        -5.4810e-02,  1.5361e+00, -7.7148e-02, -2.6562e-01, -4.7900e-01,\n",
      "        -8.3466e-03,  4.3945e-01,  1.0537e+00,  1.0333e-01,  1.0000e+00,\n",
      "        -2.3132e-01, -6.8512e-03,  3.4204e-01,  1.7656e+00, -8.6060e-02,\n",
      "         5.6000e-03,  2.3022e-01,  7.2168e-01,  1.6035e+00, -1.1322e-01,\n",
      "         2.4695e-01,  3.3105e-01,  1.8730e+00, -2.3767e-01,  6.1621e-01,\n",
      "         1.9248e+00, -1.1237e-01,  1.8164e-01,  1.5693e+00, -7.3047e-01,\n",
      "         1.5884e-02,  1.1836e+00,  2.0483e-01, -3.7012e-01,  5.6213e-02,\n",
      "         8.3374e-02,  1.1797e+00, -1.6980e-01,  1.1499e-01,  2.8516e-01,\n",
      "         1.5146e+00, -8.2666e-01,  5.0934e-02,  4.0967e-01,  5.3613e-01,\n",
      "         8.9746e-01, -4.4360e-01, -7.9407e-02, -9.2920e-01, -3.2300e-01,\n",
      "        -6.2842e-01,  4.0869e-01, -3.5767e-01,  8.6212e-03,  6.3965e-01,\n",
      "         2.1896e-02, -9.9854e-01, -5.0928e-01, -7.8809e-01, -7.5537e-01,\n",
      "        -1.5601e-01, -7.8174e-01, -2.6901e-02, -1.5344e-01,  1.8420e-01,\n",
      "        -3.3703e-03,  1.5127e+00, -1.0840e+00, -4.0942e-01,  1.9592e-02,\n",
      "         4.6753e-01,  1.4707e+00, -2.6147e-01, -3.0005e-01, -1.3389e+00,\n",
      "        -8.4961e-02, -3.7012e-01, -2.2473e-01,  4.8431e-02,  1.2207e+00,\n",
      "        -8.5840e-01,  4.7607e-01,  8.8959e-03,  1.0762e+00, -1.2139e+00,\n",
      "        -4.7913e-02,  4.4702e-01, -4.6851e-01, -3.4814e-01, -6.6455e-01,\n",
      "        -3.4448e-01,  2.5977e-01, -7.0068e-01, -6.4697e-01, -2.7563e-01,\n",
      "        -3.7891e-01,  4.7241e-01,  8.4290e-02,  1.6514e+00, -7.5146e-01,\n",
      "        -4.7150e-03, -9.5361e-01, -6.6553e-01,  7.2632e-03,  3.7866e-01,\n",
      "        -1.4038e-01, -6.7822e-01, -1.3721e-01,  1.9852e-02, -2.0483e-01,\n",
      "        -4.9316e-01, -3.5095e-02, -6.1182e-01, -2.7246e-01, -2.2644e-01,\n",
      "         3.5522e-01,  1.8047e+00, -1.6455e-01, -6.3428e-01, -1.3733e-01,\n",
      "        -4.5068e-01, -8.5059e-01, -2.5171e-01, -5.7324e-01, -2.3987e-01,\n",
      "        -3.8550e-01, -6.9629e-01, -1.9910e-01, -7.0703e-01, -1.2683e-01,\n",
      "         3.4644e-01,  1.8320e+00,  2.6343e-01,  1.0889e-01, -1.7102e-01,\n",
      "         4.4531e-01,  1.8867e+00, -3.6377e-01, -2.8641e-02, -2.2131e-01,\n",
      "        -5.7227e-01, -3.0127e-01, -3.1909e-01,  1.8024e-03, -1.3965e-01,\n",
      "        -5.0537e-02, -7.0459e-01, -5.6396e-01, -5.0195e-01, -3.1714e-01,\n",
      "        -4.1724e-01, -1.0928e+00, -5.7959e-01, -1.1426e+00, -8.3545e-01,\n",
      "        -5.9570e-01, -1.5137e-01, -4.9268e-01,  3.9368e-03, -5.1758e-01,\n",
      "        -6.0791e-02, -6.3818e-01,  1.7712e-01,  9.9658e-01,  4.6436e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: avg_val_f1 reached 0.03158 (best 0.12353), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_3.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(5.7696, device='cuda:0')\tavg_answer_loss:  tensor(5.1383, device='cuda:0')\tavg_type_loss:  tensor(0.1263, device='cuda:0')\tavg_val_f1:  0.03157894611358643\tavg_val_em:  0.0\tavg_val_prec:  0.01875\tavg_val_recall:  0.1\n",
      "sequence_output: tensor([[[ 0.6192, -0.1432,  0.3184,  ...,  1.6859,  0.7814,  0.2417],\n",
      "         [ 0.3157,  0.4302,  0.0852,  ...,  0.6303,  0.4991, -0.1599],\n",
      "         [ 0.6126,  0.1338,  0.1419,  ...,  0.6217,  0.5397, -0.5174],\n",
      "         ...,\n",
      "         [ 0.2319,  0.2575, -0.2820,  ...,  0.0793,  0.1922, -0.4109],\n",
      "         [-0.0257,  0.2761, -0.0186,  ..., -0.1650, -0.0455, -0.0780],\n",
      "         [ 0.0081,  0.0321, -0.0223,  ..., -0.0800, -0.0328, -0.0731]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.4386e-01, -4.5695e-02,  1.9930e-01,  ...,  3.7164e-01,\n",
      "           1.3586e+00,  2.9858e-01],\n",
      "         [ 5.9904e-02,  7.0469e-01, -5.6308e-02,  ...,  1.2018e+00,\n",
      "           7.7062e-01,  2.0205e-01],\n",
      "         [ 7.3067e-01,  2.3817e-01,  5.0479e-01,  ...,  6.6175e-01,\n",
      "           9.4741e-01,  1.0799e-01],\n",
      "         ...,\n",
      "         [ 6.0741e-04,  5.3867e-02, -3.3954e-02,  ..., -9.2101e-02,\n",
      "          -2.6331e-02, -9.3153e-02],\n",
      "         [-6.0798e-03,  5.0095e-02, -1.6305e-02,  ..., -7.8374e-02,\n",
      "          -2.3945e-02, -8.1538e-02],\n",
      "         [-1.1553e-02,  4.8591e-02, -3.2138e-02,  ..., -1.0963e-01,\n",
      "          -3.3577e-02, -8.3080e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1700,  0.2900,  0.0171,  ...,  1.1909,  0.6414, -0.0838],\n",
      "         [ 0.7172, -0.1185,  0.2875,  ...,  0.4042,  0.0188,  0.1454],\n",
      "         [ 0.0367,  0.1379, -0.1002,  ...,  0.7266,  0.9455, -0.3046],\n",
      "         ...,\n",
      "         [ 0.2211,  0.4999, -0.1057,  ...,  0.1693,  0.1674, -0.3884],\n",
      "         [ 0.0066,  0.0436, -0.0271,  ..., -0.0889, -0.0221, -0.0646],\n",
      "         [-0.0094,  0.0572, -0.0253,  ..., -0.0779, -0.0298, -0.0969]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9143,  0.6390,  0.1557,  ...,  1.6729,  1.2484,  0.1322],\n",
      "         [ 0.1555,  1.2293, -0.1738,  ...,  0.5487,  0.7291, -0.2305],\n",
      "         [ 0.3024,  0.4390,  0.0703,  ...,  0.8062,  0.5853, -0.3954],\n",
      "         ...,\n",
      "         [-0.0069,  0.0593, -0.0207,  ..., -0.1164, -0.0293, -0.0825],\n",
      "         [-0.0085,  0.0825, -0.0350,  ..., -0.0812, -0.0349, -0.0915],\n",
      "         [-0.0129,  0.0539, -0.0093,  ..., -0.0897, -0.0354, -0.0892]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1994e+00,  5.0068e-01,  1.3580e-03,  ...,  1.5929e+00,\n",
      "           1.1938e+00, -3.3672e-01],\n",
      "         [ 4.4016e-01,  5.3635e-01,  1.2361e-01,  ...,  9.0928e-01,\n",
      "           4.0412e-01,  2.9135e-01],\n",
      "         [ 6.6884e-01, -1.2714e-01,  2.0998e-01,  ...,  1.1557e+00,\n",
      "           1.2925e+00,  3.6655e-01],\n",
      "         ...,\n",
      "         [-8.3708e-03,  4.8955e-02, -9.9000e-03,  ..., -8.1357e-02,\n",
      "          -2.5153e-02,  1.9809e-02],\n",
      "         [ 2.5385e-02,  5.8543e-02, -2.9689e-02,  ..., -8.0329e-02,\n",
      "          -2.4590e-02, -6.6451e-02],\n",
      "         [-1.2680e-02,  5.9450e-02, -1.7238e-02,  ..., -7.4519e-02,\n",
      "          -2.4656e-02,  1.9077e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6592,  0.0557,  0.1075,  ...,  1.0409,  0.7545,  0.1216],\n",
      "         [ 0.3161,  0.4321,  0.3300,  ...,  1.3999,  0.4200,  0.0356],\n",
      "         [ 0.7180,  0.8211, -0.0025,  ...,  1.6442,  0.5231, -0.2170],\n",
      "         ...,\n",
      "         [-0.0018,  0.0413, -0.0146,  ..., -0.0900, -0.0175, -0.0810],\n",
      "         [ 0.0085,  0.0395, -0.0128,  ..., -0.0760, -0.0142, -0.0725],\n",
      "         [ 0.0190,  0.0786, -0.0545,  ..., -0.1067, -0.0256, -0.1693]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5855,  0.1507, -0.0879,  ...,  1.0125,  0.6905, -0.3153],\n",
      "         [ 0.0525,  0.6692, -0.2898,  ...,  0.4895,  0.1768, -0.5075],\n",
      "         [ 0.5811,  0.1506, -0.2286,  ...,  0.9667,  0.9962, -0.2114],\n",
      "         ...,\n",
      "         [ 0.0089,  0.0514, -0.0244,  ..., -0.0772, -0.0339, -0.0743],\n",
      "         [ 0.0536,  0.1165, -0.0230,  ..., -0.0565, -0.0335, -0.2131],\n",
      "         [-0.0131,  0.0435, -0.0171,  ..., -0.0696, -0.0155,  0.0154]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5377,  0.3599, -0.0031,  ...,  0.7488,  0.5983, -0.3810],\n",
      "         [ 0.6103,  0.1991,  0.2678,  ...,  1.3377,  0.5582, -0.1579],\n",
      "         [ 0.6406,  0.2193,  0.0305,  ...,  1.4902,  0.4009, -0.1558],\n",
      "         ...,\n",
      "         [-0.0418,  0.5194, -0.0259,  ...,  0.2070,  0.0366, -0.1736],\n",
      "         [-0.1886,  0.1908, -0.0833,  ...,  0.8386,  0.1003, -0.9552],\n",
      "         [-0.0115,  0.0645, -0.0142,  ..., -0.0646, -0.0245, -0.1213]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 3.5347e-01,  5.3057e-01,  8.3137e-02,  ...,  1.3624e+00,\n",
      "           1.1017e+00, -1.4393e-01],\n",
      "         [ 3.1092e-01,  6.5588e-01,  2.8047e-01,  ...,  8.1044e-01,\n",
      "           6.7149e-01, -3.1058e-01],\n",
      "         [ 7.7926e-01,  3.1166e-01, -1.2488e-01,  ...,  1.2139e+00,\n",
      "           1.1355e+00, -2.9352e-01],\n",
      "         ...,\n",
      "         [ 9.9630e-03,  5.8632e-02, -1.1845e-02,  ..., -1.2647e-01,\n",
      "          -1.3111e-02, -8.9391e-02],\n",
      "         [-9.2773e-02,  1.7126e-01, -5.5632e-01,  ...,  8.3307e-04,\n",
      "          -3.9329e-02, -3.2703e-01],\n",
      "         [ 3.4964e-03,  5.0997e-02, -4.6745e-02,  ..., -9.1440e-02,\n",
      "           1.1504e-02, -1.2331e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.0613,  0.3804,  0.0504,  ..., -0.2365,  0.2284, -0.4569],\n",
      "         [ 0.6406, -0.1958,  0.1610,  ...,  0.4112,  0.2988,  0.1303],\n",
      "         [ 0.8967,  0.0377,  0.0799,  ..., -0.6442,  0.3251, -0.2060],\n",
      "         ...,\n",
      "         [ 0.5198,  0.1737, -0.2483,  ...,  0.1219,  0.0339, -0.2111],\n",
      "         [ 0.0259,  0.0536, -0.0240,  ..., -0.0798, -0.0188,  0.0167],\n",
      "         [-0.0086,  0.0621,  0.0060,  ..., -0.0810, -0.0313, -0.0858]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.2070e-01,  3.3012e-01, -1.8097e-01,  ...,  1.4824e+00,\n",
      "           1.0737e+00, -1.1608e-01],\n",
      "         [-3.0215e-02,  1.6622e-01,  1.4087e-01,  ...,  2.8970e-01,\n",
      "           3.4818e-01,  2.6995e-01],\n",
      "         [ 7.6993e-01, -1.3847e-01, -6.6520e-02,  ...,  1.6883e+00,\n",
      "           7.5330e-01, -3.8234e-02],\n",
      "         ...,\n",
      "         [-1.3342e-02,  4.3463e-02, -2.3496e-02,  ..., -7.5466e-02,\n",
      "          -2.5756e-02, -8.4922e-02],\n",
      "         [-2.4929e-02,  5.7413e-02,  2.3784e-03,  ..., -8.7776e-02,\n",
      "          -2.9683e-02, -9.0430e-02],\n",
      "         [-1.1124e-03,  4.1421e-02, -1.9241e-02,  ..., -5.7411e-02,\n",
      "          -3.5607e-02,  7.5744e-03]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.0695,  0.3862, -0.0118,  ...,  0.5778,  1.1218, -0.1657],\n",
      "         [ 0.0699,  0.7408,  0.3045,  ...,  1.4458,  0.4215, -0.1893],\n",
      "         [ 0.5674,  0.3955, -0.0779,  ..., -0.0080,  1.1614, -0.0788],\n",
      "         ...,\n",
      "         [ 0.1414,  0.2363, -0.0791,  ..., -0.0110,  0.0194, -0.3480],\n",
      "         [-0.0136,  0.0557, -0.0216,  ..., -0.0864, -0.0403,  0.0157],\n",
      "         [-0.0039,  0.0464, -0.0191,  ..., -0.0810, -0.0240, -0.0751]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8004,  0.1281,  0.0521,  ...,  1.3890,  1.2328,  0.1028],\n",
      "         [ 0.0537,  0.8452, -0.1237,  ...,  1.2482,  0.3919, -0.2717],\n",
      "         [ 0.8732,  0.1328,  0.3743,  ...,  0.9150,  1.7060, -0.2245],\n",
      "         ...,\n",
      "         [ 0.2522, -0.0965,  0.0056,  ...,  0.4975,  0.3044, -0.3252],\n",
      "         [ 0.1934,  0.4151,  0.0943,  ...,  0.1725,  0.1894, -0.2501],\n",
      "         [-0.0057,  0.0515, -0.0243,  ..., -0.0870, -0.0306, -0.0813]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2878,  0.0783, -0.1878,  ...,  1.5654,  1.7378,  0.1371],\n",
      "         [ 0.3370,  0.7138,  0.0275,  ...,  0.6946,  0.5772,  0.1042],\n",
      "         [ 0.0970,  0.5425,  0.0803,  ..., -0.4832,  0.4982, -0.2626],\n",
      "         ...,\n",
      "         [-0.0090,  0.0506, -0.0199,  ..., -0.0745, -0.0268, -0.0958],\n",
      "         [-0.0453,  0.2020,  0.0295,  ..., -0.0190, -0.1209, -0.1426],\n",
      "         [-0.0024,  0.0570, -0.0248,  ..., -0.0767, -0.0172,  0.0148]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.9312,  0.5901,  0.0257,  ...,  0.6739,  0.9777, -0.2393],\n",
      "         [ 0.2377,  0.2888,  0.0500,  ...,  0.6654,  0.6597,  0.0530],\n",
      "         [-0.3562,  0.6830,  0.2505,  ...,  0.7118,  0.2436, -0.1511],\n",
      "         ...,\n",
      "         [ 0.1006,  0.1034,  0.0591,  ...,  0.2616,  0.2058,  0.0144],\n",
      "         [-0.0060,  0.0406, -0.0258,  ..., -0.0772, -0.0309,  0.0127],\n",
      "         [ 0.0661,  0.0481, -0.0813,  ..., -0.1497,  0.0330, -0.2051]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1980e+00,  2.5671e-01, -2.6478e-01,  ...,  6.5839e-01,\n",
      "           1.0982e+00, -3.7198e-01],\n",
      "         [ 7.2953e-02,  4.5934e-01,  6.1352e-02,  ...,  2.8140e-01,\n",
      "           5.5350e-02, -2.8514e-01],\n",
      "         [ 9.4704e-01,  2.0031e-01, -2.4865e-01,  ...,  1.1642e+00,\n",
      "           7.0265e-01, -3.3812e-01],\n",
      "         ...,\n",
      "         [ 9.5296e-03,  4.6526e-02, -1.5419e-02,  ..., -8.1136e-02,\n",
      "          -2.4993e-02, -7.9670e-02],\n",
      "         [-2.3836e-02,  2.2701e-01, -6.5535e-02,  ...,  4.8657e-02,\n",
      "          -8.6570e-02,  4.5816e-02],\n",
      "         [-1.0139e-03,  5.0194e-02, -7.2670e-03,  ..., -7.8175e-02,\n",
      "          -1.5972e-02, -7.6873e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9713,  0.2307,  0.0349,  ...,  0.9281,  1.2987,  0.0728],\n",
      "         [ 0.1453,  0.5194, -0.3274,  ...,  0.1965,  0.6030,  0.3662],\n",
      "         [ 0.3042,  0.0027, -0.2734,  ...,  0.9735,  1.3526, -0.6705],\n",
      "         ...,\n",
      "         [-0.0056,  0.0630, -0.0044,  ..., -0.0895, -0.0412, -0.0869],\n",
      "         [-0.0427,  0.0824, -0.1381,  ...,  0.0067, -0.0733, -0.2301],\n",
      "         [ 0.5392,  0.3429, -0.0587,  ...,  0.4668,  0.1428, -0.6754]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3709,  0.4533,  0.0624,  ...,  1.2762,  1.3399,  0.3956],\n",
      "         [ 0.4206,  0.1153,  0.3786,  ...,  0.2696,  0.4858,  0.0575],\n",
      "         [ 0.9714, -0.0794,  0.1267,  ...,  0.1290,  1.5725,  0.2538],\n",
      "         ...,\n",
      "         [-0.0167,  0.0603,  0.0025,  ..., -0.0744, -0.0280, -0.0784],\n",
      "         [-0.0086,  0.0540, -0.0181,  ..., -0.0762, -0.0240, -0.0873],\n",
      "         [-0.0035,  0.1272, -0.0296,  ..., -0.0374, -0.1121, -0.0363]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1084,  0.2295,  0.1443,  ...,  0.7253,  1.1112, -0.2663],\n",
      "         [ 0.4646,  0.5169,  0.1170,  ...,  1.0311,  0.4789,  0.0273],\n",
      "         [ 0.7112,  0.2340,  0.1089,  ...,  0.5271,  1.2702, -0.3111],\n",
      "         ...,\n",
      "         [ 0.0116,  0.0408, -0.0186,  ..., -0.0839, -0.0222, -0.0735],\n",
      "         [ 0.0240,  0.2228, -0.0764,  ..., -0.0719, -0.1050, -0.0829],\n",
      "         [-0.0120,  0.0467, -0.0274,  ..., -0.0636, -0.0191, -0.0868]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0280e+00,  4.9331e-01,  1.4581e-01,  ...,  1.2961e+00,\n",
      "           1.3085e+00, -1.5317e-01],\n",
      "         [ 1.9432e-01,  1.7985e-01,  7.1987e-01,  ...,  1.3873e+00,\n",
      "           4.4959e-01, -1.0236e-01],\n",
      "         [ 5.2556e-01, -1.0627e-02,  2.9066e-01,  ...,  1.7348e+00,\n",
      "           7.3537e-01, -8.4308e-02],\n",
      "         ...,\n",
      "         [-6.7039e-03,  6.2187e-02,  6.5951e-04,  ..., -1.0852e-01,\n",
      "          -1.9779e-02, -8.5126e-02],\n",
      "         [ 3.2250e-01,  1.5496e-01, -2.4561e-01,  ...,  5.6530e-01,\n",
      "           1.9428e-01, -8.6598e-01],\n",
      "         [-2.7890e-03,  1.6986e-01,  1.8922e-02,  ..., -7.2511e-02,\n",
      "           3.3243e-03, -2.6264e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6063,  0.4016, -0.3688,  ...,  1.4915,  0.7424, -0.1522],\n",
      "         [ 0.1147,  0.4439,  0.1188,  ...,  1.6596,  0.6117,  0.4199],\n",
      "         [ 0.4756, -0.1550,  0.0269,  ...,  1.3132,  0.5415,  0.2243],\n",
      "         ...,\n",
      "         [-0.0091,  0.0420, -0.0236,  ..., -0.0829, -0.0207, -0.0869],\n",
      "         [-0.2521,  0.7684,  0.0339,  ..., -0.4249, -0.0506, -0.1494],\n",
      "         [ 0.1164,  0.4784,  0.3213,  ..., -0.6818,  0.3180, -0.6420]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.8907e-01,  5.3533e-01,  3.1597e-02,  ...,  1.1565e+00,\n",
      "           9.5307e-01, -2.6485e-01],\n",
      "         [-1.2471e-01,  4.0265e-01,  1.4745e-01,  ...,  3.9859e-01,\n",
      "           5.0803e-01, -2.3268e-01],\n",
      "         [ 2.7306e-01,  5.3339e-01,  1.6506e-01,  ...,  4.9691e-01,\n",
      "           1.1476e-01, -1.2351e-01],\n",
      "         ...,\n",
      "         [ 3.6165e-02,  1.9037e-02, -1.5183e-01,  ..., -5.4362e-02,\n",
      "           9.4111e-02, -4.1763e-01],\n",
      "         [ 1.3691e-02,  4.0432e-02,  5.5094e-04,  ..., -7.5866e-02,\n",
      "          -2.3972e-02, -7.9785e-02],\n",
      "         [ 7.6061e-03,  2.5634e-01, -1.0954e-02,  ..., -1.2506e-01,\n",
      "          -1.1354e-02, -2.0753e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.6453e-01,  5.2878e-01,  3.8530e-02,  ...,  7.5524e-01,\n",
      "           9.8057e-01,  8.9163e-02],\n",
      "         [-1.5276e-01, -1.4573e-01, -3.0971e-02,  ...,  2.5773e-01,\n",
      "           5.0208e-01,  6.7023e-02],\n",
      "         [ 1.9878e-01, -6.4406e-02,  1.6115e-01,  ...,  1.0077e+00,\n",
      "           9.3865e-01, -4.1229e-01],\n",
      "         ...,\n",
      "         [-2.5773e-02,  5.1630e-02, -2.1505e-02,  ..., -8.6267e-02,\n",
      "          -2.8304e-02, -1.0595e-01],\n",
      "         [-1.1492e-02,  8.5643e-02, -6.1626e-02,  ..., -9.5336e-02,\n",
      "          -2.3015e-02, -2.2738e-01],\n",
      "         [ 2.6397e-02,  5.3882e-02,  3.2645e-04,  ..., -7.1927e-02,\n",
      "          -2.6203e-02, -8.8461e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5575,  0.5492,  0.5749,  ...,  1.0330,  0.8088,  0.5042],\n",
      "         [-0.1223, -0.4406,  0.1488,  ...,  1.0015,  0.5144,  0.3209],\n",
      "         [ 0.0509,  0.2713, -0.0098,  ..., -0.2014,  0.2017, -0.1806],\n",
      "         ...,\n",
      "         [ 0.0073,  0.0436, -0.0166,  ..., -0.1155, -0.0263, -0.0605],\n",
      "         [-0.0047,  0.0582, -0.0265,  ..., -0.1128, -0.0222,  0.0223],\n",
      "         [-0.0163,  0.0502, -0.0236,  ..., -0.0800, -0.0287, -0.0861]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 4.4672e-01,  3.7040e-01,  2.1660e-01,  ...,  1.4415e+00,\n",
      "           1.0881e+00, -2.6399e-01],\n",
      "         [ 4.7705e-02,  5.2452e-01,  5.2541e-01,  ...,  7.1463e-01,\n",
      "           7.0755e-01, -4.7073e-01],\n",
      "         [ 5.9272e-01,  7.5409e-01, -6.8206e-02,  ...,  1.1812e+00,\n",
      "           9.3013e-01, -1.5100e-01],\n",
      "         ...,\n",
      "         [ 1.1926e-01,  3.5241e-01, -4.4396e-02,  ..., -1.1352e-01,\n",
      "           5.7967e-02, -1.7209e-01],\n",
      "         [-4.6605e-02,  1.0912e-01, -3.6805e-02,  ..., -9.9402e-02,\n",
      "           6.0050e-04, -2.2179e-01],\n",
      "         [-1.7687e-02,  5.9596e-02, -3.2402e-02,  ..., -8.6643e-02,\n",
      "          -2.0576e-02,  1.8492e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6643,  0.1840,  0.2818,  ...,  1.5383,  1.0552, -0.3144],\n",
      "         [ 0.3810, -0.0835,  0.4482,  ...,  1.7965, -0.0973, -0.0193],\n",
      "         [ 0.0740, -0.1665,  0.0878,  ...,  0.3374,  0.6549,  0.2597],\n",
      "         ...,\n",
      "         [-0.0036,  0.0500, -0.0237,  ..., -0.0823, -0.0264, -0.0809],\n",
      "         [-0.0172,  0.0506, -0.0243,  ..., -0.0833, -0.0262, -0.0871],\n",
      "         [-0.0057,  0.0567, -0.0304,  ..., -0.0844, -0.0177, -0.1113]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3866,  0.3313, -0.1746,  ...,  0.2608,  0.2574, -0.1837],\n",
      "         [ 0.0144,  0.2053, -0.0450,  ...,  0.4763,  0.3690, -0.3170],\n",
      "         [ 0.6124,  0.5144,  0.1516,  ...,  0.0943,  0.4300, -0.3432],\n",
      "         ...,\n",
      "         [ 0.0015,  0.0605, -0.0277,  ..., -0.0863, -0.0289, -0.0874],\n",
      "         [ 0.0113,  0.0432, -0.0075,  ..., -0.0920, -0.0214, -0.0672],\n",
      "         [-0.0086,  0.0614, -0.0268,  ..., -0.0748, -0.0323, -0.0875]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9264,  0.4438, -0.0108,  ...,  1.7975,  1.4980,  0.0117],\n",
      "         [ 0.0861,  0.5284,  0.0020,  ...,  0.8601,  0.4562,  0.0500],\n",
      "         [-0.0728,  0.4840, -0.0430,  ..., -0.9212,  0.2779, -0.4314],\n",
      "         ...,\n",
      "         [ 0.0087,  0.0653, -0.0255,  ..., -0.0875, -0.0459, -0.0930],\n",
      "         [ 0.0264,  0.1940,  0.0205,  ..., -0.2166, -0.1023, -0.2421],\n",
      "         [ 0.1732,  0.2135, -0.1948,  ...,  0.1229,  0.2124, -0.6650]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.0542,  0.0905,  0.1851,  ...,  2.0506,  1.2796, -0.4089],\n",
      "         [ 0.8179, -0.2908,  0.4230,  ...,  2.6470,  0.6172, -0.1296],\n",
      "         [ 0.8364,  0.0991,  0.0746,  ...,  1.2275,  1.3540, -0.2269],\n",
      "         ...,\n",
      "         [-0.0053,  0.0519, -0.0252,  ..., -0.0695, -0.0308, -0.0896],\n",
      "         [-0.0194,  0.0417, -0.0231,  ..., -0.0864, -0.0280, -0.0878],\n",
      "         [ 0.0041,  0.0401,  0.0055,  ..., -0.0710, -0.0092, -0.0755]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2541,  0.1714,  0.1112,  ...,  1.5718,  1.3120,  0.3566],\n",
      "         [ 0.3268, -0.0578,  0.2780,  ...,  1.2980,  0.6018,  0.4577],\n",
      "         [ 0.7512,  0.1199, -0.2087,  ...,  2.0531,  1.3503, -0.0181],\n",
      "         ...,\n",
      "         [-0.0134,  0.3999,  0.1723,  ...,  0.0712,  0.0268, -0.2234],\n",
      "         [-0.0028,  0.0426, -0.0163,  ..., -0.1105, -0.0142,  0.0185],\n",
      "         [ 0.1894,  0.3048, -0.0432,  ...,  0.0445, -0.1104, -0.3535]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 2.7651e-01,  1.1649e-01,  2.3291e-01,  ...,  1.7013e+00,\n",
      "           1.4845e+00, -8.3510e-02],\n",
      "         [ 2.4335e-01,  4.0589e-01,  1.1129e-01,  ...,  9.8776e-01,\n",
      "           7.9229e-01,  5.3125e-02],\n",
      "         [ 4.7054e-02,  5.7575e-02,  7.6218e-02,  ...,  1.4273e+00,\n",
      "           8.2693e-01,  6.1227e-02],\n",
      "         ...,\n",
      "         [-4.8281e-04,  5.3870e-02, -1.6132e-02,  ..., -1.1318e-01,\n",
      "          -2.7149e-02, -9.2551e-02],\n",
      "         [ 1.5753e-01,  4.0557e-01, -3.8806e-01,  ..., -4.7559e-01,\n",
      "           2.7928e-01, -2.2473e-01],\n",
      "         [ 4.7094e-02,  1.4159e-01, -3.2668e-02,  ...,  6.2585e-02,\n",
      "           3.4094e-02, -4.0349e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5200,  0.4993, -0.2324,  ...,  1.3392,  1.4529, -0.0347],\n",
      "         [-0.1190,  0.0753, -0.3446,  ...,  1.0869,  0.6082,  0.0234],\n",
      "         [ 0.5041,  0.0427,  0.1852,  ...,  0.8497,  1.0652, -0.1251],\n",
      "         ...,\n",
      "         [-0.0104,  0.0684, -0.0240,  ..., -0.0702, -0.0420, -0.0769],\n",
      "         [ 0.0103,  0.0418, -0.0232,  ..., -0.1117, -0.0128, -0.0723],\n",
      "         [-0.0155,  0.0466, -0.0195,  ..., -0.0800, -0.0363, -0.0681]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9971,  0.4294, -0.2198,  ...,  0.7918,  1.2467, -0.0029],\n",
      "         [ 0.2630,  0.3587, -0.4369,  ..., -0.0893,  0.6756, -0.5560],\n",
      "         [ 0.5392, -0.7147, -0.1902,  ...,  0.1124,  1.0864, -0.3984],\n",
      "         ...,\n",
      "         [-0.0071,  0.0489, -0.0064,  ..., -0.0769, -0.0291, -0.0937],\n",
      "         [-0.0451,  0.0784, -0.0418,  ..., -0.0450, -0.0415,  0.0233],\n",
      "         [-0.0109,  0.0546, -0.0264,  ..., -0.1192, -0.0308, -0.0855]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8251,  0.7083,  0.2399,  ...,  2.6978,  1.1696,  0.0988],\n",
      "         [ 0.1644,  0.6445,  0.1667,  ...,  1.4259,  0.6637, -0.0951],\n",
      "         [ 0.9225,  1.0012,  0.6494,  ...,  1.4675,  0.4822,  0.3446],\n",
      "         ...,\n",
      "         [ 0.0076,  0.0578, -0.0298,  ..., -0.0743, -0.0360, -0.0917],\n",
      "         [-0.0058,  0.0764, -0.0334,  ..., -0.0726, -0.0285, -0.0690],\n",
      "         [ 0.1761, -0.1657, -0.0182,  ..., -0.3640,  0.4872, -0.1340]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5388,  0.0634, -0.1671,  ...,  1.4684,  0.7317, -0.0208],\n",
      "         [ 0.5299,  0.2074,  0.5311,  ...,  0.6795,  0.2795,  0.4597],\n",
      "         [ 0.2479,  0.2735,  0.1828,  ...,  0.0775,  0.2932,  0.0507],\n",
      "         ...,\n",
      "         [ 0.0210,  0.0710,  0.0554,  ...,  0.1187, -0.0207, -0.1967],\n",
      "         [-0.0115,  0.0431, -0.0104,  ..., -0.0775, -0.0269, -0.0818],\n",
      "         [-0.0069,  0.0553, -0.0165,  ..., -0.0883, -0.0163, -0.0901]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1336e+00,  3.3127e-01,  3.1006e-01,  ...,  1.8614e+00,\n",
      "           1.3045e+00,  1.0444e-01],\n",
      "         [ 3.4842e-01,  6.8989e-02,  2.7167e-01,  ...,  8.1156e-01,\n",
      "           1.4806e-01,  1.8421e-04],\n",
      "         [ 1.7750e-01,  2.1768e-01,  5.0346e-01,  ...,  6.6625e-01,\n",
      "           4.6788e-01, -6.9886e-02],\n",
      "         ...,\n",
      "         [-8.4307e-03,  5.4686e-02, -2.7170e-02,  ..., -8.4528e-02,\n",
      "          -2.5052e-02, -8.7921e-02],\n",
      "         [-1.0892e-02,  7.1888e-02, -1.1454e-02,  ..., -7.4472e-02,\n",
      "          -1.7145e-02, -6.5255e-02],\n",
      "         [ 1.3422e-02,  4.2505e-02, -1.8083e-02,  ..., -1.1526e-01,\n",
      "          -2.9200e-02, -8.5433e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8881,  0.5297,  0.4206,  ...,  1.9209,  1.3374, -0.2044],\n",
      "         [ 0.4966,  0.4867,  0.1037,  ...,  1.9237,  1.0560, -0.0083],\n",
      "         [ 1.1611,  1.0094,  0.0138,  ...,  1.4272,  1.2079, -0.3145],\n",
      "         ...,\n",
      "         [-0.0430,  0.0904, -0.0418,  ..., -0.0260, -0.0136, -0.2292],\n",
      "         [-0.0094,  0.0605, -0.0240,  ..., -0.0748, -0.0320,  0.0057],\n",
      "         [-0.0042,  0.0565, -0.0283,  ..., -0.0706, -0.0293, -0.0955]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8705,  0.3012, -0.1710,  ...,  2.3559,  1.2979, -0.3158],\n",
      "         [-0.1139,  0.2368, -0.0652,  ...,  1.4736,  0.2375, -0.1304],\n",
      "         [ 0.5309,  0.1067,  0.2792,  ...,  1.6603,  0.7235, -0.3511],\n",
      "         ...,\n",
      "         [ 0.0184,  0.0598,  0.0093,  ..., -0.0430,  0.0053, -0.2493],\n",
      "         [-0.0342,  0.1420,  0.0223,  ..., -0.0883, -0.0276, -0.1790],\n",
      "         [-0.0535,  0.1906,  0.0206,  ..., -0.0770, -0.0474, -0.2471]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8387,  0.5686, -0.1133,  ...,  1.2041,  1.6694, -0.3504],\n",
      "         [ 0.2683, -0.4260, -0.0237,  ...,  1.7246,  0.9182, -0.0224],\n",
      "         [ 0.9586,  0.5069,  0.0217,  ...,  1.7629,  1.7056, -0.0190],\n",
      "         ...,\n",
      "         [-0.0032,  0.0377, -0.0147,  ..., -0.0841, -0.0337, -0.0665],\n",
      "         [-0.0073,  0.0549, -0.0268,  ..., -0.0757, -0.0215, -0.0837],\n",
      "         [-0.0184,  0.0412, -0.0183,  ..., -0.0845, -0.0303, -0.0854]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.7572e-01,  8.0177e-02, -4.5265e-02,  ...,  1.8683e+00,\n",
      "           3.4886e-01, -1.0406e-01],\n",
      "         [-2.4758e-01,  4.7875e-01, -4.0727e-01,  ...,  9.5170e-01,\n",
      "           6.7578e-01, -1.5086e-01],\n",
      "         [-1.6552e-03, -3.2201e-01, -2.9510e-01,  ...,  8.9294e-01,\n",
      "           1.0456e+00, -5.5402e-02],\n",
      "         ...,\n",
      "         [-1.2752e-03,  4.9438e-02, -1.7539e-02,  ..., -7.2069e-02,\n",
      "          -2.2090e-02, -8.9587e-02],\n",
      "         [-1.0079e-02,  4.9224e-02, -2.5456e-02,  ..., -7.2632e-02,\n",
      "          -3.8589e-02, -9.5782e-02],\n",
      "         [-1.6473e-02,  4.9095e-02, -2.3810e-02,  ..., -8.3001e-02,\n",
      "          -2.9042e-02, -9.1521e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 4.4262e-01,  5.2180e-01,  5.5723e-02,  ...,  1.5379e+00,\n",
      "           9.6167e-01, -3.0857e-01],\n",
      "         [ 3.5544e-01,  5.3488e-01, -7.2647e-02,  ..., -2.2186e-01,\n",
      "           2.3724e-01,  8.6842e-02],\n",
      "         [ 5.9808e-01,  5.0044e-01,  2.9006e-01,  ...,  1.8736e+00,\n",
      "           3.2705e-01,  2.7541e-02],\n",
      "         ...,\n",
      "         [ 1.3980e-02,  1.0310e-01, -8.7999e-02,  ...,  2.0017e-01,\n",
      "           1.0403e-01, -2.3953e-01],\n",
      "         [-2.3038e-01,  3.3213e-01, -9.3166e-02,  ..., -2.2750e-01,\n",
      "           1.9630e-01, -3.4434e-01],\n",
      "         [-4.0088e-03,  5.4783e-02, -6.2625e-04,  ..., -9.3109e-02,\n",
      "          -2.6002e-02, -8.0254e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1656e+00,  1.1563e-01, -2.2194e-04,  ...,  1.3664e+00,\n",
      "           1.1929e+00, -3.2210e-01],\n",
      "         [ 4.6817e-01,  4.8170e-01,  3.7046e-01,  ...,  1.4409e+00,\n",
      "           6.7701e-01, -8.8389e-02],\n",
      "         [ 3.7828e-01,  3.3886e-01,  4.3451e-01,  ...,  7.8408e-01,\n",
      "           9.7188e-01, -1.3237e-01],\n",
      "         ...,\n",
      "         [ 5.4856e-03,  5.2134e-02, -2.5275e-02,  ..., -1.1168e-01,\n",
      "          -3.8033e-02, -8.9606e-02],\n",
      "         [ 5.0529e-03,  6.5595e-01, -1.3200e-01,  ..., -1.4996e-01,\n",
      "           2.8804e-01, -6.1558e-02],\n",
      "         [-5.5469e-03,  4.4482e-02, -2.5923e-02,  ..., -7.4591e-02,\n",
      "          -4.0691e-02,  1.2430e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.7302, -0.1104,  0.2574,  ...,  1.2851,  1.1457,  0.0715],\n",
      "         [ 0.4400,  1.1131, -0.5475,  ...,  0.0143,  0.2784, -0.1775],\n",
      "         [-0.0082,  0.1104, -0.1384,  ...,  0.6625,  0.3434,  0.2861],\n",
      "         ...,\n",
      "         [-0.0096,  0.0528, -0.0146,  ..., -0.0782, -0.0264, -0.0977],\n",
      "         [ 0.0147,  0.1447,  0.0125,  ..., -0.0318, -0.0638, -0.2284],\n",
      "         [ 0.0026,  0.0509, -0.0208,  ..., -0.0860, -0.0270,  0.0121]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0317,  0.3228, -0.1734,  ...,  1.8836,  1.4009, -0.1352],\n",
      "         [ 0.6491,  0.3724,  0.1502,  ...,  2.1372,  0.7041,  0.2707],\n",
      "         [ 1.0192,  0.5207, -0.0893,  ...,  1.1049,  0.7583, -0.4941],\n",
      "         ...,\n",
      "         [ 0.0109,  0.0512, -0.0259,  ..., -0.0805, -0.0227, -0.0899],\n",
      "         [-0.0141,  0.0514, -0.0242,  ..., -0.0765, -0.0214, -0.0916],\n",
      "         [-0.0119,  0.0472, -0.0178,  ..., -0.0805, -0.0157, -0.0802]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9067,  0.1154, -0.3075,  ...,  1.9863,  0.9617,  0.0996],\n",
      "         [ 0.1782,  0.6811, -0.0906,  ..., -0.1390,  0.7743,  0.1514],\n",
      "         [ 0.2132,  0.5574, -0.0176,  ..., -0.4219,  0.9661, -0.4275],\n",
      "         ...,\n",
      "         [ 0.0070,  0.0526, -0.0185,  ..., -0.1059, -0.0343, -0.0868],\n",
      "         [ 0.1745,  0.2762, -0.0669,  ...,  0.2359,  0.2759, -0.2364],\n",
      "         [-0.0125,  0.0553, -0.0080,  ..., -0.0793, -0.0333,  0.0101]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3071,  0.0484, -0.2212,  ...,  1.7150,  1.3665,  0.0680],\n",
      "         [-0.4460,  0.3718,  0.3749,  ...,  0.3467,  0.6259, -0.2992],\n",
      "         [ 0.7542, -0.2207,  0.0132,  ...,  1.1730,  0.6911, -0.1427],\n",
      "         ...,\n",
      "         [ 0.0072,  0.1047,  0.0118,  ..., -0.0080, -0.0815, -0.2059],\n",
      "         [ 0.2316,  0.2416, -0.1222,  ...,  0.1233,  0.2191,  0.1083],\n",
      "         [-0.0897,  0.2269, -0.0143,  ..., -0.0698, -0.1805, -0.3080]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5193,  0.2884, -0.2296,  ...,  1.4071,  0.8643, -0.4506],\n",
      "         [ 0.4112,  0.6854,  0.4864,  ...,  0.8289,  0.7373,  0.5089],\n",
      "         [ 0.6201,  0.6160,  0.0219,  ...,  0.0514,  0.9379,  0.1801],\n",
      "         ...,\n",
      "         [ 0.0055,  0.0540, -0.0165,  ..., -0.0765, -0.0265, -0.0783],\n",
      "         [ 0.0660,  0.4823,  0.0221,  ...,  0.0908,  0.1109, -0.0358],\n",
      "         [ 0.1334,  0.2814, -0.0204,  ..., -0.0344,  0.1360,  0.0130]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7625,  0.5247,  0.0596,  ...,  2.6636,  0.3128,  0.0621],\n",
      "         [ 0.6677,  0.3299, -0.2970,  ...,  2.2272,  0.5339,  0.2535],\n",
      "         [ 0.4565, -0.6752, -0.1224,  ...,  1.4694,  1.3474, -0.3609],\n",
      "         ...,\n",
      "         [-0.0040,  0.0406, -0.0203,  ..., -0.0741, -0.0182, -0.0725],\n",
      "         [ 0.0187, -0.0226,  0.0248,  ..., -0.1405, -0.0447, -0.1002],\n",
      "         [ 0.0452,  0.1957,  0.1886,  ..., -0.2157, -0.0367, -0.2525]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.9921e-01,  2.0201e-01, -1.9905e-01,  ...,  1.7763e+00,\n",
      "           8.9860e-01,  6.1477e-01],\n",
      "         [ 4.9966e-01,  4.6202e-01,  3.8997e-01,  ...,  6.5692e-01,\n",
      "           1.7286e-01,  1.6843e-01],\n",
      "         [ 8.7157e-01,  5.3627e-01,  8.3117e-02,  ...,  3.8866e-01,\n",
      "           6.0172e-01, -4.8110e-01],\n",
      "         ...,\n",
      "         [ 4.5015e-03,  4.9723e-02, -1.2057e-02,  ..., -8.8651e-02,\n",
      "          -2.9708e-02, -8.7072e-02],\n",
      "         [-1.1997e-02,  5.6467e-02, -2.0106e-02,  ..., -7.3082e-02,\n",
      "          -3.0395e-02, -8.8608e-02],\n",
      "         [-1.5968e-03,  6.1659e-02, -1.3249e-02,  ..., -7.4113e-02,\n",
      "          -3.5333e-02, -9.6386e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4452,  0.1700, -0.0509,  ...,  1.9341,  0.7045, -0.2585],\n",
      "         [-0.0499,  0.3294, -0.2157,  ...,  0.9340,  0.6393,  0.1712],\n",
      "         [ 0.2630,  0.2632,  0.0941,  ...,  1.1266,  0.0615, -0.1736],\n",
      "         ...,\n",
      "         [ 0.0045,  0.0574, -0.0223,  ..., -0.0855, -0.0284, -0.0842],\n",
      "         [-0.0043,  0.0493,  0.0090,  ..., -0.0757, -0.0279, -0.1148],\n",
      "         [-0.0189,  0.0549, -0.0299,  ..., -0.1100, -0.0357, -0.0860]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3049,  0.5371, -0.0875,  ...,  2.4168,  1.0403, -0.4984],\n",
      "         [ 0.2537, -0.1238, -0.2278,  ...,  1.0661,  0.5997, -0.4635],\n",
      "         [ 0.6284,  0.1369,  0.2653,  ...,  1.3136,  1.3758, -0.4662],\n",
      "         ...,\n",
      "         [ 0.0152,  0.0575,  0.0123,  ..., -0.0712, -0.0324,  0.0123],\n",
      "         [-0.0278,  0.1563, -0.0656,  ..., -0.0325, -0.0648, -0.1903],\n",
      "         [ 0.0210, -0.0074, -0.1313,  ...,  0.0774,  0.0626, -0.4119]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.4799e-01,  4.3413e-01,  4.7142e-01,  ...,  2.7085e+00,\n",
      "           1.5333e+00, -2.8046e-01],\n",
      "         [ 2.0172e-01,  3.0782e-01, -6.9732e-02,  ...,  4.8243e-01,\n",
      "           5.4030e-01,  2.7301e-01],\n",
      "         [ 6.0846e-01,  3.0572e-03,  3.4243e-01,  ...,  2.1847e+00,\n",
      "           1.5154e+00, -2.9576e-01],\n",
      "         ...,\n",
      "         [ 1.4441e-01,  2.8075e-01, -2.2364e-01,  ...,  1.1285e-01,\n",
      "          -7.9635e-02, -4.5101e-01],\n",
      "         [-9.1248e-03,  5.2301e-02, -4.3426e-04,  ..., -8.0242e-02,\n",
      "          -2.2788e-02, -8.7064e-02],\n",
      "         [ 5.1771e-02,  1.0780e-01, -1.7623e-01,  ..., -2.3023e-01,\n",
      "           1.2063e-01, -2.7513e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7903,  0.4324, -0.0618,  ...,  1.7667,  1.0740, -0.0498],\n",
      "         [ 0.3202,  0.6151,  0.0325,  ...,  0.3619,  0.3787, -0.0039],\n",
      "         [ 0.6717,  0.7621, -0.1441,  ...,  0.9729,  0.2119, -0.3900],\n",
      "         ...,\n",
      "         [-0.0090,  0.0500, -0.0148,  ..., -0.0812, -0.0169, -0.0847],\n",
      "         [ 0.0119,  0.0551, -0.0211,  ..., -0.0751, -0.0281, -0.0876],\n",
      "         [-0.0123,  0.0365, -0.0189,  ..., -0.0691, -0.0251, -0.0908]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9417, -0.0057, -0.0130,  ...,  1.5811,  0.7334, -0.6125],\n",
      "         [ 0.9272,  0.2355,  0.2311,  ...,  1.7975,  0.6887,  0.5940],\n",
      "         [ 0.3022,  0.3979, -0.0125,  ...,  1.3865,  0.2624,  0.1635],\n",
      "         ...,\n",
      "         [ 0.0043,  0.0735, -0.0215,  ..., -0.0719, -0.0388, -0.0678],\n",
      "         [-0.0518,  0.1568,  0.0132,  ..., -0.0837, -0.0939, -0.2399],\n",
      "         [ 0.1415,  0.3129, -0.1444,  ...,  0.1735,  0.1426, -0.3766]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9875,  0.0358, -0.1541,  ...,  1.5099,  1.9657, -0.0033],\n",
      "         [ 0.3700,  0.7455,  0.3057,  ...,  1.3218,  0.7609, -0.2443],\n",
      "         [ 0.3719,  0.1530, -0.1178,  ...,  1.2994,  0.6438, -0.0595],\n",
      "         ...,\n",
      "         [ 0.0092,  0.0417, -0.0564,  ..., -0.0585, -0.0230, -0.0877],\n",
      "         [ 0.0436,  0.0153, -0.3550,  ..., -0.2218,  0.1926,  0.0095],\n",
      "         [ 0.0103,  0.0754, -0.0270,  ..., -0.0843, -0.0242, -0.0904]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5153,  0.3597, -0.0236,  ...,  1.4701,  0.7813, -0.0756],\n",
      "         [-0.1308,  0.2110, -0.0553,  ...,  0.7629,  0.5466, -0.2372],\n",
      "         [-0.0200,  0.1674, -0.0330,  ...,  1.2657,  0.3791, -0.3653],\n",
      "         ...,\n",
      "         [ 0.0090,  0.0476, -0.0276,  ..., -0.0766, -0.0400, -0.0677],\n",
      "         [-0.0020,  0.0605, -0.0138,  ..., -0.0871, -0.0216, -0.0874],\n",
      "         [-0.0078,  0.0391, -0.0209,  ..., -0.0846, -0.0306, -0.0857]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7385,  0.1401,  0.0468,  ...,  1.5428,  0.9680, -0.0202],\n",
      "         [ 0.3640, -0.3875,  0.3137,  ...,  1.4519,  0.5847,  0.2366],\n",
      "         [ 0.1112, -0.4332,  0.0666,  ...,  0.0504,  1.3253, -0.4943],\n",
      "         ...,\n",
      "         [-0.0079,  0.0783, -0.0201,  ..., -0.0982, -0.0177, -0.0859],\n",
      "         [-0.0185,  0.0550, -0.0210,  ..., -0.0771, -0.0324, -0.0902],\n",
      "         [-0.0127,  0.0484, -0.0193,  ..., -0.0776, -0.0380, -0.0795]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0229,  0.4771, -0.1848,  ...,  2.5269,  1.0622, -0.3089],\n",
      "         [ 0.8871,  0.1305, -0.1851,  ...,  0.6001,  0.9637, -0.1045],\n",
      "         [ 0.9623, -0.0088, -0.1978,  ...,  2.5570,  0.6530, -0.4960],\n",
      "         ...,\n",
      "         [-0.0215,  0.0540, -0.0159,  ..., -0.0783, -0.0142, -0.0805],\n",
      "         [-0.0131,  0.0506, -0.0238,  ..., -0.1081, -0.0378, -0.0759],\n",
      "         [-0.0069,  0.2974, -0.0460,  ..., -0.1538, -0.1513, -0.3484]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[-0.0246,  0.3836, -0.0423,  ...,  1.5822,  1.4667, -0.1615],\n",
      "         [ 0.7794,  0.5187,  0.0954,  ...,  1.2423,  0.6529, -0.0862],\n",
      "         [-0.0676,  0.0247,  0.0242,  ...,  0.2392,  0.2376, -0.7341],\n",
      "         ...,\n",
      "         [ 0.1973,  0.2395,  0.0129,  ...,  0.1898,  0.0790, -0.3596],\n",
      "         [ 0.0120,  0.0271, -0.0161,  ..., -0.0804, -0.0338, -0.0854],\n",
      "         [-0.0103,  0.0535, -0.0299,  ..., -0.0783, -0.0345, -0.1033]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3600,  0.3957, -0.3131,  ...,  1.9389,  1.1250,  0.1881],\n",
      "         [ 0.6473,  0.3100,  0.0253,  ...,  0.7798,  0.3888,  0.2056],\n",
      "         [ 0.4102,  0.1853, -0.1741,  ...,  0.7307,  0.5721,  0.2277],\n",
      "         ...,\n",
      "         [-0.0069,  0.0631, -0.0062,  ..., -0.0712, -0.0270, -0.0930],\n",
      "         [-0.0172,  0.0564, -0.0266,  ..., -0.0817, -0.0369, -0.1014],\n",
      "         [-0.0080,  0.0376, -0.0338,  ..., -0.0841, -0.0294, -0.0829]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0319e+00,  9.1510e-02, -1.5960e-01,  ...,  2.0757e+00,\n",
      "           9.0397e-01, -2.2031e-01],\n",
      "         [-1.2703e-01, -8.7261e-04, -5.6483e-02,  ...,  1.2753e+00,\n",
      "           5.5159e-01, -2.5240e-01],\n",
      "         [ 1.3882e-01,  2.4705e-01, -6.3291e-03,  ...,  1.5978e+00,\n",
      "           5.4863e-01,  2.2057e-01],\n",
      "         ...,\n",
      "         [ 7.8596e-03,  5.0259e-02, -2.3206e-02,  ..., -7.7535e-02,\n",
      "          -2.6792e-02, -7.9912e-02],\n",
      "         [ 1.1705e-02,  5.7177e-02, -1.5937e-02,  ..., -7.7366e-02,\n",
      "          -3.6666e-02, -8.5448e-02],\n",
      "         [ 3.6840e-02,  1.2384e-01, -4.2361e-02,  ...,  2.0629e-02,\n",
      "          -1.9492e-02, -1.3622e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.1870e-01, -1.4581e-02, -1.4682e-01,  ...,  1.8487e+00,\n",
      "           1.0781e+00, -3.5898e-01],\n",
      "         [ 8.7530e-01,  2.8536e-01, -1.5455e-01,  ...,  1.8850e+00,\n",
      "           8.0651e-01,  5.2478e-01],\n",
      "         [ 8.8356e-01,  4.0292e-02,  7.1705e-02,  ...,  1.3533e+00,\n",
      "           4.3996e-01, -4.0856e-02],\n",
      "         ...,\n",
      "         [-1.1272e-02,  6.2632e-02, -2.8014e-02,  ..., -8.6069e-02,\n",
      "          -4.4771e-02, -8.3316e-02],\n",
      "         [ 1.9795e-01,  3.7783e-01, -3.2812e-01,  ...,  1.1497e-01,\n",
      "           4.6838e-01, -9.2885e-01],\n",
      "         [ 1.8410e-02,  3.9735e-01, -3.8487e-04,  ...,  2.9650e-01,\n",
      "           2.2325e-01, -3.3347e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7423,  0.2169, -0.2168,  ...,  1.4905,  0.8650, -0.2023],\n",
      "         [ 0.2301,  1.0286, -0.4466,  ...,  0.5537,  0.4326,  0.0692],\n",
      "         [ 0.1543,  0.4760, -0.1867,  ...,  1.3471,  0.5400, -0.2925],\n",
      "         ...,\n",
      "         [-0.0082,  0.0509, -0.0177,  ..., -0.0734, -0.0289, -0.0820],\n",
      "         [-0.0182,  0.0446, -0.0249,  ..., -0.0823, -0.0353, -0.0814],\n",
      "         [ 0.0036,  0.0516, -0.0091,  ..., -0.1256, -0.0167, -0.0781]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7592,  0.0799, -0.1724,  ...,  2.3682,  1.6479,  0.1327],\n",
      "         [ 0.4279,  0.6597,  0.1979,  ...,  0.8542,  0.6172,  0.4035],\n",
      "         [ 0.2884,  0.1364, -0.1594,  ...,  0.0420,  0.8065,  0.5195],\n",
      "         ...,\n",
      "         [-0.0151,  0.0397, -0.0222,  ..., -0.0760, -0.0356, -0.0916],\n",
      "         [ 0.0138,  0.3287,  0.0583,  ..., -0.1624,  0.0122, -0.1654],\n",
      "         [-0.0324,  0.4326, -0.2301,  ..., -0.1114, -0.0711, -0.3325]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.3554,  0.2099, -0.2625,  ...,  2.1924,  1.2079, -0.4106],\n",
      "         [-0.3601,  0.3955, -0.1066,  ...,  0.3882,  0.4502,  0.3755],\n",
      "         [ 0.5242,  0.5112, -0.3619,  ...,  1.6209,  1.0965, -0.8533],\n",
      "         ...,\n",
      "         [-0.0102,  0.0411, -0.0257,  ..., -0.0897, -0.0277, -0.0812],\n",
      "         [ 0.2814,  0.6051, -0.1274,  ...,  0.3217,  0.3335, -0.1358],\n",
      "         [-0.0127,  0.0464, -0.0164,  ..., -0.0738, -0.0272, -0.0907]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2546,  0.2621,  0.0720,  ...,  2.2817,  1.2948, -0.6162],\n",
      "         [ 0.2194,  0.5708, -0.3383,  ...,  0.8279,  0.4242,  0.3527],\n",
      "         [ 0.4754,  0.5674, -0.3810,  ...,  1.0578,  1.3968, -0.6195],\n",
      "         ...,\n",
      "         [-0.0164,  0.0531, -0.0092,  ..., -0.0940, -0.0186, -0.0812],\n",
      "         [-0.0043,  0.0564, -0.0166,  ..., -0.0804, -0.0258, -0.0846],\n",
      "         [ 0.0060,  0.0489, -0.0306,  ..., -0.0789, -0.0357, -0.0728]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2033,  0.4687, -0.4649,  ...,  2.1006,  1.3520, -0.1914],\n",
      "         [ 0.1367,  0.7048, -0.1538,  ...,  0.8864,  0.3910,  0.0186],\n",
      "         [ 1.0162, -0.3019, -0.2861,  ...,  0.3405,  0.6402, -0.1712],\n",
      "         ...,\n",
      "         [-0.0218,  0.0614, -0.0324,  ..., -0.0964, -0.0257,  0.0125],\n",
      "         [ 0.3359,  0.2831, -0.1500,  ...,  0.4532, -0.0651, -0.2458],\n",
      "         [-0.0044,  0.0580, -0.0166,  ..., -0.0906, -0.0277, -0.0899]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0163e+00,  7.8356e-02, -7.5337e-02,  ...,  1.9269e+00,\n",
      "           1.2435e+00, -2.8543e-01],\n",
      "         [ 3.7106e-01, -4.5384e-01, -2.2628e-01,  ...,  1.2055e+00,\n",
      "           3.2430e-01,  2.3894e-01],\n",
      "         [ 6.4332e-01,  5.5505e-01,  1.6445e-01,  ...,  8.7832e-01,\n",
      "           8.5545e-01, -3.1093e-01],\n",
      "         ...,\n",
      "         [-6.1991e-03,  7.8247e-02, -1.4016e-02,  ..., -7.6891e-02,\n",
      "          -3.5111e-02, -6.9542e-02],\n",
      "         [ 4.5536e-03,  4.6068e-02, -2.2287e-02,  ..., -8.5004e-02,\n",
      "          -2.1266e-02,  2.2565e-02],\n",
      "         [-5.6325e-03,  5.1904e-02,  1.2329e-03,  ..., -8.1085e-02,\n",
      "          -2.6542e-02, -9.1435e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.2321e-01,  1.6112e-01,  1.3995e-03,  ...,  1.7345e+00,\n",
      "           1.0457e+00, -1.6815e-01],\n",
      "         [ 2.6386e-01,  5.6936e-01, -3.5478e-01,  ...,  1.3137e+00,\n",
      "           3.9634e-01, -1.6603e-01],\n",
      "         [ 4.3367e-01,  4.4875e-01, -6.0134e-02,  ...,  1.1207e+00,\n",
      "           2.7202e-01, -3.0910e-01],\n",
      "         ...,\n",
      "         [-8.6745e-03,  9.9354e-02, -2.2853e-02,  ..., -5.7718e-02,\n",
      "          -2.2272e-02, -1.7775e-01],\n",
      "         [-2.2757e-01,  3.0675e-01, -2.3569e-01,  ...,  6.9586e-02,\n",
      "          -1.0089e-02, -7.2272e-02],\n",
      "         [ 2.8369e-03,  5.8420e-02, -3.1906e-02,  ..., -8.0695e-02,\n",
      "          -4.3950e-02, -1.0704e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5967,  0.4742, -0.2694,  ...,  1.6171,  0.8320, -0.0385],\n",
      "         [ 0.0660,  0.4697, -0.1142,  ...,  1.4937,  0.3727, -0.0188],\n",
      "         [ 0.3465,  0.5778, -0.2557,  ...,  1.2242,  0.5121, -0.3443],\n",
      "         ...,\n",
      "         [ 0.0508,  0.2258,  0.2318,  ..., -0.4217,  0.1518, -0.3453],\n",
      "         [-0.0281, -0.0675, -0.0571,  ...,  0.2685,  0.2070, -0.5468],\n",
      "         [-0.0086,  0.0625, -0.0236,  ..., -0.1027, -0.0371, -0.0962]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.8237e-01,  1.4821e-01,  2.4138e-01,  ...,  1.8961e+00,\n",
      "           1.3955e+00,  2.1459e-01],\n",
      "         [-2.8556e-01,  7.5868e-01, -3.3655e-01,  ...,  1.5341e-01,\n",
      "           4.8560e-01, -2.0201e-01],\n",
      "         [-2.6618e-01,  1.2050e-01, -7.4743e-02,  ..., -4.8505e-01,\n",
      "           4.3748e-01, -2.3449e-01],\n",
      "         ...,\n",
      "         [ 1.2886e-03,  1.1405e-01, -1.2879e-02,  ...,  6.3950e-03,\n",
      "          -5.2352e-02, -2.3166e-01],\n",
      "         [-1.8041e-02,  4.9463e-02, -2.4236e-02,  ..., -6.7604e-02,\n",
      "          -3.6429e-02, -6.9611e-02],\n",
      "         [ 2.2714e-01,  2.4626e-01, -1.8456e-02,  ..., -4.0418e-02,\n",
      "           1.2884e-01, -2.0342e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.7891e-01,  2.1603e-01,  1.7893e-02,  ...,  2.1534e+00,\n",
      "           1.2435e+00, -1.4306e-01],\n",
      "         [ 2.8650e-02,  2.4501e-01, -5.2836e-02,  ...,  7.6150e-01,\n",
      "           5.8467e-01,  5.0347e-02],\n",
      "         [ 1.0081e+00,  3.4887e-01, -5.4779e-01,  ...,  2.1221e+00,\n",
      "           4.8365e-01,  6.8182e-03],\n",
      "         ...,\n",
      "         [-6.1182e-03,  4.8938e-02, -1.8244e-02,  ..., -8.3958e-02,\n",
      "          -2.3921e-02, -7.5323e-02],\n",
      "         [-1.4204e-02,  4.6433e-02, -2.5538e-02,  ..., -1.0813e-01,\n",
      "          -2.5590e-02, -9.4017e-02],\n",
      "         [-1.5862e-02,  5.7506e-02,  2.0871e-03,  ..., -8.5675e-02,\n",
      "          -3.3249e-02, -8.7106e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.6842e-01,  5.8511e-01, -3.7862e-01,  ...,  1.9007e+00,\n",
      "           9.2142e-01, -8.4285e-02],\n",
      "         [ 1.3466e-01,  5.6202e-01, -1.7769e-01,  ...,  7.3275e-01,\n",
      "           4.8989e-01, -1.7364e-01],\n",
      "         [ 1.4387e-01,  7.0743e-01, -2.1613e-01,  ...,  1.3864e-01,\n",
      "           3.7327e-01, -2.3948e-01],\n",
      "         ...,\n",
      "         [-2.4374e-04,  5.0049e-02, -1.6142e-02,  ..., -9.0222e-02,\n",
      "          -6.6923e-03, -1.0345e-01],\n",
      "         [-1.9585e-02,  5.1463e-02, -2.8600e-02,  ..., -7.3143e-02,\n",
      "          -4.5479e-02, -8.9058e-02],\n",
      "         [-1.1039e-03,  4.5648e-02, -1.3225e-02,  ..., -8.0865e-02,\n",
      "          -1.9493e-02, -8.3890e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.1688e-01,  1.0249e-01, -4.4107e-01,  ...,  2.2329e+00,\n",
      "           6.9312e-01, -3.8178e-01],\n",
      "         [-3.2039e-01,  6.8086e-01, -1.5443e-01,  ...,  8.6192e-02,\n",
      "           5.3011e-01, -4.3985e-01],\n",
      "         [-1.3455e-01,  6.9802e-02, -3.7131e-01,  ...,  8.6633e-02,\n",
      "           5.6796e-01, -4.3662e-01],\n",
      "         ...,\n",
      "         [ 1.6716e-03,  5.4099e-02, -5.0597e-03,  ..., -8.6818e-02,\n",
      "          -1.5117e-02, -8.6059e-02],\n",
      "         [-1.7653e-02,  8.9812e-02, -2.8793e-02,  ..., -7.6496e-02,\n",
      "          -2.6977e-02, -1.0097e-01],\n",
      "         [-1.2206e-02,  4.9141e-02, -2.2732e-02,  ..., -8.0247e-02,\n",
      "          -2.5275e-02,  1.3558e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.6705e-01,  6.3855e-01, -4.1786e-01,  ...,  2.3880e+00,\n",
      "           1.1002e+00, -1.9049e-01],\n",
      "         [-4.1637e-01,  6.7475e-01, -1.0221e-02,  ...,  1.3302e+00,\n",
      "           4.0809e-01,  1.8783e-02],\n",
      "         [ 5.2399e-01,  7.4734e-01, -5.6991e-01,  ...,  2.1256e+00,\n",
      "           2.7340e-01, -4.5109e-01],\n",
      "         ...,\n",
      "         [ 1.1693e-02,  4.8889e-02, -2.4243e-02,  ..., -9.0849e-02,\n",
      "          -3.1342e-02,  1.4469e-02],\n",
      "         [-1.2694e-03,  3.9859e-02,  5.4149e-05,  ..., -8.3324e-02,\n",
      "          -2.5556e-02, -7.9745e-02],\n",
      "         [-5.6655e-02,  9.2869e-02,  2.8512e-03,  ..., -2.6535e-02,\n",
      "          -1.0072e-03, -2.2062e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3816,  0.6070,  0.1735,  ...,  1.7879,  0.9429, -0.2758],\n",
      "         [ 0.4318,  0.4607, -0.3676,  ...,  0.3537,  0.4057, -0.0500],\n",
      "         [ 0.2034,  0.5379, -0.2483,  ..., -0.1987,  0.2452, -0.0595],\n",
      "         ...,\n",
      "         [-0.0162,  0.0484, -0.0283,  ..., -0.0811, -0.0251, -0.0796],\n",
      "         [-0.0036,  0.0535, -0.0183,  ..., -0.0723, -0.0308, -0.0816],\n",
      "         [-0.0163,  0.0496, -0.0190,  ..., -0.0788, -0.0146, -0.0805]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8537,  0.5734, -0.2767,  ...,  2.4841,  1.1803,  0.1391],\n",
      "         [ 0.4018,  0.2590, -0.0829,  ...,  1.2235,  0.9030,  0.3834],\n",
      "         [ 0.2748,  0.7117, -0.3050,  ...,  0.2522,  0.8146, -0.1592],\n",
      "         ...,\n",
      "         [-0.0478,  0.0976, -0.0459,  ..., -0.0391, -0.0435, -0.2125],\n",
      "         [-0.0172,  0.0594, -0.0245,  ..., -0.0755, -0.0385, -0.0828],\n",
      "         [-0.0082,  0.0623, -0.0361,  ..., -0.0838, -0.0386, -0.0863]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 3.5268e-01,  3.4531e-01, -4.0363e-01,  ...,  2.2130e+00,\n",
      "           1.5223e+00,  7.5071e-04],\n",
      "         [-6.1080e-02,  7.2709e-01, -3.0254e-01,  ..., -1.5207e-01,\n",
      "           4.6485e-01,  8.4011e-02],\n",
      "         [ 1.8922e-01,  7.7266e-02, -3.6182e-01,  ...,  1.4513e+00,\n",
      "           1.1296e+00, -4.8233e-01],\n",
      "         ...,\n",
      "         [ 7.3868e-03,  3.8178e-02, -1.7049e-02,  ..., -8.5173e-02,\n",
      "          -3.6863e-02,  1.4561e-02],\n",
      "         [-2.2265e-02,  5.2605e-02, -2.2057e-02,  ..., -8.2435e-02,\n",
      "          -3.3685e-02,  1.5767e-02],\n",
      "         [ 1.5475e-02,  4.8759e-02, -2.7319e-02,  ..., -7.6114e-02,\n",
      "          -2.9457e-02, -8.9098e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9188,  0.6154, -0.4543,  ...,  1.6607,  1.3442, -0.3908],\n",
      "         [ 0.4431,  0.5962, -0.1452,  ...,  0.7305, -0.0734,  0.1309],\n",
      "         [ 0.3564,  0.8675, -0.3019,  ...,  0.4061,  1.0414, -0.2460],\n",
      "         ...,\n",
      "         [-0.1288,  0.5477, -0.0679,  ..., -0.1150,  0.0254, -0.1898],\n",
      "         [ 0.1494,  0.0295, -0.7641,  ...,  0.4415,  0.0921, -0.4537],\n",
      "         [-0.0136,  0.0415, -0.0175,  ..., -0.1098, -0.0278, -0.0944]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.2990,  0.2448, -0.4575,  ...,  1.0807,  0.6498, -0.3922],\n",
      "         [ 0.2404,  0.1004, -0.0586,  ...,  0.5175,  0.6031,  0.0963],\n",
      "         [ 0.1258,  0.4617, -0.3768,  ...,  0.8028,  0.5519, -0.3753],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([1.0166, 1.4473, 0.9155,  ..., 2.2422, 0.9922, 0.5337], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([1.1572, 0.4883, 1.6855,  ..., 1.0547, 3.2051, 0.7456], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.0365,  0.2234, -0.5010,  ...,  1.8757,  0.8712, -0.4441],\n",
      "         [ 0.1130,  0.1439, -0.0922,  ...,  0.5375,  0.1257,  0.1355],\n",
      "         [ 0.4896, -0.4139, -0.3111,  ...,  0.3680,  1.3603, -0.1428],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.0820e+00,  1.6270e+00,  1.4941e+00,  1.3311e+00, -5.4150e-01,\n",
      "        -1.0461e-01, -4.4507e-01, -1.7078e-01, -4.4263e-01,  7.2119e-01,\n",
      "         3.2402e+00,  1.2920e+00,  2.1973e+00,  8.0127e-01, -1.3196e-01,\n",
      "         7.5439e-01,  8.4814e-01,  2.0630e-01,  8.1689e-01,  5.2051e-01,\n",
      "         3.8555e+00,  1.3623e+00,  2.0273e+00,  4.9927e-01,  1.0869e+00,\n",
      "         3.8203e+00,  1.5107e+00,  1.1536e-02, -3.0786e-01, -1.0645e+00,\n",
      "        -2.6367e-01,  1.2070e+00, -1.0156e+00,  3.7872e-02,  2.8984e+00,\n",
      "         8.0420e-01,  1.7109e+00,  4.8218e-01,  3.7988e-01,  6.4453e-02,\n",
      "         9.1943e-01,  9.0088e-01,  5.3955e-01,  2.3750e+00,  1.1953e+00,\n",
      "         8.4082e-01, -5.7568e-01,  8.8281e-01,  4.6948e-01,  2.9517e-01,\n",
      "         1.2881e+00, -6.0254e-01,  4.2383e-01, -4.9341e-01,  2.2974e-01,\n",
      "         3.3594e+00,  8.3789e-01,  2.2012e+00,  1.0371e+00,  4.8364e-01,\n",
      "        -5.0439e-01,  1.2432e+00,  1.6199e-01,  7.1631e-01,  3.1470e-01,\n",
      "         2.6641e+00,  7.3730e-01,  8.0908e-01,  6.3623e-01,  4.4580e-01,\n",
      "         4.7339e-01,  4.0781e+00,  1.6152e+00,  2.1855e+00,  5.4736e-01,\n",
      "         2.3389e-01,  3.8843e-01,  3.0835e-01,  3.9727e+00,  1.4512e+00,\n",
      "         2.2305e+00,  5.6396e-01,  1.2812e+00, -1.2817e-01,  4.5386e-01,\n",
      "         4.6362e-01,  1.7578e-02, -2.8149e-01,  9.9023e-01,  7.2900e-01,\n",
      "         4.0015e-01,  4.0698e-01,  2.8174e-01, -2.2607e-01,  2.5859e+00,\n",
      "        -3.7500e-01,  5.5957e-01,  4.9225e-02,  8.2520e-01,  4.0898e+00,\n",
      "         1.3555e+00,  1.3262e+00,  2.2812e+00,  5.0586e-01,  1.3896e+00,\n",
      "         3.4229e-01,  1.1383e-01,  3.9886e-02,  1.6641e+00,  1.2158e+00,\n",
      "         7.5391e-01, -8.0872e-02, -7.4170e-01, -1.8054e-01, -4.6582e-01,\n",
      "         3.6270e+00,  1.3984e+00,  2.5234e+00,  9.9414e-01,  8.1055e-01,\n",
      "        -2.3804e-01,  9.5459e-02,  1.6382e-01, -4.0991e-01,  2.3418e+00,\n",
      "        -3.1250e-01,  3.6934e+00,  1.2861e+00,  2.2598e+00,  4.2480e-01,\n",
      "        -3.6304e-01,  4.5337e-01,  1.6338e+00, -5.9912e-01,  2.7734e-01,\n",
      "        -3.2764e-01,  6.9946e-02,  2.0039e+00,  4.7729e-01,  2.2498e-01,\n",
      "         3.5996e+00,  1.4043e+00,  2.9321e-01,  3.1152e+00,  1.3105e+00,\n",
      "         1.7725e+00,  7.9736e-01,  8.6182e-01,  1.7227e+00,  3.3867e+00,\n",
      "         1.2773e+00,  2.4219e+00,  3.1738e-01, -2.6709e-01,  3.3398e+00,\n",
      "         1.0195e+00,  1.1689e+00,  1.4023e+00, -3.8550e-01, -9.9976e-02,\n",
      "        -3.3350e-01,  3.2012e+00,  1.2539e+00,  2.9938e-02,  2.8301e+00,\n",
      "         8.2910e-01, -5.5225e-01,  4.0479e-01,  4.1562e+00,  1.2920e+00,\n",
      "         1.2139e+00,  2.3984e+00,  5.6299e-01, -6.5625e-01,  1.1846e+00,\n",
      "         6.3281e-01,  8.8477e-01, -3.4351e-01,  2.4336e+00,  6.1084e-01,\n",
      "         2.7295e-01,  3.7051e+00,  1.6377e+00,  2.8711e-01,  8.1494e-01,\n",
      "         4.3262e-01,  4.1328e+00,  1.7695e+00,  2.2344e+00,  8.6719e-01,\n",
      "         3.9141e+00,  1.6123e+00,  2.0352e+00,  6.7041e-01,  1.1064e+00,\n",
      "         3.5352e+00,  1.4873e+00,  8.9502e-01,  1.1035e+00,  3.6201e-03,\n",
      "        -9.4434e-01, -2.8857e-01,  3.0566e+00,  1.1611e+00,  2.4844e+00,\n",
      "         8.0664e-01, -2.9517e-01,  3.3750e+00,  2.0547e+00,  6.0254e-01,\n",
      "         1.0869e+00, -8.0664e-01,  2.3511e-01,  5.9863e-01,  8.8867e-01,\n",
      "         2.1074e+00,  4.8755e-01,  1.9404e+00,  5.9668e-01, -4.0967e-01,\n",
      "         1.7793e+00,  3.0820e+00,  2.0977e+00,  5.1758e-01,  1.1123e+00,\n",
      "         8.9697e-01, -2.8711e-01, -1.7078e-01,  1.9995e-01,  3.9526e-01,\n",
      "         3.5410e+00,  1.3662e+00,  2.1523e+00,  8.1396e-01,  1.9746e+00,\n",
      "         7.1436e-01,  2.0645e+00,  8.0420e-01,  5.6982e-01,  3.0332e+00,\n",
      "         9.8340e-01,  2.5176e+00,  5.5664e-01,  9.4482e-01,  2.7710e-01,\n",
      "         5.9906e-02,  3.6387e+00,  1.2812e+00,  1.8047e+00,  4.0161e-01,\n",
      "         5.6494e-01,  4.3286e-01,  1.2461e+00,  1.5869e+00,  8.8477e-01,\n",
      "         2.3652e+00, -3.4644e-01,  3.2891e+00,  2.0840e+00,  5.6299e-01,\n",
      "         6.9629e-01,  2.5806e-01,  1.0195e+00,  3.5547e+00,  6.6748e-01,\n",
      "         1.5400e+00,  8.0762e-01,  2.9609e+00,  1.6494e+00,  9.3506e-01,\n",
      "         3.7793e-01,  3.4531e+00,  1.4199e+00,  1.8340e+00,  6.2549e-01,\n",
      "         2.6172e+00,  1.4893e+00,  5.9717e-01,  5.9570e-01,  1.9885e-01,\n",
      "         2.9824e+00,  6.8164e-01,  1.9766e+00,  8.1592e-01,  3.9136e-01,\n",
      "         2.2729e-01, -3.0566e-01,  7.3547e-02,  3.4492e+00,  2.1619e-01,\n",
      "        -6.1182e-01,  2.7051e+00,  7.9346e-01,  1.2041e+00, -2.3755e-01,\n",
      "         2.1875e+00,  6.9727e-01,  7.1045e-01, -2.6978e-01,  2.5273e+00,\n",
      "         6.2451e-01, -3.4106e-01, -1.3782e-01,  2.4258e+00,  1.1084e+00,\n",
      "        -4.9609e-01, -2.5055e-02,  1.6709e+00, -2.0905e-02,  1.0195e+00,\n",
      "        -1.8457e-01, -5.6519e-02,  4.3530e-01, -5.1025e-01,  7.1729e-01,\n",
      "         4.3872e-01,  3.7656e+00,  1.0996e+00,  1.7578e+00,  2.8174e-01,\n",
      "         1.3740e+00,  2.2095e-02,  7.6416e-01,  4.8364e-01,  3.2666e-01,\n",
      "        -4.2749e-01,  5.0098e-01, -8.8330e-01,  3.6113e+00,  8.9990e-01,\n",
      "         1.6250e+00,  5.1123e-01,  4.0039e+00,  1.4004e+00,  5.1221e-01,\n",
      "         2.2192e-01,  2.5903e-01,  3.5840e+00,  1.2197e+00,  2.0254e+00,\n",
      "         3.6035e-01,  2.0059e+00,  8.0762e-01,  1.9902e+00,  1.1270e+00,\n",
      "         7.2461e-01,  2.6025e-01, -9.3384e-02,  3.2148e+00,  2.4531e+00,\n",
      "         6.6504e-01,  5.5908e-01,  4.5996e-01,  2.1133e+00,  5.4492e-01,\n",
      "         2.1465e+00,  8.0615e-01, -8.3496e-01,  5.0293e-01,  9.0820e-02,\n",
      "        -7.5439e-01,  1.0736e-01,  1.3643e+00, -5.6006e-01,  7.5342e-01,\n",
      "        -5.6738e-01,  9.1357e-01,  1.6904e+00,  4.6655e-01, -8.0518e-01,\n",
      "         1.6223e-01, -5.9473e-01,  7.4280e-02,  1.2344e+00,  5.6201e-01,\n",
      "         4.7046e-01,  9.4238e-01, -3.1274e-01,  2.5635e-01,  9.4287e-01,\n",
      "        -4.4531e-01,  5.7568e-01, -4.7681e-01,  9.5898e-01, -4.6216e-01,\n",
      "         8.5400e-01, -5.5615e-01, -1.2030e-01, -9.3262e-01,  3.2373e-01,\n",
      "        -2.0288e-01, -7.3096e-01,  4.5441e-02, -6.5186e-02, -7.1924e-01,\n",
      "         1.0889e+00, -2.0081e-01,  2.3486e-01,  7.4609e-01,  1.3599e-01,\n",
      "        -3.6987e-01,  6.0693e-01,  4.5117e-01, -6.3330e-01,  9.0674e-01,\n",
      "         6.6357e-01, -5.0873e-02,  9.9414e-01, -7.3535e-01,  5.4688e-01,\n",
      "         2.9712e-01, -1.3220e-01,  2.8638e-01, -4.3628e-01,  2.9541e-01,\n",
      "         7.5317e-02, -4.7095e-01,  1.0615e+00, -5.8350e-01,  2.8979e-01,\n",
      "         4.1235e-01, -1.3318e-01,  2.3022e-01,  6.7432e-01,  1.5225e+00,\n",
      "         3.6445e+00,  2.4434e+00,  6.4404e-01,  3.4473e+00,  2.1855e+00,\n",
      "         5.4639e-01,  1.0576e+00,  2.7637e+00,  1.0312e+00,  1.2012e+00,\n",
      "        -2.4243e-01,  4.1758e+00,  2.3066e+00,  1.1240e+00,  2.7754e+00,\n",
      "         2.0059e+00,  5.5029e-01,  3.2666e-01, -7.6660e-01,  5.6445e-01,\n",
      "         9.5117e-01, -1.1707e-01,  2.8979e-01,  3.5986e-01,  1.7737e-01,\n",
      "         2.2803e-01,  2.2188e+00, -8.3057e-01,  2.5317e-01, -1.2021e+00,\n",
      "         8.4863e-01,  6.4258e-01, -5.9375e-01,  3.1035e+00,  1.9617e-01,\n",
      "         3.1274e-01,  3.8359e+00,  1.2578e+00,  3.6963e-01,  5.8887e-01,\n",
      "         4.2041e-01,  2.4551e+00,  1.0801e+00,  2.0820e+00,  9.1846e-01,\n",
      "        -5.3174e-01,  1.9111e+00,  3.1680e+00,  1.9883e+00,  6.5430e-01,\n",
      "         8.2959e-01,  2.8735e-01,  2.5586e-01, -1.7322e-01,  8.7988e-01,\n",
      "        -8.1055e-01,  1.7354e+00,  1.2471e+00,  2.2583e-01,  3.2656e+00,\n",
      "         9.3359e-01,  1.6846e+00,  6.0791e-01,  3.2012e+00,  9.8242e-01,\n",
      "         1.8994e+00,  6.3574e-01,  9.7363e-01,  2.9688e-01,  7.3433e-03,\n",
      "         3.6719e+00,  1.7119e+00,  2.2617e+00,  7.1729e-01,  4.8950e-01,\n",
      "        -1.0518e+00, -1.1611e+00, -5.4102e-01,  8.5754e-02, -2.4426e-01,\n",
      "         2.8125e+00,  7.2070e-01,  5.6006e-01,  1.0293e+00,  3.9917e-01,\n",
      "         4.1821e-01,  2.6270e+00,  1.2148e+00,  1.1221e+00,  2.4902e+00,\n",
      "         1.3145e+00, -6.5771e-01,  2.0820e+00,  3.6934e+00,  7.0752e-01,\n",
      "         4.1895e-01,  1.1426e+00,  9.6484e-01, -2.1411e-01,  4.4116e-01,\n",
      "         4.2734e+00,  1.9277e+00,  4.1680e+00,  1.3662e+00,  9.6924e-01,\n",
      "        -1.1426e+00, -1.4929e-01, -2.9053e-01,  3.9375e+00,  1.3330e+00,\n",
      "         2.9805e+00,  1.3320e+00,  9.7607e-01,  6.0791e-01, -6.5723e-01,\n",
      "        -7.5684e-01,  3.1641e+00,  1.1133e+00,  3.1660e+00,  9.0479e-01,\n",
      "         1.2012e+00,  3.3301e-01,  7.7881e-01, -4.3408e-01,  2.7271e-01,\n",
      "         3.3960e-01,  1.3220e-01,  4.0156e+00,  1.3115e+00,  2.2109e+00,\n",
      "         4.6216e-01,  5.5273e-01, -3.4058e-01,  3.9727e+00,  1.0459e+00,\n",
      "         1.0566e+00,  2.0825e-01,  5.7812e-01,  5.1483e-02, -1.3367e-01,\n",
      "         2.8149e-01, -7.9102e-01, -2.0422e-01,  2.6831e-01, -5.1709e-01,\n",
      "         9.6875e-01,  6.0156e-01, -9.4177e-02, -2.4216e-02,  2.7374e-02,\n",
      "         1.6055e+00, -3.5425e-01,  1.2720e-01, -2.1408e-02,  4.7607e-01,\n",
      "        -3.8208e-01,  4.1875e+00,  2.0742e+00,  2.6309e+00,  1.0000e+00,\n",
      "         3.6445e+00,  1.1875e+00,  8.4961e-01,  3.6855e+00,  1.0986e+00,\n",
      "         2.2520e+00,  8.3984e-01,  2.9375e+00,  1.8926e+00,  2.5586e+00,\n",
      "         1.7617e+00,  2.6484e+00,  1.0430e+00, -1.7188e-01,  1.1914e+00,\n",
      "         1.3867e-01,  4.1187e-01,  3.4980e+00,  1.8359e+00,  2.5527e+00,\n",
      "         1.3330e+00,  2.8965e+00,  8.7598e-01,  2.7305e+00,  2.1992e+00,\n",
      "         4.4385e-01, -7.8271e-01,  2.5708e-01,  4.0977e+00,  1.3184e+00,\n",
      "         1.1309e+00,  2.6387e+00,  8.1055e-01, -2.6343e-01, -1.7041e-01,\n",
      "         1.1289e+00,  4.9225e-02,  7.7209e-02,  2.8003e-01,  4.1602e+00,\n",
      "         1.5312e+00,  2.7266e+00,  1.4453e+00,  2.8828e+00,  1.3164e+00,\n",
      "         2.2656e+00,  2.5293e+00,  8.5889e-01,  4.3530e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.7129e+00, -4.9023e-01, -2.6871e-02,  1.0947e+00, -1.1572e+00,\n",
      "        -8.8330e-01, -9.2529e-01,  5.0000e-01, -9.2578e-01, -4.7168e-01,\n",
      "        -4.4458e-01,  7.7393e-01,  5.4980e-01,  1.7275e+00, -4.0552e-01,\n",
      "         1.6736e-01,  2.0813e-02,  6.5186e-01,  7.3193e-01,  7.0312e-01,\n",
      "        -3.5132e-01,  1.1631e+00,  1.2314e+00,  3.6836e+00,  1.6992e+00,\n",
      "        -1.9348e-01,  2.7910e+00,  1.0332e+00,  8.3923e-02,  4.0283e-01,\n",
      "        -4.2627e-01,  9.2969e-01,  3.4155e-01, -7.0459e-01, -2.9834e-01,\n",
      "         1.0322e+00,  5.3516e-01,  6.4014e-01,  2.8203e+00,  9.2926e-03,\n",
      "         4.5776e-01,  6.7090e-01,  1.1533e+00,  7.4023e-01,  1.5234e+00,\n",
      "         2.0059e+00, -6.0791e-01, -2.7222e-01, -9.4543e-02,  1.4514e-01,\n",
      "         8.9160e-01, -7.0312e-01, -5.7520e-01,  2.1692e-01, -5.9668e-01,\n",
      "         1.6272e-01,  1.7207e+00,  9.3750e-01,  1.2510e+00,  3.0312e+00,\n",
      "        -6.0889e-01, -3.7231e-01,  1.1438e-01,  6.4404e-01,  1.2100e+00,\n",
      "        -2.3816e-01,  1.1699e+00,  1.8623e+00,  2.6113e+00,  2.6074e-01,\n",
      "         6.4648e-01, -4.6777e-01,  1.6670e+00,  1.1162e+00,  3.8633e+00,\n",
      "         3.6694e-01,  4.2139e-01,  6.4844e-01, -3.7988e-01,  1.6426e+00,\n",
      "         1.1338e+00,  3.8906e+00,  1.0742e+00, -4.3384e-01,  8.0078e-01,\n",
      "         3.7427e-01,  9.1846e-01, -1.0625e+00,  3.1104e-01,  2.0469e+00,\n",
      "        -4.1821e-01, -1.0883e-01,  1.8311e-01, -1.0596e+00,  9.8828e-01,\n",
      "         1.5234e+00, -2.3865e-01, -2.2903e-02,  4.6631e-01, -4.5190e-01,\n",
      "         3.9526e-01,  1.2969e+00,  1.0996e+00,  3.1113e+00,  1.9739e-01,\n",
      "         1.8721e+00,  9.4629e-01, -5.2100e-01,  4.6313e-01,  5.4785e-01,\n",
      "         1.6777e+00, -5.5273e-01, -9.6436e-01, -2.7124e-01, -5.3027e-01,\n",
      "        -3.6694e-01,  6.9629e-01,  3.1689e-01,  3.8945e+00,  2.4817e-01,\n",
      "        -2.2302e-01, -2.0154e-01, -6.5527e-01, -1.3555e+00,  5.5957e-01,\n",
      "         8.9941e-01,  1.3550e-01,  1.0371e+00,  1.4053e+00,  4.0039e+00,\n",
      "        -7.3145e-01, -1.0664e+00,  8.2812e-01, -3.7207e-01, -5.7031e-01,\n",
      "        -5.5029e-01, -4.9805e-01, -3.5156e-01,  1.2622e-01,  7.8955e-01,\n",
      "        -2.9517e-01,  1.8271e+00,  3.0137e+00, -2.1255e-02,  6.5674e-01,\n",
      "         4.5166e-03,  1.2920e+00,  1.6045e+00,  1.5039e+00,  7.2449e-02,\n",
      "         2.2422e+00,  1.1162e+00,  3.7578e+00, -1.2115e-01, -3.5010e-01,\n",
      "         6.8799e-01,  2.5938e+00,  1.2852e+00,  9.3945e-01, -6.8018e-01,\n",
      "        -7.5244e-01, -4.8047e-01,  2.4668e+00, -7.0117e-01,  5.3369e-01,\n",
      "         2.1797e+00, -1.2695e+00, -1.0381e+00, -7.8955e-01,  2.6489e-01,\n",
      "         9.3506e-01,  9.3604e-01,  3.2402e+00,  5.1709e-01, -8.5352e-01,\n",
      "         5.7666e-01,  6.4087e-02, -1.3229e-02,  2.4475e-01,  2.6562e+00,\n",
      "        -3.7256e-01, -2.4524e-01,  1.3760e+00,  4.3633e+00,  5.3955e-01,\n",
      "         5.9180e-01, -4.9365e-01,  1.4287e+00,  1.0498e+00,  3.6738e+00,\n",
      "        -2.4133e-01,  1.1309e+00,  1.0996e+00,  3.2539e+00,  2.9932e-01,\n",
      "        -7.1106e-02,  1.1152e+00,  1.9062e+00,  3.7617e+00, -2.5488e-01,\n",
      "         3.0029e-01, -4.2993e-01, -3.8550e-01,  1.1514e+00,  6.9824e-01,\n",
      "         2.8027e+00, -2.4646e-01,  5.4053e-01,  8.4912e-01,  2.7402e+00,\n",
      "         1.5254e+00, -5.0732e-01,  1.5107e+00, -2.8125e-01, -1.3931e-02,\n",
      "         2.3169e-01,  1.4316e+00,  1.8823e-01,  1.2764e+00, -1.1592e+00,\n",
      "         1.4932e+00,  1.6406e-01,  5.9521e-01,  2.6816e+00,  1.1953e+00,\n",
      "        -1.5784e-01, -6.2347e-02,  1.6709e+00,  1.2627e+00, -3.1372e-01,\n",
      "        -1.7749e-01,  1.2881e+00,  7.3291e-01,  2.0176e+00,  4.4849e-01,\n",
      "         2.6191e+00, -7.2168e-01,  3.3813e-01,  2.7793e+00,  4.1565e-02,\n",
      "         1.1953e+00, -2.3962e-01,  6.0742e-01,  1.0420e+00,  3.4238e+00,\n",
      "        -1.8054e-01, -2.4646e-01,  1.1025e+00,  1.0859e+00,  3.9023e+00,\n",
      "         6.0645e-01,  5.9277e-01,  6.4453e-01,  4.5923e-01,  7.9407e-02,\n",
      "         6.1914e-01, -6.9092e-01,  7.3853e-02,  4.4336e-01,  2.7559e+00,\n",
      "         7.8223e-01,  6.1865e-01, -6.0645e-01, -8.2275e-02,  1.5361e+00,\n",
      "         2.3164e+00,  5.3809e-01, -1.5112e-01, -2.2751e-02,  4.8193e-01,\n",
      "         3.2500e+00, -3.6572e-01,  1.0654e+00,  8.7646e-01,  2.9180e+00,\n",
      "         4.9292e-01,  8.1543e-01,  3.5547e-01,  2.5625e+00, -3.2764e-01,\n",
      "        -5.9961e-01,  5.9863e-01,  1.8860e-01,  5.3516e-01,  3.6133e+00,\n",
      "        -7.8564e-01, -5.7227e-01, -5.0928e-01, -2.6392e-01,  1.1279e+00,\n",
      "        -7.0410e-01, -3.1079e-01,  5.4688e-01,  1.6074e+00, -9.0234e-01,\n",
      "        -1.9543e-01,  1.7695e+00,  2.7715e+00, -3.3960e-01, -7.5879e-01,\n",
      "        -2.1179e-01,  1.5049e+00, -8.9404e-01, -5.7715e-01, -5.3467e-01,\n",
      "         1.0303e+00, -7.8564e-01, -3.1812e-01,  7.2559e-01,  3.3447e-02,\n",
      "         1.5293e+00, -6.9238e-01, -7.9163e-02,  1.1639e-01, -2.6147e-01,\n",
      "         6.0107e-01, -2.9800e-02,  1.1133e+00,  1.2061e+00,  3.8027e+00,\n",
      "         1.1426e+00, -4.5703e-01,  4.8730e-01, -3.4027e-02,  7.0605e-01,\n",
      "        -1.1113e+00, -4.7705e-01, -1.8250e-01, -7.2510e-02,  2.7441e+00,\n",
      "         1.2646e+00,  1.1250e+00,  5.1025e-02,  8.1104e-01,  3.9512e+00,\n",
      "        -8.2336e-02, -5.1953e-01,  1.2610e-01,  8.9502e-01,  4.4946e-01,\n",
      "         2.8691e+00,  6.8970e-03,  1.2051e+00,  1.0840e+00,  7.1191e-01,\n",
      "         1.0576e+00,  3.6992e+00, -2.9395e-01,  3.6987e-01,  7.9004e-01,\n",
      "         3.7988e+00,  4.3481e-01,  6.2939e-01,  7.7734e-01,  1.2627e+00,\n",
      "         4.1943e-01,  1.2119e+00, -1.1455e+00,  1.4868e-01, -1.7639e-01,\n",
      "        -1.1221e+00, -9.3555e-01,  6.3782e-02, -9.0576e-01, -7.0801e-01,\n",
      "        -8.3496e-01,  2.1667e-01,  2.0239e-01,  9.8584e-01, -1.0654e+00,\n",
      "        -6.8542e-02, -1.0322e+00, -1.0654e+00, -2.6782e-01, -3.2837e-01,\n",
      "        -8.5986e-01, -2.0789e-01, -6.8799e-01,  1.6064e-01,  3.7036e-01,\n",
      "        -8.6670e-01, -3.0615e-01, -1.5649e-01,  3.6450e-01, -7.9736e-01,\n",
      "         1.9012e-02, -4.0741e-02, -5.6122e-02, -1.4365e+00, -3.1885e-01,\n",
      "        -7.7197e-01, -1.1895e+00, -1.1152e+00, -4.9365e-01, -2.3450e-01,\n",
      "        -7.4829e-02, -1.1631e+00, -2.0544e-01, -9.9170e-01, -4.7217e-01,\n",
      "         3.4332e-02, -1.5601e-01, -7.0459e-01, -7.2852e-01, -6.5430e-01,\n",
      "        -5.5957e-01, -4.9512e-01, -3.1738e-01, -1.2529e+00, -7.1533e-01,\n",
      "        -4.0796e-01, -2.5806e-03, -3.3911e-01, -7.0947e-01, -5.3027e-01,\n",
      "        -3.9581e-02, -1.2998e+00, -2.2595e-01, -1.2189e-01, -2.9028e-01,\n",
      "        -7.5928e-02, -2.5293e-01, -4.8389e-01, -2.8149e-01, -7.6141e-03,\n",
      "         3.5400e-01,  9.2676e-01,  4.0391e+00,  6.0352e-01,  1.1494e+00,\n",
      "         3.8945e+00,  5.2881e-01,  2.9126e-01,  1.9033e+00,  3.6309e+00,\n",
      "        -3.7451e-01, -3.2739e-01,  6.5674e-01,  3.0312e+00,  7.8516e-01,\n",
      "         6.9482e-01,  2.6523e+00,  1.0547e+00, -5.5615e-01,  1.7920e+00,\n",
      "         4.3994e-01, -3.9502e-01,  2.1814e-01,  1.0010e-01,  6.9922e-01,\n",
      "        -9.9756e-01,  1.6260e+00, -1.0010e+00, -4.5557e-01, -1.9629e-01,\n",
      "         3.5076e-03,  7.3669e-02,  2.6392e-01,  2.2339e-02,  1.9082e+00,\n",
      "         4.6045e-01, -5.6488e-02,  6.0498e-01,  3.8926e+00,  2.5659e-01,\n",
      "         5.5957e-01, -6.8652e-01,  2.3691e+00, -7.3730e-01,  2.2832e+00,\n",
      "        -1.0859e+00,  1.2080e+00,  3.7671e-01,  7.1191e-01,  2.9141e+00,\n",
      "         2.1660e+00,  1.1670e+00, -2.0349e-01, -6.6895e-01,  3.0786e-01,\n",
      "        -2.9321e-01,  2.5610e-01,  3.4688e+00, -1.9324e-01, -1.1993e-01,\n",
      "         1.6504e+00,  7.6611e-01,  2.9492e+00,  4.2285e-01,  1.1543e+00,\n",
      "         1.2178e+00,  1.2754e+00,  9.4824e-01,  3.3633e+00, -1.4111e-01,\n",
      "        -1.9055e-01,  1.5234e+00,  1.2891e+00,  3.7891e+00,  2.0898e+00,\n",
      "        -6.4160e-01, -2.5513e-01, -8.6328e-01,  1.1520e-02, -4.2920e-01,\n",
      "         2.4609e-01,  4.0991e-01,  2.4746e+00,  2.6582e+00, -5.3375e-02,\n",
      "         5.5371e-01, -1.3855e-01,  2.9023e+00,  2.4292e-01,  8.4045e-02,\n",
      "         3.1016e+00, -8.5352e-01,  1.4785e+00,  4.0747e-01,  3.2715e+00,\n",
      "         1.2559e+00,  1.0176e+00,  3.6914e-01,  6.3281e-01, -3.3765e-01,\n",
      "         4.7455e-03,  2.3457e+00,  1.7615e-01,  7.9199e-01,  4.0312e+00,\n",
      "        -1.1104e+00, -3.7402e-01, -1.0010e+00, -1.4328e-02,  3.2910e+00,\n",
      "         6.8604e-01,  1.0547e+00,  1.9688e+00,  4.5078e+00, -6.6846e-01,\n",
      "        -6.5820e-01, -1.4084e-02,  2.6660e+00,  2.4002e-02,  6.8896e-01,\n",
      "         1.2246e+00,  2.9102e+00,  3.7915e-01, -1.6528e-01, -3.8965e-01,\n",
      "         2.0532e-01, -6.6895e-01, -2.8540e-01,  1.2031e+00,  1.1426e+00,\n",
      "         4.1719e+00, -1.9714e-01, -7.3389e-01, -2.4646e-01,  3.7329e-01,\n",
      "         3.4805e+00,  1.8252e+00,  8.6853e-02,  1.2573e-01, -1.0273e+00,\n",
      "         9.6863e-02, -1.1641e+00, -1.0938e+00, -4.6173e-02, -5.5225e-01,\n",
      "         4.0430e-01,  1.0101e-01,  5.8289e-02, -2.9761e-01, -6.9580e-01,\n",
      "         2.6904e-01, -1.8726e-01, -6.5576e-01, -4.4189e-01, -2.7686e-01,\n",
      "        -4.3799e-01, -3.8605e-02,  1.0850e+00,  1.2988e+00,  3.7852e+00,\n",
      "         7.5439e-01,  1.1914e+00,  3.2559e+00,  7.6611e-01,  3.3477e+00,\n",
      "         1.3643e+00,  2.4863e+00, -6.6956e-02,  1.3965e+00,  6.9385e-01,\n",
      "         2.0605e+00,  8.5156e-01,  3.8496e+00, -9.8682e-01,  4.6021e-01,\n",
      "         7.9688e-01,  8.2617e-01, -3.7524e-01,  1.2051e+00,  3.0054e-01,\n",
      "         2.6367e+00,  6.4453e-01,  2.0059e+00,  1.8096e+00,  1.4355e+00,\n",
      "         4.0195e+00, -5.0537e-01, -3.6499e-01, -3.8745e-01,  6.4111e-01,\n",
      "         2.8477e+00,  1.3105e+00,  4.4727e+00, -9.9658e-01, -1.0537e+00,\n",
      "         9.1162e-01,  1.0234e+00,  1.2207e+00, -5.5762e-01,  1.9177e-01,\n",
      "         2.1934e+00,  1.2207e+00,  3.0586e+00,  1.5615e+00,  3.2031e+00,\n",
      "         7.9053e-01,  1.0723e+00,  3.9980e+00,  5.7422e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.3271,  0.0490, -0.3570,  ...,  1.2883,  1.2211, -0.0527],\n",
      "         [-0.2621,  0.1946, -0.3524,  ...,  0.4132,  0.4171, -0.3806],\n",
      "         [ 0.8301, -0.0958, -0.1474,  ...,  1.1595,  1.2018, -0.3386],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 7.3682e-01,  1.8311e+00,  8.5156e-01, -4.6570e-02,  1.0315e-02,\n",
      "        -5.1367e-01,  6.2695e-01,  5.6299e-01, -6.0986e-01,  2.2552e-02,\n",
      "        -3.4131e-01,  1.0498e+00,  8.2568e-01,  1.1713e-01,  9.4092e-01,\n",
      "         2.9565e-01, -4.9219e-01,  1.6250e+00,  4.9780e-01,  3.6602e+00,\n",
      "         1.1572e+00,  3.3652e+00,  7.1484e-01,  1.0117e+00,  2.7051e+00,\n",
      "         5.7910e-01,  6.0840e-01,  1.2656e+00,  1.4189e+00,  2.2534e-01,\n",
      "         1.4526e-01, -3.3496e-01,  3.4033e-01,  1.5967e-01, -5.6104e-01,\n",
      "         8.0176e-01,  2.1558e-01,  2.5371e+00,  8.8318e-02,  1.3477e+00,\n",
      "        -8.7598e-01,  7.1973e-01, -7.6904e-01,  9.9512e-01,  3.8574e-01,\n",
      "        -1.9873e-01,  4.6973e-01, -4.5093e-01,  3.2734e+00,  5.1025e-01,\n",
      "         1.3564e+00, -2.7710e-01,  3.6621e+00,  1.0771e+00,  2.2812e+00,\n",
      "         7.0117e-01, -5.3101e-02,  1.7896e-01,  9.8999e-02, -4.9146e-01,\n",
      "        -4.3799e-01, -4.0259e-01,  5.8789e-01, -7.3291e-01,  3.3730e+00,\n",
      "         1.5586e+00,  4.2041e-01,  2.4097e-01,  3.3418e+00,  1.1855e+00,\n",
      "         1.1943e+00,  6.8311e-01,  4.1107e-02,  4.1055e+00,  1.0293e+00,\n",
      "         2.1699e+00,  7.9688e-01,  2.4727e+00,  3.3691e-01,  1.1084e+00,\n",
      "         3.6572e-01,  3.6699e+00,  9.8047e-01,  2.5469e+00,  8.1201e-01,\n",
      "         6.6406e-01,  1.3252e+00,  7.3096e-01,  4.5898e-01,  1.6484e+00,\n",
      "         4.4434e-01,  2.4902e-01,  6.8896e-01, -4.9878e-01, -1.1045e+00,\n",
      "         7.9688e-01,  3.9062e+00,  1.4209e+00,  2.1328e+00,  5.7764e-01,\n",
      "         1.3809e+00,  1.1025e+00, -2.1155e-01,  4.2554e-01,  4.5197e-02,\n",
      "         2.9941e+00,  1.2480e+00,  1.3896e+00,  6.6064e-01,  2.0586e+00,\n",
      "         1.4277e+00,  3.5010e-01,  3.2788e-01,  5.9521e-01,  1.7939e+00,\n",
      "         5.8447e-01,  9.6826e-01,  3.8574e-01, -7.2754e-02,  3.4727e+00,\n",
      "         9.7363e-01,  1.3877e+00, -1.2805e-01,  3.9453e-01,  5.0732e-01,\n",
      "         3.4043e+00,  6.8213e-01,  5.8301e-01,  8.8867e-01,  1.5149e-01,\n",
      "         1.8379e+00,  3.9478e-01,  1.3691e+00, -1.3159e-01,  5.3271e-01,\n",
      "         1.2871e+00,  2.9199e+00,  2.5234e+00,  1.5564e-01,  2.1172e+00,\n",
      "         7.0117e-01, -1.7297e-01,  3.2056e-01, -5.6787e-01,  5.7275e-01,\n",
      "         1.2334e+00,  2.4811e-02,  9.1357e-01, -4.2480e-01,  3.8555e+00,\n",
      "         1.6055e+00,  1.6406e+00,  6.9434e-01,  1.4844e-01,  3.8125e+00,\n",
      "         1.5049e+00,  2.2520e+00,  7.7246e-01,  7.7393e-01,  3.1445e+00,\n",
      "         1.5303e+00,  1.2451e+00,  1.2148e+00,  7.2656e-01,  1.0332e+00,\n",
      "        -2.0050e-02,  2.4512e+00,  1.1993e-01, -1.1725e-01,  1.3545e+00,\n",
      "         7.7588e-01,  8.4375e-01, -8.7891e-01, -4.3579e-02, -6.2158e-01,\n",
      "         1.2705e+00,  5.1367e-01,  7.7441e-01, -4.0698e-01,  2.2539e+00,\n",
      "         2.3438e-01,  3.3945e+00,  8.1177e-02,  1.3291e+00,  7.6758e-01,\n",
      "         1.2793e+00,  1.8896e+00, -2.4951e-01, -2.3596e-01,  9.0625e-01,\n",
      "         5.7422e-01, -8.2715e-01,  4.9048e-01, -2.2937e-01, -3.9844e-01,\n",
      "         1.3525e+00,  1.0732e+00,  3.3008e-01, -2.8760e-01,  3.9258e+00,\n",
      "         2.6738e+00,  8.8672e-01,  1.3193e+00,  7.8564e-01, -1.2090e+00,\n",
      "        -6.8848e-01, -7.8613e-01, -5.1660e-01,  2.8125e-01,  3.3984e-01,\n",
      "         1.5472e-02,  1.8982e-01,  2.1953e+00,  8.6963e-01,  9.2480e-01,\n",
      "         5.5420e-01,  4.8706e-01,  3.8262e+00,  1.0273e+00,  3.4395e+00,\n",
      "         2.2422e+00,  1.1631e+00,  8.6768e-01,  3.5820e+00,  1.0625e+00,\n",
      "        -6.1890e-02, -3.4009e-01, -7.5781e-01, -1.4624e-01,  8.1787e-01,\n",
      "         2.5059e+00,  8.5547e-01,  5.7129e-01,  1.4014e+00,  3.7720e-01,\n",
      "        -1.8225e-01, -8.8257e-02,  8.5596e-01, -3.2666e-01,  2.5957e+00,\n",
      "         1.1406e+00,  1.3379e+00,  8.4375e-01,  6.5137e-01,  4.1328e+00,\n",
      "         1.7969e+00,  2.1465e+00,  8.3643e-01, -1.4771e-01,  1.4209e+00,\n",
      "        -2.2424e-01, -2.5269e-01,  9.0088e-01,  5.2881e-01,  3.2188e+00,\n",
      "         4.1943e-01,  6.2988e-02,  1.4099e-01,  3.4414e+00,  9.9316e-01,\n",
      "         2.6392e-01,  3.4497e-01, -1.5833e-01,  9.1650e-01,  1.0000e+00,\n",
      "        -5.6104e-01, -2.1362e-01, -3.6084e-01,  3.5596e-01,  5.8154e-01,\n",
      "         3.0713e-01, -1.5027e-01,  3.3770e+00,  9.3115e-01, -2.8101e-01,\n",
      "         1.6333e-01, -1.8005e-01, -3.3838e-01,  3.4844e+00,  7.9297e-01,\n",
      "         6.2305e-01,  6.9043e-01,  4.5898e-01,  2.5762e+00, -3.6279e-01,\n",
      "         9.5361e-01,  2.3496e+00, -2.6343e-01,  8.8965e-01, -3.9331e-01,\n",
      "         1.0186e+00,  1.5027e-01,  4.1992e+00,  9.5752e-01,  1.0781e+00,\n",
      "         4.3384e-01,  3.5132e-01, -2.8198e-01,  1.1621e+00,  2.4078e-02,\n",
      "         3.9648e+00,  1.4980e+00,  1.5029e+00,  3.4863e-01,  1.7070e+00,\n",
      "         8.3838e-01,  2.4097e-01, -2.9190e-02,  3.6738e+00,  1.4092e+00,\n",
      "        -6.2158e-01,  2.0352e+00,  4.6973e-01,  3.8379e-01, -8.8013e-02,\n",
      "         3.2715e+00,  4.4312e-01,  1.3242e+00,  4.9829e-01, -1.0381e+00,\n",
      "        -2.9272e-01, -9.8877e-01, -7.8906e-01, -1.9434e-01,  1.1777e+00,\n",
      "        -3.7866e-01,  1.2079e-01, -2.5171e-01,  3.0449e+00,  2.0630e-01,\n",
      "         1.1602e+00,  2.0728e-01, -9.0771e-01, -1.6577e-01, -1.9690e-01,\n",
      "         3.5820e+00,  5.1611e-01,  1.8975e+00,  6.0156e-01,  6.7041e-01,\n",
      "        -1.3135e-01,  3.9520e-02,  3.0195e+00,  2.7715e+00,  1.5693e+00,\n",
      "         6.7480e-01,  8.3252e-01,  8.7500e-01, -7.8223e-01,  1.7639e-01,\n",
      "        -3.1299e-01, -4.6875e-01,  4.0356e-01, -7.5488e-01,  2.5366e-01,\n",
      "         2.1509e-01,  9.2334e-01,  3.6792e-01,  6.6113e-01, -6.8054e-02,\n",
      "        -6.6650e-02, -5.9082e-01, -4.6606e-01,  1.6467e-01, -3.5498e-01,\n",
      "         6.6113e-01, -1.3086e-01,  5.3662e-01,  3.3730e+00,  1.3789e+00,\n",
      "         1.0071e-01,  6.1133e-01, -3.4027e-02,  7.4072e-01,  9.6094e-01,\n",
      "         6.8359e-01, -3.3032e-01,  1.0000e+00, -2.0984e-01,  2.8271e-01,\n",
      "         6.1621e-01, -4.3994e-01,  1.7031e+00,  2.2070e+00,  2.3672e+00,\n",
      "         4.1040e-01,  2.1484e+00,  5.4639e-01, -9.2041e-01, -3.1641e-01,\n",
      "         1.0492e-01,  3.7036e-01, -7.0117e-01,  4.5801e-01, -4.2285e-01,\n",
      "         3.3457e+00,  9.6875e-01,  3.6890e-01, -5.2344e-01,  2.1729e-01,\n",
      "        -2.6970e-03,  6.8848e-01,  1.1162e+00,  1.8174e+00,  1.8469e-01,\n",
      "         2.0957e+00, -5.8643e-01,  9.2346e-02,  5.2197e-01, -2.3877e-01,\n",
      "         3.0977e+00,  1.0586e+00,  5.7129e-01,  1.0996e+00,  2.0859e+00,\n",
      "         1.3682e+00,  1.3806e-01,  1.7761e-01,  3.4888e-01,  3.2832e+00,\n",
      "         5.1074e-01,  6.2158e-01,  1.4990e+00, -7.9773e-02, -6.3416e-02,\n",
      "         2.4805e-01, -3.6694e-01,  2.7754e+00,  8.1396e-01,  8.5840e-01,\n",
      "         2.6001e-01,  3.2754e+00,  1.1895e+00,  1.7393e+00, -5.2832e-01,\n",
      "        -7.3120e-02,  3.1934e-01, -3.1567e-01,  2.7422e+00,  3.7671e-01,\n",
      "         1.2803e+00,  2.3218e-01,  3.7812e+00,  9.9023e-01,  1.3789e+00,\n",
      "         1.0176e+00,  3.9771e-01, -8.9941e-01, -6.3818e-01,  2.7026e-01,\n",
      "         1.6123e+00,  3.0579e-02, -5.6982e-01,  9.6289e-01, -4.2603e-01,\n",
      "         1.9867e-02,  2.7227e+00,  1.6895e-01,  7.7148e-01,  4.0625e-01,\n",
      "         3.5117e+00,  5.2734e-01,  8.8281e-01,  2.8086e+00,  6.1475e-01,\n",
      "         4.6973e-01,  1.7344e+00,  4.5312e-01,  1.3770e+00, -1.1334e-01,\n",
      "        -2.4194e-01, -6.1426e-01, -7.5342e-01, -6.2354e-01,  3.6652e-02,\n",
      "        -2.8735e-01,  3.6113e+00,  1.1963e+00,  7.0166e-01,  2.8839e-02,\n",
      "         2.5049e-01, -1.2891e+00,  2.7422e+00,  6.3538e-02,  1.8809e+00,\n",
      "        -1.0626e-01,  1.1787e+00, -1.6223e-01,  7.1045e-02,  1.3027e+00,\n",
      "         1.8311e-02, -5.0635e-01,  2.5332e+00,  5.0697e-03,  7.2119e-01,\n",
      "        -8.0127e-01,  5.7869e-03, -9.3945e-01,  4.4727e-01,  3.9893e-01,\n",
      "         1.5405e-01, -1.6797e-01, -9.4360e-02,  3.0898e+00,  1.6973e+00,\n",
      "         7.5928e-01, -1.4172e-01,  2.8223e+00,  1.3311e+00, -8.2520e-01,\n",
      "        -1.0020e+00, -4.6729e-01,  1.0732e+00, -2.1301e-01,  4.5312e-01,\n",
      "         3.6401e-01,  3.8721e-01,  1.4551e+00,  1.8701e-01,  4.9780e-01,\n",
      "         1.4248e+00,  4.2458e-03,  7.1924e-01, -4.8413e-01,  5.8887e-01,\n",
      "         1.6748e-01,  1.7969e+00,  2.1973e+00,  1.5791e+00,  2.6113e+00,\n",
      "         8.4131e-01, -1.0938e+00,  2.7069e-02,  2.3887e+00,  1.5449e+00,\n",
      "         6.7090e-01,  1.4316e+00,  1.9592e-01, -3.1836e-01,  1.4771e-01,\n",
      "        -3.5840e-01, -5.5029e-01,  4.8804e-01,  3.2090e+00,  7.7197e-01,\n",
      "         7.9492e-01,  1.5977e+00,  1.3086e+00,  4.5624e-02,  6.5918e-01,\n",
      "        -2.0874e-01, -7.9041e-02,  2.3613e+00,  2.0862e-01,  4.8267e-01,\n",
      "        -2.9517e-01,  8.8867e-01,  2.6184e-02,  9.6924e-01,  3.4027e-02,\n",
      "         2.0215e-01, -1.4612e-01, -1.0938e-01,  2.9102e+00,  6.5527e-01,\n",
      "         5.8301e-01, -2.3193e-01,  2.1387e+00,  6.8176e-02,  2.7710e-01,\n",
      "        -3.3887e-01,  2.1387e+00, -1.9485e-02,  3.6670e-01,  1.7666e+00,\n",
      "         4.6045e-01,  7.7979e-01,  3.0713e-01,  1.8965e+00,  1.7490e+00,\n",
      "         1.0420e+00,  5.3467e-01,  7.5391e-01,  7.6904e-01,  2.4609e+00,\n",
      "         3.8770e-01,  3.4149e-02,  2.2617e+00,  5.4932e-01,  4.9731e-01,\n",
      "         1.8274e-01, -8.0322e-01, -6.6956e-02, -1.0781e+00,  2.3193e-01,\n",
      "         3.1494e-01,  1.7002e+00,  1.4883e+00,  8.9746e-01,  2.4463e-01,\n",
      "         4.4385e-01,  2.1387e+00,  2.0337e-01, -3.8910e-02,  2.1777e+00,\n",
      "         4.2969e-01,  9.7656e-02, -1.1859e-01,  5.3314e-02, -1.7053e-01,\n",
      "        -7.0020e-01,  2.0703e-01, -5.4980e-01,  2.6992e+00,  1.7158e+00,\n",
      "         1.0508e+00,  3.9868e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 6.8994e-01, -5.3760e-01,  1.4678e+00, -5.9570e-01, -6.8604e-01,\n",
      "        -9.6387e-01, -7.3584e-01, -7.0068e-01, -1.4775e+00, -1.1562e+00,\n",
      "        -8.5400e-01, -4.6143e-01, -1.3855e-01, -8.3984e-01,  9.7852e-01,\n",
      "         4.9976e-01,  4.1040e-01,  7.1869e-03,  7.4805e-01,  5.9863e-01,\n",
      "         3.4941e+00,  5.8154e-01,  2.4668e+00,  3.9414e+00,  1.2588e+00,\n",
      "         1.6191e+00,  2.1016e+00,  1.6045e+00,  1.4238e+00,  1.8970e-01,\n",
      "         7.7930e-01, -7.1240e-01, -1.1603e-01, -1.3342e-01,  7.0166e-01,\n",
      "         2.8247e-01,  5.5957e-01,  5.4785e-01,  1.5979e-01,  1.8838e+00,\n",
      "        -7.9150e-01, -8.8721e-01, -4.5044e-01, -6.6943e-01,  5.4639e-01,\n",
      "        -2.4866e-01, -4.3164e-01, -3.7427e-01, -1.1127e-01,  6.3281e-01,\n",
      "         2.9512e+00, -6.8994e-01, -4.7168e-01,  4.4556e-01,  5.3564e-01,\n",
      "         6.1621e-01,  2.9434e+00,  4.6118e-01,  8.6304e-02, -9.1357e-01,\n",
      "        -2.0483e-01, -3.9087e-01, -5.7812e-01, -7.3389e-01,  3.2397e-01,\n",
      "         2.1680e+00,  2.9648e+00, -3.3966e-02, -2.8198e-01,  4.9243e-01,\n",
      "         3.8281e+00,  8.0566e-01,  4.3140e-01, -2.9614e-01,  9.9609e-01,\n",
      "         9.1553e-01,  3.1855e+00,  9.0527e-01,  2.6484e+00,  2.9180e+00,\n",
      "        -3.1787e-01, -4.5508e-01,  1.9824e+00,  3.6255e-01,  2.2302e-01,\n",
      "         3.2031e+00,  5.6445e-01,  1.1875e+00,  8.1909e-02, -5.6592e-01,\n",
      "        -1.4648e-01, -3.3228e-01,  2.3071e-01,  4.8657e-01,  5.9912e-01,\n",
      "        -9.4189e-01, -1.2878e-01,  9.3213e-01,  1.2090e+00,  3.2812e+00,\n",
      "         2.0957e+00,  2.4121e+00, -6.6992e-01, -2.4011e-01, -7.5781e-01,\n",
      "         7.8662e-01,  1.6797e+00,  3.5371e+00, -8.0273e-01, -6.5820e-01,\n",
      "        -3.3569e-01, -1.5442e-01, -5.3925e-02,  9.1406e-01, -2.2873e-02,\n",
      "         3.5156e-01,  3.3740e-01,  1.3799e+00, -8.0811e-01, -1.5027e-01,\n",
      "         8.8770e-01,  1.2236e+00,  3.3184e+00,  2.1774e-02, -6.2744e-01,\n",
      "        -3.7769e-01, -3.3521e-01, -2.9980e-01,  2.5757e-01,  1.0400e+00,\n",
      "         8.0322e-02,  4.2572e-02,  1.0771e+00,  2.3027e+00,  1.0469e+00,\n",
      "        -7.0801e-02,  8.7646e-01,  1.5195e+00,  2.7168e+00,  5.0586e-01,\n",
      "         6.9580e-01,  1.1221e+00, -6.3525e-01,  4.5630e-01, -8.8184e-01,\n",
      "        -2.4988e-01,  5.9961e-01, -1.9958e-01,  3.6768e-01, -4.9512e-01,\n",
      "         8.0615e-01,  8.1543e-01,  4.1641e+00, -6.8066e-01, -1.1432e-01,\n",
      "         1.4004e+00,  1.3389e+00,  1.1221e+00,  4.1992e+00,  6.6699e-01,\n",
      "         3.6548e-01,  5.3320e-01,  1.0889e+00,  1.3057e+00,  2.4883e+00,\n",
      "        -6.9336e-01,  6.1523e-01,  7.0850e-01,  8.3643e-01, -4.6655e-01,\n",
      "        -3.5327e-01,  3.2441e+00, -1.0107e+00, -1.3125e+00, -1.4512e+00,\n",
      "        -3.8624e-03,  3.3154e-01,  1.3232e-01, -4.4116e-01,  9.0479e-01,\n",
      "         1.6689e+00,  5.9326e-01,  5.0049e-01,  3.0371e+00,  8.5840e-01,\n",
      "         1.3208e-01, -8.9990e-01, -3.6768e-01, -3.4253e-01,  2.2207e+00,\n",
      "         9.4678e-01, -1.0322e+00,  1.1154e-02, -9.7198e-03, -1.0508e+00,\n",
      "         1.0664e+00,  6.3525e-01, -3.2593e-01,  4.6167e-01, -2.8027e-01,\n",
      "         1.7041e-01,  1.6055e+00,  2.7754e+00,  3.8730e+00, -1.1221e+00,\n",
      "        -1.0723e+00, -6.7920e-01, -9.9561e-01, -2.4246e-02, -4.5441e-02,\n",
      "         7.8491e-02, -7.0068e-01,  3.3423e-01,  7.3047e-01,  3.8789e+00,\n",
      "         3.8843e-01,  7.3047e-01, -2.9907e-01,  3.5000e+00,  2.3547e-01,\n",
      "         7.4609e-01,  3.8652e+00,  2.5156e+00,  9.1492e-02,  3.5039e+00,\n",
      "         1.5254e+00,  1.2744e-01,  2.2241e-01,  1.6479e-01, -1.2238e-01,\n",
      "        -4.3286e-01,  3.6963e-01,  2.9028e-01,  2.4492e+00,  2.6797e+00,\n",
      "        -6.4453e-01, -6.5576e-01, -9.0527e-01, -8.6182e-01, -6.7920e-01,\n",
      "        -9.1858e-02,  5.1807e-01,  1.7324e+00, -6.8799e-01, -3.6035e-01,\n",
      "         1.5527e+00,  1.2002e+00,  3.8223e+00, -1.0537e+00, -4.9219e-01,\n",
      "        -6.2500e-01, -2.6514e-01,  1.5225e+00, -5.9033e-01, -1.2079e-01,\n",
      "         2.8438e+00, -1.1396e+00, -7.4902e-01, -2.9810e-01,  1.7920e+00,\n",
      "         6.2598e-01,  1.8042e-01, -9.9170e-01, -3.6475e-01,  2.3984e+00,\n",
      "        -7.2754e-01, -3.0396e-01, -4.4727e-01, -4.5312e-01, -7.0361e-01,\n",
      "        -6.8506e-01, -9.0576e-01, -5.9473e-01,  2.3008e+00, -1.1484e+00,\n",
      "        -5.1025e-01, -3.1396e-01, -8.7549e-01,  8.4277e-01,  1.0059e+00,\n",
      "         3.8711e+00,  5.8936e-01,  6.9385e-01, -3.1494e-01,  3.0365e-02,\n",
      "         2.5508e+00, -3.5669e-01,  1.6272e-01,  2.7402e+00, -8.8574e-01,\n",
      "         2.9526e-02,  3.0060e-02,  5.5762e-01,  1.1387e+00,  4.2305e+00,\n",
      "         1.0879e+00,  1.6855e+00, -6.5527e-01,  3.1665e-01,  1.7842e+00,\n",
      "        -2.8320e-01,  8.9111e-01,  3.5742e+00,  9.4385e-01,  1.0098e+00,\n",
      "         5.3467e-01, -1.3184e-01,  2.3270e-02, -2.6627e-02,  2.7754e+00,\n",
      "        -7.0605e-01,  8.5303e-01,  6.3379e-01, -5.1123e-01,  2.7319e-01,\n",
      "         3.5547e-01,  3.3325e-01,  3.1348e+00, -7.6123e-01, -3.0884e-01,\n",
      "        -6.7725e-01, -1.0742e+00, -3.4375e-01, -1.0371e+00,  2.1699e+00,\n",
      "        -9.8291e-01,  1.6571e-02, -1.0859e+00,  2.1753e-01,  6.8970e-02,\n",
      "         2.9453e+00, -5.2197e-01, -1.0107e+00, -3.2324e-01, -8.5205e-01,\n",
      "         3.5059e-01,  2.3086e+00,  4.7290e-01,  1.6729e+00,  4.8926e-01,\n",
      "         6.7969e-01, -7.0410e-01,  3.8550e-01,  1.3718e-02,  2.3865e-01,\n",
      "         2.4961e+00,  3.0840e+00, -1.0785e-01, -1.4961e+00, -7.3438e-01,\n",
      "        -1.2891e+00, -5.8838e-01, -8.6768e-01, -8.0664e-01, -5.6104e-01,\n",
      "        -4.5996e-01,  3.4155e-01, -9.4629e-01, -4.4629e-01, -1.0420e+00,\n",
      "        -4.0845e-01, -2.0248e-02, -1.1406e+00, -8.3545e-01, -1.0371e+00,\n",
      "        -5.4395e-01, -4.2407e-01, -7.7441e-01,  3.2373e-01,  1.1865e+00,\n",
      "         1.8975e+00, -2.9343e-02,  4.8975e-01, -1.5930e-01, -6.5039e-01,\n",
      "        -1.5894e-01,  2.6294e-01, -2.9465e-02,  5.6885e-01,  1.4832e-01,\n",
      "        -1.0586e+00, -5.1074e-01, -4.4727e-01,  6.4844e-01,  5.7324e-01,\n",
      "         1.4526e-01,  1.5488e+00, -8.0273e-01, -1.7246e+00, -9.5557e-01,\n",
      "        -8.8818e-01,  1.5771e-01, -1.5586e+00, -1.0322e+00, -8.4473e-01,\n",
      "        -1.9995e-01,  5.6445e-01,  3.3281e+00, -4.2480e-01,  3.6743e-01,\n",
      "         8.0225e-01, -9.5032e-02, -4.0479e-01,  9.0967e-01,  1.1523e+00,\n",
      "         9.6240e-01, -3.5083e-01,  3.8879e-02, -6.8262e-01, -6.3867e-01,\n",
      "        -1.3260e-02,  3.2300e-01,  1.3652e+00,  4.6240e-01,  1.3391e-01,\n",
      "         3.5205e-01,  5.3809e-01,  1.9072e+00, -5.5322e-01, -2.5171e-01,\n",
      "         4.0308e-01,  1.8564e+00,  7.1143e-01,  1.9766e+00, -5.9052e-02,\n",
      "        -4.9756e-01, -5.7959e-01,  3.1763e-01,  5.4932e-01,  2.0762e+00,\n",
      "        -2.7148e-01, -7.7271e-02,  2.1914e+00,  2.4883e+00, -5.8447e-01,\n",
      "         1.6296e-02, -5.5908e-01, -7.0605e-01,  5.4150e-01, -2.1558e-01,\n",
      "         1.9912e+00, -2.8784e-01, -2.1362e-01,  2.2681e-01,  2.5996e+00,\n",
      "         2.1172e+00,  7.5879e-01, -1.2705e+00, -7.6709e-01, -2.9199e-01,\n",
      "         1.0651e-01,  4.2877e-02, -1.0850e+00, -2.3889e-01,  4.6802e-01,\n",
      "        -3.9502e-01, -9.6375e-02,  2.6672e-02,  1.5518e+00, -1.0059e+00,\n",
      "        -4.7119e-01,  3.6499e-01,  2.6074e+00, -4.0454e-01,  2.9922e-02,\n",
      "         2.6836e+00, -3.2031e-01,  7.1045e-01,  2.6953e+00, -8.3838e-01,\n",
      "        -6.6504e-01, -7.5684e-01, -8.4863e-01, -5.0244e-01,  1.2524e-01,\n",
      "         6.7090e-01,  7.7454e-02,  9.6777e-01,  2.7109e+00,  2.2656e+00,\n",
      "        -7.7637e-01, -6.4355e-01,  2.0178e-01,  2.1472e-01,  2.0332e+00,\n",
      "        -3.3911e-01,  2.3008e+00, -8.4424e-01, -1.1680e+00,  1.4404e-01,\n",
      "         1.9397e-01, -1.0625e+00, -1.1920e-01,  6.8665e-02,  1.9238e+00,\n",
      "        -8.5840e-01,  1.3931e-02,  5.1318e-01, -3.8013e-01,  5.9375e-01,\n",
      "         6.2793e-01,  2.5903e-01,  7.3926e-01, -2.7417e-01,  1.0889e+00,\n",
      "         2.8750e+00, -6.9531e-01, -3.4027e-02,  2.2441e+00, -6.2646e-01,\n",
      "        -3.2471e-01, -1.0244e+00,  9.0332e-01, -9.2834e-02,  9.4482e-01,\n",
      "        -3.4229e-01,  5.8008e-01, -4.7656e-01,  4.0527e-01,  2.1230e+00,\n",
      "        -4.8975e-01, -8.0872e-02,  1.7422e+00, -1.3604e+00, -4.2065e-01,\n",
      "        -8.4961e-01,  1.1133e+00,  1.5625e-02,  1.1963e+00, -4.9902e-01,\n",
      "         2.8457e+00,  1.0126e-01, -1.7725e-01,  1.0330e-02,  3.8745e-01,\n",
      "         1.4951e+00,  3.1855e+00, -1.8286e-01, -1.3984e+00,  1.9089e-02,\n",
      "        -1.0162e-01, -4.9854e-01,  4.8291e-01, -2.7197e-01,  4.6753e-01,\n",
      "         3.0645e+00,  7.6562e-01,  2.6440e-01,  6.2207e-01,  2.5684e+00,\n",
      "        -4.7656e-01, -1.0664e+00, -5.8594e-01,  1.4868e-01,  2.1621e+00,\n",
      "        -8.4570e-01, -1.4539e-01, -4.8975e-01, -4.6484e-01,  5.5267e-02,\n",
      "         1.5808e-01,  1.5137e-01, -7.8369e-01, -8.7061e-01,  2.3301e+00,\n",
      "        -5.6445e-01,  1.8994e-01, -8.1738e-01,  1.5442e-01,  2.1113e+00,\n",
      "        -8.1738e-01, -1.9348e-01,  2.6318e-01,  2.4062e+00,  6.8262e-01,\n",
      "         2.3608e-01, -8.0566e-01, -6.8164e-01, -2.1887e-01,  1.2457e-01,\n",
      "         1.3232e+00,  3.3379e+00, -3.4082e-01,  1.1953e+00,  8.8074e-02,\n",
      "         2.2422e+00, -6.8799e-01, -1.5918e-01,  3.2539e+00,  1.6533e+00,\n",
      "         3.2715e-01, -8.6133e-01, -7.8320e-01, -1.8811e-01, -7.1826e-01,\n",
      "        -8.2959e-01, -3.6133e-01,  1.2494e-01,  1.2168e+00,  3.2539e+00,\n",
      "         4.1406e-01, -9.2529e-01,  2.0547e+00, -9.6582e-01, -3.9819e-01,\n",
      "         2.8594e+00,  2.0569e-02, -7.8076e-01, -1.6858e-01,  3.6084e-01,\n",
      "        -9.7998e-01,  1.3440e-01,  7.9285e-02, -7.7686e-01,  3.7891e-01,\n",
      "         2.6543e+00,  5.8887e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.5858,  0.5146, -0.3571,  ...,  1.8299,  1.6994, -0.3132],\n",
      "         [ 0.0632,  0.4110, -0.2999,  ...,  0.6482,  0.6726,  0.2587],\n",
      "         [ 0.3056,  0.3121,  0.0430,  ..., -0.1336,  0.4111,  0.0923],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.7871e-01,  2.6016e+00,  1.6230e+00,  5.8008e-01, -3.5352e-01,\n",
      "         2.5195e+00,  3.6475e-01,  3.3374e-01, -3.7292e-02,  6.8164e-01,\n",
      "        -2.0447e-02,  1.9844e+00,  7.6367e-01,  8.7891e-01,  4.7607e-01,\n",
      "         3.9355e-01,  3.5352e+00,  9.5605e-01,  2.5439e-01, -1.7371e-01,\n",
      "         1.5645e+00,  3.4023e+00,  9.1113e-01,  1.5979e-01, -2.4707e-01,\n",
      "         1.3555e+00,  2.8574e+00,  7.0996e-01,  2.0000e+00,  2.8345e-01,\n",
      "         9.6729e-01,  4.0015e-01, -9.1858e-02,  8.0664e-01, -4.9878e-01,\n",
      "         2.9219e+00,  4.3677e-01,  4.2163e-01, -7.2705e-01,  3.3945e+00,\n",
      "         8.1641e-01,  1.8738e-01, -2.2009e-01, -3.9844e-01,  3.4766e+00,\n",
      "         6.7676e-01,  7.9285e-02,  3.0469e-01, -1.1267e-01,  2.9121e+00,\n",
      "         5.1318e-01, -9.0515e-02,  8.5938e-02, -6.8945e-01,  6.8994e-01,\n",
      "        -1.6266e-02, -1.6427e-04, -7.3193e-01,  3.2539e+00,  2.0355e-02,\n",
      "        -9.8206e-02,  3.0977e+00,  7.0679e-02, -2.4854e-01, -5.4785e-01,\n",
      "        -2.3755e-01,  3.4277e+00,  9.6143e-01,  2.9541e-01, -1.2817e-01,\n",
      "         9.6143e-01,  1.0297e-01,  2.0449e+00,  2.6055e+00,  6.4551e-01,\n",
      "        -9.9915e-02,  1.2091e-01, -1.5820e-01,  3.0137e+00,  6.4307e-01,\n",
      "         4.3628e-01,  1.2561e-01, -3.5303e-01, -1.3025e-01,  3.4062e+00,\n",
      "         1.8494e-01,  1.6711e-01, -7.5146e-01, -3.9111e-01, -3.0029e-01,\n",
      "         4.7119e-01,  4.7070e-01, -3.9844e-01, -4.8535e-01, -4.9023e-01,\n",
      "         7.9248e-01, -3.7451e-01,  3.3066e+00,  4.1333e-01, -2.3584e-01,\n",
      "        -1.4893e-01,  2.5039e+00,  3.3740e-01, -1.8631e-02,  3.1152e-01,\n",
      "         1.3633e+00,  3.3630e-02,  3.1875e+00,  1.1162e+00,  1.8958e-01,\n",
      "         4.0991e-01,  1.1346e-01,  1.9989e-02,  3.3809e+00,  2.1216e-01,\n",
      "         4.1534e-02,  3.4043e+00,  8.9160e-01,  2.6245e-01,  6.2109e-01,\n",
      "        -7.4158e-02,  3.0137e+00,  5.8154e-01, -1.1267e-01,  3.4131e-01,\n",
      "        -3.3716e-01, -8.0127e-01,  1.5271e-01,  2.4375e+00, -1.9324e-01,\n",
      "        -7.0020e-01,  1.1169e-02,  6.6309e-01,  6.4600e-01,  1.0614e-01,\n",
      "         8.7524e-02,  3.6914e+00,  1.0674e+00,  5.1074e-01,  5.5371e-01,\n",
      "         7.0166e-01,  4.6411e-01, -3.1616e-01, -3.6572e-01,  1.8438e+00,\n",
      "        -1.8945e-01, -9.4287e-01,  1.2732e-01, -6.8066e-01, -3.5522e-01,\n",
      "         3.6621e+00,  8.5303e-01,  1.7981e-01,  5.9180e-01, -1.7960e-02,\n",
      "         3.2363e+00,  5.9473e-01, -1.1151e-01,  1.2201e-01,  4.8755e-01,\n",
      "         8.7451e-01, -7.4854e-01,  1.3721e+00, -5.7227e-01,  2.6685e-01,\n",
      "        -4.2383e-01,  4.4604e-01,  3.5498e-01,  4.6356e-02, -9.6973e-01,\n",
      "         3.5391e+00,  1.6113e-01,  1.8677e-01,  1.5400e+00,  3.0020e+00,\n",
      "         6.9922e-01, -2.5223e-02,  1.8884e-01,  1.1396e+00,  1.9531e+00,\n",
      "         4.7217e-01, -6.0692e-03, -1.1865e-01,  3.0781e+00,  3.7915e-01,\n",
      "         2.2449e-01, -7.2937e-02, -8.5205e-01,  1.2238e-01, -9.7412e-01,\n",
      "        -1.6565e-01, -2.6392e-01,  1.5420e+00, -4.4922e-01,  5.8887e-01,\n",
      "         6.1279e-01,  5.0537e-01,  2.6758e+00,  1.2500e+00,  5.5389e-02,\n",
      "        -2.0349e-01,  7.3779e-01,  3.3984e-01, -2.1057e-01, -2.7441e-01,\n",
      "         1.5918e-01, -2.7197e-01,  1.2168e+00, -1.6479e-01,  8.9941e-01,\n",
      "        -3.5498e-01,  3.7012e-01, -5.0928e-01, -3.2959e-01,  6.1572e-01,\n",
      "        -1.1113e+00,  5.0732e-01,  4.4849e-01, -3.4937e-01, -2.6758e-01,\n",
      "        -3.1714e-01, -2.9565e-01,  1.1025e+00,  2.7905e-01, -1.2610e-01,\n",
      "        -3.6279e-01,  4.5197e-02, -5.9033e-01,  8.7549e-01, -3.0685e-02,\n",
      "        -2.0325e-01,  5.6641e-01,  4.3726e-01,  3.8105e+00,  1.1768e+00,\n",
      "         4.9146e-01, -5.2094e-02,  1.5195e+00,  3.3867e+00,  1.0488e+00,\n",
      "         2.1594e-01, -1.9727e-01,  1.3174e+00,  2.1504e+00,  4.2212e-01,\n",
      "         3.1470e-01, -5.7373e-01,  3.5215e+00,  9.0576e-01,  2.0483e-01,\n",
      "        -2.7979e-01, -2.0935e-01,  3.7051e+00,  5.9033e-01, -1.6370e-01,\n",
      "         4.1901e-02,  3.1016e+00,  2.2095e-01,  1.5100e-01, -5.5859e-01,\n",
      "        -2.4561e-01,  3.4688e+00,  1.1426e+00,  3.0054e-01, -7.3303e-02,\n",
      "         1.0508e+00, -2.8152e-02,  3.6543e+00,  8.7549e-01,  2.6807e-01,\n",
      "         2.5957e+00,  5.8496e-01,  2.2107e-01, -5.9766e-01, -3.1830e-02,\n",
      "        -6.7822e-01, -3.4668e-01,  3.4355e+00,  4.2749e-01, -3.1836e-01,\n",
      "        -1.8875e-02,  2.7891e+00,  3.9014e-01, -1.2781e-01, -2.6758e-01,\n",
      "         3.6562e+00,  5.9424e-01,  1.7212e-01,  4.4653e-01, -3.8525e-01,\n",
      "         5.8441e-02,  3.3105e-01,  1.3340e+00,  2.5176e+00,  3.1860e-01,\n",
      "        -4.1565e-02,  3.1543e+00,  1.9312e-01, -1.0034e-01,  1.5889e+00,\n",
      "         2.8809e+00,  2.6147e-01,  4.7559e-01,  9.5215e-01, -5.6592e-01,\n",
      "         1.9141e+00,  3.4149e-02,  3.5996e+00,  5.8105e-01, -1.3806e-01,\n",
      "        -4.0063e-01, -5.5518e-01,  3.3911e-01,  1.0293e+00,  1.0577e-01,\n",
      "         1.3232e-01, -8.2080e-01, -4.0161e-01, -9.8535e-01,  3.4297e+00,\n",
      "         8.6719e-01, -8.0017e-02, -2.6904e-01,  4.2456e-01,  1.0596e+00,\n",
      "         3.8281e+00,  5.7617e-01,  3.5195e+00,  4.2725e-01, -1.0732e+00,\n",
      "         1.1523e+00, -2.8345e-01,  3.0977e+00, -1.9141e-01, -9.2102e-02,\n",
      "         3.6309e+00,  6.9531e-01,  3.2988e+00,  1.0010e+00, -6.2408e-02,\n",
      "         2.8184e+00,  4.8853e-01,  6.4880e-02, -6.0693e-01,  2.9180e+00,\n",
      "         6.4746e-01, -2.8027e-01, -3.4766e-01,  3.2598e+00,  7.1240e-01,\n",
      "        -8.9661e-02,  3.6060e-01,  4.2188e-01,  3.6504e+00,  1.0234e+00,\n",
      "         4.8804e-01, -7.4524e-02,  1.5293e+00,  3.4062e+00,  9.5068e-01,\n",
      "         3.5034e-01, -1.9141e-01,  1.4883e+00,  2.0781e+00,  5.7471e-01,\n",
      "         3.5132e-01, -7.8906e-01,  3.4688e+00,  8.1738e-01,  3.5010e-01,\n",
      "        -1.8933e-01, -2.5244e-01, -3.8892e-01, -2.9068e-02, -1.7346e-01,\n",
      "         1.8350e+00, -8.3130e-02,  4.0586e+00,  7.7295e-01,  4.9976e-01,\n",
      "         8.8806e-02, -3.2812e-01,  3.6309e+00,  8.6377e-01, -1.1035e-01,\n",
      "        -3.3691e-02, -2.0850e-01,  3.5000e+00,  7.3242e-01, -1.2384e-01,\n",
      "         1.5857e-01, -4.6948e-01,  1.3955e+00, -1.8298e-01,  8.6719e-01,\n",
      "        -4.0576e-01,  3.4727e+00,  1.0938e+00,  3.2910e-01, -1.2512e-01,\n",
      "         1.0029e+00, -4.4507e-01,  1.7305e+00, -4.3243e-02,  3.4355e+00,\n",
      "         1.3904e-01, -2.9980e-01,  3.4551e+00,  9.9805e-01,  5.4053e-01,\n",
      "        -1.2195e-01, -2.6807e-01, -2.9468e-01,  3.7168e+00,  7.6514e-01,\n",
      "        -7.5500e-02,  1.6809e-01, -3.0518e-01,  1.4219e+00, -1.8188e-01,\n",
      "        -6.6162e-01,  2.4375e+00,  3.6206e-01, -3.6768e-01, -4.7583e-01,\n",
      "        -6.4990e-01, -7.1045e-01,  1.4258e-01,  6.1035e-01, -1.7126e-01,\n",
      "        -6.3354e-02, -9.4727e-01, -5.7422e-01, -1.0508e+00,  3.2773e+00,\n",
      "         7.9980e-01, -1.3818e-01, -3.2764e-01,  2.6611e-01,  4.0527e-01,\n",
      "         3.7383e+00,  8.7500e-01,  7.4951e-01,  6.4990e-01, -3.6774e-02,\n",
      "         5.3320e-01,  8.9661e-02,  2.9766e+00,  8.4668e-01,  5.6006e-01,\n",
      "         4.5825e-01, -8.3679e-02,  4.9438e-01, -1.3843e-01,  2.7930e+00,\n",
      "         6.1230e-01,  2.3672e+00,  7.5537e-01,  5.4590e-01,  5.1025e-01,\n",
      "         2.1350e-01,  1.1992e+00,  1.0693e+00,  1.4612e-01, -1.0947e+00,\n",
      "         3.9355e-01,  1.9641e-01,  1.4929e-01, -4.7192e-01,  1.8555e-01,\n",
      "         3.1036e-02,  2.7490e-01,  9.1357e-01, -6.1865e-01,  3.2637e+00,\n",
      "         8.2422e-01,  5.3223e-01,  4.5386e-01, -2.3267e-01,  2.5957e+00,\n",
      "         6.0010e-01,  8.1116e-02, -2.5000e-01,  3.1367e+00,  7.9639e-01,\n",
      "         1.1456e-01,  4.7681e-01,  3.2196e-02, -4.3408e-01,  1.1387e+00,\n",
      "        -3.2715e-01,  3.1504e+00,  1.1639e-01, -3.8745e-01,  3.6621e+00,\n",
      "         8.4473e-01, -3.1616e-01,  1.8726e-01,  2.7520e+00, -2.1948e-01,\n",
      "         3.1504e+00,  1.0361e+00,  6.2109e-01,  4.4336e-01, -2.6782e-01,\n",
      "         9.7656e-01, -6.0547e-02, -7.3535e-01,  1.0663e-01, -1.1025e+00,\n",
      "         3.2598e+00,  8.4668e-01,  9.0210e-02,  5.8136e-02,  2.5801e+00,\n",
      "         1.9902e+00,  3.6102e-02,  3.0664e+00,  5.0684e-01, -2.9126e-01,\n",
      "         1.3904e-01, -6.8359e-02, -7.3145e-01, -1.3695e-03, -1.1250e+00,\n",
      "         2.8359e+00,  5.2588e-01,  3.0005e-01, -4.4629e-01,  9.6582e-01,\n",
      "         1.3708e-01,  4.3115e-01,  3.7168e+00,  5.3516e-01,  3.5273e+00,\n",
      "         4.6313e-01, -1.1260e+00,  1.1582e+00, -3.6401e-01,  2.9844e+00,\n",
      "        -2.8271e-01, -1.8091e-01,  3.6914e+00,  1.0527e+00,  1.6016e-01,\n",
      "        -4.6240e-01,  3.2715e+00,  1.8384e-01,  5.8479e-03,  4.6216e-01,\n",
      "        -5.8350e-01,  3.6191e+00,  2.2107e-01,  5.9814e-01,  3.2305e+00,\n",
      "         5.3076e-01, -8.9160e-01,  4.0649e-01,  7.0605e-01, -4.0137e-01,\n",
      "         2.8945e+00, -1.1877e-01, -3.6816e-01, -3.5667e-03,  1.7651e-01,\n",
      "         4.1382e-01,  3.6758e+00,  9.5020e-01,  4.6582e-01, -1.2323e-01,\n",
      "         2.1582e+00,  2.3804e-01,  3.3965e+00,  8.1396e-01,  4.2920e-01,\n",
      "        -2.2949e-01,  1.5781e+00,  4.4220e-02,  3.0898e+00,  8.1787e-01,\n",
      "         2.4629e+00,  6.6650e-01,  1.1006e+00,  8.0908e-01, -7.4646e-02,\n",
      "         1.5557e+00, -5.1727e-02, -8.4814e-01,  1.9102e+00, -4.5630e-01,\n",
      "        -5.3369e-01,  5.9521e-01, -3.0835e-01, -3.7793e-01,  1.1113e+00,\n",
      "        -4.2212e-01,  3.4629e+00,  6.9519e-02, -4.4824e-01,  3.9844e+00,\n",
      "         9.6045e-01,  1.0283e+00,  7.7441e-01,  1.5662e-01,  7.5098e-01,\n",
      "         1.4551e-01,  2.0361e-01,  2.6055e+00, -1.0876e-01, -2.3853e-01,\n",
      "        -1.4294e-01, -1.1523e+00, -2.2937e-01,  3.4199e+00,  5.2832e-01,\n",
      "         9.3140e-02, -9.0027e-03,  2.2090e+00,  2.5520e-03, -4.2328e-02,\n",
      "         3.1465e+00,  3.5986e-01, -9.7754e-01, -4.6997e-02, -1.0199e-01,\n",
      "        -1.4197e-01, -2.0667e-01,  6.8359e-01, -9.7754e-01,  1.5332e+00,\n",
      "         2.0032e-01,  2.5085e-02,  3.2344e+00,  7.9956e-03,  1.2109e+00,\n",
      "        -1.6943e-01, -1.0977e+00, -1.9226e-01,  3.4180e+00,  3.1592e-01,\n",
      "         1.2822e+00,  2.7500e+00,  1.1709e+00,  1.3196e-01,  8.6572e-01,\n",
      "        -1.7578e-01, -2.3181e-01,  3.1191e+00,  1.8301e+00,  7.2656e-01,\n",
      "         1.1676e-01, -3.3081e-02,  3.4922e+00,  1.1328e+00,  2.5848e-02,\n",
      "         2.1912e-01,  1.9346e+00,  2.0813e-02,  3.9570e+00,  8.3105e-01,\n",
      "         8.2715e-01,  6.2793e-01,  7.0251e-02, -1.4380e-01,  1.1035e+00,\n",
      "        -1.7297e-01, -5.6982e-01, -5.1758e-01,  3.5605e+00,  9.2334e-01,\n",
      "         4.4263e-01, -3.2812e-01,  1.0156e+00,  1.0406e-01,  3.3325e-01,\n",
      "         3.5996e+00,  1.0498e+00,  4.9878e-01, -1.9141e-01,  1.6387e+00,\n",
      "         3.3086e+00,  9.9609e-01,  5.2686e-01, -1.8237e-01,  1.6641e+00,\n",
      "         1.2959e+00,  6.3232e-01,  6.8750e-01, -6.3721e-01,  3.3652e+00,\n",
      "         8.0322e-01,  3.6426e-01, -1.7627e-01, -1.2177e-01, -3.8501e-01,\n",
      "         4.3359e-01,  1.3496e+00, -4.8315e-01,  3.7578e+00,  1.2129e+00,\n",
      "         7.6123e-01,  2.6294e-01,  3.6670e-01, -1.9861e-01,  3.3027e+00,\n",
      "         7.0605e-01,  2.3584e-01, -3.5425e-01,  3.4375e+00,  9.2822e-01,\n",
      "         1.1348e+00,  1.9653e-01,  2.9336e+00,  2.3364e-01,  6.3110e-02,\n",
      "        -3.7329e-01,  3.7910e+00,  1.0098e+00,  5.6000e-02,  4.3042e-01,\n",
      "         2.6699e+00, -4.8431e-02,  1.7031e+00, -3.7402e-01, -4.0869e-01,\n",
      "        -5.0439e-01, -6.5918e-01,  6.6553e-01,  5.4535e-02,  2.9141e+00,\n",
      "         2.2773e+00,  5.4834e-01,  2.9793e-03, -6.8408e-01,  4.8413e-01,\n",
      "        -4.1968e-01,  6.5137e-01,  6.2561e-02,  8.4534e-03,  2.6816e+00,\n",
      "         1.9980e+00,  5.7275e-01, -5.3027e-01,  4.5190e-01,  1.0391e+00,\n",
      "         4.4556e-01, -6.1523e-01,  1.4038e-01, -6.7139e-01, -1.9641e-01,\n",
      "         3.5625e+00,  1.1143e+00,  5.4980e-01, -2.2449e-01,  1.0264e+00,\n",
      "        -1.5674e-01, -2.0154e-01,  1.0576e+00,  1.2805e-01, -9.1980e-02,\n",
      "         3.5410e+00,  8.2324e-01,  5.2295e-01, -1.2177e-01,  1.0205e+00,\n",
      "         2.9541e-01,  3.4985e-01,  3.0801e+00,  1.8711e+00,  1.1688e-01,\n",
      "         2.4141e+00,  1.6670e+00,  4.4403e-02,  3.1113e+00,  7.0654e-01,\n",
      "         1.8916e+00,  8.0420e-01,  8.1250e-01,  1.8604e+00,  9.3567e-02,\n",
      "        -7.9736e-01,  3.1270e+00,  8.1592e-01, -1.7426e-02,  3.6841e-01,\n",
      "         1.0488e+00, -5.9277e-01,  1.7842e+00,  1.8945e+00, -9.4223e-03,\n",
      "         8.8867e-01, -2.0435e-01,  4.9390e-01,  1.7371e-01, -1.0039e+00,\n",
      "        -1.7773e-01, -6.2828e-03, -5.1807e-01,  2.8984e+00,  2.5098e-01,\n",
      "        -5.3809e-01,  4.6753e-01,  4.9146e-01,  4.1113e-01,  2.9712e-01,\n",
      "        -5.5420e-01, -8.4277e-01,  2.1045e-01,  5.1727e-02, -5.1123e-01,\n",
      "         1.7979e+00,  1.3877e+00,  8.5388e-02,  9.1162e-01, -7.2815e-02,\n",
      "         1.4600e+00,  1.1377e+00, -3.0022e-03, -5.0879e-01, -5.7617e-01,\n",
      "         8.6121e-02,  5.0195e-01, -6.0596e-01,  7.0410e-01, -3.0640e-01,\n",
      "         2.8369e-01, -3.8770e-01, -6.1865e-01,  3.0703e+00,  2.0569e-01,\n",
      "        -8.6035e-01,  1.1041e-01, -8.3838e-01, -6.4600e-01,  1.9355e+00,\n",
      "         1.7568e+00,  7.9895e-02,  2.4648e+00,  1.3926e+00, -9.6008e-02,\n",
      "        -9.3750e-01,  3.5791e-01, -5.9668e-01, -1.4636e-01, -2.0294e-02,\n",
      "         9.8816e-02,  2.6758e-01,  4.6753e-01,  8.0127e-01,  8.7451e-01,\n",
      "         7.2607e-01, -4.3530e-01,  7.5244e-01,  9.2957e-02,  3.6426e-01,\n",
      "         3.6113e+00,  9.6387e-01,  4.6851e-01, -3.3276e-01,  1.7598e+00,\n",
      "         3.1855e+00,  9.2041e-01,  3.1470e-01, -4.3945e-01,  1.5557e+00,\n",
      "         8.0566e-01,  1.9946e-01,  9.0283e-01, -7.6807e-01,  3.2773e+00,\n",
      "         7.5732e-01,  2.9517e-01, -4.2700e-01, -3.4229e-01,  3.8418e+00,\n",
      "         1.0020e+00,  3.8391e-02, -1.1609e-01, -4.0747e-01,  3.4473e+00,\n",
      "         7.1338e-01, -1.5051e-01,  5.9961e-01,  2.2793e+00, -1.0901e-01,\n",
      "        -1.2537e-01, -9.3213e-01, -3.4521e-01,  3.1660e+00,  8.0957e-01,\n",
      "         4.3408e-01, -3.1738e-01,  9.3164e-01, -3.0835e-01,  3.2090e+00,\n",
      "         4.6313e-01, -1.9188e-03, -5.9521e-01, -2.1228e-01, -8.8330e-01,\n",
      "        -5.1660e-01,  3.2207e+00,  8.4570e-01,  2.2388e-01, -3.3203e-01,\n",
      "         1.0752e+00, -2.5562e-01,  3.7402e-01,  3.9673e-01,  1.7920e-01,\n",
      "        -3.2812e-01,  3.7402e+00,  1.2920e+00,  5.7080e-01,  2.1655e-01,\n",
      "         2.4902e-01, -8.7256e-01, -3.1421e-01,  4.5386e-01,  1.9617e-01,\n",
      "        -3.4595e-01, -5.2734e-01,  3.6660e+00,  1.0303e+00,  6.8420e-02,\n",
      "         2.7227e+00,  1.5059e+00,  5.0635e-01,  3.3984e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 9.3066e-01, -9.4824e-01, -6.6772e-02,  1.7812e+00, -9.6826e-01,\n",
      "        -1.6516e-01,  1.2051e+00, -2.0923e-01,  4.4482e-01, -7.1533e-01,\n",
      "        -1.1855e+00, -8.9941e-01,  7.6294e-02, -1.7688e-01,  2.7026e-01,\n",
      "         5.0977e-01, -7.6270e-01,  1.3965e+00,  9.0186e-01,  9.8340e-01,\n",
      "         2.4082e+00, -6.3037e-01,  1.0977e+00,  8.4326e-01,  9.8096e-01,\n",
      "         2.0215e+00, -6.5967e-01,  1.3721e+00,  5.2734e-01,  2.8857e-01,\n",
      "         6.0938e-01,  6.3721e-01,  1.6484e+00,  2.4102e+00, -3.5693e-01,\n",
      "         1.4331e-01,  9.7656e-01,  2.8379e+00, -7.7832e-01, -5.8936e-01,\n",
      "         1.3584e+00,  9.1064e-01,  1.5605e+00, -4.5215e-01,  3.0319e-02,\n",
      "         7.9346e-01,  2.1523e+00,  7.1191e-01,  9.9316e-01,  2.9419e-01,\n",
      "         7.7637e-01,  2.8594e+00,  2.4453e+00, -1.0020e+00, -7.3145e-01,\n",
      "        -7.2021e-02, -4.0015e-01, -9.0186e-01,  8.2397e-02,  2.1035e+00,\n",
      "         2.4531e+00, -5.3955e-01,  2.4434e+00,  7.2021e-01,  1.1621e-01,\n",
      "        -7.0703e-01, -5.7520e-01,  1.1553e+00,  6.9727e-01,  7.1387e-01,\n",
      "         2.1387e+00, -1.4148e-01, -1.2372e-01,  2.1545e-01,  5.9961e-01,\n",
      "         8.1885e-01,  2.9668e+00, -6.4111e-01,  1.3452e-01,  2.8101e-01,\n",
      "         4.5850e-01,  1.8369e+00,  1.1504e+00, -6.9678e-01,  8.8196e-03,\n",
      "         1.8408e+00,  2.6230e+00, -7.0264e-01, -4.0503e-01,  2.0227e-01,\n",
      "        -4.9286e-03, -1.3477e+00, -3.3716e-01, -2.4353e-01, -8.3008e-01,\n",
      "        -3.8843e-01, -5.4932e-01, -5.2197e-01,  3.4351e-01,  1.7510e+00,\n",
      "         4.0942e-01, -6.0498e-01,  2.3477e+00,  1.2930e+00, -3.4326e-01,\n",
      "        -1.2793e-01, -5.5878e-02, -2.5196e-03,  1.5540e-01,  4.8926e-01,\n",
      "         7.2021e-01,  2.7871e+00, -6.6553e-01, -1.9885e-01,  1.5225e+00,\n",
      "         1.9316e+00, -2.3425e-01,  8.3301e-01,  2.3242e+00,  9.1699e-01,\n",
      "         1.2207e+00,  3.3032e-01,  9.6826e-01,  3.4844e+00,  1.8887e+00,\n",
      "         6.5039e-01, -5.3662e-01, -8.6133e-01, -3.3618e-01,  9.8682e-01,\n",
      "         8.6230e-01, -9.7705e-01, -8.4180e-01,  2.4866e-01,  3.5474e-01,\n",
      "        -5.8154e-01,  7.1167e-02,  1.6333e-01,  5.9521e-01,  1.4746e+00,\n",
      "         8.2520e-01,  8.6182e-01,  8.3594e-01,  3.4746e+00, -6.6211e-01,\n",
      "         1.5588e-01, -5.9668e-01,  6.7920e-01, -1.3066e+00, -1.0273e+00,\n",
      "        -1.2842e-01,  7.6123e-01,  2.1816e+00,  6.8994e-01,  1.0322e+00,\n",
      "         1.6272e-01,  9.5312e-01,  3.2832e+00,  2.5195e+00, -9.3848e-01,\n",
      "        -7.8906e-01, -2.2913e-01, -7.8174e-01,  6.9238e-01, -6.8311e-01,\n",
      "        -9.6631e-01, -4.2700e-01, -1.3745e-01,  1.0553e-01, -7.5989e-02,\n",
      "        -1.8167e-03,  2.5430e+00,  2.6543e+00, -1.6045e+00, -5.9540e-02,\n",
      "         5.3857e-01,  6.3086e-01,  2.6758e+00, -1.1299e+00, -2.3914e-01,\n",
      "         4.1943e-01,  2.5039e+00, -9.8877e-01,  1.4197e-01,  8.8623e-01,\n",
      "         2.8320e+00, -1.4246e-01, -6.8359e-01,  6.5381e-01, -1.4512e+00,\n",
      "        -4.0430e-01, -9.0479e-01, -3.0322e-01, -1.2236e+00,  8.9648e-01,\n",
      "        -6.8994e-01, -1.0293e+00, -6.0205e-01,  4.6051e-02,  1.6455e+00,\n",
      "        -8.4521e-01, -7.0361e-01,  2.4646e-01, -7.2900e-01, -4.1943e-01,\n",
      "        -2.2751e-02, -3.5645e-01, -9.8975e-01, -1.0635e+00, -2.4255e-01,\n",
      "         1.3096e+00, -2.1252e-01, -6.7383e-01, -9.1162e-01, -3.9062e-01,\n",
      "        -7.6807e-01, -7.3535e-01, -2.5854e-01, -1.3730e+00, -7.6953e-01,\n",
      "        -7.0215e-01, -1.0186e+00, -3.3423e-01,  2.0885e-03, -4.0070e-02,\n",
      "        -4.9316e-01, -3.6102e-02, -1.4590e+00, -4.6680e-01, -1.1884e-01,\n",
      "         2.9834e-01, -5.7324e-01,  5.4346e-01, -9.1992e-01,  1.3213e+00,\n",
      "         6.9629e-01,  1.0811e+00,  2.5625e+00, -7.9639e-01,  1.0439e+00,\n",
      "         8.6230e-01,  1.1328e+00,  1.5068e+00, -5.9723e-02,  1.0557e+00,\n",
      "         3.2930e+00, -1.0508e+00, -7.7783e-01,  1.2793e+00,  9.1699e-01,\n",
      "         1.6289e+00, -3.8232e-01, -2.9068e-02,  1.1641e+00,  3.5488e+00,\n",
      "         2.5957e+00, -6.2842e-01,  1.7832e+00,  1.0645e+00, -4.2938e-02,\n",
      "        -1.1914e+00, -6.9824e-01,  9.7119e-01,  6.2451e-01,  4.3408e-01,\n",
      "         1.8379e+00, -1.0869e+00,  9.6375e-02,  7.2021e-01,  2.3320e+00,\n",
      "        -4.2725e-01,  1.2314e+00,  3.0527e+00, -7.7881e-01, -5.2783e-01,\n",
      "        -1.4786e-02, -1.2119e+00, -6.0107e-01,  3.7817e-01,  1.8477e+00,\n",
      "         4.9365e-01, -7.5391e-01,  2.4316e+00,  1.1523e+00, -9.4092e-01,\n",
      "        -2.1155e-01,  7.0947e-01,  3.0820e+00,  6.2793e-01, -2.1277e-01,\n",
      "        -6.0693e-01, -7.4219e-01,  6.9824e-02, -3.9551e-01,  2.3672e+00,\n",
      "        -8.2324e-01, -1.5076e-01,  2.7441e+00, -5.9912e-01,  2.1240e-01,\n",
      "         1.3464e-01,  3.2383e+00, -5.4199e-01, -5.3271e-01, -6.4258e-01,\n",
      "         2.6749e-02,  1.2469e-01, -2.5903e-01,  9.2041e-01,  3.3125e+00,\n",
      "        -9.0527e-01, -6.0352e-01,  6.7969e-01, -6.6650e-01,  3.0957e-01,\n",
      "         8.9697e-01, -9.2529e-01, -4.3140e-01, -1.1787e+00, -7.2949e-01,\n",
      "         1.1152e+00,  7.6953e-01,  1.2744e+00, -3.9355e-01, -4.8535e-01,\n",
      "        -3.0518e-01,  2.8223e+00, -2.9980e-01,  2.6602e+00, -7.8516e-01,\n",
      "        -1.7249e-01, -7.3975e-01, -5.1904e-01,  2.0684e+00, -4.1656e-03,\n",
      "        -4.2261e-01,  2.7793e+00, -7.6758e-01,  5.2979e-01,  3.1855e+00,\n",
      "        -2.7441e-01,  3.3032e-01,  1.2373e+00,  2.2578e+00, -6.6406e-01,\n",
      "         5.0781e-01,  2.9707e+00, -1.6125e-01, -2.3401e-01,  8.7207e-01,\n",
      "         3.5098e+00,  5.4004e-01,  5.2490e-01, -1.0312e+00,  1.3174e+00,\n",
      "         5.4346e-01,  1.1484e+00,  2.1816e+00, -7.1582e-01,  1.1172e+00,\n",
      "         7.4414e-01,  1.2061e+00,  1.5410e+00,  6.4636e-02,  7.7588e-01,\n",
      "         2.8379e+00, -1.0479e+00, -7.0508e-01,  1.3359e+00,  8.0029e-01,\n",
      "         1.4219e+00, -1.0283e+00, -5.3436e-02, -1.0928e+00, -7.1094e-01,\n",
      "         5.9937e-02,  5.3040e-02, -1.0638e-01,  3.1226e-01,  1.0928e+00,\n",
      "         3.2715e+00, -2.3083e-01, -2.8418e-01,  7.0361e-01,  3.2480e+00,\n",
      "         1.8047e+00, -4.4873e-01, -2.2961e-01,  9.0771e-01,  3.1641e+00,\n",
      "         1.9688e+00, -4.0552e-01,  9.4922e-01, -7.6807e-01, -1.5356e-01,\n",
      "        -4.6777e-01, -7.6172e-01,  7.7393e-01,  7.6416e-01,  8.4424e-01,\n",
      "         2.1777e+00, -3.9624e-01,  2.0935e-01,  1.3416e-01, -5.0201e-02,\n",
      "         2.3906e+00, -1.4465e-01, -3.4961e-01,  1.3049e-01,  1.3604e+00,\n",
      "         3.1426e+00,  1.7832e+00, -4.4629e-01, -1.0333e-01,  9.7461e-01,\n",
      "         3.1270e+00,  1.8076e+00, -5.9668e-01,  1.6416e+00,  7.5293e-01,\n",
      "        -9.4873e-01, -5.0684e-01,  4.7754e-01,  6.3086e-01,  1.1328e-01,\n",
      "        -9.2676e-01, -5.9082e-01,  4.4458e-01, -5.9814e-01,  4.0259e-01,\n",
      "         8.6719e-01, -8.3057e-01, -3.4302e-01, -1.1289e+00, -8.5254e-01,\n",
      "         9.1504e-01,  7.2217e-01,  1.3281e+00, -3.5815e-01,  5.0586e-01,\n",
      "        -6.4697e-01,  2.3987e-01,  2.6807e-01,  1.0938e+00,  3.2480e+00,\n",
      "        -5.0140e-02,  1.4355e+00, -7.7881e-01,  1.4502e-01,  2.1814e-01,\n",
      "         1.0156e+00,  2.5781e+00,  2.6831e-01,  1.8125e+00, -7.3682e-01,\n",
      "         1.3945e+00, -5.3418e-01,  1.5088e-01,  2.0471e-01,  9.4922e-01,\n",
      "         1.5137e+00, -1.0236e-01,  1.3062e-01,  3.1602e+00, -8.9453e-01,\n",
      "        -5.4108e-02, -3.8745e-01,  1.9275e-01, -1.1582e+00, -8.7891e-01,\n",
      "        -2.2668e-01, -4.0869e-01,  2.6001e-01, -6.5918e-02, -3.6963e-01,\n",
      "         1.2140e-01,  2.5488e-01,  1.2822e+00,  3.0293e+00,  1.1035e-01,\n",
      "         9.4141e-01,  2.9727e+00, -4.4897e-01, -2.7710e-01,  3.2544e-01,\n",
      "         1.1826e+00,  1.3730e+00,  3.0977e+00, -2.9907e-01,  3.5547e-01,\n",
      "         2.9077e-01,  2.2754e-01,  3.4746e+00,  1.9556e-01,  1.0651e-01,\n",
      "         8.3203e-01,  3.4590e+00,  2.1309e+00, -1.4832e-01,  3.0820e+00,\n",
      "        -6.8311e-01,  1.3745e-01,  2.6636e-01,  1.3037e+00,  2.8828e+00,\n",
      "         2.3906e+00, -7.5635e-01, -8.7695e-01, -8.1738e-01, -5.6299e-01,\n",
      "        -8.2129e-01,  7.1436e-01,  1.9795e+00,  6.7480e-01,  2.0508e-02,\n",
      "         8.3057e-01, -9.2920e-01,  4.3518e-02,  1.1709e+00,  3.3164e+00,\n",
      "         1.4160e+00, -8.6279e-01, -1.0098e+00, -5.7568e-01, -5.6689e-01,\n",
      "        -7.2998e-01,  1.6484e+00,  9.5605e-01,  1.4678e+00,  3.3770e+00,\n",
      "         1.3293e-01,  5.4102e-01, -2.4890e-01,  3.5938e+00, -3.7817e-01,\n",
      "         3.3047e+00, -8.3398e-01, -3.3112e-02, -7.0068e-01, -4.0894e-01,\n",
      "         2.0742e+00, -3.0228e-02, -3.7939e-01,  5.2832e-01,  3.3125e+00,\n",
      "        -1.9336e-01, -1.2985e-02,  3.1738e+00, -8.4229e-01, -3.0884e-01,\n",
      "        -1.4880e-01, -1.6089e-01,  2.4043e+00,  1.2393e+00, -3.8452e-01,\n",
      "         3.0020e+00, -9.6484e-01,  1.6833e-01, -1.0269e-02, -8.8281e-01,\n",
      "        -5.5566e-01,  2.4980e+00, -3.2104e-01, -3.9307e-01, -3.8965e-01,\n",
      "         5.2637e-01, -9.5801e-01,  1.5244e+00,  7.2754e-01,  1.3926e+00,\n",
      "         3.2080e-01,  3.4258e+00, -8.2666e-01,  1.2334e+00,  8.4375e-01,\n",
      "         1.3350e+00,  2.0544e-01,  2.7500e+00, -6.2793e-01,  1.5146e+00,\n",
      "         6.4990e-01,  3.4888e-01,  9.6240e-01,  4.4238e-01,  2.3164e+00,\n",
      "         3.0908e-01,  2.9004e+00, -9.5117e-01, -2.5464e-01,  7.7783e-01,\n",
      "         3.2422e-01,  1.1792e-01,  1.1455e+00, -6.9189e-01,  2.2522e-01,\n",
      "         3.3960e-01,  6.0333e-02,  3.3457e+00,  5.4810e-02, -7.4463e-02,\n",
      "         2.4011e-01,  7.3047e-01,  9.7266e-01,  1.0586e+00,  8.3447e-01,\n",
      "         2.8965e+00,  2.1543e+00, -1.0800e-04,  3.4648e+00, -6.0547e-01,\n",
      "        -2.9004e-01, -1.3721e-01, -9.4043e-01, -7.1826e-01,  1.5784e-01,\n",
      "         1.3838e+00,  1.5259e-01, -4.2065e-01,  2.7012e+00, -1.1055e+00,\n",
      "        -4.7021e-01,  2.3223e+00, -1.1602e+00,  9.2926e-03, -9.9268e-01,\n",
      "        -9.1650e-01, -7.8809e-01, -4.6045e-01, -7.3877e-01,  1.3232e-01,\n",
      "         5.7812e-01, -1.0858e-01, -7.7881e-02,  3.2871e+00, -2.5732e-01,\n",
      "         4.4873e-01, -4.6021e-01, -1.3301e+00, -3.2227e-01,  2.6934e+00,\n",
      "        -1.1309e+00, -1.4648e-01,  5.3223e-01,  3.3574e+00,  1.3936e+00,\n",
      "        -2.2424e-01, -7.9736e-01,  3.8135e-01,  3.6865e-01,  7.2461e-01,\n",
      "         2.7988e+00, -7.4023e-01, -3.8647e-01,  2.3157e-01,  3.1426e+00,\n",
      "        -1.6187e-01,  3.9062e-01,  4.6216e-01,  1.1072e-01,  5.0995e-02,\n",
      "         1.3350e+00,  1.4697e+00,  3.4277e+00, -8.3057e-01,  1.0898e+00,\n",
      "         7.1436e-01,  4.2847e-01, -9.0576e-01, -4.8682e-01,  1.5264e+00,\n",
      "         9.6094e-01,  1.1748e+00,  3.1484e+00,  1.1414e-02,  4.5239e-01,\n",
      "        -1.0557e+00,  1.4551e+00,  7.3975e-01,  1.0469e+00,  2.0645e+00,\n",
      "        -6.6602e-01,  1.1592e+00,  9.3945e-01,  1.0449e+00,  8.9258e-01,\n",
      "         1.1292e-01,  6.2793e-01,  2.5273e+00, -1.1084e+00, -5.7715e-01,\n",
      "         1.5723e+00,  8.4717e-01,  1.1758e+00, -1.0488e+00, -3.7671e-01,\n",
      "        -1.0791e+00,  1.6541e-01, -8.1787e-02, -3.2788e-01,  3.4595e-01,\n",
      "         4.7021e-01,  7.0459e-01,  2.5156e+00, -6.6504e-01, -4.8242e-01,\n",
      "         3.6377e-02,  2.0742e+00, -4.6118e-01, -1.1884e-01,  1.1260e+00,\n",
      "        -2.0935e-01,  5.1318e-01,  1.4001e-01,  3.1113e+00,  1.8936e+00,\n",
      "        -1.5637e-01,  1.1829e-01,  8.1006e-01,  3.4277e+00,  2.1758e+00,\n",
      "        -5.9605e-04,  3.4336e+00, -2.2131e-01,  5.1172e-01,  1.3260e-02,\n",
      "         2.9565e-01, -1.1797e+00, -1.1493e-01, -5.0146e-01, -1.4453e-01,\n",
      "        -4.6973e-01,  3.2207e+00, -7.5879e-01, -1.2334e+00, -1.0459e+00,\n",
      "        -9.2480e-01, -8.6133e-01,  8.5754e-02, -7.2559e-01, -3.2178e-01,\n",
      "        -6.0791e-01,  2.3848e+00, -6.0303e-01,  5.7080e-01, -5.5566e-01,\n",
      "         5.9509e-02, -6.4648e-01,  7.9346e-01, -4.9805e-01, -1.1641e+00,\n",
      "        -5.2490e-01,  1.1611e+00,  7.1680e-01,  8.2129e-01,  1.8799e+00,\n",
      "        -1.1562e+00, -5.0146e-01, -6.8066e-01, -6.3538e-02, -8.3691e-01,\n",
      "        -8.0383e-02,  7.0117e-01,  1.0186e+00,  3.5371e+00,  2.1641e+00,\n",
      "        -1.8143e-02,  4.6558e-01, -4.8486e-01,  3.4131e-01,  3.5742e+00,\n",
      "        -9.4189e-01,  1.5100e-01,  3.5215e+00, -7.2803e-01,  1.8779e+00,\n",
      "         1.9250e-01,  1.1511e-01,  1.8271e+00,  6.7139e-01,  3.2266e+00,\n",
      "        -1.0645e+00, -7.5928e-01,  1.0078e+00, -2.3328e-01, -1.1221e+00,\n",
      "        -4.5166e-01, -5.1807e-01, -2.1179e-01, -4.4281e-02,  3.1035e+00,\n",
      "        -1.2246e+00, -5.2490e-01, -5.9961e-01, -1.1884e-01, -1.1377e+00,\n",
      "        -9.5947e-01, -1.2598e+00, -9.2334e-01, -3.9087e-01,  2.1875e+00,\n",
      "        -1.0586e+00, -1.0801e+00, -7.1484e-01, -4.3188e-01, -7.6843e-02,\n",
      "        -4.6924e-01, -9.6729e-01, -4.4092e-01, -6.1279e-01, -7.2900e-01,\n",
      "        -7.7832e-01, -2.5684e-01,  3.0410e+00, -7.2559e-01, -2.7637e-01,\n",
      "        -8.7842e-01, -2.6880e-01,  2.5078e+00, -2.2961e-01, -9.4238e-01,\n",
      "        -1.0498e+00, -8.4033e-01, -4.7455e-03, -3.4741e-01,  1.1206e-01,\n",
      "        -7.8003e-02, -5.5615e-01, -8.5010e-01, -8.0078e-02,  2.7090e+00,\n",
      "        -1.2383e+00, -7.5049e-01, -1.3350e+00, -1.2959e+00, -1.4966e-01,\n",
      "         8.3679e-02,  3.4355e+00, -6.8799e-01,  3.7823e-03,  3.2891e+00,\n",
      "        -4.8248e-02, -7.0361e-01, -7.1777e-01, -3.5132e-01, -4.6484e-01,\n",
      "        -2.2498e-01, -9.9268e-01, -8.7500e-01,  6.9427e-03, -5.5713e-01,\n",
      "        -2.3938e-01, -1.0674e+00,  3.8916e-01, -2.0154e-01,  4.9341e-01,\n",
      "        -8.9062e-01,  1.1484e+00,  6.7969e-01,  1.0742e+00,  2.1230e+00,\n",
      "        -4.7461e-01,  1.0107e+00,  7.9199e-01,  1.1963e+00,  1.4131e+00,\n",
      "         3.6816e-01,  7.0557e-01,  2.3730e+00, -1.0020e+00, -5.8105e-01,\n",
      "         1.1582e+00,  7.2363e-01,  1.4004e+00, -4.4458e-01,  7.0496e-03,\n",
      "         5.9668e-01,  3.3145e+00,  2.3672e+00, -1.6870e-01,  1.1719e-01,\n",
      "         5.5566e-01,  3.1172e+00,  1.6953e+00, -2.7808e-01,  1.3418e+00,\n",
      "         1.1338e+00,  1.1090e-01, -1.0195e+00, -7.2363e-01,  8.1104e-01,\n",
      "         4.9316e-01,  5.7861e-01,  1.5488e+00, -1.3398e+00,  3.2166e-02,\n",
      "         5.8838e-01,  2.8477e+00, -7.1631e-01, -7.3145e-01, -5.7404e-02,\n",
      "        -9.3701e-01, -6.8115e-01,  8.4961e-01,  6.5771e-01,  4.2920e-01,\n",
      "         1.5830e+00, -9.3115e-01, -5.5615e-01, -3.1494e-01,  2.2888e-01,\n",
      "        -9.6924e-01,  1.1765e-02,  1.8518e-01,  9.7021e-01,  3.1523e+00,\n",
      "         1.0479e+00, -5.7520e-01, -9.1895e-01,  1.9458e-01, -3.0258e-02,\n",
      "        -6.1523e-01, -8.7988e-01, -1.1456e-01,  4.1406e-01,  3.4004e+00,\n",
      "         9.2834e-02,  6.6504e-01,  3.2188e+00,  4.4165e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8070,  0.1032, -0.3810,  ...,  1.3593,  1.3384, -0.2465],\n",
      "         [ 0.3303,  0.2533,  0.0561,  ...,  0.6655,  0.7335,  0.0111],\n",
      "         [ 0.4289, -0.1699,  0.0670,  ...,  0.8842,  1.1553, -0.1285],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 8.2959e-01,  1.1582e+00,  7.0166e-01,  5.2539e-01, -1.5295e-01,\n",
      "        -1.0004e-01, -4.9731e-01,  5.9619e-01, -3.5515e-03, -6.0352e-01,\n",
      "        -6.9824e-01, -1.7419e-01,  1.7383e-01, -9.5459e-01,  9.1748e-01,\n",
      "        -6.8506e-01,  1.5254e+00,  5.1074e-01, -2.3083e-01, -5.2612e-02,\n",
      "        -6.5820e-01,  1.1334e-01, -5.8252e-01, -3.7476e-01,  4.1260e-01,\n",
      "        -3.5498e-01,  2.9863e+00,  1.9609e+00,  5.9521e-01,  6.0059e-01,\n",
      "         4.8065e-02,  5.5176e-01,  3.5669e-01,  1.9707e+00,  1.1406e+00,\n",
      "        -7.8674e-02,  1.8115e-01,  1.9756e+00,  8.5400e-01, -1.6455e-01,\n",
      "         2.4780e-01, -1.2539e+00,  4.6021e-01, -7.5781e-01,  4.6069e-01,\n",
      "        -7.3096e-01, -2.0187e-02,  1.8604e-01,  1.7273e-01, -5.9668e-01,\n",
      "         3.0884e-01,  9.3445e-02, -3.8062e-01,  8.5266e-02,  1.1871e-02,\n",
      "         1.0284e-02, -6.3330e-01, -4.6631e-01,  1.7017e-01, -8.9307e-01,\n",
      "         1.1748e+00,  1.8438e+00,  7.3877e-01,  3.0640e-02, -6.1816e-01,\n",
      "         4.6191e-01, -6.3525e-01,  7.8308e-02,  8.6182e-01, -6.0791e-01,\n",
      "         1.0420e+00,  3.2593e-02, -1.0312e+00,  7.6855e-01, -4.4092e-01,\n",
      "         1.4526e-01, -1.1875e+00,  1.3730e+00, -5.2930e-01,  9.0674e-01,\n",
      "        -2.1973e-01,  1.0443e-01, -5.5225e-01,  7.0605e-01, -5.5029e-01,\n",
      "         9.8267e-02,  2.6904e-01,  2.2546e-01, -6.8896e-01,  1.7695e+00,\n",
      "         7.2217e-01, -1.3098e-01,  6.1670e-01, -1.9055e-01,  3.7383e+00,\n",
      "         1.1807e+00,  1.7598e+00,  7.9102e-01, -1.8774e-01, -5.1221e-01,\n",
      "         6.1816e-01,  2.3218e-01, -4.0723e-01,  3.5840e-01,  1.5710e-01,\n",
      "         2.5146e-01, -7.7539e-01,  2.7856e-01,  1.7896e-01,  2.5360e-02,\n",
      "        -9.5264e-01,  1.9570e+00,  7.1045e-01, -2.4011e-01,  2.3047e-01,\n",
      "        -4.1113e-01,  9.6680e-01, -5.2637e-01,  3.8916e-01, -1.6281e-02,\n",
      "         8.3398e-01,  3.6560e-02,  3.9307e-01, -3.7061e-01,  3.8574e-01,\n",
      "         7.1729e-01, -2.7863e-02,  4.9536e-01,  2.7856e-01, -5.4004e-01,\n",
      "         1.3115e+00,  2.5293e-01,  2.0532e-01, -3.4009e-01,  2.2422e+00,\n",
      "         1.0479e+00, -3.2837e-02,  1.9556e-01, -9.8730e-01,  2.4658e-01,\n",
      "        -1.3594e+00,  5.5371e-01,  8.4668e-01, -2.4255e-01, -1.7554e-01,\n",
      "        -6.4893e-01, -4.1821e-01, -8.0273e-01,  2.8516e-01,  2.1289e-01,\n",
      "         5.8789e-01,  3.7988e-01,  1.2295e+00, -8.2227e-01,  2.2188e+00,\n",
      "         8.1494e-01,  3.4062e+00,  4.9390e-01, -4.3896e-01,  3.5010e-01,\n",
      "        -5.8984e-01,  1.4417e-01,  1.2299e-02, -1.1621e+00, -1.5540e-01,\n",
      "        -3.2275e-01,  3.1934e-01,  1.1074e+00, -5.5225e-01,  1.8584e+00,\n",
      "         6.1328e-01,  3.4102e+00,  3.2959e-01,  8.7744e-01, -6.8066e-01,\n",
      "        -5.8496e-01,  2.3516e+00, -5.8154e-01,  1.1074e+00, -5.7715e-01,\n",
      "         2.1753e-01,  7.9163e-02, -1.8091e-01,  2.1912e-01, -1.7188e-01,\n",
      "        -6.8848e-01,  4.0898e+00,  1.3604e+00,  1.2822e+00,  2.3809e+00,\n",
      "         6.6016e-01,  3.2153e-01,  2.5757e-01, -5.7959e-01, -4.3384e-01,\n",
      "         1.1602e+00, -2.4780e-01,  1.2773e+00, -5.4883e-01,  5.7080e-01,\n",
      "        -6.9458e-02, -5.3076e-01,  5.0781e-01, -7.4609e-01, -4.1309e-01,\n",
      "         3.7280e-01, -4.1870e-01,  2.1152e+00,  7.0654e-01,  1.2705e+00,\n",
      "         4.3188e-01, -3.1274e-01,  6.8555e-01,  3.7842e-01,  3.6660e+00,\n",
      "         1.9980e+00,  1.5146e+00,  8.5449e-01,  2.5659e-01,  1.4502e+00,\n",
      "         6.9287e-01,  1.6738e+00,  1.3196e-01,  3.5625e+00,  1.7695e+00,\n",
      "         1.5566e+00,  7.4805e-01,  1.5527e-01,  1.3057e+00,  5.6152e-01,\n",
      "         1.7490e+00,  1.9531e-01, -7.8662e-01,  1.0234e+00,  1.8154e+00,\n",
      "         6.8311e-01,  4.4263e-01, -1.3110e-01,  3.8496e+00,  6.1816e-01,\n",
      "         1.1025e+00,  3.8555e+00,  2.1152e+00,  6.3184e-01,  8.8428e-01,\n",
      "        -9.3750e-01, -5.0537e-01, -1.5820e-01,  5.7471e-01, -9.5312e-01,\n",
      "         8.4375e-01, -6.3477e-01,  1.4219e+00,  4.9634e-01, -4.4556e-01,\n",
      "        -4.9706e-03, -7.4512e-01,  1.6833e-01, -5.2441e-01, -6.3379e-01,\n",
      "         1.0156e+00, -6.4111e-01, -5.2490e-01,  1.2715e+00,  5.9766e-01,\n",
      "         1.1055e+00,  2.1387e+00,  7.3779e-01,  2.3438e+00,  2.3008e+00,\n",
      "         1.2715e+00, -2.5711e-02,  1.0859e+00, -5.9180e-01, -3.6255e-01,\n",
      "        -3.5938e-01,  2.2681e-01,  7.2852e-01,  1.9714e-02, -2.2620e-01,\n",
      "        -5.0049e-01,  8.7256e-01, -3.8062e-01,  3.7231e-01,  7.1240e-01,\n",
      "         4.8169e-01, -6.8848e-01,  3.4668e-01, -3.4766e-01,  2.8076e-01,\n",
      "         9.9707e-01, -5.0732e-01,  7.4756e-01, -3.1830e-02, -3.3911e-01,\n",
      "         3.0518e-01, -7.7979e-01,  9.5557e-01,  5.5566e-01, -4.8950e-01,\n",
      "         7.5488e-01,  2.7588e-02, -1.8018e-01, -8.9746e-01,  2.5024e-02,\n",
      "        -1.9739e-01,  6.3330e-01,  1.2256e+00,  3.3145e+00,  1.1680e+00,\n",
      "         2.9668e+00,  1.2275e+00,  3.5938e-01,  3.0200e-01,  1.6973e+00,\n",
      "         1.2070e+00, -3.5767e-01,  7.9736e-01,  2.6917e-02, -6.8457e-01,\n",
      "        -5.0098e-01,  2.7695e+00,  3.0615e-01,  3.5332e+00,  1.3408e+00,\n",
      "         9.1797e-01,  1.5791e+00,  3.9307e-01,  1.9756e+00,  6.5674e-01,\n",
      "         3.0664e-01, -1.6223e-01,  9.8242e-01,  1.5308e-01,  6.3330e-01,\n",
      "         4.3530e-01,  3.8926e+00,  1.6230e+00,  3.2383e+00,  8.2227e-01,\n",
      "         1.9863e+00,  6.5527e-01, -1.6467e-01, -1.3351e-02,  3.9648e-01,\n",
      "         3.2598e+00,  2.5820e+00,  1.7490e+00,  3.1602e+00,  7.2363e-01,\n",
      "         3.1689e-01,  2.1973e+00,  3.2305e+00,  7.1826e-01,  1.3662e+00,\n",
      "         4.2773e-01,  8.9209e-01, -1.3721e+00,  2.0410e-01, -2.6294e-01,\n",
      "        -4.1260e-01, -9.6973e-01,  1.7041e-01, -5.7910e-01, -3.3276e-01,\n",
      "         2.2930e+00,  3.9961e+00,  1.4375e+00,  9.1406e-01,  1.0938e+00,\n",
      "         4.0938e+00,  9.0967e-01,  1.5547e+00,  2.7148e+00,  6.3721e-01,\n",
      "         5.8887e-01,  2.8174e-01, -1.3843e-01,  1.2852e+00, -5.9668e-01,\n",
      "         1.3730e+00, -2.9175e-01, -2.4841e-02,  5.8252e-01,  4.3091e-01,\n",
      "         2.7109e+00,  3.0298e-01,  1.8213e+00,  2.2617e+00,  2.4927e-01,\n",
      "         1.7090e+00, -8.4473e-01,  3.8745e-01, -3.8696e-01,  1.9023e+00,\n",
      "         5.8496e-01,  3.5391e+00,  5.1367e-01, -2.9077e-01, -1.3025e-01,\n",
      "         3.8691e+00,  2.0234e+00,  5.5566e-01, -7.1826e-01,  3.2617e-01,\n",
      "        -1.6687e-01, -1.9922e-01,  9.0527e-01, -4.8975e-01,  3.5742e+00,\n",
      "         1.7861e+00,  1.5332e+00,  7.4414e-01,  2.8491e-01,  1.1592e+00,\n",
      "         6.2256e-01,  1.6738e+00,  5.9540e-02,  5.8984e-01,  4.5386e-01,\n",
      "         3.8145e+00,  1.8643e+00,  3.6865e-01,  3.7852e+00,  1.7920e+00,\n",
      "         4.2529e-01,  1.3750e+00,  4.1445e+00,  1.1934e+00,  1.4102e+00,\n",
      "         6.7383e-01,  1.1084e+00, -4.0283e-01,  3.7617e+00,  5.3174e-01,\n",
      "         1.1904e+00, -6.3672e-01,  7.9248e-01,  9.6313e-02, -7.8906e-01,\n",
      "        -1.7761e-01,  1.2734e+00,  3.6543e+00,  1.7695e+00,  1.4170e+00,\n",
      "         6.3037e-01,  1.5686e-01,  1.0771e+00,  4.8608e-01,  1.5859e+00,\n",
      "         3.9520e-02,  5.0928e-01,  4.4507e-01,  4.0938e+00,  1.0703e+00,\n",
      "         7.8809e-01,  1.3633e+00,  3.5327e-01,  7.0264e-01,  1.9443e+00,\n",
      "         8.1445e-01,  2.4512e+00,  2.5195e-01,  8.2422e-01, -5.6982e-01,\n",
      "        -1.6870e-01, -3.5553e-02,  1.8848e+00, -1.5833e-01,  1.4717e+00,\n",
      "         4.0088e-01,  6.0156e-01,  2.9468e-01,  6.9385e-01,  2.8320e+00,\n",
      "         6.9189e-01,  2.0840e+00, -3.9482e-03, -1.8372e-01,  3.0273e+00,\n",
      "         8.9697e-01,  4.6362e-01,  7.6758e-01,  3.8730e+00,  2.7344e-01,\n",
      "         2.1055e+00,  1.3818e-01,  4.4180e+00,  1.8809e+00,  1.4824e+00,\n",
      "         7.9053e-01,  1.1436e+00,  2.7383e+00,  1.3711e+00,  1.2520e+00,\n",
      "         3.4155e-01,  1.0459e+00,  1.6660e+00, -3.9526e-01,  1.7393e+00,\n",
      "         1.4258e-01,  1.6807e+00, -4.9829e-01,  3.8232e-01,  4.0039e+00,\n",
      "         1.4189e+00,  1.4072e+00,  3.0273e+00,  1.0430e+00,  1.3318e-01,\n",
      "         1.5957e+00,  7.8125e-01,  1.0898e+00, -3.0981e-01,  5.4590e-01,\n",
      "        -6.6895e-01, -2.5220e-01, -7.2998e-01, -2.0166e-01, -7.4854e-01,\n",
      "        -3.8525e-01, -1.3867e-01,  4.3652e-01, -4.8193e-01, -5.2612e-02,\n",
      "        -6.0742e-01,  1.0117e+00, -9.6484e-01, -3.3203e-01, -1.6870e-01,\n",
      "         6.8909e-02, -4.7729e-01, -3.6865e-01, -6.3281e-01, -3.9526e-01,\n",
      "        -5.1025e-01, -2.8540e-01,  4.6411e-01,  1.9902e+00,  7.5684e-01,\n",
      "         6.8066e-01, -7.1680e-01,  1.5601e-01, -2.4084e-01, -7.3047e-01,\n",
      "         1.6211e-01,  1.3879e-01,  1.8982e-01,  8.9795e-01, -2.7759e-01,\n",
      "         7.7930e-01,  3.7915e-01,  1.2311e-01, -2.8613e-01,  2.3218e-01,\n",
      "         2.2031e+00,  1.4111e-01,  5.8057e-01, -2.9297e-01, -5.1367e-01,\n",
      "         5.9131e-01, -9.4666e-02, -6.1133e-01,  4.3373e-03, -5.5756e-02,\n",
      "        -6.3086e-01,  5.3906e-01,  1.7432e+00,  3.6934e+00,  7.3340e-01,\n",
      "         1.5225e+00,  3.5645e+00,  6.8994e-01, -4.8901e-01,  2.3008e+00,\n",
      "         1.7656e+00,  1.4072e+00,  7.5098e-01, -7.2217e-01,  1.6211e-01,\n",
      "        -2.1643e-01,  3.7227e+00,  2.0234e+00,  1.6836e+00,  8.6963e-01,\n",
      "         4.4604e-01,  1.2695e+00,  6.4160e-01,  1.7725e+00,  2.5000e-01,\n",
      "        -2.4182e-01,  1.1908e-01,  3.7656e+00,  2.1113e+00,  6.2402e-01,\n",
      "         1.0811e+00, -6.8457e-01,  1.8701e-01,  9.8511e-02, -2.4463e-01,\n",
      "         6.4062e-01, -7.3389e-01,  5.7080e-01,  4.4492e+00,  1.4688e+00,\n",
      "         1.4424e+00, -1.2909e-02,  1.4834e+00,  2.8613e-01,  7.1436e-01,\n",
      "        -3.7659e-02,  3.4707e+00,  1.9766e+00,  1.5928e+00,  8.2568e-01,\n",
      "         3.8110e-01,  5.3467e-01,  2.8594e+00,  1.7080e+00, -3.1152e-01,\n",
      "         1.6748e+00,  4.1289e+00,  1.4727e+00,  1.9678e+00,  2.2715e+00,\n",
      "         9.0137e-01,  4.4116e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.2217e+00, -1.8661e-02, -1.8478e-02,  1.6748e-01,  5.8990e-02,\n",
      "        -2.2913e-01, -8.9502e-01, -5.4004e-01, -5.9424e-01, -1.1631e+00,\n",
      "        -6.6309e-01, -9.9756e-01, -4.8828e-01, -7.8809e-01, -3.1348e-01,\n",
      "        -1.0742e+00, -1.2812e+00,  3.6133e-01,  6.1328e-01,  8.6182e-02,\n",
      "        -7.0703e-01,  3.7915e-01, -1.3049e-01, -9.1260e-01,  3.0420e-01,\n",
      "        -6.3818e-01, -7.3303e-02, -1.0394e-01,  1.3115e+00, -6.9873e-01,\n",
      "         4.7607e-02, -7.4414e-01,  5.3955e-01, -1.1250e+00,  4.2651e-01,\n",
      "         1.0391e+00,  1.7480e+00, -1.0957e+00,  5.0439e-01,  8.7842e-01,\n",
      "         1.5000e+00, -1.0918e+00,  8.1177e-02, -2.8687e-01,  2.4414e-01,\n",
      "        -5.8008e-01, -3.3350e-01, -2.9028e-01,  3.2104e-01, -9.1992e-01,\n",
      "        -5.4639e-01,  9.1248e-02, -7.5000e-01, -2.5589e-02, -1.4478e-01,\n",
      "        -7.9248e-01, -3.3740e-01, -2.0837e-01, -9.9756e-01, -5.4443e-01,\n",
      "        -1.8091e-01, -1.2686e+00,  4.7095e-01,  1.2803e+00, -1.2881e+00,\n",
      "        -1.1064e+00, -6.1133e-01, -7.2607e-01, -2.2986e-01, -9.1943e-01,\n",
      "         1.5906e-01, -1.1016e+00, -5.9717e-01, -4.8633e-01, -1.1768e+00,\n",
      "        -1.6455e-01, -4.5947e-01,  3.2764e-01, -7.5586e-01, -2.7832e-01,\n",
      "        -3.1689e-01, -6.0107e-01, -6.9824e-01, -1.0996e+00, -8.3496e-01,\n",
      "         2.9602e-02, -5.9082e-01, -5.6201e-01, -6.3281e-01, -1.4160e+00,\n",
      "         4.3530e-01,  1.2168e+00,  2.2412e-01, -6.7627e-01,  3.2739e-01,\n",
      "         1.7695e+00,  2.2656e+00,  2.8086e+00, -6.9189e-01, -4.1821e-01,\n",
      "        -8.8525e-01,  5.6213e-02, -8.2861e-01, -8.3594e-01,  4.0588e-02,\n",
      "        -5.8887e-01, -8.7463e-02, -5.3467e-01,  2.2491e-02, -6.6260e-01,\n",
      "        -5.2686e-01, -1.2549e+00,  5.6494e-01,  1.2529e+00,  9.8828e-01,\n",
      "        -1.3564e+00, -3.2990e-02, -6.7188e-01,  6.5088e-01,  9.1943e-01,\n",
      "        -3.7061e-01,  7.8857e-01, -3.5214e-04, -6.5381e-01,  5.4395e-01,\n",
      "        -2.7759e-01,  3.7451e-01, -9.5367e-03,  8.6731e-02, -1.2490e+00,\n",
      "        -3.7402e-01,  1.8213e-01,  7.8564e-01,  5.1465e-01, -1.0850e+00,\n",
      "         4.2749e-01,  9.3701e-01,  9.0137e-01, -1.0654e+00, -3.9160e-01,\n",
      "        -5.4883e-01, -2.5586e-01,  1.5503e-01, -8.4912e-01, -4.1553e-01,\n",
      "        -3.8727e-02, -4.0649e-01, -1.1582e+00, -5.0684e-01, -2.6343e-01,\n",
      "        -7.4902e-01,  5.5664e-01, -7.5342e-01, -3.3643e-01,  9.9426e-02,\n",
      "         1.2959e+00,  4.6655e-01,  3.1348e+00,  6.5674e-01, -8.1982e-01,\n",
      "        -8.0713e-01, -2.4768e-01, -4.7656e-01, -7.4609e-01,  9.4055e-02,\n",
      "        -3.9673e-01, -4.3652e-01,  9.9670e-02, -2.6929e-01, -1.0077e-01,\n",
      "         1.1328e+00,  1.7993e-01,  2.8418e+00,  1.5654e+00, -7.1729e-01,\n",
      "        -1.1396e+00,  1.5381e+00, -9.7510e-01,  1.9150e+00, -1.1738e+00,\n",
      "        -6.6016e-01,  4.1016e-01, -3.0591e-01,  1.1932e-02, -8.7109e-01,\n",
      "        -8.7451e-01, -6.2207e-01,  1.6113e-01,  1.8184e+00,  5.6738e-01,\n",
      "         6.0693e-01,  3.3926e+00,  5.1562e-01,  5.8008e-01, -1.2178e+00,\n",
      "         4.5624e-03, -1.1279e+00,  4.5068e-01, -1.0918e+00,  1.0488e+00,\n",
      "        -1.9324e-01, -1.0732e+00, -6.2402e-01,  1.6504e-01, -1.3037e+00,\n",
      "         1.8417e-02, -6.2549e-01, -4.7559e-01,  5.6543e-01,  1.1865e+00,\n",
      "        -7.4890e-02, -1.3281e-01, -5.9814e-01,  5.4150e-01, -3.8550e-01,\n",
      "         4.1650e-01,  3.3179e-01,  1.5332e+00,  4.0820e+00,  7.2852e-01,\n",
      "        -4.0796e-01, -2.0032e-01,  2.5840e+00, -4.4189e-01,  3.6743e-01,\n",
      "         1.5491e-01,  1.4141e+00,  4.1328e+00,  8.8281e-01, -4.3164e-01,\n",
      "         6.8787e-02,  2.7188e+00, -1.0615e+00,  9.5581e-02,  1.4043e+00,\n",
      "         8.6572e-01,  1.4268e+00,  2.1957e-02,  3.1958e-01,  3.6914e+00,\n",
      "         2.3184e+00,  1.7639e-01,  7.1729e-01,  4.1133e+00, -5.2686e-01,\n",
      "        -1.1865e+00, -6.1328e-01, -1.0352e+00, -8.4912e-01, -7.4854e-01,\n",
      "        -6.5381e-01, -1.1084e+00, -1.3809e+00,  6.1768e-01,  1.5039e+00,\n",
      "         5.7520e-01, -8.7988e-01,  3.8062e-01,  1.2134e-01, -8.1494e-01,\n",
      "         1.2773e+00, -3.9111e-01, -6.0693e-01,  1.1602e+00, -6.9922e-01,\n",
      "        -5.8496e-01, -4.7943e-02,  8.8721e-01,  1.0127e+00,  1.1240e+00,\n",
      "         1.0186e+00,  1.9785e+00, -2.7759e-01,  9.2529e-01,  9.8694e-02,\n",
      "        -9.9170e-01, -5.0684e-01, -3.2373e-01,  1.1090e-01, -3.6719e-01,\n",
      "        -1.2793e+00, -9.3115e-01,  3.8867e-01, -3.6621e-01, -5.8447e-01,\n",
      "        -1.8555e-01, -7.2168e-01, -1.0614e-01, -3.7628e-02, -5.1483e-02,\n",
      "        -7.6758e-01,  5.4639e-01, -1.0352e+00,  8.3923e-02,  1.8005e-01,\n",
      "        -1.1348e+00, -1.2100e+00, -5.2948e-02, -1.0635e-02, -1.0801e+00,\n",
      "         3.8354e-01, -4.0497e-02, -1.0971e-02, -1.1582e+00, -2.9077e-01,\n",
      "         1.7261e-01, -5.2393e-01, -5.2637e-01, -3.2935e-01,  9.9609e-01,\n",
      "         3.2935e-01,  5.6982e-01,  4.1094e+00, -1.9385e-01,  1.7432e+00,\n",
      "         2.2839e-01,  3.1641e-01,  1.4877e-02,  7.4561e-01,  5.2734e-01,\n",
      "        -1.0469e+00, -1.1148e-03, -5.1666e-02, -5.0293e-01,  1.3049e-01,\n",
      "         3.0391e+00,  5.4346e-01,  1.4180e+00, -1.2129e+00,  6.9092e-02,\n",
      "         1.7100e+00, -3.7134e-01,  1.5215e+00,  1.1113e+00, -5.3271e-01,\n",
      "         6.3281e-01, -6.6064e-01,  1.2773e+00, -7.7454e-02,  1.3682e+00,\n",
      "         5.1611e-01,  3.5527e+00, -6.6211e-01, -4.0332e-01,  6.0938e-01,\n",
      "        -2.6440e-01,  4.3994e-01,  2.5352e+00, -1.2189e-01,  4.5264e-01,\n",
      "         4.1602e+00,  9.9121e-01, -1.9702e-01,  1.3164e+00,  8.1738e-01,\n",
      "         3.1562e+00, -6.8311e-01,  8.3252e-01, -6.2103e-02, -1.9556e-01,\n",
      "         1.7102e-01, -8.7939e-01,  5.6787e-01,  4.2676e-01, -7.4170e-01,\n",
      "         5.1483e-02, -8.2947e-02,  3.6670e-01,  3.5879e+00,  2.3652e+00,\n",
      "        -3.6206e-01,  2.4463e-01,  2.2949e+00,  4.9585e-01,  4.4092e-01,\n",
      "         4.4141e+00,  3.3521e-01, -4.1187e-01,  5.2930e-01, -9.3896e-01,\n",
      "         2.4261e-02,  1.7798e-01,  4.6997e-01, -3.7134e-01,  6.1328e-01,\n",
      "         8.5999e-02, -2.7930e-01,  2.0508e+00, -1.3440e-01, -3.8916e-01,\n",
      "         2.1855e+00, -1.0283e+00, -2.7393e-01, -2.5464e-01,  8.8623e-01,\n",
      "         1.3984e+00, -1.8631e-02,  2.4316e+00,  3.5376e-01, -1.7603e-01,\n",
      "         1.4880e-01,  5.2441e-01,  3.9316e+00, -1.1240e+00,  3.1714e-01,\n",
      "         1.7236e-01, -1.1514e+00,  1.0083e-01,  6.8298e-02, -1.8082e-02,\n",
      "         6.3330e-01,  5.4346e-01,  1.9648e+00,  4.2617e+00,  8.4131e-01,\n",
      "        -4.2017e-01,  1.1444e-01,  2.9707e+00, -5.8990e-02,  6.4844e-01,\n",
      "        -7.0923e-02,  6.9189e-01,  4.1406e+00,  1.1780e-01,  6.1670e-01,\n",
      "         4.0742e+00,  1.0225e+00,  2.5101e-02,  7.6294e-02,  1.5449e+00,\n",
      "         2.4004e+00,  3.9688e+00,  3.4741e-01,  7.3059e-02,  3.3203e+00,\n",
      "         1.6562e+00, -5.0293e-01,  2.1504e+00, -1.8311e-01,  2.5977e-01,\n",
      "         1.6235e-01,  4.8193e-01, -2.6416e-01,  5.0537e-01,  3.4546e-01,\n",
      "         1.7939e+00,  4.1562e+00,  1.1045e+00, -2.5488e-01,  1.3525e-01,\n",
      "         3.1504e+00, -4.3121e-02,  6.3037e-01, -7.9248e-01,  4.3518e-02,\n",
      "         2.1436e-01,  7.0605e-01,  5.6250e-01,  1.7920e+00,  5.6592e-01,\n",
      "         7.7246e-01,  4.0063e-01,  3.9805e+00, -8.4229e-02, -6.5967e-01,\n",
      "         2.8915e-02, -9.9170e-01,  2.2344e+00, -1.0723e+00, -3.1885e-01,\n",
      "        -6.8848e-02, -7.0679e-02,  1.4124e-01,  1.1455e+00,  2.2876e-01,\n",
      "         1.6572e+00,  6.3135e-01,  3.7715e+00, -4.9713e-02,  1.4297e+00,\n",
      "         3.4668e-02,  8.3594e-01,  1.5625e-01, -1.2683e-01,  3.8301e+00,\n",
      "         4.8389e-01,  1.8955e+00, -3.5034e-01,  3.9795e-01,  3.3545e-01,\n",
      "         8.6230e-01,  3.1758e+00,  1.2324e+00,  7.5000e-01,  9.8584e-01,\n",
      "         3.3848e+00,  1.2471e+00,  3.5391e+00, -5.3802e-02,  1.1006e+00,\n",
      "         1.5762e+00,  1.6572e+00, -3.8672e-01,  1.2051e+00, -4.9268e-01,\n",
      "         5.2246e-01,  1.9912e+00,  9.3408e-01,  4.4971e-01,  3.3164e+00,\n",
      "         2.1738e+00, -1.7712e-01,  2.0508e+00, -9.0234e-01,  3.8757e-02,\n",
      "        -9.0576e-01, -2.8906e-01, -1.3613e+00, -7.9834e-01, -4.3970e-01,\n",
      "        -1.0186e+00,  2.9892e-02,  2.3218e-01, -2.6440e-01,  6.8555e-01,\n",
      "         5.5078e-01,  9.6924e-02,  9.5215e-01,  1.3281e-01, -1.0518e+00,\n",
      "        -6.4404e-01, -9.0283e-01, -3.2275e-01, -1.6035e+00, -2.3035e-01,\n",
      "        -1.5127e+00, -1.3896e+00, -5.5371e-01,  7.5342e-01,  2.2715e+00,\n",
      "         6.1914e-01, -9.8389e-01, -1.0557e+00, -4.8926e-01, -1.3682e+00,\n",
      "        -5.7080e-01,  4.7046e-01, -2.5806e-03, -6.6162e-01,  5.6934e-01,\n",
      "        -1.7932e-01,  1.0571e-01,  4.6600e-02,  6.2891e-01, -1.9519e-01,\n",
      "        -3.9209e-01,  5.5273e-01,  1.9788e-01,  1.1777e+00, -7.2363e-01,\n",
      "         4.6234e-02,  2.9761e-01, -6.6650e-01, -6.1084e-01,  2.8906e-01,\n",
      "         2.7148e-01, -9.2712e-02, -1.0577e-01, -7.3792e-02,  2.6504e+00,\n",
      "         1.3555e+00, -3.5547e-01,  2.1055e+00, -7.7197e-01,  8.0518e-01,\n",
      "        -3.5107e-01,  1.7051e+00,  1.8320e+00, -1.2539e+00, -1.5906e-01,\n",
      "        -3.1909e-01, -2.4695e-01,  5.8301e-01,  5.1709e-01,  2.0078e+00,\n",
      "         4.2422e+00,  7.2705e-01, -4.1064e-01,  2.3022e-01,  3.0977e+00,\n",
      "        -5.7617e-01, -2.7856e-01, -7.7881e-02,  5.4688e-01,  3.9023e+00,\n",
      "         1.0586e+00, -1.1416e+00, -2.3376e-01, -5.3027e-01, -2.4011e-01,\n",
      "         3.3301e-01, -1.1631e+00, -8.4814e-01, -4.7559e-01,  2.4194e-01,\n",
      "         3.2910e+00,  4.0283e-01,  1.3203e+00,  2.1133e+00,  2.8174e-01,\n",
      "         7.3242e-02, -3.8184e-01,  6.4404e-01,  3.8794e-01,  1.9365e+00,\n",
      "         4.2812e+00, -2.3450e-01, -2.2079e-02,  9.4873e-01,  4.4678e-01,\n",
      "         2.1738e+00, -5.7764e-01,  1.6963e+00,  8.4961e-01,  9.3066e-01,\n",
      "         3.8848e+00,  5.8789e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.9934,  0.1076, -0.6701,  ...,  1.5019,  0.9173, -0.1434],\n",
      "         [ 0.1435,  0.4163, -0.4315,  ...,  0.5600,  0.2443,  0.0163],\n",
      "         [ 0.7342,  0.2085, -0.2647,  ...,  2.0259,  0.9033, -0.5614],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([1.0264, 2.0781, 0.8721,  ..., 2.0352, 0.9785, 0.4739], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([1.5684, 0.1175, 2.0410,  ..., 0.9395, 3.4102, 0.6772], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.7970, -0.1586, -0.5464,  ...,  1.3003,  0.8787, -0.4401],\n",
      "         [ 0.5874,  0.0238,  0.0730,  ...,  0.7124,  0.5319,  0.4662],\n",
      "         [ 0.2689,  0.2449,  0.1429,  ...,  1.1596,  0.5195,  0.1106],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.1387e+00,  1.3574e+00,  1.4014e+00,  8.0908e-01,  3.4766e-01,\n",
      "         6.8262e-01, -3.8428e-01,  3.0703e+00,  1.3389e+00,  1.9189e+00,\n",
      "         4.4824e-01,  5.8301e-01, -3.9722e-01,  1.3623e-01,  5.4492e-01,\n",
      "        -1.7212e-01,  8.7158e-01,  4.9927e-01,  1.9922e+00,  5.3174e-01,\n",
      "         3.8828e+00,  8.2080e-01,  6.6748e-01,  3.9492e+00,  6.6650e-01,\n",
      "         5.2002e-01, -5.4199e-01, -4.3018e-01, -5.3125e-01,  4.0000e+00,\n",
      "         1.2646e+00,  5.0879e-01, -7.2266e-01,  2.8008e+00,  3.8544e-02,\n",
      "         2.3594e+00,  2.5122e-01,  4.7241e-01, -9.8999e-02,  7.0605e-01,\n",
      "         1.6785e-02,  2.2876e-01,  4.0469e+00,  1.2041e+00,  1.8286e-01,\n",
      "        -9.3213e-01,  3.7646e-01, -3.9453e-01, -1.6760e-01, -6.1133e-01,\n",
      "         1.5381e-01,  7.0068e-02,  3.8027e+00,  1.6982e+00,  1.5605e+00,\n",
      "         5.9766e-01,  1.5742e+00,  3.5684e+00,  7.8857e-01,  4.7290e-01,\n",
      "         2.1348e+00,  1.0078e+00,  1.7188e+00,  6.5088e-01, -2.9834e-01,\n",
      "         3.8672e+00,  6.5479e-01,  4.4141e-01, -3.5492e-02, -7.7881e-01,\n",
      "         3.1006e-02, -7.4023e-01, -2.3950e-01, -3.6523e-01,  2.1523e+00,\n",
      "         2.1790e-01,  3.6108e-01, -3.9795e-01,  2.5513e-01, -1.2085e-01,\n",
      "        -3.5205e-01,  3.6572e-01,  3.3374e-01,  5.1172e-01,  3.3613e+00,\n",
      "         7.8613e-01,  5.0684e-01,  4.1138e-01,  3.9512e+00,  5.9863e-01,\n",
      "         4.3799e-01,  3.6523e-01, -6.1621e-01,  2.5234e+00,  2.1118e-01,\n",
      "         4.3433e-01, -4.7241e-01,  4.5923e-01,  5.6427e-02, -2.6733e-01,\n",
      "         6.9092e-01, -4.1772e-01,  7.8076e-01, -8.1738e-01,  4.1328e+00,\n",
      "         6.8799e-01,  3.7158e-01,  8.2031e-01, -1.7432e-01, -4.2755e-02,\n",
      "        -5.1208e-02, -1.8066e-01,  3.7949e+00,  1.7842e+00,  1.9316e+00,\n",
      "         6.3037e-01,  1.9570e+00,  9.3701e-01,  1.4043e+00,  4.5703e-01,\n",
      "         3.4785e+00,  8.3301e-01,  2.6016e+00,  9.0674e-01,  5.4688e-01,\n",
      "        -2.1350e-01,  3.9609e+00,  7.2021e-01,  4.3994e-01,  7.6538e-02,\n",
      "        -1.0605e+00,  8.6426e-02,  6.8542e-02, -4.7339e-01,  2.2520e+00,\n",
      "         2.4805e-01,  4.5312e-01, -4.8047e-01,  4.7925e-01,  4.0405e-01,\n",
      "         4.8022e-01,  4.3047e+00,  8.8330e-01,  5.4590e-01,  5.8691e-01,\n",
      "         4.1250e+00,  9.4287e-01,  5.0684e-01,  5.3174e-01, -6.2109e-01,\n",
      "         3.0879e+00,  5.6592e-01,  6.3770e-01, -8.7952e-02,  3.2461e+00,\n",
      "         5.3223e-01,  4.9243e-01,  1.0840e+00,  1.0840e+00,  5.3516e-01,\n",
      "         2.9541e-01,  1.1582e+00,  4.2344e+00,  9.6826e-01,  6.2305e-01,\n",
      "         6.2354e-01,  3.6074e+00,  1.3936e+00,  6.5674e-01,  3.3887e+00,\n",
      "         1.2305e+00, -2.1448e-01,  9.0479e-01, -2.2925e-01,  6.4819e-02,\n",
      "         1.4941e-01, -4.1235e-01,  3.8203e+00,  1.7803e+00,  1.8525e+00,\n",
      "         5.9570e-01,  1.9785e+00,  1.1963e+00,  1.6191e+00,  5.8838e-01,\n",
      "         3.6855e+00,  9.1260e-01,  2.4824e+00,  8.0859e-01,  5.1074e-01,\n",
      "        -4.0430e-01,  4.1055e+00,  8.0811e-01,  5.0488e-01,  1.4099e-01,\n",
      "        -9.2822e-01,  1.3977e-01, -7.0020e-01, -5.9937e-02, -1.7871e-01,\n",
      "        -3.6523e-01,  2.4180e+00,  3.1177e-01,  4.3115e-01, -3.9233e-01,\n",
      "         3.3691e-01,  5.5811e-01,  1.2939e+00,  2.7812e+00,  7.9102e-01,\n",
      "         2.1367e+00,  6.7725e-01, -3.7183e-01, -2.6392e-01, -4.1333e-01,\n",
      "         4.2695e+00,  8.1592e-01,  6.1328e-01,  5.2539e-01, -2.2681e-01,\n",
      "         4.2266e+00,  1.3701e+00,  1.2236e+00,  8.0029e-01,  5.9814e-01,\n",
      "         5.1074e-01, -7.0850e-01,  1.8809e+00,  5.1758e-01,  1.9458e-01,\n",
      "         3.1348e+00,  6.1230e-01,  6.8848e-01,  1.0345e-01,  2.3477e+00,\n",
      "         4.9585e-01,  2.8125e-01, -6.6260e-01,  1.1484e+00,  1.6028e-01,\n",
      "        -3.8037e-01,  1.4658e+00, -2.3926e-01,  4.0977e+00,  1.3301e+00,\n",
      "         1.3447e+00,  9.8730e-01,  5.6738e-01,  8.7695e-01, -5.2930e-01,\n",
      "        -6.2256e-01,  1.7627e+00, -2.7905e-01,  9.5337e-02, -6.8262e-01,\n",
      "         3.2441e+00,  1.1680e+00,  3.2471e-01,  1.1172e+00,  1.2002e+00,\n",
      "        -8.2227e-01,  3.9600e-01, -3.1396e-01, -1.3452e-01,  1.9080e-01,\n",
      "         5.2393e-01,  5.8105e-01, -3.3813e-01,  4.0078e+00,  9.8242e-01,\n",
      "         8.0518e-01,  7.7832e-01,  1.0654e+00, -5.2930e-01,  3.3325e-01,\n",
      "         5.4779e-02, -5.9375e-01,  1.9409e-01,  2.7441e-01, -1.6394e-01,\n",
      "        -2.9370e-01,  2.1436e-01,  4.5898e-01,  1.0615e+00,  6.0352e-01,\n",
      "         3.8544e-02,  9.0332e-01,  5.5225e-01,  1.5723e-01, -1.3367e-01,\n",
      "         3.6865e-02,  1.0312e+00,  5.6787e-01, -8.8501e-02,  7.1631e-01,\n",
      "         3.7329e-01,  3.8989e-01, -1.8311e-01,  3.5669e-01, -4.4824e-01,\n",
      "         7.7295e-01, -8.5205e-02,  1.6968e-01,  9.5410e-01,  8.2471e-01,\n",
      "         1.0797e-01,  1.6572e+00,  2.1484e+00,  2.0762e+00,  6.8994e-01,\n",
      "         1.2979e+00,  2.8955e-01, -7.9468e-02,  7.5537e-01, -6.4062e-01,\n",
      "         1.0430e+00,  1.0566e+00,  3.7598e-01, -3.0322e-01,  1.8323e-01,\n",
      "        -2.2913e-01,  4.0820e+00,  1.6523e+00,  1.8213e+00,  4.9097e-01,\n",
      "         7.4268e-01, -4.6362e-01,  3.0078e+00,  1.0244e+00,  3.1465e+00,\n",
      "         6.8115e-01,  2.3022e-01,  1.3936e+00,  6.9763e-02, -3.0566e-01,\n",
      "         1.8823e-01,  7.8662e-01, -5.0293e-01,  4.2871e-01, -1.2646e-01,\n",
      "         3.2188e+00,  3.9336e+00,  1.2432e+00,  9.4531e-01,  5.6055e-01,\n",
      "        -4.9463e-01,  9.7607e-01,  1.2213e-01,  2.3828e+00,  1.5312e+00,\n",
      "         9.1113e-01, -1.3953e-01, -3.6072e-02,  9.8633e-01,  8.8379e-02,\n",
      "         1.1660e+00,  5.8594e-01, -4.7192e-01,  1.5840e+00,  8.6621e-01,\n",
      "         1.0186e+00,  6.8604e-01,  4.6851e-01,  5.9375e-01,  9.5898e-01,\n",
      "         8.7256e-01,  9.1187e-02, -2.7417e-01,  2.6777e+00,  1.1865e+00,\n",
      "         8.6133e-01,  9.7412e-01,  5.1123e-01,  4.3750e-01,  1.2510e+00,\n",
      "         6.9287e-01,  5.5957e-01,  2.2595e-01, -4.5044e-01,  9.9902e-01,\n",
      "         8.1299e-01, -1.5515e-01,  3.3130e-01, -8.1885e-01, -1.0046e-01,\n",
      "         1.0723e+00, -4.6533e-01,  4.0742e+00,  1.0674e+00,  9.8438e-01,\n",
      "         6.8262e-01,  5.3809e-01, -2.2974e-01,  6.6797e-01,  2.4707e-01,\n",
      "         9.0210e-02,  2.6582e+00,  9.3408e-01,  5.0342e-01, -5.6982e-01,\n",
      "         1.0651e-01, -2.5073e-01, -5.7373e-01, -6.6846e-01,  3.8223e+00,\n",
      "         9.3506e-01,  1.1426e+00,  4.1016e-01, -4.9805e-01, -1.3008e-02,\n",
      "         1.9961e+00,  5.0391e-01,  5.4932e-01, -4.5801e-01,  8.2910e-01,\n",
      "         6.7285e-01,  1.0797e-01,  5.2148e-01,  4.4824e-01,  3.0879e+00,\n",
      "         2.5605e+00,  7.3584e-01,  1.5361e+00,  3.4448e-01,  7.7553e-03,\n",
      "         3.4229e-01,  8.4131e-01, -9.8291e-01,  3.2666e-01, -1.9971e-01,\n",
      "        -3.0859e-01,  1.2988e+00,  1.1078e-01,  5.4248e-01,  9.3201e-02,\n",
      "         2.6758e+00,  1.6270e+00,  3.8867e-01,  1.3281e+00,  1.4819e-01,\n",
      "        -1.2128e-01, -4.8248e-02, -4.1040e-01,  7.2705e-01,  8.2031e-01,\n",
      "        -2.9126e-01,  2.6973e+00,  1.3418e+00, -1.3159e-01,  3.2344e+00,\n",
      "         1.0615e+00,  7.6514e-01, -3.1299e-01, -1.8701e-01, -4.0479e-01,\n",
      "        -3.7012e-01, -7.5098e-01,  1.1357e+00, -2.7783e-01,  1.5596e+00,\n",
      "         1.9458e-01,  6.1615e-02, -8.7842e-01,  6.7969e-01,  9.2969e-01,\n",
      "         2.1553e-03, -3.8110e-01,  5.9424e-01, -6.9189e-01,  5.6250e-01,\n",
      "        -4.4995e-01,  9.9609e-01,  1.7822e-01,  1.2393e+00,  4.8706e-01,\n",
      "        -7.8516e-01,  7.1338e-01, -1.6968e-01,  1.7139e-01, -6.2354e-01,\n",
      "        -3.9404e-01,  3.4204e-01,  6.5527e-01, -1.2469e-01, -2.5879e-01,\n",
      "        -1.0234e+00,  1.2817e-01,  2.9570e+00,  1.2100e+00,  1.0732e+00,\n",
      "         5.0195e-01,  4.5923e-01,  4.1602e-01,  4.1836e+00,  1.6494e+00,\n",
      "         1.9053e+00,  4.7607e-01,  7.5098e-01,  3.7871e+00,  1.6230e+00,\n",
      "         1.6777e+00,  3.6816e-01,  7.0996e-01, -9.8340e-01,  9.2041e-01,\n",
      "         8.9209e-01,  1.2561e-01,  5.2734e-01, -7.3779e-01, -4.1382e-01,\n",
      "         4.0195e+00,  1.9600e+00,  1.4014e+00,  9.6631e-01,  2.5098e-01,\n",
      "         2.7461e+00,  1.4043e+00,  1.6113e+00,  7.6611e-01, -2.1497e-01,\n",
      "         3.0176e+00,  1.2812e+00,  9.7363e-01,  3.4668e-01,  1.6221e+00,\n",
      "         3.9673e-01, -5.6396e-01,  3.1758e+00,  5.9180e-01, -4.2334e-01,\n",
      "         3.8398e+00,  1.5527e+00,  1.8203e+00,  5.4053e-01,  9.6533e-01,\n",
      "        -3.4985e-01,  1.3857e+00,  1.2744e+00,  2.6514e-01,  3.8516e+00,\n",
      "         1.4775e+00,  1.0410e+00, -2.6709e-01,  3.2051e+00,  1.6543e+00,\n",
      "        -8.5083e-02,  3.5977e+00,  1.2812e+00,  8.7598e-01,  5.9570e-01,\n",
      "         4.0625e-01,  2.9434e+00,  6.8506e-01,  5.6299e-01,  8.3618e-02,\n",
      "         1.6982e+00, -1.8530e-01,  1.6182e+00, -8.5547e-01,  2.5464e-01,\n",
      "        -4.3555e-01, -9.4043e-01,  6.3281e-01,  4.2920e-01, -6.3574e-01,\n",
      "         8.2666e-01,  2.8491e-01,  5.3125e-01, -9.3994e-01,  1.0625e+00,\n",
      "        -2.7686e-01, -7.6599e-02,  1.7151e-02,  7.4707e-02, -1.6467e-01,\n",
      "        -8.3069e-02,  5.5029e-01,  1.5176e+00,  6.9922e-01,  2.9932e-01,\n",
      "        -5.8008e-01,  3.1055e+00,  4.8657e-01,  1.3535e+00,  1.2469e-01,\n",
      "         1.1250e+00,  1.9189e-01,  6.1279e-01,  1.9763e-01, -4.2651e-01,\n",
      "         8.4473e-01, -5.7227e-01,  5.2881e-01,  9.0674e-01,  5.3223e-01,\n",
      "        -8.4961e-01,  3.1367e+00,  8.3008e-01,  5.6055e-01,  1.2783e+00,\n",
      "        -3.8452e-01,  7.3877e-01,  5.9668e-01,  3.9111e-01,  4.2812e+00,\n",
      "         1.5957e+00,  1.5264e+00,  2.9082e+00,  9.7656e-01,  8.7109e-01,\n",
      "         4.1172e+00,  1.1045e+00,  1.1377e+00,  2.5996e+00,  9.1406e-01,\n",
      "         8.3545e-01,  3.3125e+00,  2.1094e+00,  6.8311e-01,  1.6172e+00,\n",
      "         8.3350e-01,  3.1582e+00,  1.4727e+00,  7.4854e-01,  1.5186e+00,\n",
      "         4.2969e+00,  1.5518e+00,  1.3105e+00,  2.2559e+00,  8.4863e-01,\n",
      "         1.8887e+00,  9.1748e-01,  2.4243e-01,  3.1309e+00,  7.8711e-01,\n",
      "        -2.5586e-01,  6.4648e-01, -3.0908e-01,  5.5225e-01, -5.9619e-01,\n",
      "         7.0068e-01,  8.5693e-01, -1.2268e-01,  7.9688e-01, -3.2031e-01,\n",
      "        -2.2375e-01,  8.9551e-01,  4.0942e-01,  1.0820e+00, -4.7900e-01,\n",
      "         4.4897e-01, -2.7148e-01,  4.1455e-01, -4.9536e-01,  1.2812e+00,\n",
      "        -1.8591e-01,  5.9521e-01,  1.6455e-01,  9.9414e-01,  1.1582e+00,\n",
      "        -5.6122e-02,  1.1709e+00, -3.1299e-01,  2.3750e+00,  3.6060e-01,\n",
      "        -2.0349e-01,  7.5000e-01,  7.8711e-01,  2.9980e-01,  1.9995e-01,\n",
      "        -1.6870e-01,  5.5957e-01, -2.0056e-01,  1.5654e+00,  8.2703e-02,\n",
      "         5.1221e-01,  2.8477e+00,  1.0215e+00,  9.2578e-01,  1.5088e-01,\n",
      "         6.8994e-01, -5.4053e-01,  3.3765e-01,  4.1016e-01, -2.6047e-02,\n",
      "         2.9541e-01,  9.0381e-01,  2.6270e-01,  3.5742e-01,  8.9795e-01,\n",
      "         1.0010e+00,  5.6494e-01,  4.1821e-01,  1.5002e-01,  9.6484e-01,\n",
      "         9.1748e-01,  5.8496e-01,  1.4270e-01,  2.5156e+00,  6.1279e-01,\n",
      "         9.8438e-01,  1.0771e+00,  3.8887e+00,  1.8457e+00,  2.5801e+00,\n",
      "         1.8643e+00,  1.5908e+00,  3.3750e+00,  1.9404e+00,  1.0312e+00,\n",
      "         1.5391e+00], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.4248e+00,  2.2034e-01,  1.0059e+00,  1.1743e-01,  1.8447e+00,\n",
      "         1.8457e+00, -1.1064e+00, -3.7842e-01,  3.7305e-01,  6.3086e-01,\n",
      "         1.7324e+00,  1.5176e+00, -9.8096e-01, -5.8691e-01, -3.9746e-01,\n",
      "        -7.1826e-01, -8.1592e-01, -3.8208e-01, -2.7344e-01,  6.6211e-01,\n",
      "         3.8239e-02,  2.6953e+00,  3.0371e+00, -2.5317e-01,  2.2090e+00,\n",
      "         3.1328e+00, -1.2656e+00, -3.4546e-01, -9.1504e-01,  5.7373e-01,\n",
      "         2.9141e+00,  3.0840e+00, -1.1074e+00,  2.9980e-01,  1.1562e+00,\n",
      "        -3.3887e-01,  5.6458e-02,  5.0146e-01,  2.1719e+00,  1.8408e+00,\n",
      "         9.9060e-02,  2.5830e-01,  5.6396e-01,  3.2227e+00,  2.2900e-01,\n",
      "        -8.4521e-01, -2.7832e-01,  1.0284e-01,  1.4795e-01, -1.3184e+00,\n",
      "        -1.3271e+00, -8.1104e-01, -2.5073e-01,  7.3730e-01,  1.5986e+00,\n",
      "         4.3047e+00,  1.8203e+00,  5.4688e-01,  5.0098e-01,  3.8535e+00,\n",
      "         8.6230e-01,  2.7812e+00,  1.0664e+00,  2.7793e+00, -2.5195e-01,\n",
      "         3.3130e-01,  1.1143e+00,  3.8594e+00,  2.4824e+00, -1.2227e+00,\n",
      "        -8.5938e-01, -7.1973e-01, -9.6289e-01,  2.3181e-01, -4.9512e-01,\n",
      "        -9.2957e-02,  4.6777e-01,  1.9541e+00,  1.9111e+00,  3.5596e-01,\n",
      "        -7.3242e-01, -2.9272e-01, -6.7578e-01,  6.1963e-01, -9.1797e-02,\n",
      "         1.2324e+00,  3.6484e+00,  2.6328e+00, -2.3157e-01,  7.2412e-01,\n",
      "         3.1523e+00,  2.5547e+00, -1.4219e+00, -9.2188e-01, -1.3000e-01,\n",
      "         4.4775e-01,  1.8965e+00,  1.5488e+00, -1.5796e-01, -1.2061e+00,\n",
      "         7.3340e-01, -1.0605e+00, -9.3506e-01, -6.7773e-01,  7.9407e-02,\n",
      "         1.1475e+00,  3.5273e+00,  2.8594e+00, -4.9414e-01, -3.5059e-01,\n",
      "        -3.0933e-01, -1.0635e+00, -3.2349e-01,  7.9004e-01,  1.4775e+00,\n",
      "         3.4082e+00,  1.0186e+00,  2.9062e+00,  1.0996e+00,  3.0566e+00,\n",
      "        -1.2659e-01,  2.6504e+00,  1.2568e+00,  6.8506e-01,  4.4492e+00,\n",
      "        -6.6223e-02,  5.7422e-01,  5.1562e-01,  4.1133e+00,  3.8550e-01,\n",
      "        -8.8330e-01, -1.0947e+00,  3.6475e-01, -4.2090e-01, -9.2871e-01,\n",
      "        -2.3401e-01,  4.5068e-01,  2.1797e+00,  2.0352e+00, -4.2114e-01,\n",
      "         5.8887e-01,  6.3049e-02,  5.2881e-01,  3.7090e+00,  2.9648e+00,\n",
      "        -2.8076e-01,  4.5471e-02,  2.9688e+00,  3.0879e+00, -1.2617e+00,\n",
      "        -6.6260e-01,  5.7678e-02,  9.8975e-01,  3.3145e+00, -9.2041e-02,\n",
      "         2.4951e-01,  2.2305e+00,  2.9805e+00,  5.0830e-01,  1.4551e+00,\n",
      "        -3.8867e-01, -4.8242e-01,  1.8384e-01,  3.3545e-01,  3.9375e+00,\n",
      "         1.1270e+00, -3.8965e-01,  2.4883e+00,  1.5596e+00,  7.6758e-01,\n",
      "         3.4805e+00, -8.1396e-01,  1.4404e+00, -1.3291e+00, -9.9268e-01,\n",
      "        -7.8003e-02, -3.8379e-01, -1.5051e-01,  8.2861e-01,  1.4951e+00,\n",
      "         3.3770e+00,  9.5410e-01,  2.6758e+00,  9.7803e-01,  2.6484e+00,\n",
      "        -2.3816e-01,  2.4219e+00,  9.6045e-01,  7.7441e-01,  4.5117e+00,\n",
      "        -2.9834e-01,  3.0029e-01,  1.2627e+00,  4.2422e+00,  6.8420e-02,\n",
      "        -1.0498e+00, -9.2773e-01, -8.2520e-01, -1.0703e+00,  2.7637e-01,\n",
      "        -1.9104e-01, -6.2109e-01, -1.2115e-01,  5.4883e-01,  2.4805e+00,\n",
      "         2.0840e+00, -3.9966e-01, -4.7412e-01,  2.8516e-01,  2.9609e+00,\n",
      "        -1.4404e-01,  2.5957e+00, -1.4639e+00, -7.3779e-01, -1.3232e+00,\n",
      "         1.1719e-01,  4.1211e-01,  3.7070e+00,  2.5996e+00, -1.2295e+00,\n",
      "         1.2976e-01,  3.4570e-01,  1.6045e+00,  2.6211e+00,  3.4434e+00,\n",
      "         2.9375e+00, -1.1504e+00,  7.2327e-02,  7.9834e-01,  1.4551e+00,\n",
      "        -4.7021e-01, -5.6061e-02,  8.4033e-01,  3.1953e+00, -3.6084e-01,\n",
      "         1.3904e-01, -7.8186e-02,  6.6650e-01,  1.2207e+00, -3.3398e-01,\n",
      "        -9.0771e-01,  8.5059e-01, -5.4150e-01,  2.4670e-01,  3.4692e-01,\n",
      "         1.7158e+00,  2.6758e+00,  3.3398e+00,  1.1436e+00,  5.1855e-01,\n",
      "        -1.3643e+00,  1.0879e+00,  3.0859e-01, -2.4036e-01, -6.1328e-01,\n",
      "         3.4204e-01,  1.0762e+00,  3.8281e+00, -7.4561e-01,  2.2207e+00,\n",
      "        -1.0840e+00, -9.3262e-01, -3.3545e-01, -8.1348e-01, -4.0576e-01,\n",
      "        -2.7124e-01, -2.4353e-01, -1.5391e+00,  1.2384e-01,  2.7090e+00,\n",
      "         2.7715e+00, -1.2715e+00,  4.0625e-01, -1.0410e+00, -2.9199e-01,\n",
      "         7.1289e-02, -1.2549e+00, -8.6377e-01, -4.2822e-01, -8.3740e-01,\n",
      "        -4.7168e-01, -6.5283e-01, -6.8750e-01,  5.5713e-01,  5.8990e-02,\n",
      "        -5.1416e-01, -9.0283e-01, -1.1006e+00, -2.0859e-02, -4.5117e-01,\n",
      "        -1.3613e+00,  3.6768e-01, -7.6953e-01, -9.6875e-01, -4.8340e-01,\n",
      "        -8.6963e-01, -5.6299e-01, -5.1562e-01,  1.0187e-01, -3.2324e-01,\n",
      "         3.4595e-01, -7.3877e-01, -3.9398e-02, -2.4426e-01, -2.7002e-01,\n",
      "        -2.1326e-01, -1.1670e+00, -9.6094e-01, -3.1055e-01,  2.5215e+00,\n",
      "         3.6377e-01, -4.0649e-01,  4.1080e-04, -7.1167e-02, -2.8247e-01,\n",
      "        -2.5830e-01,  2.0488e+00, -1.0186e+00, -5.4541e-01, -5.6885e-01,\n",
      "        -1.1338e+00, -2.1927e-02,  9.7900e-01,  1.2793e+00,  3.3750e+00,\n",
      "         3.3711e+00, -1.6533e+00,  3.0542e-01,  2.3594e+00,  2.4078e-02,\n",
      "         7.5049e-01,  3.9082e+00,  1.3965e+00,  3.5254e+00, -3.6890e-01,\n",
      "        -1.8530e-01,  4.9536e-01, -1.3857e+00, -4.0747e-01, -1.1895e+00,\n",
      "         9.3213e-01, -3.7183e-01,  8.2861e-01,  3.3887e+00,  3.0742e+00,\n",
      "        -3.3838e-01,  1.2634e-01, -6.0254e-01,  1.6321e-01,  1.8076e+00,\n",
      "        -7.4756e-01, -3.4277e-01, -1.6642e-03,  6.6797e-01, -1.0068e+00,\n",
      "        -4.9023e-01, -3.4644e-01, -2.1765e-01,  1.7480e-01, -3.7305e-01,\n",
      "         1.4490e-01, -2.9858e-01,  2.6685e-01, -4.0039e-01, -1.9324e-01,\n",
      "         1.3748e-02,  2.6978e-01, -1.1533e+00,  3.2617e-01,  2.6025e-01,\n",
      "         4.5483e-01,  6.5527e-01, -3.5791e-01,  5.8057e-01,  1.0724e-01,\n",
      "        -2.6099e-01, -1.7651e-01, -1.0675e-01,  4.5850e-01,  2.6831e-01,\n",
      "        -1.2164e-01,  2.9883e-01,  7.2083e-02, -1.4785e+00, -4.7070e-01,\n",
      "        -1.2607e+00, -8.4277e-01, -1.8469e-01,  7.1680e-01,  3.2578e+00,\n",
      "         1.8271e+00,  8.0371e-01,  2.3945e+00,  5.8350e-01,  1.5417e-01,\n",
      "         7.1289e-01, -1.4185e-01,  9.1553e-01, -9.2285e-02,  2.8320e-01,\n",
      "         8.8806e-02,  3.8525e-01, -4.3457e-01, -6.4990e-01,  1.8201e-01,\n",
      "         2.7734e+00,  2.7422e+00,  1.2754e+00, -5.1367e-01, -2.9541e-01,\n",
      "         3.0908e-01,  9.8975e-01,  6.6260e-01,  1.1348e+00,  4.4019e-01,\n",
      "        -2.2498e-01,  8.6377e-01, -2.9639e-01,  6.0352e-01,  9.3359e-01,\n",
      "         2.6779e-02,  1.7041e+00,  7.9004e-01,  5.3662e-01,  1.5771e+00,\n",
      "         3.9824e+00,  2.5020e+00, -1.0918e+00,  1.1426e-01, -6.5625e-01,\n",
      "        -5.5127e-01, -4.8779e-01,  4.3921e-01,  8.3008e-03, -8.8135e-01,\n",
      "         5.7861e-01,  7.6233e-02,  1.3896e+00,  4.1284e-01,  5.3955e-01,\n",
      "         1.3799e+00,  3.4473e+00, -4.2847e-01, -2.2131e-01,  5.5969e-02,\n",
      "        -2.6172e-01,  1.5703e+00,  2.0195e+00,  7.1338e-01,  1.8555e-02,\n",
      "         1.7607e+00,  3.5039e+00,  5.3760e-01, -1.1289e+00, -9.4629e-01,\n",
      "        -2.4744e-01, -7.0654e-01, -7.4561e-01, -6.2354e-01,  5.5029e-01,\n",
      "        -2.2424e-01, -1.0693e+00, -1.0566e+00, -3.7329e-01, -1.8335e-01,\n",
      "        -6.8817e-03,  5.6787e-01, -2.8345e-01, -1.1943e+00, -1.8323e-01,\n",
      "        -8.1152e-01, -6.4062e-01, -9.7900e-01,  3.3032e-01, -6.1670e-01,\n",
      "        -1.4746e+00, -3.6133e-01, -4.5654e-01, -7.0605e-01, -1.2012e+00,\n",
      "        -8.8574e-01, -6.0254e-01, -5.5957e-01,  2.8369e-01,  6.9873e-01,\n",
      "        -9.5703e-01, -2.8564e-01, -4.1577e-01,  1.0059e+00,  2.5020e+00,\n",
      "         2.2324e+00, -3.0127e-01,  5.3418e-01, -3.0566e-01,  1.4004e+00,\n",
      "         1.2959e+00,  3.2422e+00,  3.6641e+00, -4.0381e-01,  1.1191e+00,\n",
      "         1.3115e+00,  3.0508e+00,  3.6250e+00, -8.1152e-01,  2.7515e-01,\n",
      "        -9.4299e-02,  8.1982e-01, -3.7329e-01, -2.1167e-01, -2.1875e-01,\n",
      "         6.4453e-02,  1.4541e+00,  2.4746e+00,  3.4941e+00,  7.9395e-01,\n",
      "         6.9971e-01,  1.5674e+00,  1.6494e+00,  3.8594e+00, -2.8613e-01,\n",
      "         1.1279e-01,  1.0977e+00,  1.6299e+00,  3.9434e+00,  2.1008e-01,\n",
      "         1.7812e+00, -1.1689e+00, -2.4451e-01,  2.0215e+00, -1.0977e+00,\n",
      "        -3.0591e-01,  1.0898e+00,  1.4404e+00,  3.5781e+00,  3.9844e+00,\n",
      "         1.5857e-01,  3.3643e-01,  1.9756e+00,  5.7568e-01,  2.4011e-01,\n",
      "         1.2842e+00,  4.4805e+00,  2.7344e-01,  1.0508e+00,  1.2773e+00,\n",
      "         5.2246e-01,  5.4834e-01,  1.4199e+00,  3.7383e+00,  3.0005e-01,\n",
      "         5.2246e-01,  6.2988e-01,  1.8984e+00,  3.6377e-01, -5.6104e-01,\n",
      "         2.0684e+00, -6.3672e-01,  1.6084e+00, -1.0781e+00, -8.1152e-01,\n",
      "        -8.5840e-01, -4.0454e-01, -2.8369e-01,  1.3354e-01, -1.0225e+00,\n",
      "        -7.6416e-01, -5.6494e-01, -5.3320e-01, -7.5391e-01,  1.2764e+00,\n",
      "        -2.4231e-01, -1.1543e+00, -9.6631e-01,  1.0748e-01, -5.6787e-01,\n",
      "        -9.0479e-01, -3.8403e-01,  2.5635e-01, -1.0541e-01,  5.6396e-01,\n",
      "        -1.0576e+00,  7.0618e-02,  1.7090e-01,  7.9639e-01,  2.4512e+00,\n",
      "         7.3730e-01, -7.6123e-01, -2.3035e-01, -4.1113e-01, -7.0850e-01,\n",
      "         1.1780e-01, -1.1934e+00, -1.0773e-01,  3.9551e-01,  7.5977e-01,\n",
      "        -1.2764e+00,  5.0342e-01,  1.1396e+00,  3.0254e+00,  1.3418e+00,\n",
      "        -4.1382e-01,  1.6445e+00, -1.5393e-01,  5.1904e-01, -4.9561e-01,\n",
      "         5.3418e-01,  2.0332e+00,  1.2012e+00,  1.3594e+00,  4.6211e+00,\n",
      "        -4.9512e-01,  3.2886e-01,  1.5557e+00,  5.9570e-01,  1.0088e+00,\n",
      "         4.3789e+00,  6.2988e-01,  3.0542e-01,  2.6230e+00,  2.4668e+00,\n",
      "         4.1445e+00,  6.6553e-01,  4.1870e-01,  2.3008e+00,  3.4961e+00,\n",
      "        -2.7100e-01,  2.8027e-01,  2.1543e+00,  1.2910e+00,  1.8857e+00,\n",
      "         2.7520e+00,  3.9590e+00,  1.1221e+00,  1.5371e+00,  2.4182e-01,\n",
      "        -1.1045e+00, -2.6636e-01, -7.3145e-01, -2.4207e-01, -3.9282e-01,\n",
      "         5.8044e-02,  6.6650e-01,  8.7219e-02,  1.4197e-01,  7.6660e-01,\n",
      "        -9.4434e-01,  2.2485e-01,  5.5469e-01,  6.9397e-02, -8.8623e-01,\n",
      "        -5.9814e-01, -7.0166e-01, -7.6233e-02, -4.1821e-01,  1.2812e+00,\n",
      "        -7.6904e-01, -2.6709e-01, -1.9485e-02,  9.1357e-01,  4.5581e-01,\n",
      "        -5.7861e-01, -1.4661e-01, -7.0459e-01,  9.2676e-01,  2.7168e+00,\n",
      "        -1.0146e+00,  1.0864e-01,  8.5156e-01, -7.8223e-01, -4.0375e-02,\n",
      "        -3.2373e-01, -2.3376e-01, -7.7588e-01,  2.4646e-01,  1.5977e+00,\n",
      "        -1.2432e+00,  1.6754e-02,  9.9609e-01,  4.1016e+00,  9.3140e-02,\n",
      "        -4.6387e-01, -1.2578e+00, -2.5131e-02,  2.3218e-01, -5.5084e-03,\n",
      "        -1.1846e+00, -3.3496e-01,  1.1432e-01, -9.0234e-01, -4.1089e-01,\n",
      "        -3.2684e-02,  1.4099e-01, -8.9014e-01, -6.9336e-01, -1.5918e-01,\n",
      "         9.3994e-01,  5.8545e-01, -5.8643e-01, -7.4072e-01,  1.1389e-01,\n",
      "         1.6475e+00,  3.5278e-01, -1.4111e-01,  9.7510e-01,  1.3643e+00,\n",
      "         7.0361e-01,  1.2295e+00, -1.6479e-01,  1.6143e+00,  4.0742e+00,\n",
      "         7.7881e-01], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.9864,  0.2595, -0.4435,  ...,  1.4268,  1.2928, -0.2077],\n",
      "         [ 0.0666,  0.4630, -0.0777,  ...,  0.5400,  0.4262, -0.0412],\n",
      "         [-0.1463,  0.4551,  0.0285,  ...,  0.0525,  0.3147, -0.3133],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([1.2803, 1.6992, 1.1436,  ..., 1.0410, 0.6631, 0.4529], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.7437, -0.0925,  0.6860,  ...,  0.2893,  1.9219,  0.6626],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n",
      "sequence_output: tensor([[[-0.0193,  0.1238,  0.0289,  ...,  0.1426,  0.0757, -0.0499],\n",
      "         [-0.0350,  0.1632, -0.1433,  ...,  0.2062,  0.3191, -0.1485],\n",
      "         [ 0.2020,  0.1285,  0.0413,  ..., -0.2131,  0.1377, -0.0342],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0780, -0.0291, -0.0993],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0780, -0.0291, -0.0993],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0780, -0.0291, -0.0993]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.2476,  0.6108,  0.3845,  0.1527,  0.4031,  0.0374,  0.3298,  0.2072,\n",
      "        -0.0176,  0.4346,  0.5840,  0.6553,  0.7275,  0.6177,  0.6606,  0.2515],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1669, -0.1584, -0.1429,  0.0059, -0.1858, -0.1874, -0.2856, -0.0367,\n",
      "        -0.3484,  0.0154, -0.1831, -0.1698, -0.2333, -0.2389,  0.0404,  0.2283],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.7850,  0.0903, -0.0940,  ...,  1.9261,  0.6823, -0.5281],\n",
      "         [ 0.6551, -0.0290,  0.0926,  ...,  1.4439,  0.7715,  0.1366],\n",
      "         [ 0.5991,  0.1679,  0.1954,  ...,  1.3166,  1.0325, -0.2600],\n",
      "         ...,\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991],\n",
      "         [-0.0102,  0.0585, -0.0227,  ..., -0.0781, -0.0292, -0.0991]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 2.4854e-01,  1.0361e+00,  4.8553e-02,  9.5605e-01, -3.7378e-01,\n",
      "         3.4692e-01, -3.9398e-02,  4.7241e-01, -2.1667e-01,  3.0938e+00,\n",
      "         6.2158e-01,  5.0934e-02,  2.9980e+00,  6.9580e-01,  3.0493e-01,\n",
      "        -1.4758e-01,  3.3809e+00,  1.1357e+00,  3.8208e-01,  3.2886e-01,\n",
      "        -6.9702e-02,  3.1035e+00,  5.2002e-01,  4.2969e+00,  9.5703e-01,\n",
      "         2.7734e-01,  1.5625e-02, -5.4169e-02,  4.1641e+00,  1.0713e+00,\n",
      "         3.9795e-01,  1.2756e-01,  5.2277e-02,  7.7637e-01,  3.8516e+00,\n",
      "         9.8926e-01, -7.4707e-02, -1.7981e-01,  2.4863e+00,  7.4805e-01,\n",
      "        -7.2900e-01,  4.7681e-01, -8.2910e-01, -1.0889e-01,  1.8857e+00,\n",
      "         1.7920e+00,  6.6406e-01,  1.8030e-01, -1.0962e-01,  1.1139e-01,\n",
      "         7.7515e-02,  4.0312e+00,  8.0127e-01,  6.0840e-01,  2.5024e-01,\n",
      "        -6.1523e-01, -2.7271e-01, -4.3237e-01,  2.9961e+00,  6.9727e-01,\n",
      "        -8.4521e-01, -3.7964e-01,  1.6016e+00, -5.1709e-01,  2.5527e+00,\n",
      "         5.2637e-01,  1.1318e+00,  3.7891e+00,  9.4580e-01,  4.1602e+00,\n",
      "         1.4092e+00,  5.2246e-01,  3.1836e-01,  2.0654e-01,  1.1641e+00,\n",
      "         4.6265e-01, -5.3857e-01, -2.4146e-01,  6.2744e-01,  4.6631e-01,\n",
      "         2.5330e-02,  3.0918e+00,  9.3359e-01, -7.0947e-01, -2.3682e-01,\n",
      "         5.5225e-01,  7.0923e-02, -4.0552e-01,  3.9160e+00,  1.2891e+00,\n",
      "         1.0078e+00,  1.3550e-01,  2.4004e+00,  1.1250e+00,  1.0811e+00,\n",
      "         1.1139e-01, -1.2988e-01, -5.4102e-01,  2.8027e+00,  4.5074e-02,\n",
      "        -3.5254e-01,  3.8159e-01, -2.3584e-01, -7.7686e-01, -2.0483e-01,\n",
      "         6.6040e-02, -5.4785e-01, -2.5952e-01, -5.3320e-01,  4.2212e-01,\n",
      "        -4.3359e-01,  2.3906e+00,  6.1279e-01, -1.2793e-01, -4.4775e-01,\n",
      "        -1.1365e-01,  2.5098e+00,  6.2988e-01, -1.8872e-01, -6.6101e-02,\n",
      "        -1.4664e-02,  3.9473e+00,  1.3223e+00,  3.3643e-01,  5.9521e-01,\n",
      "         4.9487e-01, -5.6836e-01,  2.1426e+00,  3.5938e-01, -9.0515e-02,\n",
      "         3.3618e-01, -6.4758e-02,  4.0078e+00,  8.5156e-01,  5.5615e-01,\n",
      "         3.4082e-01,  1.3855e-01,  1.0117e+00,  8.5449e-01,  1.0957e+00,\n",
      "         1.1523e+00,  2.4805e+00,  5.2832e-01,  1.3184e+00,  1.8281e+00,\n",
      "         6.5234e-01,  4.8615e-02,  1.0586e+00, -1.0039e+00, -6.7725e-01,\n",
      "        -5.1660e-01,  4.2266e+00,  1.0928e+00,  5.8990e-02,  8.4814e-01,\n",
      "         3.4961e-01,  1.2100e+00, -4.1455e-01,  7.4219e-01, -7.3096e-01,\n",
      "         3.8574e+00,  7.8418e-01,  1.5686e-01,  8.9893e-01,  1.0010e+00,\n",
      "         4.1484e+00,  1.3516e+00,  3.4180e-01,  1.1729e+00,  1.7363e+00,\n",
      "         1.0785e-01,  3.9917e-01,  4.0547e+00,  1.2715e+00,  1.8481e-01,\n",
      "         8.5596e-01,  1.4746e+00, -2.7328e-02,  3.0493e-01, -3.4814e-01,\n",
      "         3.7383e+00,  8.8232e-01,  3.6621e-01,  1.1481e-01,  2.4341e-01,\n",
      "        -6.2500e-01,  9.8242e-01,  1.5442e-01, -3.5742e-01,  6.3660e-02,\n",
      "        -3.7036e-01,  3.7988e+00,  9.4922e-01,  5.4199e-01,  2.2441e+00,\n",
      "         4.7607e-01,  1.5649e-01,  3.8926e+00,  1.2188e+00,  9.5154e-02,\n",
      "        -6.1279e-01,  3.5645e+00,  4.7632e-01, -1.3379e-01,  1.3989e-01,\n",
      "        -9.1019e-03, -4.6484e-01,  2.4590e+00,  3.3867e+00,  6.6748e-01,\n",
      "         7.2571e-02,  1.8774e-01, -3.4766e-01,  3.8184e+00,  1.1328e+00,\n",
      "         3.3234e-02, -8.9307e-01, -1.4539e-01,  2.6099e-01,  2.7295e-01,\n",
      "         4.1562e+00,  6.3525e-01, -3.0933e-01,  2.9980e-01, -2.4158e-01,\n",
      "         3.5605e+00,  5.6006e-01, -6.3591e-03,  4.9683e-01,  2.0273e+00,\n",
      "         4.2694e-02,  2.0007e-01, -7.1167e-02,  3.6992e+00,  7.3633e-01,\n",
      "        -2.1210e-02,  2.4629e+00,  5.7709e-02, -4.2822e-01,  4.1172e+00,\n",
      "         8.4082e-01, -1.9067e-01,  2.9238e+00,  4.7412e-01,  7.9688e-01,\n",
      "         1.0908e+00, -2.6123e-01,  7.9297e-01,  5.1758e-01,  2.8027e+00,\n",
      "         9.7229e-02,  1.2109e+00,  1.0162e-01, -1.0583e-01,  5.4443e-01,\n",
      "        -3.1519e-01,  2.2285e+00,  7.5623e-02,  1.1025e+00,  6.7566e-02,\n",
      "        -1.0376e-01,  6.1963e-01, -5.0781e-01, -7.3926e-01,  2.8730e+00,\n",
      "        -4.3604e-01, -3.4717e-01,  1.3306e-01,  4.5972e-01,  4.3286e-01,\n",
      "        -6.5186e-02,  8.2886e-02,  7.0190e-02, -3.2544e-01, -7.6318e-01,\n",
      "        -4.1895e-01,  1.9258e+00,  2.0166e-01, -9.9243e-02,  5.7764e-01,\n",
      "        -3.8989e-01,  5.2734e-01, -2.6001e-01,  4.5874e-01,  1.2861e+00,\n",
      "         4.8004e-02,  4.8950e-01,  1.6426e+00,  2.5635e-01,  4.6484e-01,\n",
      "         1.2900e+00, -3.5492e-02,  5.5939e-02,  1.1260e+00,  2.0544e-01,\n",
      "         2.0488e+00,  1.3477e-01,  2.5049e-01,  8.4717e-01,  9.1309e-01,\n",
      "         2.6973e+00,  2.8184e+00,  1.5898e+00,  6.7285e-01,  1.0322e+00,\n",
      "         5.0488e-01,  7.1484e-01,  1.1631e+00,  2.4961e+00,  3.0347e-01,\n",
      "         4.3921e-01,  2.6387e+00,  2.4160e+00,  1.3115e+00,  4.1357e-01,\n",
      "         7.4756e-01,  3.1299e-01,  4.7681e-01,  9.4482e-01,  2.0723e+00,\n",
      "         6.1981e-02,  3.4692e-01, -2.3950e-01,  4.0547e+00,  1.0654e+00,\n",
      "        -4.7546e-02,  4.3066e-01,  1.1738e+00,  5.1855e-01,  1.6504e+00,\n",
      "         1.7148e+00,  3.8086e-01,  9.2627e-01,  3.4766e-01,  1.1572e+00,\n",
      "         3.4961e-01,  7.9248e-01,  3.3130e-01, -2.0593e-01, -7.5977e-01,\n",
      "         5.9131e-01, -4.9146e-01,  1.6296e-01, -4.0796e-01,  3.8828e+00,\n",
      "         9.2725e-01,  3.8574e-01,  2.0098e+00,  4.7266e-01,  3.0615e-01,\n",
      "         3.7129e+00,  1.4834e+00,  6.3525e-01,  8.0469e-01,  3.0371e-01,\n",
      "         3.6768e-01,  3.7012e-01,  8.4277e-01, -4.0192e-02,  3.8086e+00,\n",
      "         7.3096e-01,  3.0103e-01, -4.5752e-01,  3.8320e+00,  7.7539e-01,\n",
      "         1.0236e-01,  2.8540e-01, -2.7930e-01,  2.4180e+00,  2.8442e-01,\n",
      "        -4.7852e-01, -1.5344e-01,  4.9438e-01, -8.3496e-01, -5.5078e-01,\n",
      "        -3.1030e-01, -5.9570e-01,  2.0020e+00,  3.1289e+00,  6.2891e-01,\n",
      "         1.1993e-01,  3.3472e-01,  4.5441e-02,  7.0508e-01, -8.3008e-01,\n",
      "         3.1776e-03,  1.6150e-01, -2.0581e-01,  2.6245e-01, -3.3545e-01,\n",
      "         2.2324e+00,  3.4512e+00,  4.9585e-01, -3.6230e-01,  2.5952e-01,\n",
      "        -1.0052e-01,  3.4219e+00,  7.9590e-01,  2.4316e+00,  1.4575e-01,\n",
      "         2.4004e+00,  5.1117e-02,  2.9824e+00,  6.3330e-01,  6.4307e-01,\n",
      "         2.0020e-01,  3.2832e+00,  1.2549e+00,  1.1481e-01,  3.1230e+00,\n",
      "         5.4004e-01, -1.2103e-01, -4.0601e-01,  3.5625e+00,  7.7734e-01,\n",
      "         8.6365e-02, -5.6488e-02, -2.3401e-01, -2.4780e-01,  7.9688e-01,\n",
      "        -1.7505e-01,  1.2500e-01,  9.3933e-02,  2.2290e-01, -1.2817e-01,\n",
      "         2.3633e+00,  3.2305e+00,  7.2266e-01,  1.9702e-01,  4.4019e-01,\n",
      "        -9.1309e-02, -6.4941e-01, -5.7275e-01, -5.1074e-01,  3.7422e+00,\n",
      "         1.3154e+00,  2.0957e+00,  4.2993e-01, -1.2231e-01,  4.2480e-01,\n",
      "        -1.2756e-01,  1.1699e+00, -5.7422e-01,  3.4453e+00,  9.4580e-01,\n",
      "         8.3984e-01,  9.6631e-01,  6.2891e-01, -1.6504e-01,  6.3916e-01,\n",
      "        -2.5513e-01,  1.7383e+00,  1.6973e+00,  3.3887e-01, -2.9663e-01,\n",
      "         5.7959e-01, -5.1074e-01,  4.8706e-01, -2.2070e-01,  3.6660e+00,\n",
      "         1.2451e+00,  1.0127e+00,  1.5420e+00,  6.9519e-02, -4.5068e-01,\n",
      "         3.5762e+00,  8.7012e-01,  7.6758e-01,  1.8477e+00,  1.2432e+00,\n",
      "         2.6538e-01, -1.6675e-01, -5.4736e-01,  3.4399e-01, -4.5410e-01,\n",
      "        -2.7100e-01,  9.5020e-01, -3.8916e-01,  6.6748e-01,  4.2114e-01,\n",
      "         4.4897e-01,  4.0391e+00,  7.6953e-01,  4.3652e-01,  1.8154e+00,\n",
      "        -4.6313e-01,  8.0225e-01,  9.7949e-01,  3.2891e+00,  5.1025e-01,\n",
      "        -9.9121e-01, -1.1298e-01, -4.2480e-02, -2.9492e-01, -1.3342e-01,\n",
      "         9.2871e-01, -8.4717e-02,  6.3362e-03, -6.5771e-01, -2.4304e-01,\n",
      "        -1.1367e+00,  3.8599e-01, -6.3086e-01,  3.0054e-01, -2.6978e-01,\n",
      "         3.8887e+00,  8.4766e-01,  5.0684e-01,  4.7217e-01,  4.8901e-01,\n",
      "         3.4316e+00,  2.8516e+00,  1.6870e-01,  5.8301e-01, -1.7505e-01,\n",
      "        -2.0703e-01, -4.8779e-01,  3.1270e+00,  1.0020e+00,  3.4106e-01,\n",
      "         3.5391e+00,  1.4824e+00,  8.8916e-01, -1.0266e-01,  8.0383e-02,\n",
      "         3.4473e+00,  7.1729e-01,  4.5703e-01,  8.9941e-01,  3.5449e+00,\n",
      "         9.3994e-01,  7.3584e-01,  1.2549e+00,  3.5566e+00,  9.9609e-01,\n",
      "         5.9766e-01,  2.3926e-01,  2.4141e+00,  2.0938e+00,  1.1709e+00,\n",
      "         8.6230e-01,  8.3496e-01,  3.2446e-01,  3.1367e+00,  9.3457e-01,\n",
      "         9.7351e-02,  3.2988e+00,  9.5703e-01, -1.0201e-02,  2.6035e+00,\n",
      "         6.3086e-01, -9.4299e-02,  3.0391e+00,  1.4355e+00,  8.6377e-01,\n",
      "         4.1870e-01,  2.5391e+00,  7.0996e-01, -6.3232e-02,  3.5352e+00,\n",
      "         5.7910e-01,  6.1426e-01,  1.6321e-01,  3.4707e+00,  1.1650e+00,\n",
      "         8.5107e-01,  2.7026e-01,  2.1948e-01, -4.2920e-01, -1.9629e-01,\n",
      "         3.3594e+00,  2.6309e+00,  8.3887e-01,  6.6846e-01, -9.0771e-01,\n",
      "        -2.0776e-01, -4.2847e-01,  3.8574e-01,  7.1045e-01,  3.6309e+00,\n",
      "         9.5459e-01,  7.8613e-01,  5.4150e-01,  4.5947e-01,  3.9883e+00,\n",
      "         8.3838e-01,  2.7661e-01,  4.0586e+00,  6.5430e-01,  3.7793e-01,\n",
      "         3.9043e+00,  7.6074e-01,  2.7979e-01, -9.6631e-01,  1.0420e+00,\n",
      "        -1.9910e-01,  2.4316e+00, -1.0602e-01,  3.6289e+00,  9.3652e-01,\n",
      "         5.1221e-01,  1.2852e+00,  1.4961e+00, -3.7646e-01,  3.2246e+00,\n",
      "         6.8408e-01,  1.7051e+00, -4.5166e-02,  2.1704e-01, -8.1885e-01,\n",
      "         6.9153e-02, -9.6582e-01,  2.3477e+00,  1.1104e+00,  6.6357e-01,\n",
      "        -4.3610e-02,  1.3110e-01, -6.8176e-02, -5.1367e-01, -8.6719e-01,\n",
      "         8.8232e-01, -2.4988e-01,  1.1884e-01,  1.6687e-01, -9.9414e-01,\n",
      "         2.7100e-01, -3.8574e-01, -5.2393e-01,  3.3828e+00,  4.6118e-01,\n",
      "         1.3809e+00, -1.9241e-02,  6.0107e-01,  4.0308e-01,  4.1484e+00,\n",
      "         8.6523e-01,  4.8364e-01,  3.9258e+00,  9.0137e-01,  3.9624e-01,\n",
      "         3.4863e+00,  1.2930e+00,  8.9014e-01,  1.9934e-01,  1.2900e+00,\n",
      "         1.0126e-01,  4.5996e-01,  2.6270e-01,  6.6357e-01,  5.8350e-01,\n",
      "        -7.2815e-02,  1.3831e-01,  5.0244e-01, -1.5881e-01,  1.2578e+00,\n",
      "         3.4141e+00,  8.2422e-01,  2.6328e+00,  1.1553e+00,  9.8877e-02,\n",
      "         6.4404e-01,  3.9414e+00,  9.3311e-01,  5.8936e-01,  3.3828e+00,\n",
      "         1.1602e+00,  7.9395e-01,  1.4026e-01,  6.1768e-01, -3.6230e-01,\n",
      "        -8.7061e-01,  7.5342e-01,  2.6367e-01,  4.0308e-01,  3.0957e-01,\n",
      "         6.6284e-02,  3.4199e+00,  7.8271e-01, -4.4897e-01,  2.2383e+00,\n",
      "        -7.7588e-01,  1.6709e+00,  4.0664e+00,  1.0850e+00,  7.6172e-01,\n",
      "        -5.6055e-01,  2.7002e-01,  4.0820e-01,  1.3232e-01,  3.8027e+00,\n",
      "         1.4561e+00,  2.3462e-01,  1.7227e+00,  8.8989e-02, -1.2432e+00,\n",
      "        -6.5723e-01, -4.4019e-01,  3.4551e+00,  7.1680e-01,  4.0781e+00,\n",
      "         1.1279e+00,  6.9092e-01, -4.3823e-01,  9.1064e-01, -1.9092e-01,\n",
      "         3.6543e+00,  9.1162e-01,  8.1104e-01,  2.6719e+00,  7.6221e-01,\n",
      "         3.8125e+00,  1.2471e+00,  1.0635e+00,  5.6201e-01,  3.5527e+00,\n",
      "         1.3164e+00,  1.0068e+00,  6.1035e-01,  4.8169e-01,  3.4688e+00,\n",
      "         1.0596e+00,  8.3887e-01,  4.0747e-01,  3.5098e+00,  1.4072e+00,\n",
      "         2.9883e-01,  3.3965e+00,  1.1230e+00,  4.3921e-01,  5.7037e-02,\n",
      "         2.4824e+00,  6.3672e-01,  1.9375e+00,  3.5605e+00,  1.3672e+00,\n",
      "         5.1416e-01,  2.7686e-01,  3.8477e+00,  1.5137e+00,  8.3936e-01,\n",
      "         4.2944e-01,  2.2058e-01,  3.1992e+00,  8.9307e-01,  2.0430e+00,\n",
      "         2.6270e-01,  1.6377e+00,  1.3660e-01,  5.4199e-01,  5.8594e-01,\n",
      "        -9.4531e-01,  4.1367e+00,  1.5469e+00,  1.1396e+00,  6.0840e-01,\n",
      "        -3.3374e-01, -3.9746e-01, -2.2070e-01, -6.6895e-01,  1.0492e-01,\n",
      "         1.1377e+00, -3.5425e-01,  3.7134e-01, -8.0859e-01,  4.1445e+00,\n",
      "         1.0850e+00,  7.5244e-01, -2.9346e-01, -2.8003e-01,  3.8691e+00,\n",
      "         1.3096e+00,  6.1475e-01,  7.2852e-01, -2.8183e-02, -1.7700e-01,\n",
      "         1.2861e+00,  8.3374e-02,  3.7969e+00,  1.1738e+00,  7.0801e-01,\n",
      "         3.0444e-01,  3.9297e+00,  9.3164e-01,  5.4346e-01,  1.2238e-01,\n",
      "         1.8768e-02, -3.6255e-01,  6.8262e-01,  6.4697e-01, -4.3481e-01,\n",
      "         4.6338e-01, -3.8208e-01, -4.0924e-02, -5.2094e-02,  3.3252e-01,\n",
      "        -1.2042e-01,  4.0703e+00,  1.0469e+00,  7.5977e-01, -5.1123e-01,\n",
      "         1.1865e+00, -6.9824e-01,  1.5247e-01,  3.3047e+00,  2.7197e-01,\n",
      "        -1.6956e-01,  3.8965e-01,  5.3174e-01, -4.9706e-03,  1.0773e-01,\n",
      "         7.3047e-01,  1.0527e+00,  1.0146e+00, -1.0361e+00,  3.8672e+00,\n",
      "         9.9023e-01,  8.3350e-01,  7.2607e-01, -4.6997e-01, -3.6621e-02,\n",
      "        -4.8553e-02, -4.0356e-01,  4.0698e-01, -1.3408e+00,  9.0759e-02,\n",
      "         6.4893e-01, -5.9229e-01,  1.5076e-01, -3.1616e-01,  3.9102e+00,\n",
      "         1.4648e+00,  7.9736e-01,  2.2402e+00, -3.1958e-01,  3.6758e+00,\n",
      "         7.9932e-01,  6.3428e-01,  9.9121e-01, -4.8730e-01,  7.1973e-01,\n",
      "        -3.5107e-01,  1.8530e-01, -2.2974e-01, -5.7459e-05, -3.0859e-01,\n",
      "         1.4709e-01, -7.6074e-01,  4.7583e-01, -7.4902e-01,  3.8794e-01,\n",
      "         4.4727e-01,  1.3159e-01,  9.5312e-01, -7.1875e-01, -2.0862e-01,\n",
      "        -1.1774e-01,  5.2295e-01,  2.0801e-01,  2.6660e-01,  3.7964e-01,\n",
      "         6.5088e-01,  3.6621e+00,  1.6152e+00,  7.2217e-01,  4.3359e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.4131e+00,  3.7109e-01,  8.2422e-01, -8.6816e-01, -8.8867e-01,\n",
      "        -4.5142e-01, -1.4287e+00,  4.4342e-02, -3.6206e-01, -4.6191e-01,\n",
      "         2.6880e-01,  2.2402e+00,  2.4231e-01,  5.4199e-02,  2.0957e+00,\n",
      "        -8.3984e-01, -1.2634e-01,  4.6558e-01,  4.5190e-01,  7.6172e-01,\n",
      "         2.4590e+00, -7.0215e-01,  7.4316e-01, -3.8647e-01,  4.1602e-01,\n",
      "         6.3184e-01,  9.4385e-01,  3.6543e+00, -1.3489e-01,  4.5605e-01,\n",
      "         6.4844e-01,  9.4531e-01,  3.4961e+00, -5.0446e-02, -1.5762e-02,\n",
      "         2.8394e-01,  3.5742e+00, -6.0156e-01,  6.3086e-01,  1.9316e+00,\n",
      "         1.2878e-01, -4.8584e-01, -6.5527e-01, -5.6201e-01,  1.2256e-01,\n",
      "        -9.8083e-02,  1.0635e-02,  1.5303e+00,  1.8936e+00, -6.0693e-01,\n",
      "        -5.5371e-01,  5.5115e-02,  3.0811e-01,  7.0654e-01,  2.8340e+00,\n",
      "        -1.4636e-01,  1.0168e-01, -1.0260e-01,  5.1855e-01,  2.3457e+00,\n",
      "        -7.2266e-01, -7.3145e-01, -2.3108e-01, -1.8457e-01,  1.8335e-01,\n",
      "         1.0059e+00,  1.7939e+00,  5.3369e-01,  3.2031e+00,  1.2421e-02,\n",
      "         5.9375e-01,  7.4854e-01,  1.3271e+00,  3.7539e+00,  6.7432e-01,\n",
      "         2.3477e+00, -5.9912e-01, -2.3499e-01,  3.4229e-01,  5.8643e-01,\n",
      "        -1.8896e-01, -3.3081e-01,  2.7559e+00, -7.6465e-01, -8.6133e-01,\n",
      "        -1.0596e+00, -4.8096e-01, -4.1577e-01,  3.1616e-01,  4.6729e-01,\n",
      "         3.7266e+00, -3.7744e-01,  2.9272e-01,  1.8984e+00,  2.3008e+00,\n",
      "        -3.4277e-01, -1.0713e+00, -1.6904e+00, -2.9321e-01,  5.9033e-01,\n",
      "         2.3132e-01,  1.1823e-01,  2.9251e-02, -1.0967e+00, -4.4287e-01,\n",
      "         4.0741e-02, -9.6130e-02, -1.1445e+00, -8.8916e-01,  9.5654e-01,\n",
      "        -1.4775e+00, -6.6846e-01, -9.2041e-02,  1.0186e+00, -5.9277e-01,\n",
      "        -1.1816e+00, -5.3320e-01, -7.6538e-02,  1.3262e+00, -2.8491e-01,\n",
      "        -6.9727e-01, -6.0699e-02,  1.4587e-01,  2.9668e+00,  1.2051e+00,\n",
      "         3.7744e-01,  1.6089e-01, -8.2568e-01,  2.7856e-01,  6.0742e-01,\n",
      "         5.2979e-01,  1.5991e-01,  3.5669e-01,  9.2773e-02,  5.4102e-01,\n",
      "         3.1172e+00, -5.6299e-01,  1.3643e+00,  1.7139e-01, -3.9111e-01,\n",
      "         1.1934e+00,  5.2051e-01,  2.4707e+00,  1.9717e+00, -1.6785e-01,\n",
      "        -3.0594e-02,  7.3389e-01,  1.0254e+00, -1.8665e-01, -8.5449e-01,\n",
      "        -6.3965e-01,  3.6816e-01,  3.8184e-01,  3.0840e+00,  2.5625e+00,\n",
      "        -9.0674e-01, -5.0098e-01,  7.6709e-01, -3.7061e-01,  3.5742e-01,\n",
      "        -1.2250e-01,  2.2937e-01,  3.7598e+00,  2.5122e-01, -2.8052e-01,\n",
      "        -4.4629e-01,  8.1848e-02,  3.3496e+00, -8.4277e-01, -1.7593e-02,\n",
      "         2.9512e+00,  2.3828e+00, -5.7471e-01, -2.1350e-01,  2.7246e+00,\n",
      "        -1.0596e+00, -8.9874e-03,  2.9805e+00,  1.7432e+00, -8.0078e-01,\n",
      "        -4.0771e-01, -2.5342e-01,  6.7529e-01,  2.0254e+00,  2.0977e+00,\n",
      "        -8.7061e-01, -6.0254e-01, -3.0502e-02, -2.6270e-01,  8.5156e-01,\n",
      "         1.8372e-01,  9.4580e-01,  1.0879e+00,  3.4902e+00,  1.8584e+00,\n",
      "         3.7656e+00, -1.2314e+00, -1.4929e-01,  1.4587e-01,  3.5938e+00,\n",
      "        -7.0312e-01, -3.2471e-01,  1.4661e-01,  3.3945e+00,  9.8877e-01,\n",
      "        -9.6289e-01, -7.4609e-01,  5.9021e-02,  2.9980e-01,  6.0449e-01,\n",
      "         3.3672e+00,  1.2607e+00, -1.0898e+00, -4.6094e-01,  1.3074e-01,\n",
      "         3.4512e+00, -1.3252e+00, -1.3831e-01, -5.9229e-01, -8.6719e-01,\n",
      "         5.0262e-02,  8.8721e-01,  2.9434e+00,  2.5371e+00, -1.1748e+00,\n",
      "        -5.9326e-01,  1.0797e-01,  3.3691e+00, -2.4109e-01, -1.3867e-01,\n",
      "         3.0410e+00, -6.6064e-01, -9.5996e-01, -5.3564e-01, -1.7944e-01,\n",
      "         2.8594e+00, -1.5833e-01,  2.6152e+00, -9.4922e-01,  1.3843e-01,\n",
      "         1.4612e-01,  2.9941e+00, -9.7717e-02,  2.4512e-01,  4.3652e-01,\n",
      "         8.7354e-01,  3.7129e+00,  2.7539e-01,  7.1387e-01, -5.2686e-01,\n",
      "         1.9934e-01, -6.2988e-01, -1.1664e-01,  6.9519e-02, -5.5939e-02,\n",
      "         1.2734e+00, -4.0308e-01,  1.1639e-01, -7.4756e-01, -6.1798e-02,\n",
      "         1.0565e-01,  3.7622e-01,  2.0508e+00, -5.3564e-01,  1.1758e+00,\n",
      "        -6.9946e-02, -1.3989e-01, -8.1445e-01, -1.1566e-01,  4.5483e-01,\n",
      "        -4.4556e-01, -3.2812e-01,  1.8555e-01, -4.2017e-01, -3.8916e-01,\n",
      "        -8.4912e-01, -9.3018e-01, -5.9967e-02,  8.8989e-02, -7.9224e-02,\n",
      "         6.0010e-01,  4.4617e-02,  3.0566e-01, -3.0591e-01,  7.8613e-02,\n",
      "         3.9722e-01,  4.1284e-01, -4.9023e-01,  3.6670e-01,  1.0176e+00,\n",
      "        -2.4377e-01,  6.5137e-01, -4.9023e-01,  5.8838e-01,  8.4131e-01,\n",
      "         1.6815e-02,  4.6631e-01,  2.8477e+00,  1.2805e-01, -3.1616e-01,\n",
      "         8.9990e-01, -2.1851e-01,  1.1154e-02, -6.7261e-02,  3.1030e-01,\n",
      "         2.7344e-01,  2.5195e+00, -7.6367e-01, -5.4718e-02,  2.6191e+00,\n",
      "         2.8105e+00,  3.4595e-01, -1.6125e-01, -1.8778e-03, -4.8370e-02,\n",
      "         1.8298e-01,  1.5320e-01,  1.9219e+00, -8.9014e-01, -1.9214e-01,\n",
      "         2.4492e+00,  2.4238e+00, -1.0322e+00,  1.8478e-02,  1.9727e-01,\n",
      "         2.2422e+00,  1.9062e+00,  3.1421e-01,  9.0088e-01, -1.0439e+00,\n",
      "        -3.8910e-02,  6.1230e-01, -5.7404e-02,  8.3350e-01, -5.7568e-01,\n",
      "         1.0273e+00,  1.9531e-01,  4.0063e-01,  2.3398e+00, -8.5010e-01,\n",
      "        -4.4653e-01, -2.9395e-01,  1.1035e+00, -2.4426e-01,  6.5820e-01,\n",
      "         3.8330e-01,  3.5391e+00,  1.8223e+00,  3.7051e+00, -1.0381e+00,\n",
      "        -1.0760e-01,  1.6980e-01,  1.5707e-03,  4.2236e-01,  4.8975e-01,\n",
      "         2.9980e+00, -1.0297e-01,  4.6924e-01,  2.4011e-01,  6.2842e-01,\n",
      "         2.8906e-01,  3.5410e+00, -7.6709e-01,  2.2266e-01, -3.8544e-02,\n",
      "         2.7598e+00,  2.4141e+00, -1.3232e+00,  4.2389e-02,  5.1123e-01,\n",
      "         1.5938e+00, -8.5693e-01, -5.7178e-01, -1.2734e+00, -5.5078e-01,\n",
      "        -6.7285e-01, -1.1641e+00,  3.6768e-01,  4.6289e-01,  4.2505e-01,\n",
      "         3.5586e+00,  2.2480e+00, -4.2407e-01, -7.5781e-01, -5.7324e-01,\n",
      "        -1.6907e-01,  8.7695e-01, -7.8662e-01,  5.6201e-01, -6.3916e-01,\n",
      "         4.4775e-01, -1.2115e-01,  8.3057e-01,  3.0078e+00,  2.9375e+00,\n",
      "        -1.0010e+00, -2.3572e-01,  1.8721e+00,  1.8140e-01,  2.8008e+00,\n",
      "         1.2988e-01,  2.3457e+00,  2.3169e-01, -9.8419e-03,  2.7393e-01,\n",
      "         2.5371e+00, -4.6411e-01,  2.5488e-01,  3.4902e+00, -9.0881e-02,\n",
      "         3.9038e-01,  3.0156e+00, -7.9712e-02,  1.5906e-01, -4.2206e-02,\n",
      "         2.1934e+00,  2.2539e+00, -5.8154e-01, -1.0127e+00,  1.7461e+00,\n",
      "        -5.8740e-01,  6.5155e-03,  8.4229e-02, -3.2202e-01, -4.0747e-01,\n",
      "         4.7021e-01,  4.6265e-01,  4.0649e-01,  3.5469e+00,  2.3398e+00,\n",
      "        -9.9463e-01, -1.0430e+00, -2.5073e-01, -8.2227e-01, -6.3232e-01,\n",
      "         1.1768e+00,  9.4287e-01,  7.4365e-01,  3.1738e+00,  1.4922e+00,\n",
      "        -3.8550e-01, -1.8997e-02, -1.1973e+00, -1.7957e-01,  7.4585e-02,\n",
      "         6.2695e-01,  8.5156e-01,  7.6807e-01,  3.0781e+00,  5.7568e-01,\n",
      "        -1.5112e-01,  6.1371e-02, -6.1621e-01,  2.1543e+00, -2.5488e-01,\n",
      "         1.4734e-01, -9.8877e-01, -6.2646e-01, -3.8379e-01, -4.8730e-01,\n",
      "        -7.7515e-02,  1.4580e+00,  5.0098e-01,  3.3457e+00, -4.5557e-01,\n",
      "         1.4307e-01, -1.0797e-01,  8.1787e-01,  5.8105e-01,  7.7783e-01,\n",
      "         8.6670e-01,  2.7949e+00,  7.5439e-01, -4.3433e-01, -7.3364e-02,\n",
      "        -4.6680e-01,  5.9387e-02, -5.8936e-01,  4.1406e-01, -2.8540e-01,\n",
      "         6.4795e-01,  1.1487e-01,  1.3428e-01,  3.4766e+00,  1.2461e+00,\n",
      "        -1.9446e-01,  3.0078e-01,  2.6074e+00, -2.0615e-02,  3.5293e+00,\n",
      "         2.5195e-01, -5.5859e-01, -2.4304e-01, -5.6152e-01, -6.4648e-01,\n",
      "        -1.9348e-01,  2.2461e-01,  1.1787e+00, -9.3994e-01, -3.3374e-01,\n",
      "        -4.3018e-01,  1.5515e-01, -7.2217e-01,  7.9199e-01, -8.0688e-02,\n",
      "         5.5420e-01,  2.4561e-01,  3.7871e+00, -5.2460e-02,  7.0850e-01,\n",
      "         4.1431e-01,  9.9945e-03,  2.3848e+00,  2.7500e+00,  3.2544e-01,\n",
      "         2.1406e+00, -1.2578e+00, -7.0898e-01,  6.7578e-01,  3.6719e+00,\n",
      "        -1.5381e-01,  3.4692e-01,  1.5615e+00,  3.2793e+00,  2.5312e+00,\n",
      "         6.4941e-01,  9.3506e-01,  3.4336e+00,  2.3496e+00,  1.1261e-01,\n",
      "         5.4688e-01,  1.1191e+00,  5.9277e-01, -2.4451e-01,  1.3440e-01,\n",
      "         1.0264e+00,  3.7266e+00, -6.4062e-01,  7.1436e-01,  8.2666e-01,\n",
      "         4.9438e-01,  1.4658e+00,  3.0625e+00, -4.1577e-01,  1.2152e-01,\n",
      "         3.2812e+00, -4.1406e-01,  6.4258e-01,  3.0059e+00, -4.8389e-01,\n",
      "         6.2207e-01,  2.6719e+00, -3.7793e-01,  1.0907e-01,  3.4863e-01,\n",
      "         2.3672e+00,  2.1533e-01,  1.0156e+00,  3.2383e+00,  3.2788e-01,\n",
      "         4.8877e-01,  9.9512e-01,  3.2344e+00, -3.8623e-01,  4.1699e-01,\n",
      "         6.1230e-01,  1.0850e+00,  3.5820e+00, -6.8298e-02,  4.8682e-01,\n",
      "         3.4277e-01, -3.5693e-01,  3.7280e-01,  3.3594e+00, -2.1436e-01,\n",
      "        -9.6533e-01, -7.4219e-01, -1.3046e-02, -5.6934e-01, -3.1323e-01,\n",
      "         4.2090e-01,  3.4395e+00,  3.6816e-01,  6.5869e-01, -2.5293e-01,\n",
      "         6.3037e-01,  4.2461e+00,  7.9541e-01,  8.0566e-01,  4.1250e+00,\n",
      "        -2.8174e-01,  5.0830e-01,  3.9160e+00, -6.6797e-01,  3.1494e-01,\n",
      "        -8.2471e-01,  7.9834e-01,  9.2920e-01,  7.5732e-01,  6.5674e-01,\n",
      "         3.7461e+00,  2.2695e+00,  1.7188e+00,  1.1025e+00,  9.3896e-01,\n",
      "         3.9707e+00,  7.7686e-01,  2.9531e+00, -1.2646e+00, -1.8982e-01,\n",
      "        -8.8232e-01, -1.4365e+00, -1.9006e-01,  8.4961e-02,  2.5859e+00,\n",
      "         1.3945e+00, -5.1562e-01, -1.3831e-01, -3.1152e-01, -6.2305e-01,\n",
      "         5.3497e-02,  9.2725e-01,  6.5460e-03,  1.5121e-02, -1.4424e+00,\n",
      "        -1.0205e+00, -6.1816e-01, -9.4531e-01, -4.1962e-02,  3.6836e+00,\n",
      "         9.6484e-01,  3.2051e+00,  1.6602e-01,  5.8447e-01,  1.5417e-01,\n",
      "         4.9316e-01,  3.9707e+00,  5.3027e-01,  5.3076e-01,  3.9141e+00,\n",
      "         5.0018e-02,  4.6997e-01,  1.9092e+00,  4.0391e+00,  1.5613e-01,\n",
      "        -1.3895e-03,  1.1676e-01,  1.2537e-01, -2.5903e-01, -3.9038e-01,\n",
      "         4.3213e-01,  1.7322e-01,  6.9775e-01,  1.1328e+00,  8.0469e-01,\n",
      "        -4.4409e-01,  1.4844e+00,  9.0527e-01,  1.3271e+00,  3.8359e+00,\n",
      "         6.4160e-01,  2.1594e-01,  3.1494e-01,  3.6133e+00, -1.4709e-01,\n",
      "         3.7256e-01,  1.6680e+00,  3.9766e+00,  2.9150e-01,  1.7451e+00,\n",
      "        -7.7490e-01, -2.6050e-01, -3.1555e-02, -8.3008e-01, -1.3879e-01,\n",
      "        -7.8760e-01,  5.7178e-01,  3.9023e+00, -4.4287e-01,  1.0078e+00,\n",
      "         2.0935e-01,  1.5732e+00,  1.1060e-01,  4.0845e-01,  3.6836e+00,\n",
      "        -6.9141e-01, -3.3643e-01, -1.1200e-01, -7.8760e-01, -5.3857e-01,\n",
      "         9.3701e-01,  3.8184e+00,  2.9141e+00, -9.2676e-01, -1.4414e+00,\n",
      "        -4.4067e-01, -8.4570e-01,  4.8022e-01,  3.8047e+00,  2.6367e-01,\n",
      "         4.3042e-01,  3.6387e+00,  3.5254e-01, -3.4497e-01, -4.5264e-01,\n",
      "         1.3947e-02,  7.3730e-01,  3.0215e+00,  5.2100e-01,  3.0508e+00,\n",
      "        -3.6108e-01,  9.2590e-02,  8.5107e-01,  3.9707e+00, -3.2251e-01,\n",
      "         1.7456e-02,  4.2285e-01,  1.5703e+00,  3.7207e+00, -2.8467e-01,\n",
      "         3.9551e-01,  1.0518e+00,  3.9688e+00, -3.4277e-01,  1.1504e+00,\n",
      "         3.8809e+00, -3.3423e-01,  5.4932e-01,  3.6992e+00, -6.1523e-01,\n",
      "         1.6577e-01,  2.7969e+00,  1.3359e+00, -4.3408e-01,  5.6213e-02,\n",
      "         4.7461e-01,  3.6328e+00, -1.2482e-01,  3.6572e-01,  1.0215e+00,\n",
      "         3.9199e+00, -7.2266e-01, -6.0822e-02,  1.3535e+00,  1.1973e+00,\n",
      "         3.1523e+00, -1.6739e-02,  8.8721e-01, -1.2725e+00, -6.0205e-01,\n",
      "        -8.3838e-01,  3.3447e-01, -1.1810e-01,  8.4180e-01,  2.3477e+00,\n",
      "         4.1406e-01, -1.3984e+00, -7.3877e-01, -9.3945e-01, -8.1885e-01,\n",
      "        -1.0504e-01, -1.0508e+00,  1.0150e-01,  1.9336e-01,  1.4331e-01,\n",
      "         5.7520e-01,  3.6113e+00, -9.8193e-01, -4.2334e-01,  1.5411e-02,\n",
      "         8.5986e-01,  3.6973e+00,  1.1017e-01,  4.5532e-01, -1.6348e+00,\n",
      "        -9.2712e-02, -1.6895e-01, -1.2927e-01,  3.1714e-01,  3.3828e+00,\n",
      "        -5.4932e-01,  3.4375e-01,  2.3755e-01,  3.3145e+00, -1.4834e+00,\n",
      "        -3.1689e-01,  7.1729e-01, -6.4746e-01, -3.9087e-01, -9.6533e-01,\n",
      "        -4.4287e-01,  4.1431e-01, -8.3643e-01, -8.0078e-01, -4.2822e-01,\n",
      "        -4.1473e-02,  4.7925e-01,  6.7578e-01,  3.6367e+00, -1.0430e+00,\n",
      "         1.0625e+00, -9.8682e-01, -8.0322e-01,  1.8420e-01,  1.4619e+00,\n",
      "        -2.1729e-01, -9.4971e-01, -1.8665e-01,  3.0960e-02,  1.3257e-01,\n",
      "        -1.2711e-02,  8.3984e-02, -9.1846e-01, -3.3130e-01, -1.4075e-01,\n",
      "         7.5195e-01,  4.0078e+00,  5.1788e-02, -9.3652e-01,  7.6721e-02,\n",
      "        -6.1768e-01, -1.0156e+00, -5.9521e-01, -8.1152e-01, -3.4668e-01,\n",
      "        -4.7803e-01, -9.8682e-01, -2.2107e-01, -5.8447e-01, -1.2128e-01,\n",
      "         7.2705e-01,  3.8262e+00,  6.8457e-01,  8.3887e-01, -2.5098e-01,\n",
      "         7.2021e-01,  4.0273e+00, -6.5234e-01, -2.0313e-03, -3.0200e-01,\n",
      "        -7.4561e-01, -5.8594e-01, -5.4785e-01, -7.7820e-02, -3.1689e-01,\n",
      "        -1.7261e-01, -9.8779e-01, -7.4609e-01, -7.0703e-01, -6.7676e-01,\n",
      "        -3.6963e-01, -1.3379e+00, -4.4214e-01, -1.1982e+00, -1.2139e+00,\n",
      "        -9.2090e-01, -2.1838e-01, -8.2471e-01, -1.0004e-01, -6.5332e-01,\n",
      "        -1.4084e-02, -5.3125e-01,  7.8613e-01,  3.6836e+00,  6.6455e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(5.3854, device='cuda:0')\tavg_answer_loss:  tensor(4.8687, device='cuda:0')\tavg_type_loss:  tensor(0.1034, device='cuda:0')\tavg_val_f1:  0.0\tavg_val_em:  0.0\tavg_val_prec:  0.0\tavg_val_recall:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.0 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00004: avg_val_f1 reached 0.00000 (best 0.12353), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_4.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.0316e+00,  2.5643e-01, -3.9115e-01,  ...,  1.2963e+00,\n",
      "           8.7722e-01, -2.6191e-01],\n",
      "         [ 2.1327e-01,  6.2534e-01,  8.2497e-02,  ...,  1.0651e+00,\n",
      "           3.5865e-01, -2.4098e-01],\n",
      "         [ 1.1645e+00,  5.0961e-02,  8.7885e-02,  ...,  7.9419e-01,\n",
      "           8.7891e-01, -5.9704e-01],\n",
      "         ...,\n",
      "         [-1.4056e-02,  7.3354e-02, -1.0317e-02,  ..., -3.6255e-02,\n",
      "           1.9414e-04, -1.0202e-01],\n",
      "         [-8.5495e-03,  4.7897e-02, -5.6827e-03,  ..., -8.1570e-02,\n",
      "          -1.8639e-02, -1.2270e-01],\n",
      "         [-9.7032e-03,  5.7671e-02,  4.8857e-03,  ..., -7.6355e-02,\n",
      "          -4.1608e-02, -9.9946e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7629,  0.1602, -0.4137,  ...,  1.6839,  1.4657,  0.1300],\n",
      "         [ 0.1667,  0.2829, -0.1534,  ...,  1.0029,  0.7938, -0.3442],\n",
      "         [ 0.5715,  0.4781, -0.7499,  ...,  1.9170,  1.0646, -0.0814],\n",
      "         ...,\n",
      "         [-0.0135,  0.0534, -0.0170,  ..., -0.0668, -0.0193, -0.0720],\n",
      "         [-0.0170,  0.0397, -0.0291,  ..., -0.0822, -0.0284, -0.1009],\n",
      "         [-0.0280,  0.0879, -0.0360,  ..., -0.0325, -0.0332, -0.2071]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2446,  0.2049, -0.2777,  ...,  1.7347,  1.3011, -0.1288],\n",
      "         [ 0.1932, -0.0247, -0.3170,  ...,  0.6480,  0.6365,  0.1823],\n",
      "         [ 0.9757,  0.4424, -0.2945,  ...,  0.4419,  1.2154, -0.0733],\n",
      "         ...,\n",
      "         [ 0.0052,  0.0402, -0.0143,  ..., -0.0852, -0.0264, -0.0762],\n",
      "         [-0.0875,  0.2378, -0.0994,  ...,  0.1500,  0.0713, -0.1018],\n",
      "         [-0.0063,  0.0523, -0.0130,  ..., -0.0817, -0.0320, -0.0724]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.2823e-01,  1.6481e-01, -5.5249e-01,  ...,  1.9970e+00,\n",
      "           7.6155e-01, -5.1451e-01],\n",
      "         [ 6.1898e-01, -5.3519e-01, -2.6785e-01,  ...,  1.4587e+00,\n",
      "           6.8452e-01,  2.7760e-01],\n",
      "         [ 6.6916e-01,  3.0176e-01, -1.1150e-01,  ...,  2.6430e+00,\n",
      "           7.3605e-01, -7.1539e-01],\n",
      "         ...,\n",
      "         [ 1.4350e-03,  4.2401e-02, -8.7878e-03,  ..., -7.8639e-02,\n",
      "          -1.5766e-02, -7.3441e-02],\n",
      "         [-1.2383e-02,  5.1104e-02, -3.1114e-02,  ..., -8.1245e-02,\n",
      "          -4.4912e-02,  3.3436e-02],\n",
      "         [ 6.8719e-03,  6.2874e-02, -2.5644e-02,  ..., -8.6733e-02,\n",
      "          -2.7904e-02, -1.0321e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2014,  0.1277, -0.1816,  ...,  2.3103,  0.4816, -0.2772],\n",
      "         [ 0.0365,  0.6558, -0.2850,  ...,  0.6180,  0.3936, -0.2816],\n",
      "         [ 0.1604,  0.6077, -0.3475,  ...,  1.3333,  0.9667, -0.3768],\n",
      "         ...,\n",
      "         [-0.0082,  0.0816, -0.0142,  ..., -0.0807, -0.0260, -0.0817],\n",
      "         [-0.0156,  0.0600, -0.0194,  ..., -0.0927, -0.0345, -0.0922],\n",
      "         [-0.0246,  0.1932,  0.0243,  ..., -0.0444, -0.0594, -0.1778]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7849,  0.5374, -0.5017,  ...,  1.7127,  0.9573, -0.3667],\n",
      "         [-0.0432,  0.5670, -0.2029,  ...,  0.5452,  0.5144, -0.5053],\n",
      "         [ 0.8509,  0.6742, -0.5990,  ...,  0.8950,  0.7066, -0.6620],\n",
      "         ...,\n",
      "         [-0.0549,  0.6355,  0.0784,  ..., -0.2306,  0.3244, -0.4170],\n",
      "         [-0.0381,  0.0850, -0.0441,  ..., -0.0305, -0.0194, -0.2358],\n",
      "         [-0.0046,  0.0496, -0.0125,  ..., -0.0848, -0.0120, -0.0650]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0024,  0.3545, -0.5390,  ...,  1.5695,  1.1769, -0.2761],\n",
      "         [ 0.2380,  0.7517, -0.6055,  ...,  1.0649,  0.6169,  0.1353],\n",
      "         [ 0.0885,  0.5542, -0.4479,  ...,  1.7805,  0.5246, -0.5298],\n",
      "         ...,\n",
      "         [-0.0118,  0.0554, -0.0204,  ..., -0.0848, -0.0276, -0.0778],\n",
      "         [-0.0245,  0.0459, -0.0208,  ..., -0.0662, -0.0285, -0.0857],\n",
      "         [-0.0133,  0.0539, -0.0167,  ..., -0.1096, -0.0150, -0.0863]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.0322e-01,  7.4672e-01, -4.9351e-01,  ...,  1.5371e+00,\n",
      "           1.2603e+00, -6.4724e-01],\n",
      "         [ 1.7252e-01,  6.4574e-01,  4.6210e-01,  ...,  1.2231e+00,\n",
      "           6.6651e-01, -1.3271e-01],\n",
      "         [ 6.9751e-02,  6.0158e-01, -5.5273e-01,  ...,  4.6021e-01,\n",
      "           5.1838e-01, -9.4657e-01],\n",
      "         ...,\n",
      "         [ 8.4799e-04,  3.2842e-02, -1.4314e-02,  ..., -1.1142e-01,\n",
      "          -2.3280e-02, -8.5109e-02],\n",
      "         [-1.3584e-02,  6.2025e-02, -2.0298e-02,  ..., -7.3931e-02,\n",
      "          -2.8123e-02,  1.2125e-02],\n",
      "         [-1.0065e-02,  5.1012e-02,  2.0114e-03,  ..., -8.6220e-02,\n",
      "          -2.7638e-02, -8.8863e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.1710e-01,  3.8211e-01, -5.9062e-01,  ...,  1.4440e+00,\n",
      "           1.1713e+00,  5.0376e-02],\n",
      "         [ 3.6992e-01, -3.3067e-01,  9.3866e-02,  ...,  7.2346e-01,\n",
      "           7.4358e-01,  2.0595e-01],\n",
      "         [ 4.0167e-01,  4.0349e-01, -3.7484e-01,  ...,  1.8747e+00,\n",
      "           1.3774e+00, -1.4872e-01],\n",
      "         ...,\n",
      "         [-6.3973e-02,  3.3531e-01,  9.2141e-02,  ...,  3.5812e-01,\n",
      "           1.8536e-01, -8.0256e-01],\n",
      "         [-3.5319e-02,  9.1158e-02, -5.6650e-02,  ..., -3.2746e-02,\n",
      "          -4.6144e-02, -2.0556e-01],\n",
      "         [-6.1787e-04,  7.3234e-02, -1.4545e-02,  ..., -1.1319e-01,\n",
      "          -2.9664e-02, -8.2985e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5454,  0.2039, -0.5209,  ...,  1.2741,  1.0351, -0.1957],\n",
      "         [-0.1293,  0.7728, -0.0697,  ...,  0.1903,  0.5967, -0.2554],\n",
      "         [ 0.1105,  0.7375, -0.1642,  ..., -0.8752,  0.2345, -0.4995],\n",
      "         ...,\n",
      "         [ 0.0350,  0.0456, -0.0114,  ..., -0.0313,  0.0016, -0.1005],\n",
      "         [-0.0060,  0.0556, -0.0241,  ..., -0.0837, -0.0264,  0.0176],\n",
      "         [ 0.0360,  0.5578, -0.2715,  ...,  0.5671,  0.0669, -0.4759]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.2826e-01,  5.0063e-01, -3.0480e-01,  ...,  2.2879e+00,\n",
      "           6.9056e-01, -5.6176e-01],\n",
      "         [ 4.3098e-01, -4.3987e-01,  1.3709e-01,  ...,  1.2212e+00,\n",
      "           3.2400e-01, -3.0701e-01],\n",
      "         [ 4.0425e-01,  2.0750e-01, -2.1642e-01,  ...,  1.6525e+00,\n",
      "           1.1773e+00, -2.8415e-01],\n",
      "         ...,\n",
      "         [-9.6904e-03,  5.7847e-02, -1.7475e-02,  ..., -8.3875e-02,\n",
      "          -1.2306e-02, -9.3601e-02],\n",
      "         [-1.4692e-02,  8.5766e-02, -1.2515e-03,  ...,  6.6700e-03,\n",
      "          -3.4968e-02, -1.0663e-01],\n",
      "         [ 1.1222e-02,  4.8064e-02, -1.3726e-02,  ..., -7.1174e-02,\n",
      "          -2.8397e-02, -8.4418e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2060,  0.5868, -0.3149,  ...,  0.8913,  1.1108, -0.3201],\n",
      "         [-0.0400,  0.5082, -0.1295,  ...,  0.3346,  0.4439,  0.1414],\n",
      "         [-0.1677,  0.1989, -0.4646,  ..., -0.4113,  0.5673, -0.0093],\n",
      "         ...,\n",
      "         [ 0.0065,  0.0554, -0.0143,  ..., -0.0741, -0.0244, -0.0751],\n",
      "         [ 0.0072,  0.0564, -0.0033,  ..., -0.5332,  0.1439, -0.3025],\n",
      "         [ 0.0143,  0.0478,  0.0050,  ..., -0.0771, -0.0298, -0.0794]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.9660e-01,  4.3845e-01,  2.2905e-02,  ...,  2.3101e+00,\n",
      "           1.4522e+00, -5.1041e-01],\n",
      "         [-1.2033e-01,  4.0011e-01, -1.3333e-01,  ...,  1.2954e+00,\n",
      "           4.6989e-01, -2.3136e-01],\n",
      "         [ 4.6795e-01, -3.0471e-02, -5.2696e-01,  ...,  1.4663e+00,\n",
      "           6.9617e-01, -1.2580e-01],\n",
      "         ...,\n",
      "         [ 1.8473e-02,  9.0837e-02, -4.0607e-02,  ..., -1.0157e-01,\n",
      "          -3.9848e-03, -8.7028e-03],\n",
      "         [ 3.6928e-01,  4.8809e-01, -1.8556e-03,  ...,  7.3399e-02,\n",
      "           4.4246e-01, -6.0272e-01],\n",
      "         [-1.9500e-02,  5.0840e-02, -1.7648e-02,  ..., -8.4521e-02,\n",
      "          -2.9054e-02, -9.0317e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.7683e-01,  6.6777e-01, -2.8216e-01,  ...,  6.8361e-01,\n",
      "           1.5313e+00, -1.0002e-02],\n",
      "         [ 6.5846e-01,  4.7759e-01,  4.7350e-01,  ...,  1.2303e+00,\n",
      "           6.1177e-01,  3.2210e-01],\n",
      "         [ 3.7931e-01,  4.4039e-01,  5.2694e-01,  ...,  3.6741e-01,\n",
      "           2.2660e-01, -2.3470e-01],\n",
      "         ...,\n",
      "         [-1.5567e-02,  5.4613e-02,  1.5052e-03,  ..., -7.0959e-02,\n",
      "          -2.8805e-02, -7.0709e-02],\n",
      "         [-1.1772e-02,  4.6644e-02, -1.7412e-02,  ..., -7.3733e-02,\n",
      "          -3.5326e-02, -8.8976e-02],\n",
      "         [ 1.1472e-02,  4.6011e-02, -2.4523e-02,  ..., -7.5409e-02,\n",
      "          -2.7562e-02, -8.3565e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.9031,  0.3847, -0.3320,  ...,  2.0405,  0.5401,  0.1618],\n",
      "         [ 0.2855,  0.4800, -0.0824,  ...,  0.4790,  0.2405,  0.2356],\n",
      "         [ 0.4759, -0.1265, -0.2377,  ...,  0.0382,  1.0537, -0.3165],\n",
      "         ...,\n",
      "         [-0.0605,  0.1259, -0.0498,  ..., -0.0483, -0.0549, -0.2413],\n",
      "         [-0.0136,  0.0460, -0.0265,  ..., -0.1159, -0.0377,  0.0110],\n",
      "         [ 0.1999,  0.6648, -0.0929,  ..., -0.3391,  0.3658, -0.5536]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0385,  0.0720, -0.2394,  ...,  0.7914,  1.2400, -0.1776],\n",
      "         [ 0.0853,  0.3435,  0.0787,  ...,  0.6277,  0.4261,  0.1425],\n",
      "         [ 0.3516,  0.5463, -0.1762,  ...,  0.6004,  0.3965,  0.2190],\n",
      "         ...,\n",
      "         [-0.0084,  0.0811, -0.0201,  ..., -0.0846, -0.0341, -0.0843],\n",
      "         [-0.0163,  0.0532, -0.0245,  ..., -0.0772, -0.0265, -0.0880],\n",
      "         [ 0.0098,  0.0569, -0.0228,  ..., -0.0748, -0.0326, -0.0678]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0212e+00, -2.0303e-01, -4.5288e-01,  ...,  1.8517e+00,\n",
      "           1.2633e+00, -4.5551e-01],\n",
      "         [-9.5169e-02,  6.4572e-01,  1.0006e-02,  ...,  4.9736e-01,\n",
      "           7.7475e-01, -1.5729e-02],\n",
      "         [ 6.2716e-01,  2.3675e-01, -2.2260e-01,  ...,  1.1438e+00,\n",
      "           1.0530e+00, -3.5643e-01],\n",
      "         ...,\n",
      "         [ 7.9993e-03,  4.9805e-02, -1.1372e-02,  ..., -9.5344e-02,\n",
      "          -2.1275e-02, -7.2966e-02],\n",
      "         [ 4.4300e-04,  4.0462e-02, -3.4018e-02,  ..., -1.0362e-01,\n",
      "          -2.4654e-02, -7.3609e-02],\n",
      "         [-2.1493e-02,  6.0173e-02,  1.3121e-03,  ..., -8.1215e-02,\n",
      "          -4.0098e-02, -9.6567e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7928,  0.4517, -0.4702,  ...,  1.2041,  1.6027, -0.1326],\n",
      "         [ 0.2051,  0.0500,  0.4513,  ...,  0.0100,  0.4902, -0.2305],\n",
      "         [ 0.2651, -0.0776, -0.1361,  ..., -0.5250,  0.2890, -0.0738],\n",
      "         ...,\n",
      "         [ 0.3547, -0.2892, -0.1037,  ...,  0.5734,  0.2760, -0.4321],\n",
      "         [-0.0450,  0.2224, -0.1114,  ..., -0.0100, -0.1461, -0.0669],\n",
      "         [-0.0303,  0.3626, -0.0565,  ...,  0.3819,  0.0487, -0.0641]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8833, -0.0175, -0.4476,  ...,  1.7726,  1.0636, -0.2842],\n",
      "         [ 0.3721,  0.2126,  0.0437,  ...,  0.9072,  0.5034, -0.0242],\n",
      "         [ 0.4105,  0.9229, -0.1319,  ...,  0.7768,  0.3659, -0.2918],\n",
      "         ...,\n",
      "         [-0.0134,  0.0443, -0.0212,  ..., -0.0938, -0.0258, -0.0774],\n",
      "         [-0.1059,  0.4657, -0.2077,  ...,  0.0489,  0.1125, -0.1083],\n",
      "         [-0.0095,  0.0585, -0.0160,  ..., -0.0759, -0.0315, -0.0864]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0083e+00,  6.1370e-01, -1.6738e-01,  ...,  1.9597e+00,\n",
      "           1.2643e+00, -1.7912e-01],\n",
      "         [ 1.4279e-01,  5.7717e-01, -7.3768e-03,  ...,  3.1069e-01,\n",
      "           5.5227e-01, -3.4571e-01],\n",
      "         [-2.6420e-01,  7.7081e-01, -3.0281e-01,  ...,  3.1100e-01,\n",
      "           1.3466e+00, -3.0612e-01],\n",
      "         ...,\n",
      "         [ 1.3849e-03,  1.5883e-01, -1.4111e-01,  ..., -1.7667e-01,\n",
      "           1.5206e-01, -5.1491e-01],\n",
      "         [ 3.7844e-03,  4.0638e-02, -1.6043e-02,  ..., -7.7473e-02,\n",
      "          -1.7696e-02, -6.3508e-02],\n",
      "         [-1.0090e-02,  5.4661e-02, -2.2108e-02,  ..., -7.9788e-02,\n",
      "          -2.3701e-02, -9.2135e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3940,  0.5863, -0.4827,  ...,  1.6069,  0.6526, -0.2418],\n",
      "         [ 0.1177,  0.0862, -0.2794,  ...,  0.6060,  1.0137,  0.1415],\n",
      "         [ 0.3731,  0.7386,  0.2102,  ...,  1.3547,  1.1886,  0.3394],\n",
      "         ...,\n",
      "         [-0.0142,  0.0397, -0.0211,  ..., -0.0802, -0.0345, -0.0921],\n",
      "         [-0.0062,  0.0583, -0.0184,  ..., -0.0754, -0.0299,  0.0162],\n",
      "         [-0.0077,  0.0607, -0.0219,  ..., -0.0901, -0.0299, -0.0890]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7456,  0.2916, -0.1913,  ...,  1.3756,  0.8035,  0.1255],\n",
      "         [ 0.2811,  0.6733, -0.0735,  ...,  0.8194,  0.5065, -0.0340],\n",
      "         [-0.1535, -0.1396, -0.1450,  ...,  0.4994,  0.1193,  0.5162],\n",
      "         ...,\n",
      "         [ 0.0102,  0.0494, -0.0263,  ..., -0.0684, -0.0250, -0.0680],\n",
      "         [-0.0108,  0.0577, -0.0302,  ..., -0.0851, -0.0374, -0.0920],\n",
      "         [-0.0093,  0.0499, -0.0218,  ..., -0.1142, -0.0408,  0.0076]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4948,  0.3784, -0.0987,  ...,  0.9592,  0.7741, -0.1422],\n",
      "         [ 0.3551,  0.6212,  0.2280,  ...,  0.5875,  0.5919, -0.1338],\n",
      "         [ 0.2238,  0.6762, -0.2510,  ...,  0.3238,  1.0253, -0.3926],\n",
      "         ...,\n",
      "         [-0.0014,  0.0535, -0.0364,  ..., -0.0978, -0.0362, -0.0735],\n",
      "         [-0.0413,  0.1114, -0.0224,  ..., -0.0154, -0.0181, -0.2072],\n",
      "         [-0.0204,  0.0576, -0.0301,  ..., -0.0741, -0.0323, -0.0986]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5899,  0.5249, -0.3117,  ...,  0.8421,  1.3797, -0.0614],\n",
      "         [-0.0039,  0.3287, -0.1345,  ...,  0.0172,  0.4548, -0.1181],\n",
      "         [-0.1205,  0.3672, -0.0328,  ..., -0.4047,  0.3282, -0.1571],\n",
      "         ...,\n",
      "         [-0.0168,  0.0514, -0.0213,  ..., -0.0785, -0.0236, -0.0821],\n",
      "         [-0.0473,  0.3187, -0.1628,  ...,  0.0659,  0.0772, -0.0200],\n",
      "         [-0.0176,  0.1339, -0.0157,  ..., -0.0041,  0.1207, -0.2634]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6589,  0.5323, -0.2482,  ...,  0.9581,  1.3734, -0.2695],\n",
      "         [ 0.4970,  0.3641,  0.0058,  ...,  0.3122,  0.6827,  0.2302],\n",
      "         [-0.2971,  0.6192, -0.5126,  ...,  1.1149,  1.2044,  0.8379],\n",
      "         ...,\n",
      "         [ 0.0175,  0.0485, -0.0208,  ..., -0.0795, -0.0261, -0.0916],\n",
      "         [-0.0067,  0.0575, -0.0348,  ..., -0.0761, -0.0353, -0.0017],\n",
      "         [ 0.3746,  0.0202, -0.2561,  ..., -0.0793,  0.3686, -0.3080]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.5911e-01,  3.8554e-01, -2.5233e-01,  ...,  1.4147e+00,\n",
      "           1.1963e+00, -2.6620e-01],\n",
      "         [-4.5378e-01,  8.2231e-01,  7.7594e-02,  ...,  6.2215e-01,\n",
      "           2.8245e-01, -1.8542e-01],\n",
      "         [ 6.5745e-01,  2.8968e-01,  7.7183e-02,  ..., -1.5933e-01,\n",
      "           9.0284e-01, -5.9655e-01],\n",
      "         ...,\n",
      "         [-8.7303e-03,  4.6200e-02, -2.4198e-02,  ..., -7.5649e-02,\n",
      "          -2.2360e-02, -6.4273e-02],\n",
      "         [-8.1756e-04, -5.9272e-02,  1.7992e-02,  ..., -2.2236e-01,\n",
      "           1.8267e-01, -1.5151e-01],\n",
      "         [-7.3633e-03,  4.0164e-02, -1.6223e-02,  ..., -9.0353e-02,\n",
      "          -2.6971e-02, -8.2721e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0463e+00,  3.5481e-01, -4.0289e-01,  ...,  1.2431e+00,\n",
      "           1.0457e+00,  1.7237e-01],\n",
      "         [ 1.7789e-01,  4.5183e-01, -5.8701e-04,  ...,  9.3448e-01,\n",
      "           9.7465e-02,  2.5303e-01],\n",
      "         [ 4.9745e-01,  7.4615e-01,  7.7641e-02,  ..., -1.2461e-01,\n",
      "           6.7099e-01,  2.3393e-01],\n",
      "         ...,\n",
      "         [-2.2170e-02,  4.7901e-02, -2.0007e-02,  ..., -8.9617e-02,\n",
      "          -3.4080e-02, -9.3035e-02],\n",
      "         [-4.9517e-03,  4.2258e-02, -3.8263e-02,  ..., -8.2517e-02,\n",
      "          -2.5611e-02, -8.9474e-02],\n",
      "         [-1.4692e-02,  2.5209e-01, -2.0607e-01,  ...,  7.3487e-02,\n",
      "           7.9915e-02, -2.2487e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9875, -0.1137, -0.6063,  ...,  1.6595,  1.1281, -0.2261],\n",
      "         [ 0.1398, -0.0801, -0.1454,  ...,  1.3535,  0.1811,  0.4486],\n",
      "         [ 0.1765, -0.1549, -0.1012,  ...,  1.2608,  1.4171, -0.0963],\n",
      "         ...,\n",
      "         [ 0.0064,  0.0521, -0.0180,  ..., -0.0690, -0.0307, -0.0916],\n",
      "         [-0.1763,  0.1970, -0.1846,  ...,  0.0833, -0.1002,  0.0665],\n",
      "         [ 0.0051,  0.0714, -0.0240,  ..., -0.0787, -0.0289, -0.0879]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 4.4942e-01,  2.6623e-01, -1.6596e-01,  ...,  1.9607e+00,\n",
      "           4.2975e-01,  2.2682e-01],\n",
      "         [ 6.4000e-02,  1.5965e-01, -1.5238e-01,  ...,  2.6116e-01,\n",
      "           6.7898e-01,  2.3751e-01],\n",
      "         [-2.0142e-01,  2.9951e-01, -4.7446e-01,  ...,  2.7056e-01,\n",
      "           1.7624e+00,  6.9466e-02],\n",
      "         ...,\n",
      "         [ 1.2352e-02,  3.5740e-02,  1.4014e-03,  ..., -7.9434e-02,\n",
      "          -1.6070e-02, -7.6934e-02],\n",
      "         [ 2.6649e-01,  2.0832e-01, -1.8683e-01,  ...,  8.9072e-03,\n",
      "          -2.6462e-02, -4.9015e-01],\n",
      "         [-4.2789e-02,  7.9667e-02, -1.6320e-01,  ..., -2.9962e-01,\n",
      "           1.2632e-01, -1.9436e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.2950,  0.1274, -0.7393,  ...,  2.1401,  0.9964, -0.1313],\n",
      "         [ 0.1483,  0.1852,  0.0319,  ...,  0.7792,  0.1923,  0.3822],\n",
      "         [ 0.3304,  0.0448, -0.6166,  ..., -0.6864,  1.2412, -0.5967],\n",
      "         ...,\n",
      "         [ 0.0194,  0.0345, -0.0208,  ..., -0.0784, -0.0089, -0.0640],\n",
      "         [ 0.0260,  0.0511, -0.0211,  ..., -0.0752, -0.0229, -0.0707],\n",
      "         [-0.0385, -0.0784, -0.0829,  ..., -0.2213, -0.0034, -0.1483]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8394,  0.3817, -0.3903,  ...,  1.8169,  1.0619, -0.2561],\n",
      "         [ 0.3902,  0.1887,  0.0288,  ...,  0.7083,  0.5563, -0.1372],\n",
      "         [-0.4218,  0.4059, -0.1300,  ...,  0.3147,  0.3875,  0.2605],\n",
      "         ...,\n",
      "         [ 0.5988,  0.8346,  0.0468,  ...,  1.0109,  0.8712, -0.1379],\n",
      "         [-0.0261,  0.0991, -0.0198,  ..., -0.0308, -0.0242,  0.0292],\n",
      "         [ 0.0307,  0.0536, -0.0195,  ..., -0.1273, -0.0544, -0.0669]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8836,  0.1341, -0.3374,  ...,  1.5462,  1.2796,  0.0109],\n",
      "         [ 0.3348, -0.3045,  0.2780,  ..., -0.7352,  0.1384,  0.0758],\n",
      "         [ 0.2250,  0.0288, -0.3911,  ..., -0.2053,  1.3023,  0.0755],\n",
      "         ...,\n",
      "         [-0.0070,  0.0539,  0.0016,  ..., -0.0779, -0.0296, -0.0894],\n",
      "         [-0.0093,  0.0515, -0.0183,  ..., -0.0747, -0.0340, -0.0890],\n",
      "         [-0.0080,  0.0413, -0.0213,  ..., -0.0802, -0.0167, -0.0882]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.6982e-01,  6.4345e-01, -3.3781e-01,  ...,  2.0788e+00,\n",
      "           1.3912e+00,  2.6813e-01],\n",
      "         [ 6.0466e-01,  6.5878e-01, -6.2423e-02,  ...,  8.0659e-01,\n",
      "           1.0548e+00, -1.0449e-01],\n",
      "         [ 7.1952e-01,  6.3133e-01,  1.7518e-02,  ...,  7.0674e-01,\n",
      "           8.4794e-01,  3.4107e-02],\n",
      "         ...,\n",
      "         [ 4.8668e-04,  8.9990e-02, -1.5273e-02,  ..., -7.4545e-02,\n",
      "          -2.4227e-02, -9.0085e-02],\n",
      "         [-2.5612e-03,  7.9582e-02, -3.6756e-02,  ..., -2.1913e-02,\n",
      "          -3.7231e-02, -1.4247e-01],\n",
      "         [ 1.4684e-02,  1.5407e-01, -2.2174e-02,  ..., -6.3314e-02,\n",
      "          -1.8314e-02, -2.1127e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5154, -0.2341, -0.8340,  ...,  0.8691,  1.3916,  0.4015],\n",
      "         [-0.1518,  0.5314,  0.0264,  ...,  0.4921,  0.5267,  0.3095],\n",
      "         [-0.1236,  0.0607, -0.2559,  ..., -0.0046,  0.4489,  0.1537],\n",
      "         ...,\n",
      "         [-0.0132,  0.0459, -0.0184,  ..., -0.0697, -0.0483, -0.0760],\n",
      "         [-0.0022,  0.0552, -0.0243,  ..., -0.0847, -0.0268, -0.0892],\n",
      "         [-0.0168,  0.0462, -0.0108,  ..., -0.1212, -0.0314, -0.0851]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4639,  0.6291, -0.2377,  ...,  1.9009,  0.7512, -0.2530],\n",
      "         [ 0.4044,  0.6795, -0.0379,  ...,  1.2267,  0.1473,  0.2081],\n",
      "         [ 0.2546,  0.7587, -0.3738,  ...,  0.6970,  0.5481, -0.4103],\n",
      "         ...,\n",
      "         [-0.0439,  0.1559, -0.0506,  ..., -0.1007, -0.0429, -0.2142],\n",
      "         [-0.0411,  0.0610, -0.0182,  ..., -0.0394, -0.0152, -0.2118],\n",
      "         [ 0.2322,  0.3122, -0.0988,  ...,  0.1485,  0.2424, -0.5595]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[-0.3477,  0.7153,  0.3098,  ..., -0.7179,  0.3588, -0.6860],\n",
      "         [ 0.5421, -0.1012,  0.2578,  ...,  0.7222,  0.0894, -0.1072],\n",
      "         [ 0.6936,  0.3344,  0.0566,  ..., -0.4509,  0.4414,  0.1424],\n",
      "         ...,\n",
      "         [-0.0113,  0.0561, -0.0217,  ..., -0.0731, -0.0328, -0.0854],\n",
      "         [-0.0257,  0.0675, -0.0250,  ..., -0.1197, -0.0314, -0.0947],\n",
      "         [-0.2447,  0.2185, -0.2710,  ...,  0.4385,  0.0403, -0.1856]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6741,  0.5687, -0.3872,  ...,  1.4630,  1.5038, -0.1329],\n",
      "         [-0.2784, -0.3511, -0.6428,  ...,  0.5106,  0.5350,  0.5764],\n",
      "         [ 0.1002,  0.0879, -0.2175,  ...,  0.9959,  0.8304,  0.2440],\n",
      "         ...,\n",
      "         [ 0.0073,  0.0564, -0.0202,  ..., -0.0955, -0.0269, -0.0939],\n",
      "         [ 0.0112,  0.0434, -0.0234,  ..., -0.0738, -0.0328, -0.0875],\n",
      "         [-0.0232,  0.0817, -0.0293,  ..., -0.1187, -0.0472, -0.0857]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4708,  0.5343, -0.6410,  ...,  1.1191,  0.7612, -0.0620],\n",
      "         [-0.0889,  0.4431, -0.1472,  ...,  1.4181,  0.1180,  0.0481],\n",
      "         [ 0.2729, -0.1369, -0.8309,  ...,  0.4027,  1.2418,  0.0551],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0199, -0.0163,  ...,  0.1889,  0.2561, -0.6570],\n",
      "         [ 0.0045,  0.0494, -0.0021,  ..., -0.0853, -0.0358,  0.0231],\n",
      "         [ 0.0093,  0.0609, -0.0107,  ..., -0.1275, -0.0403, -0.0832]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.3268,  0.2909, -0.3923,  ...,  1.6950,  1.2591,  0.2728],\n",
      "         [ 0.4403,  0.4898,  0.0468,  ...,  0.3932,  0.1424, -0.4431],\n",
      "         [ 0.8233, -0.7785, -0.1620,  ...,  0.2566,  0.4352, -0.2406],\n",
      "         ...,\n",
      "         [ 0.0552,  0.1305, -0.1232,  ...,  0.0257,  0.0372, -0.3805],\n",
      "         [-0.0186,  0.0558, -0.0287,  ..., -0.0841, -0.0210, -0.0844],\n",
      "         [ 0.0092,  0.0862, -0.0372,  ..., -0.0698,  0.0209, -0.2209]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5608,  0.2425, -0.7222,  ...,  1.6455,  0.8918, -0.1867],\n",
      "         [-0.0340,  0.5381,  0.4952,  ...,  0.0283,  0.3085, -0.0323],\n",
      "         [-0.1246,  0.3425,  0.0557,  ..., -0.0448,  0.0463, -0.1121],\n",
      "         ...,\n",
      "         [-0.0110,  0.0482, -0.0238,  ..., -0.0657, -0.0148, -0.0911],\n",
      "         [ 0.0455,  0.0353, -0.1519,  ...,  0.0090,  0.0456, -0.3119],\n",
      "         [-0.0538,  0.1009, -0.0295,  ..., -0.0498, -0.0321, -0.2127]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0685,  0.7894, -0.2337,  ...,  2.4738,  1.2086, -0.3159],\n",
      "         [ 0.0985,  0.6039,  0.1231,  ...,  0.5705,  0.4173,  0.3798],\n",
      "         [ 0.3319,  0.1118, -0.5327,  ...,  1.6506,  0.2584, -0.1153],\n",
      "         ...,\n",
      "         [ 0.0436,  0.0761, -0.0375,  ..., -0.0371, -0.0244, -0.2032],\n",
      "         [-0.0065,  0.0766, -0.0125,  ..., -0.0699, -0.0292, -0.0865],\n",
      "         [-0.0077,  0.0588, -0.0266,  ..., -0.0749, -0.0309, -0.0743]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7325,  0.7187, -0.2697,  ...,  2.4152,  1.2907, -0.2336],\n",
      "         [ 0.2645,  0.5921,  0.1927,  ...,  1.7575,  0.4688,  0.3867],\n",
      "         [ 0.4832,  0.4127, -0.1354,  ...,  1.1107,  0.5944, -0.4581],\n",
      "         ...,\n",
      "         [ 0.0243,  0.1853,  0.0038,  ..., -0.0738, -0.0806, -0.2811],\n",
      "         [-0.0083,  0.0329, -0.0188,  ..., -0.0731, -0.0199, -0.0868],\n",
      "         [-0.0130,  0.1131, -0.0149,  ..., -0.0659, -0.0618, -0.2066]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.8713e-01,  1.9664e-01,  2.2875e-01,  ...,  6.7154e-01,\n",
      "           1.1237e+00,  3.2902e-01],\n",
      "         [ 1.8683e-01,  5.2120e-01,  4.3796e-01,  ...,  1.7090e-01,\n",
      "           1.0157e-01, -1.9277e-01],\n",
      "         [ 6.3690e-01,  5.6849e-01,  7.6943e-03,  ...,  1.1215e+00,\n",
      "           1.3114e+00, -2.1211e-01],\n",
      "         ...,\n",
      "         [-3.6411e-04,  5.5349e-02, -1.4051e-02,  ..., -1.0094e-01,\n",
      "          -1.3857e-02, -8.6977e-02],\n",
      "         [-1.1953e-02,  5.8805e-02, -1.0947e-02,  ..., -8.4469e-02,\n",
      "          -3.4996e-02, -8.8932e-02],\n",
      "         [-1.2763e-03,  5.7284e-02, -2.3845e-02,  ..., -7.4779e-02,\n",
      "          -2.4777e-02, -9.4691e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0358e+00,  4.9873e-02, -4.2691e-01,  ...,  1.5623e+00,\n",
      "           1.3216e+00,  2.3640e-01],\n",
      "         [ 4.6193e-01,  3.1876e-04,  1.4663e-01,  ...,  2.5921e-01,\n",
      "           4.0123e-01,  1.3510e-01],\n",
      "         [ 2.7612e-01,  7.4313e-01, -7.8199e-02,  ...,  2.9319e-01,\n",
      "           6.2983e-01, -5.0384e-02],\n",
      "         ...,\n",
      "         [-2.0738e-02,  8.7281e-02, -2.3384e-02,  ..., -8.8026e-02,\n",
      "          -3.7310e-02,  9.4337e-03],\n",
      "         [-1.5480e-02,  4.2057e-02, -3.1144e-02,  ..., -7.9862e-02,\n",
      "          -2.6779e-02, -9.3067e-02],\n",
      "         [-1.1609e-02,  8.0694e-02, -2.1498e-02,  ..., -7.7593e-02,\n",
      "          -2.8216e-02, -9.4983e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.1070,  0.1247, -0.3302,  ...,  1.6045,  1.1917,  0.1600],\n",
      "         [ 0.1061,  0.4984, -0.3616,  ...,  0.5734,  0.3602,  0.0302],\n",
      "         [-0.1309,  0.0152, -0.4040,  ...,  0.0165,  0.8547, -0.1323],\n",
      "         ...,\n",
      "         [-0.0108,  0.0566, -0.0290,  ..., -0.1153, -0.0409, -0.0923],\n",
      "         [-0.0152,  0.0457, -0.0206,  ..., -0.1000, -0.0299, -0.0875],\n",
      "         [ 0.1840,  0.1896,  0.1068,  ...,  0.4892,  0.1615,  0.0155]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6441,  0.6290, -0.4173,  ...,  2.0557,  0.9256,  0.4618],\n",
      "         [ 0.4783,  0.1586,  0.2746,  ...,  0.6716,  0.2012,  0.0552],\n",
      "         [ 0.0650,  0.3712, -0.2272,  ...,  1.2824,  0.9559, -0.2057],\n",
      "         ...,\n",
      "         [ 0.0441,  0.1591, -0.0686,  ..., -0.0973, -0.0182, -0.2167],\n",
      "         [-0.0205,  0.0610, -0.0384,  ..., -0.0874, -0.0369, -0.1059],\n",
      "         [-0.0203,  0.0535, -0.0294,  ..., -0.0832, -0.0111, -0.1036]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.7218,  0.3719, -0.0454,  ...,  2.1187,  0.5598, -0.1839],\n",
      "         [ 0.2486, -0.0936, -0.0854,  ...,  0.5484,  0.3504, -0.1404],\n",
      "         [ 0.1298,  0.1359, -0.1088,  ..., -0.2088,  0.0499, -0.5399],\n",
      "         ...,\n",
      "         [-0.0172,  0.0362, -0.0089,  ..., -0.0705, -0.0345, -0.0932],\n",
      "         [-0.0338,  0.1631, -0.0127,  ..., -0.0822, -0.1022, -0.2422],\n",
      "         [-0.0086,  0.0427, -0.0031,  ..., -0.0859, -0.0324, -0.0804]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2179,  0.1736, -0.3458,  ...,  2.0697,  1.2694, -0.0861],\n",
      "         [-0.0148,  0.7366, -0.4177,  ..., -0.6430,  0.2975,  0.3661],\n",
      "         [ 0.4644,  0.3470,  0.1270,  ...,  0.9455,  0.5756,  0.2892],\n",
      "         ...,\n",
      "         [-0.0144,  0.0444, -0.0221,  ..., -0.0733, -0.0348, -0.0811],\n",
      "         [ 0.2548,  0.4017, -0.0065,  ...,  0.4376,  0.1456, -0.3047],\n",
      "         [-0.0130,  0.0525, -0.0261,  ..., -0.0868, -0.0290, -0.0965]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6921,  0.3983, -0.2166,  ...,  1.0456,  1.0278, -0.1876],\n",
      "         [ 0.3331,  0.4218, -0.0321,  ...,  0.9523,  0.5564,  0.1720],\n",
      "         [ 0.3292, -0.0947,  0.1639,  ...,  1.8289,  0.5922, -0.1752],\n",
      "         ...,\n",
      "         [-0.0094,  0.0575, -0.0153,  ..., -0.0834, -0.0202, -0.0866],\n",
      "         [-0.3374,  0.4350, -0.0775,  ...,  0.3048,  0.0780, -0.1257],\n",
      "         [-0.0168,  0.0239, -0.0282,  ..., -0.0766, -0.0275, -0.0863]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.0871e-01,  3.6274e-01, -3.4774e-01,  ...,  1.8769e+00,\n",
      "           8.3921e-01, -2.1539e-01],\n",
      "         [ 4.9697e-01,  6.8251e-01,  6.1872e-01,  ...,  1.4765e+00,\n",
      "           2.2110e-01, -7.3979e-02],\n",
      "         [ 2.8554e-01,  2.8843e-01, -2.5886e-01,  ...,  1.6426e+00,\n",
      "           4.1838e-01, -1.0075e-01],\n",
      "         ...,\n",
      "         [ 5.6085e-04,  5.5991e-02, -3.1864e-02,  ..., -8.4419e-02,\n",
      "          -3.9813e-02, -7.5798e-02],\n",
      "         [-1.7099e-02,  5.3074e-02,  2.6873e-03,  ..., -7.9420e-02,\n",
      "          -3.2013e-02, -8.9632e-02],\n",
      "         [-3.7901e-02,  1.4869e-01, -3.6089e-02,  ..., -4.9100e-02,\n",
      "          -1.9482e-02, -3.7539e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 5.4160e-01,  2.3359e-01,  2.0661e-01,  ...,  1.5712e+00,\n",
      "           8.2537e-01, -3.9292e-01],\n",
      "         [ 3.1299e-02,  5.0024e-01, -1.3008e-01,  ...,  8.8746e-01,\n",
      "           6.8254e-01,  3.2073e-01],\n",
      "         [ 6.5632e-01,  3.5716e-02, -9.4512e-02,  ...,  2.2545e+00,\n",
      "           1.3473e+00, -4.2715e-01],\n",
      "         ...,\n",
      "         [ 7.0536e-05,  1.3067e-01,  2.7763e-02,  ..., -1.0714e-01,\n",
      "          -8.2938e-02, -2.2117e-01],\n",
      "         [-4.3397e-03,  5.0072e-02, -1.0839e-02,  ..., -8.7207e-02,\n",
      "          -2.9085e-02, -9.3661e-02],\n",
      "         [-1.0356e-02,  8.2432e-02, -2.7592e-02,  ..., -7.5354e-02,\n",
      "          -4.2138e-02, -8.9444e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.1527,  0.5798, -0.0411,  ...,  1.3541,  1.1047, -0.2419],\n",
      "         [ 0.6173,  0.6409,  0.2824,  ...,  0.0254, -0.2183, -0.4817],\n",
      "         [ 0.4696,  0.8174, -0.0304,  ...,  0.8696,  0.2394,  0.5160],\n",
      "         ...,\n",
      "         [-0.0110,  0.0425, -0.0189,  ..., -0.0716, -0.0344, -0.0881],\n",
      "         [-0.0839,  0.3193, -0.0702,  ..., -0.0945, -0.1575, -0.3152],\n",
      "         [-0.0038,  0.0484, -0.0250,  ..., -0.0777, -0.0401, -0.0647]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.2315,  0.2319, -0.4164,  ...,  1.3963,  1.1866,  0.0077],\n",
      "         [ 0.0427,  0.2173,  0.5169,  ...,  0.2321,  0.0348,  0.3031],\n",
      "         [-0.0409,  0.5973, -0.1158,  ...,  0.0601,  1.0363,  0.5181],\n",
      "         ...,\n",
      "         [-0.0154,  0.0455, -0.0135,  ..., -0.0793, -0.0251,  0.0158],\n",
      "         [ 0.0164,  0.0603, -0.0134,  ..., -0.0918, -0.0288, -0.0863],\n",
      "         [ 0.2807,  0.0637,  0.0889,  ...,  0.3665,  0.1291, -0.1200]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 7.2865e-01, -8.4752e-02, -3.3436e-01,  ...,  1.5982e+00,\n",
      "           7.6862e-01, -2.6958e-01],\n",
      "         [ 3.0339e-01, -9.1243e-02, -4.5119e-01,  ...,  6.6348e-01,\n",
      "           7.1030e-02,  1.9259e-01],\n",
      "         [ 3.2372e-01, -2.7128e-02, -1.4933e-01,  ...,  1.8309e+00,\n",
      "           9.1410e-01, -3.4197e-01],\n",
      "         ...,\n",
      "         [ 1.7084e-01,  7.2010e-02, -2.1807e-01,  ...,  3.1996e-01,\n",
      "           2.5575e-01, -3.1791e-01],\n",
      "         [-1.1523e-02,  5.0026e-02, -1.3928e-02,  ..., -7.5860e-02,\n",
      "          -2.5621e-02,  1.6260e-02],\n",
      "         [-3.4968e-02,  1.0092e-01,  1.6792e-03,  ..., -9.7170e-02,\n",
      "          -8.5982e-02, -1.9580e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.1044,  0.2389, -0.4491,  ...,  2.0835,  0.9839, -0.4459],\n",
      "         [-0.2863,  0.7017, -0.5625,  ...,  1.2165,  0.1560, -0.0324],\n",
      "         [ 0.8692,  0.8508, -0.5741,  ...,  0.5901,  1.5455, -0.8625],\n",
      "         ...,\n",
      "         [-0.0261,  0.0604, -0.0096,  ..., -0.0808, -0.0457, -0.1073],\n",
      "         [-0.0228,  0.0664, -0.0132,  ..., -0.0798, -0.0322, -0.0802],\n",
      "         [-0.0523,  0.1452, -0.0297,  ..., -0.0462, -0.0219, -0.2024]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.2733e-01,  2.3551e-01, -3.3758e-01,  ...,  2.0892e+00,\n",
      "           5.6286e-01,  3.9939e-01],\n",
      "         [ 3.9178e-01,  5.4299e-01, -5.5145e-01,  ...,  1.2923e+00,\n",
      "           3.6695e-01,  3.4947e-01],\n",
      "         [ 4.6511e-01,  4.3454e-01, -4.2305e-01,  ...,  6.9055e-01,\n",
      "           1.1165e+00,  2.4264e-03],\n",
      "         ...,\n",
      "         [-1.1762e-02,  5.7502e-02, -1.2697e-02,  ..., -8.1566e-02,\n",
      "          -3.4895e-02, -8.7700e-02],\n",
      "         [-1.2489e-03,  4.6384e-02, -1.7190e-03,  ..., -8.0963e-02,\n",
      "          -2.4130e-02, -8.7962e-02],\n",
      "         [-5.4230e-02, -7.8204e-02, -1.7527e-01,  ..., -3.4286e-02,\n",
      "          -1.8782e-02, -3.9184e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.4373, -0.1622, -0.3400,  ...,  1.7071,  1.3698,  0.0276],\n",
      "         [ 0.0161,  0.5201,  0.7668,  ...,  1.2052, -0.0060, -0.1521],\n",
      "         [-0.0232, -0.1921, -0.3367,  ...,  0.2030,  0.8928, -0.1917],\n",
      "         ...,\n",
      "         [ 0.0865,  0.0842, -0.1242,  ..., -0.1152,  0.0461, -0.4503],\n",
      "         [-0.0096,  0.0405, -0.0226,  ..., -0.0829, -0.0393, -0.0884],\n",
      "         [-0.0541,  0.0721, -0.0374,  ..., -0.0280, -0.0216, -0.2105]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 8.3836e-01,  6.3231e-01, -2.9985e-01,  ...,  1.6991e+00,\n",
      "           9.5689e-01, -2.7658e-01],\n",
      "         [-1.5254e-01,  6.6180e-01, -5.2241e-02,  ...,  1.1308e+00,\n",
      "           1.6977e-01, -2.7431e-02],\n",
      "         [ 6.7984e-01,  4.2484e-01, -3.7623e-02,  ..., -7.6520e-01,\n",
      "           1.4479e-01, -6.0719e-01],\n",
      "         ...,\n",
      "         [ 5.5844e-02,  2.3654e-01,  5.7998e-02,  ..., -5.9339e-02,\n",
      "          -3.2886e-02, -2.5464e-01],\n",
      "         [-1.1982e-02,  4.4959e-02, -1.7611e-02,  ..., -8.8359e-02,\n",
      "          -3.0627e-02, -8.4311e-02],\n",
      "         [-6.3605e-02,  7.8347e-02, -3.9217e-02,  ..., -8.4658e-03,\n",
      "          -9.4073e-04, -2.2177e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.0651,  0.2000,  0.1184,  ...,  1.6944,  0.4002, -0.2462],\n",
      "         [-0.2642,  0.1852,  0.1880,  ...,  0.5398, -0.0090,  0.2208],\n",
      "         [ 0.1557,  0.1383, -0.2496,  ...,  0.6965,  0.0513, -0.0109],\n",
      "         ...,\n",
      "         [-0.0068,  0.0750, -0.0462,  ..., -0.0293, -0.0102, -0.2107],\n",
      "         [ 0.1408,  0.1412, -0.1580,  ...,  0.0813,  0.1543, -0.6836],\n",
      "         [ 0.0092,  0.0307, -0.0199,  ..., -0.0844, -0.0317,  0.0185]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9194,  0.2383, -0.2117,  ...,  1.8582,  0.1379, -0.1114],\n",
      "         [ 0.1878,  0.0843,  0.4122,  ...,  0.4115,  0.0340,  0.0797],\n",
      "         [ 0.3305,  0.0280, -0.1538,  ...,  0.8583,  0.8987,  0.0143],\n",
      "         ...,\n",
      "         [-0.0114,  0.0399, -0.0240,  ..., -0.0840, -0.0306, -0.0825],\n",
      "         [-0.0197,  0.0489, -0.0144,  ..., -0.0727, -0.0332, -0.0821],\n",
      "         [ 0.0140,  0.0503, -0.0535,  ..., -0.0249, -0.0190, -0.2001]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5570, -0.2073, -0.4625,  ...,  2.4761,  0.2055, -0.5897],\n",
      "         [ 0.1032,  0.2276,  0.5753,  ...,  0.6295,  0.3894,  0.2800],\n",
      "         [-0.2390,  0.2727,  0.2357,  ...,  0.2527,  0.5439, -0.2280],\n",
      "         ...,\n",
      "         [-0.0411,  0.0795, -0.0202,  ..., -0.0141, -0.0264, -0.1878],\n",
      "         [-0.0155,  0.0598, -0.0207,  ..., -0.0715, -0.0316, -0.0891],\n",
      "         [ 0.1266,  0.1666, -0.1916,  ...,  0.0899,  0.3682, -0.3155]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 9.8188e-01,  5.0872e-01,  1.5130e-01,  ...,  1.8821e+00,\n",
      "           6.7859e-01, -2.8971e-01],\n",
      "         [-1.5963e-01,  8.0120e-01, -1.5609e-01,  ...,  9.2777e-01,\n",
      "           3.2402e-01,  2.5693e-01],\n",
      "         [-1.2231e-01,  1.6607e-01, -5.1947e-02,  ..., -3.9506e-01,\n",
      "           1.0859e-03, -2.1999e-02],\n",
      "         ...,\n",
      "         [-5.1185e-02,  4.7410e-02, -2.0477e-02,  ..., -2.6561e-02,\n",
      "          -8.0038e-03,  2.7296e-02],\n",
      "         [-9.8176e-03,  5.3384e-02, -4.1127e-03,  ..., -8.8690e-02,\n",
      "          -2.6740e-02, -8.5591e-02],\n",
      "         [-3.0198e-03,  4.6522e-02, -2.5845e-02,  ..., -8.0303e-02,\n",
      "          -2.7442e-02, -8.4470e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0724e+00,  1.5387e-01, -1.5587e-01,  ...,  2.1468e+00,\n",
      "           5.1355e-01, -4.7380e-01],\n",
      "         [-7.5445e-02,  7.0340e-01, -1.5859e-01,  ...,  5.3493e-01,\n",
      "           5.5169e-01,  4.1811e-01],\n",
      "         [ 6.4432e-01, -1.4399e-02,  2.3040e-01,  ...,  1.3712e+00,\n",
      "           1.0777e+00, -6.7677e-01],\n",
      "         ...,\n",
      "         [ 2.7473e-02,  4.8575e-02, -2.3619e-02,  ..., -7.5320e-02,\n",
      "          -3.0238e-02, -8.1245e-02],\n",
      "         [ 1.0578e-03,  2.2236e-02,  3.2037e-02,  ..., -1.4166e-02,\n",
      "          -1.5351e-02, -2.5188e-01],\n",
      "         [-7.1980e-03,  6.3822e-02, -1.4445e-02,  ..., -7.7635e-02,\n",
      "          -3.4076e-02, -8.4049e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.4252e-01,  3.4342e-01, -2.5431e-01,  ...,  1.8114e+00,\n",
      "           1.0833e+00, -3.2600e-02],\n",
      "         [ 7.3052e-02,  4.3247e-01, -1.6330e-01,  ...,  6.0536e-01,\n",
      "           1.8028e-01, -5.8833e-02],\n",
      "         [-1.3727e-01, -8.3597e-02,  3.6552e-01,  ...,  6.1828e-01,\n",
      "           3.9459e-01, -3.3357e-01],\n",
      "         ...,\n",
      "         [ 4.9214e-02,  1.2105e-01, -1.2168e-03,  ...,  1.4183e-02,\n",
      "           6.7121e-02, -2.9774e-01],\n",
      "         [-1.0947e-02,  4.3073e-02, -1.7936e-02,  ..., -1.0833e-01,\n",
      "          -2.8810e-02, -6.3080e-02],\n",
      "         [-6.9021e-02,  1.8063e-01,  9.4492e-02,  ...,  4.1827e-01,\n",
      "           2.4154e-01, -3.6084e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.2444e+00,  2.6660e-01, -3.9549e-01,  ...,  1.4921e+00,\n",
      "           1.3679e+00, -2.0204e-01],\n",
      "         [-9.2752e-02,  3.2301e-01, -2.0320e-01,  ...,  5.9208e-01,\n",
      "           2.2536e-02,  4.6761e-01],\n",
      "         [ 6.4143e-02,  8.9667e-01,  2.4197e-02,  ...,  1.3581e+00,\n",
      "           3.7440e-01,  3.7109e-02],\n",
      "         ...,\n",
      "         [ 1.1038e-03,  5.8182e-02, -1.7002e-02,  ..., -6.7107e-02,\n",
      "          -2.4670e-02,  1.7250e-02],\n",
      "         [ 1.8164e-02,  5.4575e-02, -1.5365e-02,  ..., -6.6075e-02,\n",
      "          -2.4368e-02, -9.2304e-02],\n",
      "         [-9.0586e-03,  7.9219e-02, -1.2331e-02,  ..., -8.3531e-02,\n",
      "          -1.4340e-02, -8.6647e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.6179e-01,  1.0151e-01, -9.9128e-02,  ...,  6.0098e-02,\n",
      "           2.0652e-01,  2.3734e-01],\n",
      "         [ 2.0295e-01, -2.3824e-01,  4.3034e-01,  ...,  1.5110e+00,\n",
      "           8.7092e-03, -2.3239e-01],\n",
      "         [ 3.0287e-01,  2.6727e-01, -6.5633e-02,  ..., -4.1367e-01,\n",
      "           8.4559e-02,  9.1856e-02],\n",
      "         ...,\n",
      "         [-1.0677e-02,  5.3400e-02, -2.5780e-02,  ..., -8.4212e-02,\n",
      "          -3.0783e-02, -8.8623e-02],\n",
      "         [-1.7693e-02,  2.5646e-02, -2.8696e-02,  ..., -2.1249e-04,\n",
      "           1.4505e-02, -1.7897e-01],\n",
      "         [ 2.3305e-02,  6.9531e-01, -1.0849e-01,  ..., -2.0159e-01,\n",
      "           4.2460e-01, -6.4656e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8711, -0.1477,  0.0470,  ...,  1.8617,  1.1975,  0.3468],\n",
      "         [-0.5915,  0.1797,  0.0436,  ...,  0.5742,  0.0090,  0.2045],\n",
      "         [ 0.1226,  0.1535,  0.0901,  ..., -0.1937,  0.3602, -0.1791],\n",
      "         ...,\n",
      "         [ 0.0122,  0.0463, -0.0273,  ..., -0.0782, -0.0232, -0.0798],\n",
      "         [-0.0735,  0.3876, -0.1419,  ...,  0.2235,  0.1000, -0.0601],\n",
      "         [ 0.1209, -0.0138,  0.0490,  ...,  0.2308,  0.0988, -0.2526]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.9090,  0.0798, -0.1261,  ...,  2.1476,  0.7330,  0.0713],\n",
      "         [ 0.1557, -0.0530,  0.2198,  ...,  1.5655,  0.0866, -0.1108],\n",
      "         [-0.1147, -0.1937, -0.1267,  ...,  0.3926,  0.7449,  0.1802],\n",
      "         ...,\n",
      "         [-0.0134,  0.2159,  0.0415,  ..., -0.1136, -0.0644, -0.2046],\n",
      "         [-0.0140,  0.0575,  0.0047,  ..., -0.0807, -0.0316, -0.0919],\n",
      "         [-0.0156,  0.0592, -0.0203,  ..., -0.0199,  0.0197, -0.2068]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.8133,  0.2595, -0.0359,  ...,  2.5350,  0.4425, -0.5725],\n",
      "         [ 0.3740,  0.2598,  0.5935,  ...,  1.2856,  0.3563,  0.1069],\n",
      "         [ 0.5072,  0.7114,  0.2385,  ...,  0.4590,  0.4198, -0.4672],\n",
      "         ...,\n",
      "         [-0.0344,  0.0624, -0.0487,  ..., -0.0954,  0.0250, -0.2830],\n",
      "         [ 0.1147, -0.2006, -0.1015,  ...,  0.4682,  0.0218, -0.6840],\n",
      "         [-0.0485,  0.1280, -0.0491,  ..., -0.0492, -0.0232,  0.0204]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0583, -0.3186, -0.2072,  ...,  2.5898,  0.4695, -0.0838],\n",
      "         [ 0.5558,  0.4102, -0.2506,  ...,  1.6509,  0.4595,  0.0852],\n",
      "         [ 0.2599,  0.4439,  0.1554,  ...,  0.6368,  0.0410,  0.0368],\n",
      "         ...,\n",
      "         [-0.0451,  0.0872, -0.0289,  ..., -0.0489, -0.0309, -0.2150],\n",
      "         [-0.0086,  0.0459, -0.0189,  ..., -0.0868, -0.0313, -0.0936],\n",
      "         [ 0.1482,  0.1664, -0.0854,  ...,  0.0164,  0.1993, -0.2825]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5363,  0.0348, -0.2219,  ...,  1.8570,  0.5691, -0.1037],\n",
      "         [-0.1184,  0.7804, -0.2486,  ...,  0.2442,  0.3809,  0.1313],\n",
      "         [ 0.0701,  0.0487, -0.1782,  ...,  0.0153,  0.4162, -0.3473],\n",
      "         ...,\n",
      "         [-0.0108,  0.0814, -0.0251,  ..., -0.1146, -0.0327, -0.0781],\n",
      "         [-0.0036,  0.0409, -0.0159,  ..., -0.0749, -0.0258, -0.0852],\n",
      "         [ 0.0084,  0.0039, -0.0606,  ..., -0.0665,  0.0297, -0.1716]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6364,  0.0206, -0.3721,  ...,  3.0724,  1.0984,  0.4317],\n",
      "         [ 0.1362,  0.4696,  0.1683,  ...,  0.6558,  0.3014,  0.1589],\n",
      "         [ 1.0010, -0.2887, -0.1716,  ...,  2.5667,  0.8390,  0.0227],\n",
      "         ...,\n",
      "         [-0.0172,  0.0436,  0.0036,  ..., -0.0734, -0.0313, -0.0947],\n",
      "         [-0.0125,  0.0546, -0.0281,  ..., -0.0713, -0.0341, -0.0996],\n",
      "         [-0.0163,  0.0492, -0.0217,  ..., -0.0840, -0.0242, -0.0834]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.4612, -0.1616, -0.1630,  ...,  2.4239,  0.9283,  0.2499],\n",
      "         [ 0.1837, -0.0242,  0.3881,  ...,  1.2177, -0.0386,  0.3222],\n",
      "         [ 0.1440,  0.0171, -0.1371,  ...,  0.8039,  0.9764,  0.4666],\n",
      "         ...,\n",
      "         [-0.0132,  0.0463, -0.0188,  ..., -0.0794, -0.0257, -0.0908],\n",
      "         [-0.1086,  0.3433, -0.0595,  ..., -0.1434,  0.1659, -0.1533],\n",
      "         [-0.0122,  0.0508, -0.0162,  ..., -0.0780, -0.0338, -0.0795]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5814,  0.0120,  0.1730,  ...,  2.1467,  0.7514, -0.0044],\n",
      "         [ 0.0508,  0.0513,  0.1563,  ...,  0.7547,  0.4186,  0.4667],\n",
      "         [ 0.4550,  0.2164,  0.3842,  ...,  0.4099, -0.2173, -0.0236],\n",
      "         ...,\n",
      "         [ 0.0127,  0.0401, -0.0064,  ..., -0.0743, -0.0256, -0.0778],\n",
      "         [-0.0138,  0.0518, -0.0166,  ..., -0.0741, -0.0305,  0.0175],\n",
      "         [-0.0031,  0.0364, -0.0178,  ..., -0.0941, -0.0314, -0.0847]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0800e+00, -2.4758e-01, -1.2233e-02,  ...,  2.2241e+00,\n",
      "           4.9643e-01, -4.8245e-02],\n",
      "         [-1.6027e-02,  3.0247e-01, -1.7323e-01,  ...,  2.2750e+00,\n",
      "          -4.4938e-02,  1.9566e-01],\n",
      "         [ 5.3345e-01, -4.4416e-01,  6.7047e-01,  ...,  1.5336e+00,\n",
      "           7.1031e-01,  4.5310e-01],\n",
      "         ...,\n",
      "         [ 1.7166e-03,  3.9517e-02, -1.6540e-02,  ..., -8.8599e-02,\n",
      "          -1.2172e-02, -7.2306e-02],\n",
      "         [-1.5960e-02,  5.2744e-02, -2.0995e-02,  ..., -7.4216e-02,\n",
      "          -3.6939e-02, -7.6915e-02],\n",
      "         [ 1.9937e-01,  1.9628e-01, -2.4176e-01,  ...,  5.9595e-02,\n",
      "           4.5650e-02, -3.3047e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 1.0309, -0.0291, -0.1876,  ...,  1.8689,  0.8486, -0.1811],\n",
      "         [ 0.0708,  0.8655, -0.1555,  ...,  0.9113,  0.5784, -0.1796],\n",
      "         [ 0.1163,  0.0415, -0.0031,  ...,  0.4200, -0.2132, -0.1395],\n",
      "         ...,\n",
      "         [-0.1382,  0.2219, -0.0800,  ..., -0.4944,  0.0498,  0.0069],\n",
      "         [-0.0051,  0.0469, -0.0096,  ..., -0.0790, -0.0326, -0.0862],\n",
      "         [-0.0106,  0.0399, -0.0206,  ..., -0.0876, -0.0164, -0.0997]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 6.6809e-01,  3.6657e-01,  4.3400e-01,  ...,  2.2122e+00,\n",
      "           1.4532e+00, -2.4201e-04],\n",
      "         [-1.0444e-01, -5.9877e-01,  6.4323e-01,  ...,  9.1739e-01,\n",
      "           6.4080e-01,  4.0435e-01],\n",
      "         [ 4.1042e-01,  4.4795e-01, -2.0108e-01,  ...,  1.3843e+00,\n",
      "           5.6288e-01,  1.6793e-01],\n",
      "         ...,\n",
      "         [-7.6316e-03,  4.3035e-02, -3.6653e-02,  ..., -8.6995e-02,\n",
      "          -4.5073e-02, -8.3051e-02],\n",
      "         [-1.6530e-02,  3.2018e-02, -2.4796e-02,  ..., -8.9039e-02,\n",
      "          -3.1336e-02, -8.3647e-02],\n",
      "         [-7.1537e-02,  2.1280e-01,  2.9006e-02,  ...,  9.6093e-02,\n",
      "          -9.1497e-02, -1.7184e-02]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.6555,  0.4109, -0.4799,  ...,  1.8263,  0.4151,  0.0567],\n",
      "         [ 0.1971,  0.2784,  0.1382,  ...,  1.3049,  0.4404, -0.0172],\n",
      "         [ 0.2178,  0.4388, -0.1322,  ...,  0.6567,  0.2070,  0.0447],\n",
      "         ...,\n",
      "         [-0.0119,  0.0417, -0.0205,  ..., -0.0864, -0.0392, -0.0935],\n",
      "         [-0.1478,  0.4164,  0.0371,  ...,  0.0832,  0.0868, -0.1156],\n",
      "         [-0.0042,  0.0469, -0.0105,  ..., -0.1173, -0.0316, -0.0779]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
      "sequence_output: tensor([[[ 0.5232,  0.4987, -0.0870,  ...,  0.9976,  0.9423,  0.3685],\n",
      "         [-0.2840,  0.6911, -0.7069,  ...,  1.1178,  0.2473, -0.3865],\n",
      "         [ 0.0125, -0.2433, -0.5016,  ...,  0.1655,  0.8995,  0.0733],\n",
      "         ...,\n",
      "         [ 0.0541,  0.1632,  0.0400,  ..., -0.0748, -0.0368, -0.2074],\n",
      "         [-0.0117,  0.0341, -0.0298,  ..., -0.0830, -0.0373, -0.0857],\n",
      "         [ 0.0696,  0.1865, -0.1403,  ..., -0.1742,  0.0332, -0.0466]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  0\n",
      "qid:  5adfe0de55429925eb1afae9\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 1.3272,  0.0337, -0.0536,  ...,  1.1715,  0.5718, -0.2857],\n",
      "         [ 0.0989,  0.0191,  0.1177,  ...,  0.6020,  0.2977,  0.1233],\n",
      "         [ 0.3119,  0.2673, -0.1249,  ...,  1.1107,  0.7067, -0.2803],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([1.6055, 0.9624, 0.6201,  ..., 1.5869, 1.1250, 0.3718], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.2773, -0.1388,  1.9316,  ..., -0.0377,  2.7461,  0.4766],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  1\n",
      "qid:  5ac3e80b554299076e296ccd\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.6679, -0.1575, -0.3152,  ...,  2.2563,  0.4480, -0.0764],\n",
      "         [ 0.2683, -0.2630,  0.1946,  ...,  1.0802,  0.1396,  0.2076],\n",
      "         [ 0.3657, -0.5258, -0.1498,  ...,  1.1249,  0.9795,  0.0077],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.4199e+00,  1.1416e+00,  1.8643e+00,  1.1865e+00, -1.1709e+00,\n",
      "        -2.5952e-01, -9.9561e-01, -6.9043e-01, -5.8789e-01,  2.4731e-01,\n",
      "         3.5020e+00,  1.2891e+00,  1.7334e+00,  8.2666e-01, -4.5996e-01,\n",
      "         7.0459e-01,  1.0950e-01,  6.7871e-02,  4.5166e-02,  4.7168e-01,\n",
      "         3.9277e+00,  1.2305e+00,  1.2285e+00,  7.5635e-01,  1.2676e+00,\n",
      "         3.3926e+00,  1.5039e+00, -1.2756e-01, -6.7090e-01, -1.2354e+00,\n",
      "        -5.9912e-01,  1.0010e+00, -1.2637e+00, -3.7988e-01,  2.3730e+00,\n",
      "         3.4912e-01,  8.5303e-01,  1.7566e-01,  1.9531e-01, -3.5889e-01,\n",
      "         5.3760e-01,  5.6104e-01,  1.7505e-01,  2.1738e+00,  6.6016e-01,\n",
      "         7.6514e-01, -7.8760e-01,  4.9243e-01,  1.7371e-01,  7.2998e-02,\n",
      "         1.4619e+00, -9.7998e-01,  3.0225e-01, -6.1182e-01,  6.7871e-02,\n",
      "         2.9473e+00,  2.9883e-01,  9.8828e-01,  4.0625e-01,  3.4375e-01,\n",
      "        -8.6279e-01,  7.6904e-01, -1.7310e-01,  4.0088e-01,  1.3208e-01,\n",
      "         2.8438e+00,  5.8301e-01,  5.8691e-01,  8.5254e-01,  1.1975e-01,\n",
      "         4.3970e-01,  4.4961e+00,  1.3818e+00,  1.0693e+00,  5.8496e-01,\n",
      "        -1.5173e-01,  7.5806e-02,  1.5137e-01,  4.4375e+00,  1.1016e+00,\n",
      "         1.0283e+00,  5.3760e-01,  1.0957e+00, -4.6069e-01,  2.1436e-01,\n",
      "         1.1462e-01, -2.0471e-01, -5.2246e-01,  7.5293e-01,  1.1982e+00,\n",
      "         1.9913e-02, -1.8579e-01, -1.2817e-01, -5.5664e-01,  2.9199e+00,\n",
      "        -8.8867e-01, -2.0935e-01,  7.0068e-02,  5.2344e-01,  3.6738e+00,\n",
      "         7.3438e-01,  4.6631e-01,  1.0352e+00,  3.9307e-02,  5.8838e-01,\n",
      "         8.6823e-03, -1.1475e-01, -5.8984e-01,  1.1309e+00,  6.6016e-01,\n",
      "         4.7876e-01, -3.7939e-01, -9.8486e-01, -3.2422e-01, -6.9336e-01,\n",
      "         3.3789e+00,  1.0381e+00,  1.4951e+00,  7.3291e-01, -7.2998e-02,\n",
      "        -3.9307e-01, -8.2642e-02,  9.8633e-02, -7.2803e-01,  2.9102e+00,\n",
      "        -7.1533e-01,  3.1426e+00,  6.0205e-01,  1.1982e+00,  3.3936e-01,\n",
      "        -4.0210e-01,  1.4331e-01,  2.3066e+00, -8.6963e-01,  1.0339e-01,\n",
      "        -5.1709e-01, -1.9629e-01,  1.5830e+00,  3.7256e-01, -6.2256e-03,\n",
      "         2.2461e+00,  6.3672e-01, -2.2998e-01,  1.0303e+00,  3.9087e-01,\n",
      "         1.1240e+00,  4.1943e-01,  1.4087e-01,  8.1396e-01,  2.7109e+00,\n",
      "         5.9717e-01,  9.8535e-01, -2.0984e-01, -8.1299e-01,  2.9727e+00,\n",
      "         3.1348e-01,  1.0400e+00,  1.1885e+00, -6.3818e-01, -3.4521e-01,\n",
      "        -3.2007e-01,  2.7246e+00,  4.8755e-01, -5.3662e-01,  2.1152e+00,\n",
      "         4.9683e-01, -6.6748e-01, -1.0300e-02,  4.2539e+00,  7.1484e-01,\n",
      "         5.0195e-01,  1.2637e+00,  1.3159e-01, -7.5537e-01,  5.0928e-01,\n",
      "         2.7881e-01,  4.9683e-01, -7.7539e-01,  1.4688e+00,  2.6978e-01,\n",
      "        -4.4849e-01,  2.7266e+00,  7.7344e-01,  1.1121e-01,  3.9551e-01,\n",
      "         3.9062e-01,  4.3320e+00,  1.6309e+00,  1.4980e+00,  1.1348e+00,\n",
      "         4.4141e+00,  1.3340e+00,  1.1426e+00,  5.8594e-01,  8.8232e-01,\n",
      "         4.0820e+00,  1.2842e+00,  1.0029e+00,  1.6074e+00, -5.6543e-01,\n",
      "        -1.1113e+00, -5.4590e-01,  2.4395e+00,  9.3457e-01,  1.6426e+00,\n",
      "         6.3770e-01, -6.8994e-01,  4.0586e+00,  1.5635e+00,  4.0356e-01,\n",
      "         8.5156e-01, -9.6533e-01,  1.1536e-01,  1.9922e-01,  3.8477e-01,\n",
      "         1.9785e+00,  4.9390e-01,  1.5195e+00,  4.6338e-01, -8.5400e-01,\n",
      "         2.5957e+00,  2.7832e+00,  1.4814e+00,  2.0435e-01,  5.4736e-01,\n",
      "         3.6841e-01, -4.8608e-01, -1.9641e-01, -1.9379e-03,  1.9861e-01,\n",
      "         3.1719e+00,  1.2246e+00,  1.1113e+00,  4.0723e-01,  4.4287e-01,\n",
      "         2.8467e-01,  9.8340e-01,  2.9272e-01,  2.5049e-01,  1.6250e+00,\n",
      "         1.1536e-01,  1.4287e+00,  7.4219e-02,  3.1934e-01,  6.6162e-02,\n",
      "        -3.8599e-01,  3.4453e+00,  9.5215e-01,  1.0771e+00,  4.1748e-01,\n",
      "         2.8638e-01,  3.9966e-01,  1.4561e+00,  1.1504e+00,  4.6045e-01,\n",
      "         1.9072e+00, -8.6133e-01,  3.6367e+00,  1.4170e+00,  2.7588e-01,\n",
      "         3.6938e-01, -1.1337e-02,  5.7178e-01,  2.9062e+00,  2.2681e-01,\n",
      "         7.5879e-01,  7.6172e-01,  2.7559e+00,  1.0615e+00,  4.3945e-01,\n",
      "         8.0383e-02,  2.9668e+00,  1.3076e+00,  1.0088e+00,  4.6509e-01,\n",
      "         2.4688e+00,  9.1943e-01,  2.1899e-01,  3.5181e-01, -3.2129e-01,\n",
      "         3.0859e+00,  3.1812e-01,  9.7607e-01,  4.5996e-01,  1.9678e-01,\n",
      "        -3.2617e-01, -5.9033e-01, -1.6370e-01,  2.1211e+00, -6.5308e-02,\n",
      "        -8.2324e-01,  2.6504e+00,  5.6201e-01,  1.0967e+00, -5.6738e-01,\n",
      "         1.9287e+00,  5.7080e-01,  1.1035e+00, -5.5762e-01,  2.0898e+00,\n",
      "         3.2104e-01, -4.3774e-01, -4.7485e-01,  1.8799e+00,  8.0957e-01,\n",
      "        -4.7632e-01, -2.7173e-01,  1.0352e+00, -2.5293e-01,  7.7490e-01,\n",
      "        -1.9263e-01, -2.7246e-01,  3.3032e-01, -6.4795e-01,  2.3315e-01,\n",
      "         4.0967e-01,  3.6133e+00,  6.2012e-01,  8.7598e-01,  3.2104e-01,\n",
      "         1.2598e+00, -3.7427e-01,  5.1172e-01,  1.7883e-01,  1.0449e-01,\n",
      "        -8.8135e-01,  1.9250e-01, -1.2461e+00,  3.1309e+00,  3.4790e-02,\n",
      "         1.0713e+00,  2.7759e-01,  3.9766e+00,  7.3047e-01,  4.6680e-01,\n",
      "        -4.3262e-01, -2.3572e-01,  3.2793e+00,  7.3975e-01,  1.0244e+00,\n",
      "        -8.4290e-02,  6.2549e-01,  4.3677e-01,  1.0986e+00,  4.3115e-01,\n",
      "         3.4497e-01,  1.3330e-01, -5.9619e-01,  3.4355e+00,  1.5430e+00,\n",
      "         6.2061e-01,  1.1938e-01,  4.2749e-01,  2.2969e+00,  5.0244e-01,\n",
      "         1.7471e+00,  7.2754e-01, -1.2686e+00,  4.2334e-01, -1.4209e-01,\n",
      "        -1.1904e+00, -2.6367e-01,  1.1885e+00, -7.5977e-01,  6.3867e-01,\n",
      "        -8.2471e-01,  1.1494e+00,  6.7969e-01,  3.1372e-01, -1.2500e+00,\n",
      "         1.6174e-01, -6.9775e-01, -1.9739e-01,  1.2246e+00, -1.9934e-01,\n",
      "         8.3923e-02,  4.7144e-01, -4.6240e-01, -1.8005e-01,  6.5625e-01,\n",
      "        -6.9189e-01,  3.7573e-01, -6.8018e-01,  5.7178e-01, -7.1484e-01,\n",
      "         4.2358e-01, -9.5020e-01, -2.8687e-01, -1.2812e+00,  1.9421e-01,\n",
      "        -4.2358e-01, -9.5898e-01, -3.9624e-01, -3.1592e-01, -9.4434e-01,\n",
      "         7.8564e-01, -4.1846e-01,  7.3547e-02,  6.0791e-01,  3.8452e-03,\n",
      "        -6.2158e-01,  4.7168e-01,  3.6084e-01, -8.0566e-01,  7.8223e-01,\n",
      "         6.0205e-01, -2.4170e-01,  5.6104e-01, -1.0342e+00,  3.8037e-01,\n",
      "         1.3269e-01, -3.0444e-01,  1.7993e-01, -6.1328e-01, -4.3640e-02,\n",
      "        -3.0542e-01, -7.5488e-01,  7.5244e-01, -8.0029e-01,  1.0852e-01,\n",
      "         2.6733e-01, -2.9224e-01,  2.9953e-02,  1.2061e-01,  4.0210e-01,\n",
      "         4.2812e+00,  1.5400e+00,  8.9307e-01,  3.6758e+00,  1.3193e+00,\n",
      "         6.0205e-01,  8.3008e-01,  3.0391e+00,  9.0820e-01,  1.6123e+00,\n",
      "        -5.1807e-01,  4.2969e+00,  1.3379e+00,  7.6807e-01,  2.4160e+00,\n",
      "         1.4238e+00,  3.7134e-01,  2.8516e-01, -1.1973e+00,  2.1851e-01,\n",
      "         2.6245e-01, -5.1709e-01, -6.2073e-02,  4.3091e-02, -1.0364e-01,\n",
      "        -5.8441e-03,  2.6289e+00, -1.2549e+00, -7.2632e-02, -1.5312e+00,\n",
      "         5.9863e-01,  3.4717e-01, -7.0264e-01,  2.1426e+00, -8.3618e-02,\n",
      "         1.5454e-01,  3.8301e+00,  7.2559e-01,  5.3027e-01,  1.3074e-01,\n",
      "         3.9453e-01,  2.9434e+00,  1.0850e+00,  2.1621e+00,  8.9307e-01,\n",
      "        -1.0381e+00,  2.9941e+00,  3.1836e+00,  1.4004e+00,  4.2822e-01,\n",
      "         5.2002e-01,  2.5903e-01,  3.0136e-02, -5.0439e-01,  6.4648e-01,\n",
      "        -1.2969e+00,  1.4619e+00,  1.2432e+00, -8.5693e-02,  3.4258e+00,\n",
      "         4.8560e-01,  9.1553e-01,  2.8052e-01,  1.8828e+00,  2.4463e-01,\n",
      "         1.1328e+00,  7.5195e-02,  3.5815e-01,  1.7163e-01, -4.8486e-01,\n",
      "         3.9766e+00,  1.6475e+00,  1.5078e+00,  8.1006e-01,  2.4683e-01,\n",
      "        -1.4160e+00, -1.3096e+00, -7.8613e-01, -3.2013e-02, -3.8965e-01,\n",
      "         2.8848e+00,  5.2197e-01,  5.8887e-01,  1.5859e+00,  5.7007e-02,\n",
      "         4.1479e-01,  3.8340e+00,  1.6094e+00,  1.0430e+00,  3.1777e+00,\n",
      "         1.5342e+00, -1.2734e+00,  3.2109e+00,  3.4199e+00,  2.3169e-01,\n",
      "         1.9958e-01,  9.5801e-01,  9.9121e-01, -3.5352e-01, -3.3618e-01,\n",
      "         4.8828e+00,  2.1016e+00,  3.1992e+00,  7.1582e-01,  8.3252e-01,\n",
      "        -1.5908e+00, -2.3193e-01, -7.6514e-01,  4.3555e+00,  1.6953e+00,\n",
      "         2.2188e+00,  5.1465e-01,  7.2021e-01,  4.8706e-01, -9.3408e-01,\n",
      "        -1.0566e+00,  3.2559e+00,  1.2090e+00,  1.2188e+00,  2.9932e-01,\n",
      "         2.9004e-01,  1.2439e-01,  6.8506e-01, -6.2402e-01,  4.0131e-03,\n",
      "         2.0340e-02, -3.0542e-01,  4.4258e+00,  9.6631e-01,  1.1680e+00,\n",
      "         6.1621e-01, -1.8970e-01, -5.8838e-01,  4.2031e+00,  7.1533e-01,\n",
      "         9.4678e-01, -5.9601e-02,  4.8071e-01, -2.7295e-01, -4.4385e-01,\n",
      "         1.7090e-01, -1.1133e+00, -4.5947e-01,  2.1313e-01, -6.6895e-01,\n",
      "         1.0527e+00,  3.6987e-01, -2.3291e-01, -1.7822e-01, -2.1252e-01,\n",
      "         1.9004e+00, -5.9277e-01, -2.5146e-01, -5.2734e-01, -4.7095e-01,\n",
      "        -5.3174e-01,  4.7617e+00,  2.1094e+00,  2.0840e+00,  1.0059e+00,\n",
      "         2.1211e+00,  5.5713e-01,  6.0010e-01,  3.0684e+00,  8.2422e-01,\n",
      "         1.5801e+00,  7.6514e-01,  2.8574e+00,  1.8936e+00,  2.0195e+00,\n",
      "         1.5908e+00,  1.5811e+00,  9.2139e-01, -4.3506e-01,  9.1748e-01,\n",
      "        -3.7628e-02,  2.0581e-01,  2.6289e+00,  1.8330e+00,  1.5186e+00,\n",
      "         1.1611e+00,  1.7158e+00,  6.1670e-01,  2.0020e+00,  1.2471e+00,\n",
      "         3.5840e-01, -1.2715e+00,  2.8442e-01,  4.9766e+00,  9.5068e-01,\n",
      "         1.0576e+00,  1.7734e+00,  8.4473e-01, -8.6670e-01, -4.0454e-01,\n",
      "         1.4590e+00, -8.6914e-02, -1.0931e-01,  2.3209e-02,  4.4414e+00,\n",
      "         1.7637e+00,  2.3867e+00,  1.5176e+00,  2.9961e+00,  1.3984e+00,\n",
      "         2.7734e+00,  1.7070e+00,  1.0547e+00,  4.4092e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 2.2070e+00, -7.7490e-01,  1.8652e-01,  1.5312e+00, -1.3496e+00,\n",
      "        -1.1025e+00, -1.2324e+00,  1.9943e-02, -1.1133e+00, -8.6768e-01,\n",
      "        -5.7031e-01,  5.1123e-01,  3.7842e-01,  2.2871e+00, -7.8760e-01,\n",
      "         5.8441e-02,  1.0399e-02,  6.2158e-01,  3.8818e-01,  6.0400e-01,\n",
      "        -2.4768e-01,  7.7686e-01,  8.2178e-01,  4.2617e+00,  2.0938e+00,\n",
      "        -6.4941e-02,  3.5820e+00,  1.3818e+00, -2.3132e-01,  8.0750e-02,\n",
      "        -6.2598e-01,  1.1553e+00,  4.7760e-02, -1.0576e+00, -3.6646e-01,\n",
      "         1.9727e-01,  1.3586e-01,  2.8394e-01,  1.9863e+00, -5.1318e-01,\n",
      "         2.7051e-01,  1.3989e-01,  1.3000e-01,  5.3320e-01,  6.6211e-01,\n",
      "         2.1699e+00, -9.0625e-01, -4.5776e-01, -2.9102e-01, -2.7759e-01,\n",
      "         1.1621e+00, -1.0059e+00, -7.7588e-01,  1.8661e-02, -9.4287e-01,\n",
      "         3.1860e-02,  7.0020e-01,  3.1470e-01,  8.2373e-01,  2.5371e+00,\n",
      "        -9.8389e-01, -5.2930e-01, -8.3374e-02,  3.4619e-01,  5.6836e-01,\n",
      "        -4.2773e-01,  6.1914e-01,  1.2090e+00,  3.6016e+00,  4.8615e-02,\n",
      "         5.6787e-01, -4.8315e-01,  1.0449e+00,  9.7363e-01,  4.5391e+00,\n",
      "         1.7914e-02,  6.2347e-02,  1.1554e-01, -4.4531e-01,  8.5107e-01,\n",
      "         9.8291e-01,  4.3906e+00,  1.4805e+00, -8.2520e-01,  4.3115e-01,\n",
      "         5.1453e-02,  7.2314e-01, -1.2627e+00,  2.5467e-02,  2.3594e+00,\n",
      "        -6.9824e-01, -5.0781e-01, -3.9014e-01, -1.4834e+00,  8.1299e-01,\n",
      "         8.9307e-01, -6.6357e-01, -2.6416e-01, -2.2058e-01, -6.3672e-01,\n",
      "        -2.3669e-01, -6.7566e-02,  4.1455e-01,  2.0586e+00, -2.8711e-01,\n",
      "         1.4346e+00,  3.9575e-01, -9.4238e-01,  1.1896e-01,  3.0273e-01,\n",
      "         7.5342e-01, -9.1309e-01, -1.2148e+00, -4.9194e-01, -8.9844e-01,\n",
      "        -2.0056e-01,  2.6318e-01,  9.4177e-02,  4.3750e+00, -3.2812e-01,\n",
      "        -4.1504e-01, -3.5938e-01, -9.6240e-01, -1.6387e+00,  6.2061e-01,\n",
      "         3.2104e-01,  1.3953e-01,  2.4548e-01,  9.3701e-01,  4.6367e+00,\n",
      "        -7.6367e-01, -1.1396e+00,  1.0264e+00, -7.2510e-01, -7.4512e-01,\n",
      "        -8.8379e-01, -8.1055e-01, -6.7969e-01, -8.7891e-02,  1.5576e-01,\n",
      "        -6.4941e-01,  6.5576e-01,  1.8311e+00, -3.5083e-01, -1.5198e-01,\n",
      "        -6.6309e-01,  4.2139e-01,  6.5527e-01,  9.0625e-01, -2.6904e-01,\n",
      "         1.5684e+00,  7.9297e-01,  3.2734e+00, -7.1631e-01, -3.8818e-01,\n",
      "         1.1627e-01,  1.4092e+00,  1.0381e+00,  5.1367e-01, -1.0068e+00,\n",
      "        -1.0918e+00, -6.2500e-01,  1.8584e+00, -9.8096e-01,  4.6582e-01,\n",
      "         1.6133e+00, -1.5410e+00, -1.4580e+00, -7.6904e-01, -3.5449e-01,\n",
      "        -1.8896e-01,  2.6367e-01,  2.3691e+00, -3.0273e-02, -9.8877e-01,\n",
      "         3.4717e-01, -2.2522e-01, -3.6157e-01,  1.9885e-01,  2.0430e+00,\n",
      "        -1.1230e+00, -5.3076e-01,  6.5576e-01,  3.5898e+00,  7.1350e-02,\n",
      "         5.1611e-01, -3.0737e-01,  8.7891e-01,  6.5625e-01,  4.5352e+00,\n",
      "        -3.6426e-01,  6.9971e-01,  6.9580e-01,  3.9102e+00, -2.0129e-01,\n",
      "        -4.1675e-01,  8.1299e-01,  1.3965e+00,  4.9453e+00, -6.8799e-01,\n",
      "        -6.0608e-02, -7.1826e-01, -6.0449e-01,  5.4150e-01,  3.6719e-01,\n",
      "         3.1426e+00, -4.5703e-01,  5.3223e-01,  3.4912e-01,  2.0020e+00,\n",
      "         1.5967e+00, -6.7383e-01,  1.5771e+00, -6.5186e-01,  5.0146e-01,\n",
      "        -2.2400e-01,  7.7393e-01, -3.1372e-01,  8.7695e-01, -1.6289e+00,\n",
      "         1.6943e+00, -7.5256e-02,  6.9763e-02,  1.7461e+00,  2.8833e-01,\n",
      "        -4.4971e-01, -4.1357e-01,  8.6035e-01,  6.5137e-01, -8.3789e-01,\n",
      "        -1.9067e-01,  5.4053e-01,  3.2135e-02,  9.5850e-01, -1.8298e-01,\n",
      "         1.8145e+00, -1.1289e+00,  1.0704e-02,  7.3486e-01, -5.3027e-01,\n",
      "         2.7075e-01, -6.7871e-01,  1.5112e-01,  5.2881e-01,  2.7324e+00,\n",
      "        -5.2588e-01, -3.3350e-01,  4.8511e-01,  4.6851e-01,  4.0742e+00,\n",
      "         3.1958e-01,  5.2881e-01,  8.3350e-01,  8.2336e-02, -4.9622e-02,\n",
      "         9.9268e-01, -1.6035e+00, -1.6663e-02, -3.4180e-02,  1.9482e+00,\n",
      "         5.1855e-01,  4.6240e-01, -1.0508e+00, -9.4055e-02,  4.0625e-01,\n",
      "         2.1211e+00,  6.0889e-01, -4.8120e-01, -6.2500e-01, -1.4221e-01,\n",
      "         1.5186e+00, -6.4014e-01,  5.0586e-01,  2.3254e-01,  2.3359e+00,\n",
      "         2.9614e-01,  4.8901e-01,  6.2561e-02,  1.9854e+00, -6.1475e-01,\n",
      "        -7.9150e-01,  9.5825e-02, -2.7979e-01,  2.3755e-01,  2.9590e+00,\n",
      "        -1.2236e+00, -9.1943e-01, -8.8184e-01, -1.5088e-01,  5.8447e-01,\n",
      "        -1.0381e+00, -4.5752e-01,  3.2275e-01,  7.0410e-01, -1.1494e+00,\n",
      "        -3.1445e-01,  8.0762e-01,  3.5156e+00, -5.6494e-01, -1.0742e+00,\n",
      "        -4.7876e-01,  8.4033e-01, -1.1133e+00, -8.3887e-01, -8.2227e-01,\n",
      "         5.1270e-01, -9.7607e-01, -5.4590e-01,  2.0056e-01, -3.5547e-01,\n",
      "         1.0947e+00, -7.9688e-01, -3.1860e-01,  2.8381e-02, -6.9385e-01,\n",
      "         5.4980e-01, -6.2378e-02,  3.4448e-01,  7.7393e-01,  4.0273e+00,\n",
      "         1.1543e+00, -8.9893e-01,  1.2585e-01, -2.9346e-01,  4.3018e-01,\n",
      "        -1.5605e+00, -7.5049e-01, -8.1055e-01, -2.0483e-01,  1.1025e+00,\n",
      "         1.0547e+00,  1.7139e-01, -2.2278e-01,  3.2861e-01,  4.4219e+00,\n",
      "        -6.7334e-01, -1.0557e+00, -4.7668e-02,  8.4915e-03, -1.5442e-01,\n",
      "         7.7686e-01, -2.2644e-01,  5.6494e-01,  6.9434e-01,  3.5962e-01,\n",
      "         7.6904e-01,  3.2539e+00, -8.0566e-01,  2.3682e-01,  4.4043e-01,\n",
      "         3.9980e+00, -7.1167e-02,  5.6836e-01,  9.5020e-01,  1.1016e+00,\n",
      "         5.0293e-01,  1.5674e+00, -1.4111e+00, -1.8713e-01, -3.3594e-01,\n",
      "        -1.3848e+00, -1.2559e+00,  2.8671e-02, -9.8877e-01, -1.0156e+00,\n",
      "        -1.0371e+00, -1.5283e-01, -1.1304e-01,  1.2168e+00, -1.4453e+00,\n",
      "        -1.2512e-01, -1.0605e+00, -1.1689e+00, -5.6299e-01, -5.6445e-01,\n",
      "        -1.0693e+00, -3.0981e-01, -7.9004e-01, -7.8613e-02,  2.2107e-01,\n",
      "        -9.3555e-01, -3.0054e-01, -2.5879e-01,  1.9983e-01, -8.7842e-01,\n",
      "        -1.2952e-01, -3.0713e-01, -2.0605e-01, -1.7363e+00, -4.8950e-01,\n",
      "        -9.6533e-01, -1.3945e+00, -1.4082e+00, -6.2598e-01, -4.4800e-01,\n",
      "        -1.5430e-01, -1.3223e+00, -3.7695e-01, -1.2607e+00, -6.2061e-01,\n",
      "        -1.8665e-01, -2.7002e-01, -9.2188e-01, -8.4521e-01, -8.5059e-01,\n",
      "        -7.2314e-01, -6.3721e-01, -3.7500e-01, -1.4268e+00, -8.8379e-01,\n",
      "        -5.6104e-01, -1.9177e-01, -4.5532e-01, -8.2617e-01, -7.7588e-01,\n",
      "        -1.0315e-01, -1.3936e+00, -2.9053e-01, -2.7124e-01, -4.3042e-01,\n",
      "        -2.3621e-01, -4.3286e-01, -6.4502e-01, -6.4746e-01,  5.2148e-01,\n",
      "         5.8496e-01,  7.1924e-01,  4.5312e+00,  8.1592e-01,  9.9365e-01,\n",
      "         4.8047e+00, -4.7485e-02,  2.6562e-01,  1.3076e+00,  4.3672e+00,\n",
      "        -5.4004e-01, -1.8359e-01,  3.3667e-01,  2.5410e+00,  7.7344e-01,\n",
      "         2.8491e-01,  2.1484e+00,  1.0303e+00, -7.3926e-01,  1.7002e+00,\n",
      "        -4.6436e-01, -8.0908e-01, -1.6406e-01, -1.3806e-01,  4.9487e-01,\n",
      "        -1.2412e+00,  1.9668e+00, -1.3477e+00, -7.2119e-01, -6.4355e-01,\n",
      "        -2.6855e-01, -9.7534e-02, -8.8013e-02, -1.8481e-01,  1.2168e+00,\n",
      "        -4.5044e-02, -1.2524e-01,  2.2009e-01,  4.6484e+00, -1.5125e-01,\n",
      "         5.1855e-01, -5.8203e-01,  2.2598e+00, -9.7314e-01,  2.2070e+00,\n",
      "        -1.3057e+00,  1.7568e+00,  5.0879e-01,  2.4170e-01,  2.1836e+00,\n",
      "         2.3965e+00,  1.3750e+00, -4.7681e-01, -1.0918e+00, -5.0659e-02,\n",
      "        -7.2656e-01,  3.6163e-02,  3.8828e+00, -6.5283e-01, -1.9458e-01,\n",
      "         5.0098e-01,  1.6089e-01,  1.3945e+00,  2.0593e-01,  5.3369e-01,\n",
      "         7.8271e-01,  8.2568e-01,  8.2422e-01,  3.5039e+00, -3.4375e-01,\n",
      "        -1.1792e-01,  1.0371e+00,  1.1279e+00,  4.6992e+00,  2.9961e+00,\n",
      "        -9.3994e-01, -4.7827e-01, -1.0703e+00,  8.6060e-02, -7.2803e-01,\n",
      "         4.2871e-01,  3.0664e-01,  1.9268e+00,  3.9512e+00, -1.9580e-01,\n",
      "         5.5420e-01,  6.5039e-01,  3.5039e+00,  4.3726e-01, -1.2122e-01,\n",
      "         3.4785e+00, -1.1934e+00,  1.7871e+00,  9.5703e-01,  2.8184e+00,\n",
      "         1.1914e+00,  5.6641e-01, -3.9526e-01,  2.5122e-01, -8.1494e-01,\n",
      "         8.8562e-02,  1.6953e+00,  5.8936e-01,  1.1182e+00,  5.1680e+00,\n",
      "        -1.4443e+00, -6.5137e-01, -1.3701e+00,  2.0190e-01,  2.4844e+00,\n",
      "         1.2891e+00,  1.1240e+00,  2.7812e+00,  4.7578e+00, -8.2129e-01,\n",
      "        -9.0430e-01,  1.3110e-01,  2.0098e+00,  1.8335e-01,  2.0093e-01,\n",
      "         4.4019e-01,  9.0039e-01,  1.3403e-01, -3.8550e-01, -7.3242e-01,\n",
      "        -1.2354e-01, -9.2383e-01, -1.1609e-01,  6.2598e-01,  1.2852e+00,\n",
      "         5.0586e+00, -7.6074e-01, -1.1777e+00,  1.0902e-02,  1.1188e-01,\n",
      "         3.6953e+00,  1.5713e+00, -4.6753e-02, -6.7017e-02, -1.2148e+00,\n",
      "        -3.2654e-02, -1.4316e+00, -1.4180e+00, -1.7249e-01, -6.3721e-01,\n",
      "         4.8022e-01, -1.4099e-01, -1.9989e-02, -5.5469e-01, -1.0225e+00,\n",
      "         2.5195e-01, -4.8804e-01, -9.3799e-01, -9.7900e-01, -6.5723e-01,\n",
      "        -1.3359e+00,  6.2065e-03,  6.4697e-01,  1.4229e+00,  3.0703e+00,\n",
      "         8.4766e-01,  9.4629e-01,  1.9678e+00,  1.7168e+00,  2.0020e+00,\n",
      "         1.7197e+00,  1.6807e+00, -5.4834e-01,  9.7510e-01,  5.1758e-01,\n",
      "         1.6689e+00,  1.5850e+00,  4.1016e+00, -1.2939e+00,  1.3416e-01,\n",
      "         3.1274e-01,  2.6807e-01, -3.9404e-01,  4.6509e-01,  7.2449e-02,\n",
      "         2.3223e+00,  4.5020e-01,  8.2422e-01,  1.8096e+00,  2.1113e+00,\n",
      "         4.6406e+00, -8.0469e-01, -1.1201e+00, -1.9617e-01,  2.9199e-01,\n",
      "         2.0957e+00,  2.1875e+00,  5.3438e+00, -1.0098e+00, -1.4004e+00,\n",
      "         9.4727e-01,  6.7236e-01,  1.0322e+00, -8.9990e-01,  2.6611e-01,\n",
      "         1.3389e+00,  1.7295e+00,  3.3633e+00,  2.7656e+00,  3.2168e+00,\n",
      "        -3.5572e-04,  2.4160e+00,  5.3828e+00,  5.7910e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  2\n",
      "qid:  5ae7ff745542994a481bbe6e\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 1.0503, -0.0926, -0.1156,  ...,  1.2460,  0.5839, -0.0081],\n",
      "         [-0.1483,  0.2095, -0.1387,  ...,  1.9610,  0.3808, -0.2610],\n",
      "         [ 0.8863, -0.3746, -0.0481,  ...,  1.4488,  1.2870, -0.2388],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.1396e+00,  1.2666e+00,  8.3447e-01, -2.9199e-01, -1.8774e-01,\n",
      "        -7.1680e-01,  3.1104e-01,  1.8750e-01, -1.0137e+00, -2.1472e-01,\n",
      "        -5.3955e-01,  8.9600e-01,  7.1680e-01, -2.3657e-01,  2.1326e-01,\n",
      "        -2.4023e-01, -9.9023e-01,  7.0361e-01,  3.5938e-01,  4.3750e+00,\n",
      "         1.4785e+00,  4.2617e+00,  5.8057e-01,  1.1807e+00,  3.3203e+00,\n",
      "         7.5830e-01,  7.7051e-01,  1.9619e+00,  1.3164e+00, -2.0630e-01,\n",
      "         4.7302e-02, -8.5840e-01,  9.1858e-02, -7.0343e-03, -1.0039e+00,\n",
      "         2.4146e-01, -2.4902e-02,  2.6992e+00, -7.3486e-02,  1.2578e+00,\n",
      "        -1.3340e+00,  5.1758e-01, -1.0518e+00,  5.1953e-01,  1.7993e-01,\n",
      "        -3.6035e-01,  4.3628e-01, -6.4062e-01,  3.1543e+00,  2.6562e-01,\n",
      "         1.1240e+00, -6.3184e-01,  4.4922e+00,  7.2949e-01,  1.2041e+00,\n",
      "         3.8843e-01, -3.7842e-01,  1.1917e-02, -2.9205e-02, -6.5869e-01,\n",
      "        -7.0166e-01, -5.0439e-01,  4.3555e-01, -9.3457e-01,  4.2891e+00,\n",
      "         1.0381e+00,  2.0654e-01, -6.1829e-02,  4.4570e+00,  6.3184e-01,\n",
      "         1.0986e+00,  2.0227e-01, -2.8125e-01,  4.4219e+00,  7.9346e-01,\n",
      "         1.1670e+00,  5.5859e-01,  2.5195e+00,  1.5564e-01,  1.0830e+00,\n",
      "        -1.6187e-01,  4.0977e+00,  5.1709e-01,  1.0020e+00,  4.9292e-01,\n",
      "        -7.1228e-02,  4.5898e-01,  1.3220e-01, -1.3013e-01,  1.9473e+00,\n",
      "         1.3452e-01, -5.8289e-03,  5.0684e-01,  3.7500e-01, -1.6631e+00,\n",
      "         2.3950e-01,  4.0859e+00,  9.7021e-01,  1.1562e+00,  4.8279e-02,\n",
      "         5.1074e-01,  9.8926e-01, -4.7168e-01,  2.4683e-01, -2.6904e-01,\n",
      "         4.1602e+00,  8.0859e-01,  1.4199e+00,  9.5886e-02,  1.7256e+00,\n",
      "         1.1504e+00,  2.0142e-01,  1.7505e-01,  4.2163e-01,  7.2656e-01,\n",
      "         4.0381e-01,  6.6260e-01,  3.2935e-01, -5.3369e-01,  3.2734e+00,\n",
      "         3.7036e-01,  6.4404e-01, -4.9902e-01,  4.1931e-02, -9.3506e-02,\n",
      "         2.7793e+00,  2.9956e-01,  1.6248e-01,  6.6162e-01,  1.6052e-01,\n",
      "         8.9893e-01,  3.1519e-01,  9.9512e-01, -3.5248e-02,  2.7051e-01,\n",
      "         3.4717e-01,  3.9375e+00,  1.6953e+00,  1.0742e-01,  3.1074e+00,\n",
      "         4.2212e-01, -3.1201e-01, -4.7028e-02, -8.3545e-01,  1.7517e-01,\n",
      "         1.0713e+00, -1.5771e-01,  5.8838e-01, -2.5439e-01,  4.4922e+00,\n",
      "         1.3447e+00,  1.3135e+00,  7.0703e-01, -2.4927e-01,  4.3047e+00,\n",
      "         1.2891e+00,  1.3896e+00,  2.6685e-01,  6.0938e-01,  1.7607e+00,\n",
      "         1.1543e+00,  4.6387e-01,  5.2344e-01,  3.2104e-01,  3.8867e-01,\n",
      "        -7.9492e-01,  1.5518e+00, -6.5869e-01, -8.0469e-01,  4.6216e-01,\n",
      "         3.5474e-01,  5.8301e-01, -1.1992e+00, -2.1716e-01, -7.4023e-01,\n",
      "         9.9512e-01,  1.2195e-01,  3.5205e-01, -1.1787e+00,  1.6748e+00,\n",
      "         1.5173e-01,  4.1562e+00, -2.0190e-01,  1.3584e+00,  3.4106e-01,\n",
      "         3.5132e-01,  1.4932e+00, -5.6738e-01, -3.6426e-01,  7.6758e-01,\n",
      "         4.1797e-01, -1.1279e+00,  3.8257e-01, -3.7427e-01, -7.1436e-01,\n",
      "         1.9473e+00,  1.0400e+00,  1.6992e-01, -6.1279e-01,  4.8008e+00,\n",
      "         2.2266e+00,  3.8379e-01,  9.7510e-01,  1.2070e+00, -1.3828e+00,\n",
      "        -9.1064e-01, -1.0117e+00, -6.8848e-01,  1.0486e-01, -6.8207e-03,\n",
      "        -6.0059e-02, -1.1194e-01,  2.2051e+00,  7.6953e-01,  1.4932e+00,\n",
      "         1.4563e-01,  3.5522e-01,  4.7500e+00,  1.0254e+00,  3.9258e+00,\n",
      "         1.2148e+00,  8.9844e-01,  9.7119e-01,  4.4492e+00,  1.1025e+00,\n",
      "        -4.5410e-01, -6.8457e-01, -9.0820e-01, -4.0063e-01,  7.5928e-01,\n",
      "         1.9805e+00,  6.9580e-01,  3.1372e-01,  4.6167e-01,  1.2964e-01,\n",
      "        -5.2393e-01, -2.3181e-01,  6.6650e-01, -5.8594e-01,  2.2910e+00,\n",
      "         8.7646e-01,  9.0918e-01,  7.3828e-01,  6.2378e-02,  4.7070e+00,\n",
      "         1.1660e+00,  1.2666e+00,  5.8057e-01, -5.9961e-01,  1.1465e+00,\n",
      "        -6.9336e-01, -5.4883e-01,  6.3867e-01, -1.1298e-01,  2.8535e+00,\n",
      "         8.8684e-02, -5.1367e-01, -6.0089e-02,  3.7734e+00,  3.2568e-01,\n",
      "         3.1982e-02,  4.9133e-02,  3.3960e-01,  6.7969e-01,  1.5576e+00,\n",
      "        -7.0410e-01, -4.3115e-01, -5.6006e-01,  1.3501e-01,  3.5645e-01,\n",
      "        -3.2837e-02, -3.8916e-01,  2.7695e+00,  8.2861e-01, -6.0010e-01,\n",
      "        -1.2262e-01, -4.7534e-01, -5.7080e-01,  3.3262e+00,  3.4888e-01,\n",
      "         1.1650e+00,  2.6123e-01,  3.5205e-01,  3.0449e+00, -4.2578e-01,\n",
      "         7.3145e-01,  3.0215e+00, -4.1089e-01,  8.3447e-01, -7.9395e-01,\n",
      "         1.2666e+00, -2.8857e-01,  4.2656e+00,  3.9331e-01,  1.3652e+00,\n",
      "         1.9519e-01,  5.0476e-02, -6.9531e-01,  1.0127e+00, -3.2593e-02,\n",
      "         4.2812e+00,  9.8926e-01,  1.5527e+00, -1.0779e-01,  2.2461e+00,\n",
      "         4.4043e-01,  8.6060e-02, -1.8176e-01,  3.5566e+00,  9.8438e-01,\n",
      "        -7.3242e-01,  2.5801e+00,  1.9067e-01,  1.7822e-01, -4.0332e-01,\n",
      "         4.3125e+00, -1.6693e-02,  9.9609e-01,  1.8481e-01, -1.2178e+00,\n",
      "        -5.5371e-01, -1.2793e+00, -9.8438e-01, -2.6440e-01,  1.8281e+00,\n",
      "        -7.1484e-01, -7.2083e-02, -7.7832e-01,  3.8438e+00, -2.9248e-01,\n",
      "         8.3691e-01, -5.1416e-01, -1.1592e+00, -3.0566e-01, -3.6572e-01,\n",
      "         3.7930e+00,  1.5710e-01,  7.1143e-01,  2.9663e-01,  3.3667e-01,\n",
      "        -3.8623e-01, -2.6465e-01,  3.2656e+00,  2.3047e+00,  1.2705e+00,\n",
      "         4.1260e-01,  1.1484e+00,  6.4209e-02, -1.1514e+00, -1.9638e-02,\n",
      "        -6.3525e-01, -7.6953e-01,  2.1448e-01, -1.0078e+00,  3.6835e-02,\n",
      "        -7.7637e-02,  3.3398e-01,  1.3733e-03,  1.4136e-01, -4.1211e-01,\n",
      "        -3.1079e-01, -1.0400e+00, -9.0039e-01, -2.0898e-01, -6.4990e-01,\n",
      "         4.6729e-01, -4.0039e-01,  2.9736e-01,  2.9062e+00,  3.6475e-01,\n",
      "        -3.8647e-01, -9.9243e-02, -1.9324e-01,  1.7310e-01,  3.2886e-01,\n",
      "         1.0615e+00, -4.0601e-01,  2.4072e-01, -3.9746e-01,  3.4576e-02,\n",
      "         3.9087e-01, -7.3047e-01,  1.8926e+00,  1.9551e+00,  2.0938e+00,\n",
      "        -2.6611e-01,  1.9297e+00,  1.4648e-01, -1.3379e+00, -6.0059e-01,\n",
      "        -1.1560e-01, -4.0466e-02, -1.0684e+00,  1.6211e-01, -7.0996e-01,\n",
      "         3.7637e+00,  6.7871e-01,  4.8999e-01, -1.0039e+00, -9.2896e-02,\n",
      "        -1.8799e-01,  1.6211e-01,  3.2861e-01,  1.7549e+00, -1.2347e-01,\n",
      "         2.0547e+00, -1.0410e+00, -8.4351e-02,  3.3643e-01, -5.1270e-01,\n",
      "         2.2070e+00,  4.1187e-01,  1.2537e-01,  5.0439e-01,  1.2842e+00,\n",
      "         5.3467e-01, -3.6621e-01, -9.2163e-02, -3.6621e-01,  3.1484e+00,\n",
      "        -1.3989e-01, -8.4839e-02,  5.1074e-01, -3.5229e-01, -2.9663e-01,\n",
      "        -3.8513e-02, -7.1875e-01,  2.1953e+00,  4.4727e-01,  5.3516e-01,\n",
      "        -3.2959e-01,  2.7793e+00,  7.6221e-01,  1.2021e+00, -9.3750e-01,\n",
      "        -2.6904e-01,  4.4373e-02, -6.4160e-01,  2.8105e+00, -1.9836e-01,\n",
      "         8.7451e-01, -4.5020e-01,  3.5996e+00,  4.6777e-01,  8.5352e-01,\n",
      "         4.6655e-01, -1.3379e-01, -1.2588e+00, -9.5801e-01,  2.1716e-01,\n",
      "         9.3311e-01, -3.6426e-01, -9.1260e-01,  4.5288e-01, -5.1953e-01,\n",
      "        -3.5400e-01,  3.0938e+00, -2.6587e-01,  3.6328e-01,  7.6782e-02,\n",
      "         2.7695e+00,  7.3242e-02,  5.8789e-01,  1.4326e+00, -6.6345e-02,\n",
      "         1.1047e-01,  5.7080e-01, -5.7770e-02,  9.3457e-01, -3.4106e-01,\n",
      "        -3.5620e-01, -9.0869e-01, -1.2266e+00, -7.9150e-01, -3.7817e-01,\n",
      "        -3.6328e-01,  4.0078e+00,  5.2637e-01,  4.8608e-01, -5.2588e-01,\n",
      "        -2.1326e-01, -1.2734e+00,  2.3027e+00, -5.0244e-01,  1.7939e+00,\n",
      "        -4.0698e-01,  1.7197e+00, -3.3447e-01, -3.0151e-01,  7.8760e-01,\n",
      "        -3.3569e-01, -9.9219e-01,  2.4512e+00, -4.8267e-01,  3.1177e-01,\n",
      "        -1.1152e+00, -1.5320e-01, -1.2188e+00, -1.7383e-01,  3.2397e-01,\n",
      "         4.4873e-01, -3.4448e-01, -7.1472e-02,  3.7695e+00,  1.3350e+00,\n",
      "         4.2554e-01, -5.2686e-01,  2.8398e+00,  1.3457e+00, -1.3623e+00,\n",
      "        -1.2656e+00, -6.5479e-01,  1.5449e+00, -7.6416e-01,  2.9077e-01,\n",
      "        -2.6562e-01,  3.1519e-01,  1.5020e+00, -2.7069e-02, -1.6235e-01,\n",
      "         1.1025e+00, -4.2993e-01,  4.0894e-02, -9.8633e-01,  5.9131e-01,\n",
      "        -7.7979e-01,  2.3086e+00,  8.8330e-01,  1.6377e+00,  2.1543e+00,\n",
      "         3.8574e-01, -1.6953e+00, -1.1829e-01,  3.0020e+00,  1.5625e+00,\n",
      "         5.1758e-01,  1.9521e+00, -2.7002e-01, -1.1143e+00, -6.8481e-02,\n",
      "        -5.7715e-01, -8.6328e-01,  1.0046e-01,  3.7988e+00,  2.0129e-01,\n",
      "         4.0869e-01,  7.9199e-01,  1.0430e+00, -5.0293e-01,  1.2195e-01,\n",
      "        -5.5322e-01, -3.0054e-01,  1.8086e+00, -2.1643e-01,  7.2571e-02,\n",
      "        -5.1465e-01,  5.7031e-01, -1.5625e-01,  7.3682e-01, -2.3206e-01,\n",
      "        -1.2054e-03, -2.5903e-01, -4.4141e-01,  2.2988e+00,  2.5711e-02,\n",
      "         2.2232e-02, -6.6016e-01,  1.6943e+00, -4.7192e-01,  1.5235e-04,\n",
      "        -8.6523e-01,  1.2607e+00, -4.4189e-01,  4.4250e-02,  9.3164e-01,\n",
      "         8.5022e-02,  3.1226e-01,  4.6240e-01,  1.7090e+00,  7.6562e-01,\n",
      "         4.1235e-01,  1.5125e-01,  1.6858e-01,  2.8931e-01,  1.8838e+00,\n",
      "        -3.0811e-01, -6.3623e-01,  1.4434e+00, -1.3586e-01,  1.8188e-01,\n",
      "        -3.1836e-01, -1.2637e+00, -3.4570e-01, -1.2627e+00, -5.8746e-03,\n",
      "         2.6880e-01,  1.6426e+00,  4.8804e-01,  2.1973e-01, -3.9551e-01,\n",
      "         1.9128e-01,  1.3857e+00, -5.3613e-01, -5.5078e-01,  1.3379e+00,\n",
      "        -3.2080e-01, -2.2937e-01, -5.0000e-01, -2.9272e-01, -5.3027e-01,\n",
      "        -1.2236e+00,  4.1199e-02, -9.1553e-01,  1.7129e+00,  1.0664e+00,\n",
      "         1.0283e+00,  3.1470e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 3.1299e-01, -6.7285e-01,  1.7480e+00, -9.5459e-01, -1.0430e+00,\n",
      "        -1.1357e+00, -9.0283e-01, -1.0566e+00, -1.7803e+00, -1.4004e+00,\n",
      "        -1.0430e+00, -3.9209e-01, -4.5972e-01, -1.1367e+00,  2.1106e-01,\n",
      "        -7.9834e-02, -2.1655e-01, -5.9912e-01,  4.7095e-01,  1.0400e+00,\n",
      "         3.6758e+00,  9.3213e-01,  2.5566e+00,  4.7148e+00,  1.4980e+00,\n",
      "         1.1221e+00,  2.0059e+00,  2.3379e+00,  1.3867e+00, -1.5327e-02,\n",
      "         6.4014e-01, -9.9463e-01, -4.7974e-01, -1.9299e-01,  3.9038e-01,\n",
      "        -2.3279e-01, -1.5991e-01,  6.2109e-01, -3.6499e-02,  1.1445e+00,\n",
      "        -1.1816e+00, -1.2695e+00, -8.1445e-01, -8.8818e-01,  1.9507e-01,\n",
      "        -3.9990e-01, -5.7129e-01, -6.0205e-01, -3.9856e-02,  2.1716e-01,\n",
      "         2.7441e+00, -1.1162e+00, -5.8887e-01, -6.1340e-02,  2.8296e-01,\n",
      "         3.4619e-01,  2.1621e+00,  2.4268e-01, -8.8684e-02, -1.0215e+00,\n",
      "        -3.9795e-01, -6.5430e-01, -8.3887e-01, -9.9463e-01,  5.3711e-01,\n",
      "         1.6953e+00,  2.4531e+00, -8.7305e-01, -1.0974e-01,  2.0801e-01,\n",
      "         3.7109e+00,  2.9834e-01, -4.6069e-01, -1.2219e-01,  5.4150e-01,\n",
      "         7.9150e-01,  2.4121e+00,  7.2656e-01,  1.5029e+00,  3.8203e+00,\n",
      "        -1.0254e+00, -4.0820e-01,  5.5273e-01, -1.3562e-01, -4.4629e-01,\n",
      "         1.7529e+00, -1.7883e-02,  3.9746e-01, -9.4629e-01, -9.4434e-01,\n",
      "        -5.2051e-01, -6.2402e-01,  5.8075e-02,  4.9756e-01,  3.8971e-02,\n",
      "        -1.4092e+00,  8.3008e-02,  4.7363e-01,  3.9429e-01,  1.9277e+00,\n",
      "         1.6699e+00,  1.5518e+00, -1.0068e+00, -4.1089e-01, -1.0742e+00,\n",
      "         5.1270e-01,  9.3066e-01,  3.7422e+00, -1.0703e+00, -8.7891e-01,\n",
      "        -5.7031e-01, -4.2700e-01, -3.9648e-01,  1.7798e-01, -3.6987e-01,\n",
      "        -1.8542e-01, -1.1024e-02,  4.7656e-01, -1.1855e+00, -1.6504e-01,\n",
      "         2.1350e-01,  5.2686e-01,  1.6943e+00, -4.1699e-01, -8.6523e-01,\n",
      "        -2.3938e-01, -5.0342e-01, -4.9243e-01, -1.7261e-01,  7.2144e-02,\n",
      "        -3.0835e-01, -2.8320e-01,  4.4727e-01,  1.2871e+00,  4.4336e-01,\n",
      "         4.6265e-01,  9.4727e-01,  1.9961e+00,  3.5059e+00,  7.4951e-01,\n",
      "         3.1787e-01,  6.7505e-02, -9.0723e-01, -6.2073e-02, -1.0254e+00,\n",
      "        -5.4688e-01, -3.4119e-02, -3.6108e-01, -1.4551e-01, -4.2529e-01,\n",
      "         3.0225e-01,  6.7676e-01,  4.1641e+00, -1.1260e+00, -9.9487e-02,\n",
      "         6.7139e-01,  1.0898e+00,  7.1826e-01,  4.1484e+00,  1.5637e-01,\n",
      "        -5.2393e-01, -1.7078e-01,  2.5192e-02, -3.8989e-01,  3.2188e+00,\n",
      "        -1.7002e+00, -2.7319e-01,  3.2444e-03, -7.6294e-02, -1.1084e+00,\n",
      "        -9.1357e-01,  3.5762e+00, -1.6064e+00, -1.6572e+00, -1.6816e+00,\n",
      "        -4.4678e-01, -2.2449e-01, -4.2554e-01, -1.0215e+00,  4.2603e-01,\n",
      "         2.9224e-01,  7.5488e-01, -2.7441e-01,  3.1328e+00, -1.3403e-01,\n",
      "         4.6191e-01, -1.2197e+00, -8.4717e-01, -9.7705e-01,  2.0977e+00,\n",
      "         2.2400e-02, -1.5303e+00, -4.6533e-01, -5.3418e-01, -1.2090e+00,\n",
      "         1.1455e+00,  3.5864e-01, -7.0117e-01, -1.6309e-01, -1.3257e-01,\n",
      "        -3.6304e-01,  4.0747e-01,  2.0938e+00,  4.8125e+00, -1.4102e+00,\n",
      "        -1.4912e+00, -1.1797e+00, -1.2656e+00, -5.2856e-02, -4.4922e-01,\n",
      "        -2.3059e-01, -9.4971e-01, -8.1863e-03,  3.3997e-02,  4.8125e+00,\n",
      "        -4.5044e-01,  4.6948e-01, -7.6538e-02,  2.7930e+00,  3.7598e-01,\n",
      "         5.2979e-01,  3.5312e+00,  1.5479e+00, -6.0158e-03,  3.5176e+00,\n",
      "         8.8916e-01, -4.7778e-01, -2.4939e-01, -2.9614e-01, -5.6885e-01,\n",
      "        -5.6396e-01, -3.5010e-01, -3.8574e-01,  1.1602e+00,  1.7832e+00,\n",
      "        -1.2939e+00, -9.7998e-01, -1.1699e+00, -1.2607e+00, -1.2549e+00,\n",
      "        -7.6270e-01, -1.2292e-01,  7.5488e-01, -1.1055e+00, -4.0405e-01,\n",
      "         5.5762e-01,  7.3291e-01,  3.6309e+00, -1.6094e+00, -7.9248e-01,\n",
      "        -9.9268e-01, -6.1035e-01,  1.6133e+00, -9.5215e-01, -1.5759e-01,\n",
      "         2.2734e+00, -1.5771e+00, -1.2275e+00, -4.4214e-01,  4.7266e-01,\n",
      "        -4.8706e-02, -2.0532e-01,  4.4971e-01, -6.8896e-01,  3.0195e+00,\n",
      "        -8.1836e-01, -4.8169e-01, -6.7773e-01, -7.5195e-01, -1.0752e+00,\n",
      "        -9.0918e-01, -1.1924e+00, -6.5381e-01,  1.0293e+00, -1.4189e+00,\n",
      "        -9.9561e-01, -6.1719e-01, -1.1533e+00,  9.1748e-01,  5.0195e-01,\n",
      "         4.2539e+00, -1.3977e-01,  4.7583e-01, -5.8447e-01, -2.0557e-01,\n",
      "         2.7910e+00, -6.0498e-01, -1.0095e-01,  3.0859e+00, -1.1602e+00,\n",
      "        -2.2351e-01, -4.5093e-01,  7.2070e-01,  6.7285e-01,  4.5156e+00,\n",
      "         5.5078e-01,  1.3486e+00, -1.0527e+00, -1.0657e-01,  5.7959e-01,\n",
      "        -3.4546e-01,  5.9326e-01,  3.6328e+00, -2.8735e-01,  9.8828e-01,\n",
      "        -1.2085e-01, -4.0747e-01, -5.4590e-01, -4.7632e-01,  2.8535e+00,\n",
      "        -1.1416e+00,  7.2900e-01,  1.6187e-01, -7.6221e-01, -2.4817e-01,\n",
      "         2.0874e-01, -5.5371e-01,  3.1270e+00, -1.2607e+00, -6.3623e-01,\n",
      "        -1.1123e+00, -1.6533e+00, -7.4170e-01, -1.2188e+00,  2.6777e+00,\n",
      "        -1.3037e+00, -4.3994e-01, -1.2100e+00, -7.7759e-02, -8.9111e-01,\n",
      "         2.7969e+00, -1.6836e+00, -1.5498e+00, -5.9473e-01, -1.1602e+00,\n",
      "         2.8979e-01,  8.4668e-01, -4.3604e-01,  6.1475e-01,  9.3613e-03,\n",
      "         3.7549e-01, -1.0684e+00,  7.2266e-02,  1.7347e-03,  2.4612e-02,\n",
      "         1.4736e+00,  3.8574e+00, -7.3486e-01, -1.9141e+00, -1.0879e+00,\n",
      "        -1.6113e+00, -9.0479e-01, -1.2471e+00, -1.1436e+00, -7.7930e-01,\n",
      "        -6.2012e-01, -1.8053e-03, -1.2344e+00, -8.7646e-01, -1.3975e+00,\n",
      "        -6.4258e-01, -4.1211e-01, -1.5527e+00, -1.1484e+00, -1.2217e+00,\n",
      "        -8.5498e-01, -7.5195e-01, -1.0723e+00,  3.3521e-01,  3.1958e-01,\n",
      "         1.1592e+00, -2.5684e-01,  1.9995e-01, -5.8154e-01,  4.4482e-01,\n",
      "        -8.4473e-02, -3.2227e-01, -2.7710e-01, -4.9744e-02, -2.4915e-01,\n",
      "        -1.3057e+00, -8.9160e-01, -6.2891e-01,  1.1743e-01,  1.2559e+00,\n",
      "        -5.5908e-01,  1.4648e+00, -1.1230e+00, -2.0430e+00, -1.1836e+00,\n",
      "        -1.0371e+00, -4.2053e-02, -1.7764e+00, -1.2578e+00, -1.2109e+00,\n",
      "        -3.8892e-01,  1.0962e-01,  3.6426e+00, -7.4463e-01,  1.5247e-01,\n",
      "         6.1279e-01, -5.0000e-01,  4.4629e-01,  1.2119e+00, -9.9365e-02,\n",
      "         7.3779e-01, -8.4424e-01, -1.3904e-01, -9.5752e-01, -9.6338e-01,\n",
      "        -3.7183e-01, -1.6821e-01,  5.2881e-01, -1.2195e-01, -8.6365e-02,\n",
      "         2.1851e-01,  2.4329e-01,  8.9600e-01, -9.3457e-01, -4.7290e-01,\n",
      "        -6.9923e-03,  4.3823e-01,  2.3148e-02,  4.0283e-01, -2.3840e-01,\n",
      "        -7.9834e-01, -9.1455e-01,  2.1655e-01,  1.6891e-02,  1.4355e+00,\n",
      "        -6.1328e-01, -4.9622e-02,  4.1309e-01,  2.6484e+00, -9.7900e-01,\n",
      "        -1.4465e-01, -8.3545e-01, -9.6094e-01,  3.8306e-01, -6.5479e-01,\n",
      "         1.5615e+00, -7.5488e-01, -2.6685e-01,  4.5166e-02,  1.6865e+00,\n",
      "         1.3975e+00,  1.7310e-01, -1.6650e+00, -1.0586e+00, -4.4727e-01,\n",
      "        -1.9788e-01, -4.1309e-01, -1.3193e+00, -3.2422e-01,  9.1675e-02,\n",
      "        -8.2568e-01, -5.1709e-01, -4.6875e-01,  7.0752e-01, -1.1973e+00,\n",
      "        -4.8706e-01, -1.0535e-01,  3.8110e-01, -3.8916e-01, -2.7490e-01,\n",
      "         8.0762e-01, -5.8301e-01,  4.2114e-01,  1.7598e+00, -1.1807e+00,\n",
      "        -8.5986e-01, -1.1123e+00, -1.2373e+00, -8.3643e-01, -1.9238e-01,\n",
      "        -2.1179e-01,  1.6504e-01,  6.6846e-01,  1.9258e+00,  1.8350e+00,\n",
      "        -1.1582e+00, -9.4824e-01,  2.6001e-02, -3.2373e-01,  1.8789e+00,\n",
      "        -1.0254e+00,  2.5684e+00, -1.0176e+00, -1.5127e+00, -1.0681e-01,\n",
      "        -2.0898e-01, -1.3604e+00, -5.1123e-01, -4.2163e-01,  1.2910e+00,\n",
      "        -1.0361e+00, -4.4861e-02,  3.4717e-01, -7.4219e-01,  4.2993e-01,\n",
      "         7.0166e-01,  1.4832e-01,  6.4014e-01, -4.2554e-01,  5.3320e-01,\n",
      "         3.1094e+00, -1.1689e+00, -1.6956e-01,  1.9053e+00, -9.6045e-01,\n",
      "        -6.7090e-01, -1.0820e+00,  7.6367e-01, -6.7090e-01,  4.8608e-01,\n",
      "        -8.6279e-01,  4.1968e-01, -1.2520e+00, -8.0078e-01,  2.7930e+00,\n",
      "        -1.2305e+00, -1.0576e+00,  1.8389e+00, -1.4902e+00, -7.0850e-01,\n",
      "        -1.3555e+00,  7.4023e-01, -1.9092e-01,  1.4277e+00, -6.9531e-01,\n",
      "         2.3730e+00, -2.8271e-01, -5.1514e-01,  1.2134e-01,  2.6929e-01,\n",
      "         8.1934e-01,  3.8789e+00, -2.2717e-01, -1.7969e+00, -2.7637e-01,\n",
      "        -2.5635e-01, -8.0518e-01,  1.0773e-01, -6.2695e-01, -3.8428e-01,\n",
      "         2.5879e+00,  2.3706e-01, -5.6299e-01, -7.0898e-01,  2.9316e+00,\n",
      "        -8.2715e-01, -1.3721e+00, -1.0645e+00, -5.3516e-01,  1.5195e+00,\n",
      "        -9.6094e-01, -1.7200e-01, -5.4541e-01, -4.1357e-01, -1.2817e-01,\n",
      "         7.4829e-02, -2.1228e-01, -8.7842e-01, -1.1816e+00,  1.5371e+00,\n",
      "        -9.3066e-01, -3.0273e-01, -1.5820e+00, -6.1475e-01,  1.5361e+00,\n",
      "        -1.1572e+00, -6.3574e-01, -2.9297e-01,  1.8633e+00,  7.6025e-01,\n",
      "        -4.7144e-01,  4.2041e-01, -8.6816e-01, -9.4043e-01, -3.4351e-01,\n",
      "         4.6289e-01,  2.7715e+00, -5.4492e-01,  6.4160e-01, -5.4492e-01,\n",
      "         1.5410e+00, -1.2051e+00, -7.3584e-01,  2.4238e+00,  1.2441e+00,\n",
      "         1.1652e-01, -1.4658e+00, -1.0225e+00, -5.6348e-01, -8.3496e-01,\n",
      "        -1.0391e+00, -1.0723e+00, -3.4351e-01,  3.8232e-01,  2.7109e+00,\n",
      "        -3.7598e-02, -1.5488e+00,  9.3896e-01, -1.4922e+00, -1.0283e+00,\n",
      "         1.8438e+00, -3.6963e-01, -1.0723e+00, -5.2148e-01, -6.6650e-02,\n",
      "        -1.3496e+00, -1.0048e-02, -3.7402e-01, -1.8066e+00, -4.3823e-01,\n",
      "         2.6348e+00,  4.1821e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  3\n",
      "qid:  5ab83bd055429919ba4e2279\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.4116,  0.4371, -0.5388,  ...,  1.7468,  1.2804, -0.1347],\n",
      "         [-0.1261,  0.3832, -0.2495,  ...,  1.4878,  0.5744,  0.3198],\n",
      "         [ 0.1031,  0.2028,  0.0497,  ..., -0.1145,  0.0169, -0.0600],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 5.8350e-01,  2.3262e+00,  9.4531e-01,  2.5098e-01, -6.3477e-01,\n",
      "         2.2891e+00,  2.5659e-01,  9.4604e-02, -2.3621e-01,  3.7769e-01,\n",
      "        -8.3069e-02,  1.4824e+00,  4.5728e-01,  8.8184e-01,  1.3147e-01,\n",
      "         3.5278e-01,  4.1172e+00,  6.9775e-01,  1.3013e-01, -2.5073e-01,\n",
      "         1.9785e+00,  4.0391e+00,  5.6738e-01, -5.1117e-02, -3.6011e-01,\n",
      "         1.5195e+00,  2.3770e+00,  4.8120e-01,  1.3926e+00,  1.2317e-01,\n",
      "         6.3428e-01,  1.9775e-01, -1.4490e-01,  9.6191e-01, -7.4805e-01,\n",
      "         2.6973e+00,  1.9495e-01,  4.4434e-01, -1.1582e+00,  3.7383e+00,\n",
      "         5.6836e-01, -5.5023e-02, -3.5400e-01, -4.5288e-01,  3.9844e+00,\n",
      "         6.8408e-01,  5.3467e-02,  8.7158e-02, -2.4451e-01,  2.6855e+00,\n",
      "         1.6846e-01, -1.6675e-01, -4.5807e-02, -1.0361e+00,  4.4995e-01,\n",
      "        -2.0947e-01, -1.3208e-01, -9.4482e-01,  3.1387e+00, -1.2244e-01,\n",
      "        -3.1055e-01,  1.8320e+00, -7.3181e-02, -7.4268e-01, -6.6699e-01,\n",
      "        -5.2832e-01,  3.5234e+00,  6.3672e-01, -4.7211e-02, -3.1885e-01,\n",
      "         1.2979e+00, -7.0801e-02,  1.9121e+00,  1.8457e+00,  2.7295e-01,\n",
      "        -2.8687e-01,  1.4331e-01, -4.3701e-01,  2.2148e+00,  3.8916e-01,\n",
      "         3.5742e-01,  8.8135e-02, -5.5225e-01, -4.4019e-01,  3.2148e+00,\n",
      "        -7.6233e-02,  9.6924e-02,  3.6108e-01, -8.3643e-01, -5.6006e-01,\n",
      "         3.4058e-01,  9.5215e-02, -7.7832e-01, -8.6475e-01, -7.7344e-01,\n",
      "         6.2939e-01, -6.5967e-01,  2.5078e+00,  1.0547e-01, -2.9468e-01,\n",
      "        -2.6489e-01,  1.8789e+00,  3.1201e-01, -5.0049e-01,  9.4849e-02,\n",
      "         1.2793e+00, -2.2388e-01,  2.7910e+00,  7.7637e-01, -4.9896e-02,\n",
      "         1.4124e-01,  1.0815e-01, -2.3486e-01,  2.7891e+00, -1.2733e-02,\n",
      "        -9.4727e-02,  3.2188e+00,  8.8916e-01,  9.7351e-02,  2.1692e-01,\n",
      "        -2.6343e-01,  2.7227e+00,  1.7639e-01, -1.5332e-01, -8.4045e-02,\n",
      "        -9.8193e-01, -1.1084e+00, -7.3730e-02,  2.9824e+00, -2.2302e-01,\n",
      "        -7.7588e-01, -2.5464e-01,  4.6509e-01,  6.2402e-01, -2.4780e-02,\n",
      "        -1.0876e-01,  3.0586e+00,  7.3193e-01,  9.9060e-02,  1.7249e-01,\n",
      "         2.3584e-01,  3.0591e-01, -4.3579e-01, -3.0615e-01,  1.4795e+00,\n",
      "        -3.1250e-01, -1.2354e+00, -2.2876e-01, -1.1816e+00, -4.7900e-01,\n",
      "         4.0352e+00,  8.0371e-01,  8.9050e-02,  2.0581e-01, -1.9678e-01,\n",
      "         3.1758e+00,  1.6931e-01, -2.1326e-01, -3.5919e-02,  2.3730e-01,\n",
      "         3.2373e-01, -8.6035e-01,  7.9102e-01, -6.5820e-01, -8.8440e-02,\n",
      "        -6.0986e-01,  2.7905e-01,  1.7957e-01, -6.4697e-02, -1.1768e+00,\n",
      "         3.5156e+00, -3.4821e-02,  9.8694e-02,  1.3369e+00,  2.2910e+00,\n",
      "         3.3521e-01, -2.1069e-01,  2.7817e-02,  8.6865e-01,  1.1445e+00,\n",
      "         1.7224e-01, -1.9669e-02, -4.5581e-01,  2.0000e+00,  3.3264e-02,\n",
      "         1.6101e-01, -4.2261e-01, -1.1816e+00, -2.5610e-01, -1.5107e+00,\n",
      "        -3.7598e-01, -5.1514e-01,  1.4170e+00, -7.3047e-01,  4.2041e-01,\n",
      "         3.2886e-01,  8.0505e-02,  1.9092e+00,  8.5693e-01,  8.1421e-02,\n",
      "        -5.2686e-01,  4.0942e-01,  1.3855e-01, -3.4570e-01, -3.3447e-01,\n",
      "         9.4177e-02, -3.4619e-01,  8.0566e-01, -5.1514e-01,  5.5566e-01,\n",
      "        -5.9912e-01, -2.0422e-01, -8.0469e-01, -5.4150e-01,  4.7266e-01,\n",
      "        -1.3779e+00,  2.9590e-01,  2.4927e-01, -6.2744e-01, -4.6924e-01,\n",
      "        -4.6167e-01, -5.2393e-01,  9.6582e-01,  1.5771e-01, -3.4888e-01,\n",
      "        -6.0010e-01, -3.3057e-01, -8.9600e-01,  4.6167e-01, -2.2522e-01,\n",
      "        -6.3379e-01,  2.4463e-01,  3.6304e-01,  4.3672e+00,  8.7109e-01,\n",
      "         3.0835e-01, -1.6748e-01,  2.1777e+00,  3.8086e+00,  6.7773e-01,\n",
      "         3.4393e-02, -2.7759e-01,  1.9541e+00,  7.2461e-01,  2.1729e-01,\n",
      "         4.2920e-01, -1.0293e+00,  3.7207e+00,  4.6436e-01, -5.7831e-02,\n",
      "        -3.7256e-01, -3.2104e-01,  3.7207e+00,  2.5586e-01, -3.0884e-01,\n",
      "        -1.4551e-01,  1.4883e+00, -2.3303e-01, -4.4458e-01, -8.5596e-01,\n",
      "        -4.3506e-01,  3.6113e+00,  8.0518e-01,  6.6711e-02, -2.8394e-01,\n",
      "         1.1875e+00, -3.6450e-01,  3.4707e+00,  8.1934e-01,  1.9702e-01,\n",
      "         1.4873e+00,  2.3511e-01,  3.5571e-01, -8.4766e-01, -3.0200e-01,\n",
      "        -8.6865e-01, -4.9780e-01,  2.4707e+00,  9.8083e-02, -2.6660e-01,\n",
      "        -1.2372e-01,  2.0039e+00,  3.4058e-01, -4.1089e-01, -5.0684e-01,\n",
      "         3.1543e+00,  1.7712e-01,  7.7698e-02, -2.2742e-01, -7.6074e-01,\n",
      "        -1.1603e-01,  1.3818e-01,  1.1270e+00,  1.5898e+00,  1.0718e-01,\n",
      "        -3.4253e-01,  2.4512e+00, -2.4567e-02, -4.0479e-01,  1.5762e+00,\n",
      "         2.3418e+00,  3.0078e-01,  2.3755e-01,  6.8750e-01, -9.5898e-01,\n",
      "         1.8701e+00, -3.6719e-01,  3.5254e+00,  8.8318e-02, -3.6346e-02,\n",
      "        -5.0977e-01, -6.1426e-01,  3.3447e-01,  6.1218e-02, -3.0469e-01,\n",
      "        -6.0889e-01, -1.1260e+00, -5.7910e-01, -1.2891e+00,  3.5742e+00,\n",
      "         4.6997e-01, -3.9868e-01, -4.2090e-01,  9.0515e-02,  3.6328e-01,\n",
      "         4.0742e+00,  7.2900e-01,  2.9258e+00,  2.1265e-01, -1.5781e+00,\n",
      "         1.1035e+00, -4.7681e-01,  2.0938e+00, -3.7842e-01, -9.0210e-02,\n",
      "         3.3926e+00,  5.6250e-01,  1.7793e+00,  4.9536e-01, -3.2495e-01,\n",
      "         1.5996e+00, -5.7770e-02, -2.8296e-01, -7.6318e-01,  2.1270e+00,\n",
      "         1.2268e-01, -4.1260e-01, -6.7676e-01,  2.9258e+00,  2.2681e-01,\n",
      "        -1.9263e-01,  4.1199e-02,  3.9014e-01,  4.2734e+00,  6.7188e-01,\n",
      "         2.6416e-01, -2.3425e-01,  2.3340e+00,  3.7578e+00,  5.5859e-01,\n",
      "         5.4688e-02, -3.6133e-01,  2.1465e+00,  5.0928e-01,  1.9995e-01,\n",
      "         4.7461e-01, -1.3379e+00,  3.8555e+00,  4.4946e-01,  4.0741e-03,\n",
      "        -3.4766e-01, -8.4521e-01, -6.4258e-01, -2.6025e-01, -3.0933e-01,\n",
      "         1.8828e+00, -3.1421e-01,  4.1875e+00,  2.6587e-01, -9.4833e-03,\n",
      "         2.2681e-01, -5.1221e-01,  3.6641e+00,  6.3281e-01, -1.4282e-01,\n",
      "        -1.9128e-01, -4.8145e-01,  3.4824e+00,  2.2180e-01, -3.3691e-01,\n",
      "        -9.2529e-02, -7.6953e-01,  1.2021e+00, -3.6157e-01,  6.5381e-01,\n",
      "        -6.8652e-01,  3.8379e+00,  6.9336e-01,  2.0828e-02, -3.5938e-01,\n",
      "         1.1367e+00, -5.6982e-01,  1.7051e+00, -3.3276e-01,  3.3750e+00,\n",
      "         1.2402e-01, -4.8706e-01,  3.5234e+00,  3.8892e-01,  1.4359e-02,\n",
      "        -2.8662e-01, -5.8838e-01, -5.6299e-01,  3.5898e+00,  3.1812e-01,\n",
      "        -2.8442e-01, -1.9922e-01, -6.0693e-01,  1.2227e+00, -7.8760e-01,\n",
      "        -1.0420e+00,  2.4688e+00,  2.3828e-01, -5.4053e-01, -8.5205e-01,\n",
      "        -8.3154e-01, -8.2031e-01,  8.1360e-02, -2.3389e-01, -5.8350e-01,\n",
      "        -7.3096e-01, -1.2705e+00, -7.5635e-01, -1.3486e+00,  3.4570e+00,\n",
      "         3.8989e-01, -4.9048e-01, -4.5190e-01, -2.3285e-02,  3.7622e-01,\n",
      "         3.4258e+00,  5.3369e-01,  4.2773e-01,  5.2734e-01, -2.5293e-01,\n",
      "         4.1321e-02, -2.1582e-01,  3.0430e+00,  5.6689e-01,  3.0591e-01,\n",
      "         4.2212e-01, -3.3301e-01,  2.9373e-02, -4.1846e-01,  1.7275e+00,\n",
      "         3.1885e-01,  1.8564e+00,  4.7681e-01,  2.8882e-01,  4.1357e-01,\n",
      "         1.6006e-02,  5.9619e-01,  3.4302e-01,  2.0581e-01, -1.7070e+00,\n",
      "         2.5732e-01, -1.8140e-01, -2.2473e-01, -9.1455e-01, -1.6492e-01,\n",
      "        -2.2131e-01,  1.6370e-01,  7.5977e-01, -8.7549e-01,  2.7891e+00,\n",
      "         5.3369e-01,  2.8711e-01,  3.9233e-01, -3.1909e-01,  1.6855e+00,\n",
      "         2.4170e-01, -1.7285e-01, -5.8984e-01,  2.6152e+00,  3.7524e-01,\n",
      "        -1.4062e-01,  2.7051e-01, -1.2054e-01, -6.8896e-01,  1.0205e+00,\n",
      "        -5.6641e-01,  3.0098e+00,  1.9385e-01, -6.9629e-01,  3.7090e+00,\n",
      "         3.6401e-01, -4.8804e-01, -1.6821e-01,  1.1064e+00, -3.9893e-01,\n",
      "         2.6797e+00,  8.0176e-01,  3.4082e-01,  3.6621e-01, -2.3511e-01,\n",
      "         1.0557e+00, -5.3320e-01, -1.0381e+00, -1.0553e-01, -1.3867e+00,\n",
      "         3.4199e+00,  4.8462e-01,  6.6797e-01, -3.7048e-02,  2.3770e+00,\n",
      "         1.1465e+00, -3.5864e-01,  2.9863e+00,  3.3423e-01, -5.0732e-01,\n",
      "         4.4312e-02, -4.5801e-01, -1.0244e+00, -2.6050e-01, -1.3809e+00,\n",
      "         2.3359e+00,  1.7441e-02, -1.8127e-01, -6.6846e-01,  1.5059e+00,\n",
      "        -1.3733e-01,  3.9746e-01,  4.0742e+00,  1.0205e+00,  2.3672e+00,\n",
      "         3.9185e-01, -1.7266e+00,  1.1191e+00, -5.7666e-01,  2.2598e+00,\n",
      "        -4.1064e-01, -2.4194e-01,  3.7109e+00,  6.7822e-01,  7.3059e-02,\n",
      "        -8.3594e-01,  3.1328e+00,  5.5908e-02, -3.4473e-01,  3.0493e-01,\n",
      "        -6.6211e-01,  3.8145e+00,  1.2805e-01,  5.4590e-01,  2.4492e+00,\n",
      "         3.3350e-01, -1.3223e+00,  5.1611e-01,  5.0195e-01, -6.1328e-01,\n",
      "         2.3828e+00, -4.4629e-01, -5.6787e-01, -1.9824e-01, -1.1310e-01,\n",
      "         3.8086e-01,  4.2656e+00,  6.2061e-01,  2.6465e-01, -2.7075e-01,\n",
      "         2.5254e+00,  3.2642e-01,  3.7930e+00,  5.4297e-01,  2.5122e-01,\n",
      "        -4.2603e-01,  1.7588e+00, -1.3184e-01,  2.5566e+00,  4.0820e-01,\n",
      "         1.3867e+00,  2.5854e-01,  6.3525e-01,  3.5352e-01, -9.6741e-02,\n",
      "         1.1211e+00, -1.7749e-01, -1.3955e+00,  2.4766e+00, -6.6162e-01,\n",
      "        -7.5781e-01,  2.2534e-01, -6.2402e-01, -6.7871e-01,  1.0498e+00,\n",
      "        -6.7334e-01,  3.2559e+00,  1.4868e-01, -6.8506e-01,  4.4766e+00,\n",
      "         6.0449e-01,  6.1035e-01,  6.5576e-01, -8.3435e-02,  3.0225e-01,\n",
      "         8.9966e-02, -2.3270e-02,  1.3516e+00, -3.4912e-01, -8.5645e-01,\n",
      "        -3.1885e-01, -1.1895e+00, -4.0161e-01,  2.9922e+00,  1.8176e-01,\n",
      "         3.2288e-02, -9.8206e-02,  1.6709e+00, -7.1838e-02, -3.7720e-01,\n",
      "         2.3789e+00,  3.0859e-01, -1.2119e+00, -2.4438e-01, -2.2119e-01,\n",
      "        -3.2739e-01, -3.2471e-01,  4.6289e-01, -1.1689e+00,  1.0625e+00,\n",
      "         3.2288e-02, -3.6938e-01,  2.9102e+00, -4.1962e-02,  6.7236e-01,\n",
      "        -4.6631e-01, -1.1689e+00, -4.3286e-01,  3.2969e+00,  1.1682e-01,\n",
      "         1.0439e+00,  2.6074e+00,  9.0723e-01,  1.2439e-01,  6.4600e-01,\n",
      "        -1.7773e-01, -3.0493e-01,  3.3555e+00,  1.0039e+00,  5.1953e-01,\n",
      "         5.6519e-02, -2.2205e-01,  3.4277e+00,  8.6328e-01,  1.0391e-02,\n",
      "         1.5152e-02,  2.0762e+00, -3.2324e-01,  4.1602e+00,  5.5566e-01,\n",
      "         3.9233e-01,  4.4775e-01,  1.1841e-01, -5.9717e-01,  7.1143e-01,\n",
      "        -4.7412e-01, -6.7432e-01, -8.3008e-01,  4.1562e+00,  6.5771e-01,\n",
      "         1.8433e-01, -4.5117e-01,  1.6025e+00, -6.4026e-02,  3.6621e-01,\n",
      "         4.1523e+00,  7.9102e-01,  3.0518e-01, -3.3911e-01,  2.2441e+00,\n",
      "         3.6328e+00,  6.5479e-01,  3.4009e-01, -3.1689e-01,  1.8018e+00,\n",
      "         4.4238e-01,  3.2153e-01,  8.2227e-01, -1.0859e+00,  3.6719e+00,\n",
      "         4.9536e-01,  1.0156e-01, -3.8477e-01, -6.2695e-01, -7.0605e-01,\n",
      "         1.6492e-01,  1.2939e+00, -6.6992e-01,  3.6777e+00,  7.1143e-01,\n",
      "         4.6265e-01,  6.1401e-02,  2.7124e-01, -5.4590e-01,  2.6484e+00,\n",
      "         5.0488e-01,  1.0229e-01, -5.8740e-01,  2.7949e+00,  4.6021e-01,\n",
      "         6.0791e-01, -2.5223e-02,  2.3711e+00,  1.9666e-01, -1.2915e-01,\n",
      "        -5.6494e-01,  3.9844e+00,  5.7520e-01, -5.7861e-02,  2.7368e-01,\n",
      "         1.0840e+00, -3.5254e-01,  1.5254e+00, -6.7432e-01, -6.8262e-01,\n",
      "        -9.4141e-01, -8.5156e-01,  4.7437e-01, -1.6541e-01,  2.3008e+00,\n",
      "         1.5889e+00,  2.4536e-01, -6.6455e-01, -9.6973e-01,  2.0312e-01,\n",
      "        -6.2012e-01,  4.3311e-01, -1.4832e-01, -2.1631e-01,  2.0391e+00,\n",
      "         1.2314e+00,  1.8140e-01, -6.7188e-01,  2.5854e-01,  6.9238e-01,\n",
      "         3.2812e-01, -8.1689e-01, -1.1353e-01, -8.8623e-01, -4.0454e-01,\n",
      "         3.9844e+00,  7.5830e-01,  2.3901e-01, -3.9697e-01,  1.1494e+00,\n",
      "        -4.6631e-01, -3.6743e-01,  8.3691e-01,  8.8013e-02, -1.6895e-01,\n",
      "         3.0430e+00,  3.6475e-01,  2.1289e-01, -9.1370e-02,  1.1631e+00,\n",
      "         1.2598e-01,  3.6914e-01,  3.5156e+00,  9.0234e-01,  3.1812e-01,\n",
      "         2.1543e+00,  8.0225e-01,  1.2402e-01,  3.1055e+00,  5.2295e-01,\n",
      "         1.1143e+00,  5.6494e-01,  5.4932e-01,  6.6211e-01,  7.0374e-02,\n",
      "        -1.3037e+00,  3.1309e+00,  4.7388e-01, -2.6416e-01,  1.5854e-02,\n",
      "         9.0967e-01, -9.3555e-01,  1.2207e+00,  3.5938e-01, -2.6514e-01,\n",
      "         5.3369e-01, -4.0479e-01,  2.7002e-01,  5.4382e-02, -1.2412e+00,\n",
      "        -4.3628e-01, -2.4463e-01, -9.0332e-01,  1.9082e+00, -1.3440e-01,\n",
      "        -6.1865e-01,  2.8955e-01,  2.9956e-01,  2.8223e-01,  2.0728e-01,\n",
      "        -8.0957e-01, -1.0703e+00, -1.3342e-01, -1.8677e-01, -8.4033e-01,\n",
      "         1.2715e+00,  5.0342e-01, -6.8909e-02,  5.8740e-01, -2.6392e-01,\n",
      "         8.9355e-01,  3.9990e-01, -9.8938e-02, -7.5928e-01, -8.4033e-01,\n",
      "        -1.9543e-01,  2.7466e-01, -7.1875e-01,  4.0479e-01, -3.8330e-01,\n",
      "         8.8989e-02, -6.4111e-01, -8.7109e-01,  2.3438e+00,  2.9150e-01,\n",
      "        -1.0654e+00, -5.2307e-02, -1.0312e+00, -8.1250e-01,  1.2832e+00,\n",
      "         6.0303e-01, -1.2225e-01,  1.8643e+00,  4.8706e-01, -2.0251e-01,\n",
      "        -1.0371e+00,  2.1558e-01, -7.5049e-01, -2.9175e-01, -1.0852e-01,\n",
      "        -8.4839e-02,  5.7129e-02,  2.4878e-01,  5.1465e-01,  4.8560e-01,\n",
      "         3.5645e-01, -6.3232e-01,  5.9082e-01, -1.0394e-01,  3.7720e-01,\n",
      "         3.9746e+00,  6.0596e-01,  2.0813e-01, -4.0820e-01,  2.4922e+00,\n",
      "         3.4453e+00,  5.3125e-01, -1.4580e-02, -5.0684e-01,  1.9883e+00,\n",
      "         2.7222e-01,  5.3589e-02,  1.1699e+00, -1.1611e+00,  3.6328e+00,\n",
      "         3.9062e-01, -3.0518e-03, -5.3760e-01, -4.5410e-01,  4.1406e+00,\n",
      "         7.6758e-01,  1.3257e-01,  3.5767e-02, -5.7324e-01,  3.4414e+00,\n",
      "         3.5425e-01, -1.8311e-01,  3.0908e-01,  8.9746e-01, -5.1416e-01,\n",
      "        -5.5029e-01, -1.0381e+00, -4.5459e-01,  3.3555e+00,  4.7827e-01,\n",
      "         3.5309e-02, -4.3970e-01,  1.2422e+00, -5.6836e-01,  2.8301e+00,\n",
      "         7.0557e-02,  1.8152e-01, -7.9492e-01, -5.5225e-01, -9.0527e-01,\n",
      "        -5.9766e-01,  3.3730e+00,  4.5190e-01, -1.4429e-01, -4.4922e-01,\n",
      "         1.2627e+00, -4.6533e-01,  1.2695e-01,  1.9995e-01,  6.6162e-02,\n",
      "        -5.2344e-01,  3.8145e+00,  1.0010e+00,  2.8662e-01,  9.5276e-02,\n",
      "        -2.1790e-01, -1.0957e+00, -4.2603e-01,  3.9941e-01,  5.1880e-02,\n",
      "        -6.0986e-01, -6.4795e-01,  4.0156e+00,  7.9102e-01, -3.3417e-03,\n",
      "         9.7559e-01,  1.3545e+00,  9.7168e-01,  3.5034e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.2031e+00, -1.2939e+00, -7.7881e-02,  1.9932e+00, -1.2871e+00,\n",
      "        -2.5513e-01,  5.2637e-01, -3.4570e-01,  2.2229e-01, -8.3252e-01,\n",
      "        -1.2148e+00, -1.1348e+00, -4.1724e-01, -6.2793e-01, -5.1709e-01,\n",
      "         3.9258e-01, -8.4717e-01,  8.6279e-01,  5.9717e-01,  6.5381e-01,\n",
      "         2.4980e+00, -5.9033e-01,  5.5176e-01,  4.5044e-01,  7.1680e-01,\n",
      "         1.8369e+00, -1.0791e+00,  6.2744e-01,  2.4902e-01,  8.7524e-02,\n",
      "         3.1348e-01,  2.1411e-01,  7.7197e-01,  2.6621e+00, -6.4160e-01,\n",
      "        -6.6872e-03,  4.5361e-01,  2.5020e+00, -9.3701e-01, -8.3203e-01,\n",
      "         6.3672e-01,  4.5044e-01,  1.6055e+00, -8.8379e-01, -2.6050e-01,\n",
      "         4.4580e-01,  1.6787e+00,  1.2793e-01,  4.6313e-01, -2.4506e-02,\n",
      "         3.4424e-01,  2.1816e+00,  2.3301e+00, -1.4229e+00, -1.1680e+00,\n",
      "        -3.8208e-01, -7.5879e-01, -1.1787e+00, -1.9116e-01,  1.2334e+00,\n",
      "         1.7559e+00, -7.8027e-01,  1.4512e+00,  1.3379e-01, -2.5708e-01,\n",
      "        -1.1445e+00, -9.1260e-01,  3.8794e-01,  1.5942e-01, -3.5458e-03,\n",
      "         1.7354e+00, -3.9819e-01, -7.0801e-01, -2.0251e-01,  3.0457e-02,\n",
      "         1.9263e-01,  2.0996e+00, -1.0449e+00, -9.7656e-02, -6.8054e-02,\n",
      "        -4.8279e-02,  5.8594e-01,  3.9404e-01, -1.0791e+00, -2.6758e-01,\n",
      "         7.5049e-01,  2.1035e+00,  4.0991e-01, -8.7402e-01, -2.4036e-01,\n",
      "        -3.5059e-01, -5.9277e-01, -6.8994e-01, -6.2891e-01, -1.1504e+00,\n",
      "        -5.9473e-01, -7.3291e-01, -9.7803e-01, -1.7883e-01,  8.0615e-01,\n",
      "        -2.0020e-01, -8.5645e-01,  1.5410e+00,  8.5498e-01, -6.5332e-01,\n",
      "        -4.4995e-01, -4.2310e-01, -2.0459e-01, -3.3203e-02,  1.5393e-01,\n",
      "         1.4771e-01,  2.3652e+00, -1.0918e+00, -3.5132e-01,  8.2715e-01,\n",
      "         8.0127e-01, -8.8672e-01,  4.3970e-01,  1.7178e+00,  1.2225e-01,\n",
      "         5.1904e-01, -1.2610e-01,  4.2310e-01,  2.7422e+00,  1.5996e+00,\n",
      "         1.9312e-01, -8.4473e-01, -1.2109e+00, -2.2644e-01,  7.2607e-01,\n",
      "         5.9961e-01, -1.1572e+00, -9.5312e-01,  1.9592e-01,  1.2659e-01,\n",
      "        -8.6133e-01, -2.5244e-01, -2.0984e-01,  7.8659e-03,  2.5293e-01,\n",
      "         2.2913e-01,  3.9673e-01,  3.7695e-01,  2.5957e+00, -1.2490e+00,\n",
      "        -2.8467e-01, -1.0752e+00,  2.6123e-01, -1.8164e+00, -1.3955e+00,\n",
      "        -4.3970e-01,  3.9038e-01,  1.5518e+00, -8.1909e-02,  3.8062e-01,\n",
      "        -2.8735e-01,  3.9941e-01,  2.4883e+00,  2.1641e+00, -1.5000e+00,\n",
      "        -9.1553e-01, -8.1104e-01, -1.1562e+00, -2.9999e-02, -7.3389e-01,\n",
      "        -1.2598e+00, -7.1240e-01, -4.8853e-01, -2.6221e-01, -6.1328e-01,\n",
      "        -3.0518e-01,  1.2471e+00,  2.1719e+00, -2.1816e+00, -6.1475e-01,\n",
      "        -1.2952e-01, -7.1899e-02,  1.3398e+00, -1.1660e+00, -2.7368e-01,\n",
      "        -1.9104e-02,  1.0820e+00, -1.4424e+00, -4.7058e-02,  2.3120e-01,\n",
      "         1.3936e+00, -4.4482e-01, -9.8193e-01,  3.0371e-01, -1.9697e+00,\n",
      "        -7.3438e-01, -1.1348e+00, -6.4355e-01, -1.4414e+00,  3.6792e-01,\n",
      "        -9.8877e-01, -1.4023e+00, -1.0342e+00, -3.4839e-01, -1.2817e-01,\n",
      "        -1.0010e+00, -9.1992e-01,  2.3157e-01, -9.4092e-01, -5.6641e-01,\n",
      "        -4.0259e-01, -6.2012e-01, -1.3350e+00, -1.3057e+00, -4.1455e-01,\n",
      "         3.6548e-01, -6.7041e-01, -1.0430e+00, -1.1387e+00, -7.7588e-01,\n",
      "        -1.1045e+00, -9.9951e-01, -5.1904e-01, -1.4688e+00, -9.6631e-01,\n",
      "        -9.6338e-01, -1.3623e+00, -4.1309e-01, -2.6831e-01, -3.5962e-01,\n",
      "        -7.5342e-01, -4.9194e-01, -1.6846e+00, -7.5098e-01, -4.2383e-01,\n",
      "         9.2407e-02, -7.6855e-01,  4.1846e-01, -9.5898e-01,  6.5820e-01,\n",
      "         2.7710e-01,  6.3379e-01,  2.9980e+00, -8.5156e-01,  3.8428e-01,\n",
      "         3.2812e-01,  6.3232e-01,  1.5322e+00, -2.1228e-01,  2.1924e-01,\n",
      "         3.2637e+00, -1.3096e+00, -9.8389e-01,  4.5166e-01,  3.8062e-01,\n",
      "         1.4277e+00, -8.8428e-01, -1.2366e-01,  5.4248e-01,  2.9316e+00,\n",
      "         2.0977e+00, -9.7314e-01,  3.3325e-01,  5.1953e-01, -4.0845e-01,\n",
      "        -1.4219e+00, -9.2139e-01,  1.8652e-01,  9.1309e-02, -2.1887e-01,\n",
      "         2.2461e+00, -1.5576e+00, -5.4382e-02,  1.7126e-01,  8.6133e-01,\n",
      "        -3.8110e-01,  6.6602e-01,  2.3750e+00, -1.2617e+00, -9.2529e-01,\n",
      "        -4.2505e-01, -1.4580e+00, -1.0039e+00, -2.0715e-01,  7.2412e-01,\n",
      "        -3.2104e-01, -9.7217e-01,  1.4600e+00,  4.7510e-01, -1.6221e+00,\n",
      "        -3.7793e-01,  2.0557e-01,  1.7832e+00,  1.1176e-01, -5.5371e-01,\n",
      "        -9.5312e-01, -1.1484e+00, -3.5840e-01, -5.8887e-01,  1.7324e+00,\n",
      "        -1.2549e+00, -2.8711e-01,  1.8906e+00, -1.1260e+00, -3.1860e-01,\n",
      "         1.3257e-01,  2.7695e+00, -8.5107e-01, -8.2471e-01, -9.5264e-01,\n",
      "        -3.8452e-01, -3.2837e-01, -5.2588e-01,  5.5518e-01,  3.1133e+00,\n",
      "        -1.0029e+00, -8.2959e-01,  5.5371e-01, -9.1504e-01, -1.8774e-01,\n",
      "         6.3525e-01, -1.2861e+00, -7.5098e-01, -1.4395e+00, -1.0127e+00,\n",
      "         4.1333e-01,  3.3008e-01,  1.3496e+00, -7.2168e-01,  4.2358e-01,\n",
      "        -3.8062e-01,  1.5898e+00, -5.3369e-01,  2.4082e+00, -1.1445e+00,\n",
      "        -4.1138e-01, -1.1602e+00, -6.7480e-01,  9.0918e-01, -4.5239e-01,\n",
      "        -5.1465e-01,  1.0293e+00, -1.3184e+00, -2.6880e-01,  1.8848e+00,\n",
      "        -5.1025e-01, -1.1646e-01,  3.7988e-01,  4.2871e-01, -9.4531e-01,\n",
      "         1.8408e-01,  2.4688e+00, -4.6606e-01, -3.4912e-01,  3.2520e-01,\n",
      "         3.2520e+00,  2.3193e-01,  4.6826e-01, -9.5068e-01,  6.5771e-01,\n",
      "         9.6375e-02,  6.8994e-01,  2.5547e+00, -6.9189e-01,  4.8096e-01,\n",
      "         2.5854e-01,  8.3984e-01,  1.7588e+00, -4.6021e-02,  3.4399e-01,\n",
      "         3.1367e+00, -1.3389e+00, -7.8760e-01,  6.7432e-01,  4.0771e-01,\n",
      "         1.4463e+00, -1.2646e+00, -3.2959e-01, -1.3828e+00, -1.0127e+00,\n",
      "        -1.5759e-01, -3.5034e-01, -4.6570e-02,  3.1921e-02,  6.6260e-01,\n",
      "         3.2891e+00, -5.9814e-01, -1.0840e-01,  3.4570e-01,  2.9961e+00,\n",
      "         1.6797e+00, -7.5146e-01, -1.0571e-01,  3.7744e-01,  2.6953e+00,\n",
      "         1.9043e+00, -7.0264e-01,  6.0010e-01, -9.4922e-01, -2.7856e-01,\n",
      "        -5.9180e-01, -8.1787e-01,  1.2115e-01,  2.5122e-01,  2.2107e-01,\n",
      "         2.2324e+00, -5.9912e-01,  2.0157e-02, -2.8931e-01,  7.3608e-02,\n",
      "         2.2402e+00, -4.2163e-01, -3.7329e-01, -1.5137e-01,  1.0703e+00,\n",
      "         2.8105e+00,  1.6885e+00, -7.9053e-01,  6.8909e-02,  5.1953e-01,\n",
      "         2.7266e+00,  1.6699e+00, -8.9746e-01,  1.3076e+00,  2.5562e-01,\n",
      "        -1.6230e+00, -5.3906e-01,  3.7061e-01,  1.8347e-01, -2.3181e-01,\n",
      "        -1.0400e+00, -8.0518e-01,  3.5913e-01, -7.7344e-01, -4.3762e-02,\n",
      "         6.5625e-01, -1.2393e+00, -6.6406e-01, -1.3789e+00, -1.0225e+00,\n",
      "         3.5815e-01,  3.2202e-01,  1.5400e+00, -6.0156e-01,  4.5630e-01,\n",
      "        -5.0342e-01,  2.0050e-02, -1.0864e-01,  6.0986e-01,  1.9180e+00,\n",
      "        -4.1260e-01,  7.0898e-01, -7.0215e-01, -8.3435e-02, -8.0750e-02,\n",
      "         5.4053e-01,  1.3760e+00, -1.8140e-01,  6.2646e-01, -1.2412e+00,\n",
      "         2.3108e-01, -6.4893e-01, -8.1482e-02, -1.0083e-01,  5.1221e-01,\n",
      "         7.9443e-01, -4.5898e-01, -1.2683e-01,  3.1523e+00, -1.2402e+00,\n",
      "        -4.5435e-01, -5.2686e-01, -9.6558e-02, -1.3467e+00, -1.3145e+00,\n",
      "        -5.2051e-01, -5.9229e-01, -1.8341e-02, -5.1807e-01, -3.0811e-01,\n",
      "        -1.1072e-01, -8.5938e-02,  7.4365e-01,  1.7617e+00,  9.8206e-02,\n",
      "         4.2529e-01,  2.7910e+00, -7.3242e-01, -2.7393e-01,  1.0321e-01,\n",
      "         7.1484e-01,  9.1064e-01,  2.8750e+00, -6.7432e-01,  8.3862e-02,\n",
      "        -1.3391e-01,  3.6133e-01,  3.4395e+00, -4.0186e-01,  9.3201e-02,\n",
      "         5.7178e-01,  2.6816e+00,  1.1445e+00, -2.1912e-01,  2.0176e+00,\n",
      "        -7.2705e-01, -6.3416e-02, -9.0637e-02,  7.1191e-01,  1.4609e+00,\n",
      "         2.0898e+00, -1.3311e+00, -1.1406e+00, -1.2832e+00, -8.7646e-01,\n",
      "        -8.2275e-01, -5.7068e-02,  2.1973e+00,  2.6465e-01,  1.8774e-01,\n",
      "         1.3037e+00, -1.2695e+00,  1.2225e-01,  6.0254e-01,  2.9219e+00,\n",
      "         1.2559e+00, -1.1924e+00, -1.0938e+00, -8.8623e-01, -7.4023e-01,\n",
      "        -9.1162e-01,  7.5684e-01,  5.6299e-01,  9.6143e-01,  4.0078e+00,\n",
      "        -1.5356e-01,  4.7632e-01, -2.2546e-01,  2.7090e+00, -3.1885e-01,\n",
      "         3.7676e+00, -1.0635e+00, -2.0605e-01, -1.0234e+00, -4.8071e-01,\n",
      "         1.0762e+00, -4.1602e-01, -4.5215e-01, -4.6814e-02,  3.1855e+00,\n",
      "        -5.3516e-01,  2.1683e-02,  2.9785e+00, -9.7705e-01, -6.3135e-01,\n",
      "        -3.6133e-01, -2.2742e-01,  1.4629e+00,  2.6147e-01, -4.0918e-01,\n",
      "         3.3301e+00, -1.4229e+00,  2.0667e-01, -2.7515e-01, -1.1680e+00,\n",
      "        -7.2510e-01,  1.4082e+00, -3.5767e-01, -5.9424e-01, -5.5811e-01,\n",
      "         4.5825e-01, -8.2715e-01,  7.9004e-01,  3.3643e-01,  1.1494e+00,\n",
      "         2.5903e-01,  3.3555e+00, -7.9590e-01,  3.7036e-01,  3.1226e-01,\n",
      "         9.5703e-01, -2.0642e-01,  2.0391e+00, -1.1016e+00,  4.7046e-01,\n",
      "         1.1621e-01,  3.5645e-02,  2.1619e-01,  1.2500e-01,  1.2471e+00,\n",
      "         1.3013e-01,  2.8027e+00, -1.2793e+00, -5.3857e-01,  3.4033e-01,\n",
      "        -7.9407e-02, -1.3342e-01,  9.8291e-01, -1.0869e+00, -4.5410e-02,\n",
      "        -1.3391e-01,  2.6718e-02,  3.2773e+00, -6.1182e-01, -6.2294e-03,\n",
      "        -7.5684e-02,  1.0120e-01,  5.7666e-01,  5.6445e-01,  3.1641e-01,\n",
      "         1.7119e+00,  9.7314e-01, -1.5051e-01,  3.1543e+00, -1.3438e+00,\n",
      "        -5.9082e-01, -4.3433e-01, -1.1895e+00, -8.7451e-01, -1.7273e-01,\n",
      "         6.4209e-01, -4.4531e-01, -6.0205e-01,  2.2695e+00, -1.3047e+00,\n",
      "        -6.2451e-01,  1.7764e+00, -1.4424e+00, -2.4109e-01, -1.2148e+00,\n",
      "        -1.1934e+00, -1.0322e+00, -8.1836e-01, -1.1348e+00, -4.6216e-01,\n",
      "         1.4551e-01, -4.2920e-01, -2.0483e-01,  2.8555e+00, -7.2412e-01,\n",
      "        -1.0938e-01, -7.4072e-01, -1.5703e+00, -5.5029e-01,  1.9150e+00,\n",
      "        -1.3525e+00, -3.0322e-01,  3.4332e-02,  3.0273e+00,  9.6387e-01,\n",
      "        -3.7280e-01, -9.5215e-01,  2.8540e-01,  1.1975e-01,  3.2422e-01,\n",
      "         2.3223e+00, -1.0420e+00, -5.4834e-01, -1.2378e-01,  2.8574e+00,\n",
      "        -4.1724e-01,  1.3806e-01, -4.7180e-02,  3.6670e-01, -1.0535e-01,\n",
      "         8.3008e-01,  1.1123e+00,  3.7969e+00, -1.2686e+00,  4.1016e-01,\n",
      "         5.3955e-01,  1.3672e-01, -1.3037e+00, -5.3906e-01,  7.6074e-01,\n",
      "         4.2871e-01,  6.6406e-01,  4.1289e+00, -2.3230e-01,  4.3750e-01,\n",
      "        -1.0283e+00,  6.4697e-01,  2.3193e-01,  5.0586e-01,  2.5820e+00,\n",
      "        -8.1201e-01,  2.7148e-01,  2.9565e-01,  4.9927e-01,  6.6602e-01,\n",
      "        -1.8701e-01,  1.6345e-01,  2.4727e+00, -1.4072e+00, -8.6035e-01,\n",
      "         6.7822e-01,  3.8306e-01,  9.4336e-01, -1.2627e+00, -7.1533e-01,\n",
      "        -1.4365e+00, -1.3135e-01, -5.2197e-01, -3.7207e-01,  4.1656e-02,\n",
      "         1.7042e-03,  2.8613e-01,  1.9844e+00, -1.1494e+00, -6.9629e-01,\n",
      "        -1.8127e-01,  1.2959e+00, -1.0576e+00, -8.6792e-02,  3.1104e-01,\n",
      "        -6.2354e-01,  8.2642e-02,  3.6774e-02,  2.8574e+00,  1.7744e+00,\n",
      "        -7.8076e-01,  2.3083e-01,  2.8174e-01,  2.8809e+00,  1.1318e+00,\n",
      "        -2.8320e-01,  2.3730e+00, -4.6802e-01, -1.0425e-01, -6.3232e-01,\n",
      "        -3.4363e-02, -1.4570e+00, -4.7144e-01, -8.7109e-01, -8.1885e-01,\n",
      "        -1.0342e+00,  2.4043e+00, -1.4492e+00, -1.6494e+00, -1.5293e+00,\n",
      "        -1.2119e+00, -1.2061e+00, -2.1594e-01, -1.1455e+00, -9.8291e-01,\n",
      "        -1.0371e+00,  1.4678e+00, -1.4297e+00, -2.8906e-01, -7.9443e-01,\n",
      "        -3.5522e-01, -8.6279e-01,  4.5459e-01, -8.8281e-01, -1.4814e+00,\n",
      "        -8.1738e-01,  3.2007e-01,  1.5112e-01,  1.4648e-01,  2.1172e+00,\n",
      "        -1.4678e+00, -7.9590e-01, -7.9150e-01, -1.2109e-01, -9.1406e-01,\n",
      "        -2.0050e-02,  1.4941e-01,  5.4590e-01,  3.0898e+00,  2.1680e+00,\n",
      "        -2.9272e-01,  4.3750e-01, -4.9341e-01, -7.3914e-02,  3.3574e+00,\n",
      "        -1.5361e+00, -3.0884e-01,  3.4004e+00, -8.3643e-01,  5.9131e-01,\n",
      "         1.2451e-01, -3.9856e-02,  8.0371e-01,  3.8672e-01,  3.1172e+00,\n",
      "        -1.4863e+00, -1.0010e+00,  3.9722e-01, -8.2275e-01, -1.2100e+00,\n",
      "        -5.6104e-01, -7.1289e-01, -6.5186e-01, -5.1123e-01,  1.9824e+00,\n",
      "        -1.6357e+00, -8.8232e-01, -9.0430e-01, -3.6353e-01, -1.5156e+00,\n",
      "        -1.2959e+00, -1.7148e+00, -1.1641e+00, -6.8311e-01,  1.4150e+00,\n",
      "        -1.3574e+00, -1.3154e+00, -8.3398e-01, -6.5771e-01, -3.6255e-01,\n",
      "        -8.3301e-01, -1.2676e+00, -6.2109e-01, -8.5059e-01, -9.8291e-01,\n",
      "        -1.2363e+00, -5.6396e-01,  2.4297e+00, -8.3691e-01, -7.1729e-01,\n",
      "        -1.2422e+00, -7.1826e-01,  1.3672e+00, -5.7422e-01, -1.1934e+00,\n",
      "        -1.4834e+00, -1.2393e+00, -4.3018e-01, -5.2930e-01, -1.6711e-01,\n",
      "        -3.7305e-01, -1.0771e+00, -1.0557e+00,  7.1533e-02,  2.0820e+00,\n",
      "        -1.6172e+00, -1.0469e+00, -1.5889e+00, -1.5049e+00, -5.4980e-01,\n",
      "        -4.1211e-01,  2.3418e+00, -1.7656e+00, -6.2500e-01,  2.4238e+00,\n",
      "        -5.8838e-01, -1.0264e+00, -9.0039e-01, -7.0312e-01, -7.2900e-01,\n",
      "        -6.0107e-01, -1.1494e+00, -1.3662e+00, -4.6997e-01, -8.9795e-01,\n",
      "        -6.0645e-01, -1.1484e+00,  5.1422e-02, -5.9961e-01,  4.3921e-01,\n",
      "        -8.9502e-01,  4.1675e-01,  2.1155e-01,  5.0977e-01,  2.2949e+00,\n",
      "        -6.7432e-01,  2.7686e-01,  2.0776e-01,  6.0986e-01,  1.3369e+00,\n",
      "         5.6366e-02,  2.2437e-01,  2.7480e+00, -1.2676e+00, -6.8018e-01,\n",
      "         4.4263e-01,  3.0054e-01,  1.1924e+00, -7.5342e-01,  9.1980e-02,\n",
      "         3.2202e-01,  3.2285e+00,  2.9473e+00, -5.8936e-01,  2.7808e-01,\n",
      "         1.6235e-01,  2.5469e+00,  1.2041e+00, -6.6357e-01,  2.3523e-01,\n",
      "         5.7812e-01, -3.0249e-01, -1.3184e+00, -8.9795e-01,  1.1438e-01,\n",
      "        -1.6064e-01, -8.2397e-02,  1.8252e+00, -1.5928e+00, -5.1208e-02,\n",
      "         1.6528e-01,  2.7168e+00, -1.0332e+00, -1.0479e+00, -4.4263e-01,\n",
      "        -1.2236e+00, -9.4287e-01,  1.1987e-01,  1.3159e-01, -1.6077e-01,\n",
      "         2.0176e+00, -1.1768e+00, -8.3936e-01, -3.7793e-01, -1.0950e-01,\n",
      "        -1.2295e+00,  1.0797e-01, -2.1240e-01,  3.5400e-01,  2.6211e+00,\n",
      "         1.0010e+00, -9.3555e-01, -1.1670e+00,  2.4673e-02, -3.7524e-01,\n",
      "        -9.9854e-01, -1.1895e+00,  2.1027e-02,  1.4722e-01,  2.7578e+00,\n",
      "        -1.8860e-01,  5.2588e-01,  3.6426e+00,  3.9453e-01], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  4\n",
      "qid:  5a7df5635542990b8f503b0a\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.4940, -0.1702,  0.0296,  ...,  1.6669,  0.9634, -0.0324],\n",
      "         [ 0.5694,  0.1498,  0.5265,  ...,  1.3809,  0.3924, -0.3564],\n",
      "         [ 0.3722, -0.1074,  0.3156,  ...,  0.9231,  0.7170, -0.2264],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.2197e+00,  1.8079e-01,  2.9883e-01,  4.8413e-01, -5.9521e-01,\n",
      "        -2.5928e-01, -7.5049e-01,  4.0771e-01, -5.8447e-01, -8.1445e-01,\n",
      "        -9.5947e-01, -4.2432e-01, -1.9250e-01, -1.0957e+00,  6.3037e-01,\n",
      "        -1.0293e+00,  1.0420e+00,  6.7200e-02, -5.7471e-01, -5.4199e-02,\n",
      "        -8.9453e-01, -1.0608e-01, -9.3164e-01, -6.7676e-01,  1.6785e-01,\n",
      "        -7.0996e-01,  3.1953e+00,  1.2246e+00,  1.8481e-01,  3.6597e-01,\n",
      "        -2.1375e-01, -2.6099e-01,  3.0811e-01,  1.9053e+00,  7.3047e-01,\n",
      "        -3.2788e-01,  4.9878e-01,  1.5625e+00,  5.1953e-01, -4.0527e-01,\n",
      "         3.5864e-01, -1.6846e+00,  3.7329e-01, -1.0371e+00,  3.5742e-01,\n",
      "        -1.0635e+00, -2.0740e-01, -6.1798e-02,  8.8074e-02, -8.8281e-01,\n",
      "         8.9294e-02, -1.2079e-01, -5.7471e-01, -1.8640e-01, -1.4941e-01,\n",
      "        -2.1118e-01, -9.8193e-01, -7.4756e-01, -2.0056e-01, -1.0459e+00,\n",
      "         7.2998e-01,  9.8340e-01,  2.5317e-01, -2.7075e-01, -1.0000e+00,\n",
      "         1.2744e-01, -9.1992e-01, -3.1714e-01,  3.5303e-01, -9.4092e-01,\n",
      "         3.2007e-01, -3.6060e-01, -1.1865e+00,  5.1709e-01, -6.6211e-01,\n",
      "        -1.0608e-01, -1.3184e+00,  1.0820e+00, -1.0137e+00,  7.9639e-01,\n",
      "        -4.5654e-01, -9.2102e-02, -8.1885e-01,  4.1724e-01, -7.8760e-01,\n",
      "         4.8340e-02,  8.8074e-02,  4.0100e-02, -9.3896e-01,  1.4102e+00,\n",
      "         2.3975e-01, -4.4727e-01,  6.8164e-01, -5.0928e-01,  3.1426e+00,\n",
      "         4.9097e-01,  1.4922e+00,  7.3975e-01, -4.4604e-01, -7.1436e-01,\n",
      "         4.9585e-01,  3.7750e-02, -6.6455e-01,  1.9641e-01, -5.3833e-02,\n",
      "        -6.9519e-02, -9.6289e-01,  1.7468e-01,  2.4109e-03, -2.1570e-01,\n",
      "        -1.2178e+00,  1.8994e+00,  2.4512e-01, -4.6509e-01,  1.3428e-01,\n",
      "        -7.9004e-01,  5.1514e-01, -9.0283e-01,  1.3538e-01, -3.8062e-01,\n",
      "         5.7764e-01, -3.4082e-01,  1.6101e-01, -6.9238e-01,  4.9316e-02,\n",
      "         5.0684e-01, -3.6572e-01,  2.5635e-01, -5.3680e-02, -8.7402e-01,\n",
      "         1.1387e+00,  4.1382e-02, -8.0261e-03, -6.3135e-01,  1.8779e+00,\n",
      "         6.8066e-01, -3.0347e-01,  7.1655e-02, -1.5635e+00, -3.2074e-02,\n",
      "        -1.6094e+00,  3.6938e-01,  6.3965e-01, -5.0488e-01, -3.5156e-01,\n",
      "        -8.4619e-01, -5.8887e-01, -1.0889e+00,  1.6833e-01, -9.2102e-02,\n",
      "         2.5317e-01,  3.2544e-01,  9.4434e-01, -9.4238e-01,  1.9326e+00,\n",
      "         5.5859e-01,  2.5801e+00, -8.4473e-02, -5.5713e-01, -5.4352e-02,\n",
      "        -8.2422e-01, -5.3009e-02, -2.8418e-01, -1.3896e+00, -4.9902e-01,\n",
      "        -6.6309e-01,  6.2744e-02,  8.3740e-01, -8.9648e-01,  1.5049e+00,\n",
      "         4.2236e-01,  2.1680e+00, -3.6914e-01,  6.3574e-01, -1.0264e+00,\n",
      "        -8.5352e-01,  2.8887e+00, -8.3643e-01,  1.2852e+00, -8.7256e-01,\n",
      "        -7.3975e-02, -1.6150e-01, -3.3252e-01, -1.7053e-01, -3.9404e-01,\n",
      "        -9.1455e-01,  4.1055e+00,  8.2910e-01,  8.1006e-01,  1.0801e+00,\n",
      "        -1.7593e-02, -1.0809e-01, -1.9080e-01, -8.2275e-01, -9.5605e-01,\n",
      "         7.6514e-01, -4.8340e-01,  9.8340e-01, -8.7793e-01,  1.9788e-01,\n",
      "        -6.1279e-01, -7.6611e-01,  2.4976e-01, -1.1426e+00, -5.9229e-01,\n",
      "         2.2656e-01, -6.7285e-01,  1.2256e+00,  8.7341e-02,  5.1074e-01,\n",
      "         2.4658e-01, -5.1953e-01,  2.2278e-01,  3.3447e-01,  3.6777e+00,\n",
      "         1.0957e+00,  6.6895e-01,  2.3193e-01, -2.0679e-01,  7.3633e-01,\n",
      "         1.0864e-01,  1.0654e+00, -2.2266e-01,  3.9375e+00,  9.6777e-01,\n",
      "         6.3672e-01,  1.4819e-01, -4.0674e-01,  5.8447e-01,  5.9204e-03,\n",
      "         1.1230e+00, -1.1896e-01, -1.3926e+00,  8.6230e-01,  1.4268e+00,\n",
      "         3.8257e-01,  4.8218e-01, -5.2100e-01,  4.3438e+00,  5.9668e-01,\n",
      "         1.5283e+00,  4.6406e+00,  1.3262e+00,  3.6157e-01,  5.2490e-01,\n",
      "        -1.5518e+00, -8.3545e-01, -3.9673e-01,  3.0933e-01, -1.1455e+00,\n",
      "         6.2109e-01, -1.0371e+00,  9.4629e-01,  1.0727e-02, -6.1963e-01,\n",
      "        -3.3813e-02, -1.0615e+00, -5.8197e-02, -8.8477e-01, -8.9990e-01,\n",
      "         6.2891e-01, -8.8867e-01, -6.7920e-01,  1.3965e+00,  1.0040e-01,\n",
      "         3.5229e-01,  1.7861e+00,  1.2524e-01,  1.7871e+00,  1.1885e+00,\n",
      "         6.6895e-01, -5.8350e-01,  6.0986e-01, -7.8711e-01, -7.6855e-01,\n",
      "        -6.0303e-01, -6.0730e-03,  5.6055e-01, -1.5137e-01, -5.2881e-01,\n",
      "        -7.5098e-01,  5.6641e-01, -5.0586e-01, -7.0374e-02,  2.9639e-01,\n",
      "        -7.4524e-02, -1.0430e+00, -1.0413e-01, -8.0273e-01, -2.0361e-01,\n",
      "         5.7617e-01, -7.8125e-01,  4.3530e-01, -2.2595e-01, -5.8252e-01,\n",
      "        -1.8591e-01, -1.1963e+00,  6.4111e-01,  1.1255e-01, -9.8096e-01,\n",
      "         3.1519e-01, -2.7930e-01, -4.8218e-01, -1.3047e+00, -3.4521e-01,\n",
      "        -5.9082e-01,  1.3464e-01,  3.5522e-01,  3.3574e+00,  1.0156e+00,\n",
      "         1.7988e+00,  7.4072e-01,  4.4458e-01, -8.5144e-02,  2.1426e+00,\n",
      "         6.2695e-01, -5.5566e-01,  3.2202e-01, -2.3474e-01, -9.7656e-01,\n",
      "        -8.5352e-01,  2.4805e+00, -1.7322e-01,  3.4863e+00,  8.3154e-01,\n",
      "         5.0146e-01,  7.6416e-01, -3.1396e-01,  6.9189e-01,  1.2659e-01,\n",
      "        -1.4783e-01, -1.0420e+00,  4.6729e-01, -2.0068e-01,  1.6821e-01,\n",
      "         3.5840e-01,  3.3184e+00,  1.3525e+00,  1.9980e+00,  2.0874e-01,\n",
      "         3.9380e-01,  5.4297e-01, -6.3330e-01, -3.0029e-01,  2.0483e-01,\n",
      "         3.4629e+00,  1.6992e+00,  1.4336e+00,  1.9590e+00,  2.8271e-01,\n",
      "         3.6469e-02,  2.5156e+00,  2.0918e+00,  1.5149e-01,  2.5073e-01,\n",
      "         2.6001e-01,  5.9766e-01, -1.4590e+00, -1.1157e-01, -6.2939e-01,\n",
      "        -5.5859e-01, -1.2852e+00, -1.0199e-01, -8.9990e-01, -7.4365e-01,\n",
      "         2.5723e+00,  3.2188e+00,  6.0400e-01,  5.6934e-01,  1.1289e+00,\n",
      "         3.9766e+00,  5.3516e-01,  1.1689e+00,  1.4004e+00,  2.3755e-01,\n",
      "         4.8315e-01, -4.8608e-01, -3.7793e-01,  6.5039e-01, -8.0811e-01,\n",
      "         8.8135e-01, -3.9038e-01, -1.7932e-01,  1.3318e-01,  3.5767e-01,\n",
      "         3.0508e+00, -3.3911e-01,  1.7090e+00,  2.7285e+00, -3.9136e-01,\n",
      "         1.7529e+00, -1.4697e+00,  2.4634e-01, -7.2461e-01,  2.0625e+00,\n",
      "         3.6353e-01,  3.2305e+00,  2.0532e-01, -6.0254e-01, -4.4043e-01,\n",
      "         4.5352e+00,  1.1836e+00,  3.3765e-01, -1.0947e+00,  1.5833e-01,\n",
      "        -3.3618e-01, -4.4800e-01,  4.8608e-01, -1.0996e+00,  3.5293e+00,\n",
      "         6.4209e-01,  5.3857e-01,  1.1829e-01, -2.0312e-01,  3.3008e-01,\n",
      "        -6.8054e-02,  1.0098e+00, -1.1487e-01,  1.4539e-01,  3.6523e-01,\n",
      "         4.6641e+00,  1.0137e+00,  3.2251e-01,  4.6211e+00,  9.5459e-01,\n",
      "         2.3755e-01,  9.6631e-01,  4.8047e+00,  7.7686e-01,  1.0088e+00,\n",
      "         8.6719e-01,  1.6699e+00, -6.2646e-01,  4.0312e+00,  4.3481e-01,\n",
      "         1.1250e+00, -9.5410e-01,  9.5752e-01, -4.0234e-01, -9.3555e-01,\n",
      "        -2.9541e-01,  9.9023e-01,  3.4961e+00,  6.4746e-01,  4.4312e-01,\n",
      "         2.3727e-02, -3.7280e-01,  3.3057e-01, -1.8970e-01,  9.8145e-01,\n",
      "        -1.6638e-01,  6.9092e-02,  3.5742e-01,  4.0430e+00,  5.6299e-01,\n",
      "         2.6465e-01,  6.6895e-01,  2.1286e-02,  4.0503e-01,  1.0166e+00,\n",
      "         3.8794e-01,  1.3711e+00,  1.6064e-01,  3.7402e-01, -8.0322e-01,\n",
      "        -3.6743e-01, -1.7908e-01,  2.4512e+00, -6.2793e-01,  1.3037e+00,\n",
      "         1.9543e-01,  2.9980e-01,  7.8613e-02,  3.3691e-01,  1.8291e+00,\n",
      "         3.9868e-01,  1.0879e+00,  2.6550e-03, -8.7012e-01,  2.9805e+00,\n",
      "         3.4814e-01,  1.7407e-01, -3.7384e-02,  3.6895e+00, -2.7893e-02,\n",
      "         1.1348e+00,  5.4932e-04,  4.4766e+00,  1.4346e+00,  9.2432e-01,\n",
      "         4.6973e-01,  7.1436e-01,  1.2305e+00,  8.9697e-01,  7.5391e-01,\n",
      "         2.6440e-01,  7.2363e-01,  2.3418e+00, -9.9316e-01,  1.6748e+00,\n",
      "        -2.4243e-01,  1.1572e+00, -7.7588e-01,  1.3892e-01,  4.7578e+00,\n",
      "         1.0488e+00,  9.5166e-01,  2.2090e+00,  5.4346e-01,  2.0691e-01,\n",
      "         1.7578e+00,  4.6582e-01,  1.8730e+00, -6.2988e-01,  2.1887e-01,\n",
      "        -9.1406e-01, -6.4941e-01, -1.1426e+00, -5.8887e-01, -9.7363e-01,\n",
      "        -5.4297e-01, -3.7646e-01, -2.0142e-01, -8.3838e-01, -4.0039e-01,\n",
      "        -9.8096e-01,  3.8354e-01, -9.8584e-01, -6.8164e-01, -8.1348e-01,\n",
      "        -2.6489e-01, -8.7695e-01, -7.2461e-01, -9.1357e-01, -7.4658e-01,\n",
      "        -8.1299e-01, -6.5820e-01, -1.1005e-01,  2.0117e+00,  2.2803e-01,\n",
      "         2.7783e-01, -1.0566e+00, -3.0591e-01, -5.9912e-01, -1.2314e+00,\n",
      "        -1.7480e-01, -3.2397e-01, -5.4443e-01,  4.0161e-01, -5.4932e-01,\n",
      "         3.3203e-01,  1.4990e-01, -7.2632e-02, -5.3760e-01, -2.8760e-01,\n",
      "         1.0605e+00, -1.0425e-01,  2.3193e-01, -5.1025e-01, -1.0000e+00,\n",
      "         2.1777e-01, -3.2397e-01, -9.9756e-01, -4.0259e-01, -4.0576e-01,\n",
      "        -1.0146e+00,  1.4917e-01,  3.9038e-01,  4.1250e+00,  6.4307e-01,\n",
      "         1.7695e+00,  3.0391e+00,  2.4487e-01, -8.0762e-01,  3.0586e+00,\n",
      "         5.8057e-01,  9.8535e-01,  8.6523e-01, -1.1484e+00, -1.4175e-02,\n",
      "        -6.0107e-01,  3.7734e+00,  1.0068e+00,  8.4229e-01,  2.8223e-01,\n",
      "         8.1940e-03,  6.5479e-01,  1.1139e-03,  1.1699e+00,  8.5266e-02,\n",
      "        -6.7627e-01, -2.5098e-01,  4.7617e+00,  1.2227e+00,  3.5889e-01,\n",
      "         5.1807e-01, -1.1943e+00, -2.0187e-02, -1.0962e-01, -4.4312e-01,\n",
      "         5.5322e-01, -1.0889e+00,  3.4766e-01,  4.8945e+00,  1.0957e+00,\n",
      "         1.2100e+00, -4.5117e-01,  1.4141e+00, -5.9265e-02,  4.8950e-01,\n",
      "        -3.7598e-01,  2.8711e+00,  8.8037e-01,  6.8359e-01,  2.4048e-01,\n",
      "        -5.7495e-02, -7.0679e-02,  1.9463e+00,  1.0020e+00, -7.8662e-01,\n",
      "         1.8936e+00,  3.9883e+00,  8.8867e-01,  1.2871e+00,  1.5693e+00,\n",
      "         9.2041e-01,  3.8574e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 9.7998e-01, -3.1323e-01, -1.0327e-01, -9.3765e-03, -2.9663e-01,\n",
      "        -4.4092e-01, -9.4238e-01, -8.7939e-01, -7.4072e-01, -1.3398e+00,\n",
      "        -8.0273e-01, -1.1572e+00, -8.2764e-01, -8.3984e-01, -4.8315e-01,\n",
      "        -1.3818e+00, -1.7734e+00,  1.1298e-01,  3.9185e-02,  8.6304e-02,\n",
      "        -8.3057e-01,  1.1578e-01, -3.3618e-01, -1.0488e+00,  1.1304e-01,\n",
      "        -8.0615e-01, -3.2324e-01, -3.3545e-01,  8.3936e-01, -7.5488e-01,\n",
      "        -8.0078e-02, -1.2461e+00,  4.2871e-01, -1.2324e+00,  2.5903e-01,\n",
      "         3.6694e-01,  2.3652e+00, -1.2598e+00,  2.1216e-01,  1.4099e-01,\n",
      "         1.8975e+00, -1.3086e+00, -2.9968e-02, -5.3223e-01,  4.1113e-01,\n",
      "        -8.5498e-01, -5.8105e-01, -6.4600e-01,  7.1239e-04, -1.2061e+00,\n",
      "        -8.4961e-01, -2.7563e-01, -9.3213e-01, -2.2278e-01, -3.8916e-01,\n",
      "        -1.1357e+00, -7.4072e-01, -5.2734e-01, -1.4414e+00, -8.4766e-01,\n",
      "        -6.9043e-01, -1.8066e+00,  1.5173e-01,  6.3770e-01, -1.6123e+00,\n",
      "        -1.4297e+00, -8.5986e-01, -1.0537e+00, -5.5176e-01, -1.1553e+00,\n",
      "        -1.8347e-01, -1.3135e+00, -9.4287e-01, -9.1260e-01, -1.4697e+00,\n",
      "        -3.9648e-01, -7.0215e-01,  1.5881e-01, -1.2891e+00, -5.8496e-01,\n",
      "        -4.9927e-01, -7.6416e-01, -9.9463e-01, -1.5771e+00, -1.2637e+00,\n",
      "        -3.2007e-01, -7.2754e-01, -9.4336e-01, -1.0674e+00, -1.9893e+00,\n",
      "         8.8257e-02,  5.2063e-02,  3.5620e-01, -9.9658e-01,  3.1982e-01,\n",
      "         5.4004e-01,  1.6318e+00,  1.1826e+00, -8.5010e-01, -7.7148e-01,\n",
      "        -1.1680e+00, -1.9873e-01, -1.0928e+00, -1.0869e+00, -1.3184e-01,\n",
      "        -8.1982e-01, -4.0259e-01, -7.2070e-01, -7.7332e-02, -1.2109e+00,\n",
      "        -9.8779e-01, -1.6455e+00,  2.1594e-01,  2.9712e-01,  1.3467e+00,\n",
      "        -1.6006e+00, -4.1748e-01, -1.2139e+00,  2.7759e-01,  4.6460e-01,\n",
      "        -1.0254e+00,  1.8127e-01, -3.8525e-01, -9.4141e-01,  1.7664e-01,\n",
      "        -5.0635e-01,  4.4556e-02, -2.9736e-01,  5.6458e-02, -1.4873e+00,\n",
      "        -6.3281e-01, -1.7358e-01,  4.2163e-01,  3.0225e-01, -1.2979e+00,\n",
      "         1.0468e-01,  1.5857e-01,  1.2207e+00, -1.3164e+00, -7.4756e-01,\n",
      "        -8.1348e-01, -3.4766e-01,  1.6809e-01, -9.7705e-01, -6.6064e-01,\n",
      "        -3.9746e-01, -7.2461e-01, -1.4502e+00, -5.8643e-01, -4.9829e-01,\n",
      "        -8.6670e-01,  4.3018e-01, -9.2725e-01, -6.0840e-01, -2.7563e-01,\n",
      "         4.5923e-01,  1.9800e-01,  2.5449e+00,  4.3579e-01, -1.0938e+00,\n",
      "        -1.0361e+00, -5.4736e-01, -7.7588e-01, -1.0811e+00, -2.0020e-01,\n",
      "        -7.0166e-01, -6.7871e-01, -1.4221e-01, -8.1055e-01, -5.6055e-01,\n",
      "         2.4805e-01, -1.0071e-01,  2.1328e+00,  6.2061e-01, -1.3359e+00,\n",
      "        -1.5654e+00,  1.9717e+00, -1.3018e+00,  2.2422e+00, -1.5439e+00,\n",
      "        -1.0146e+00, -1.5149e-01, -6.2158e-01, -1.9629e-01, -1.1934e+00,\n",
      "        -1.2246e+00, -4.2139e-01, -2.3523e-01,  5.0146e-01,  1.8494e-01,\n",
      "         2.3315e-01,  1.8945e+00,  1.6553e-01,  1.2964e-01, -1.7051e+00,\n",
      "        -4.2944e-01, -1.4971e+00,  1.1969e-01, -1.4502e+00,  7.4414e-01,\n",
      "        -3.1396e-01, -1.4541e+00, -8.1055e-01, -1.0315e-01, -1.2500e+00,\n",
      "        -1.9946e-01, -7.1240e-01, -1.0498e+00, -1.8542e-01,  1.8140e-01,\n",
      "        -2.1191e-01, -3.5156e-01, -1.0059e+00,  4.3701e-01, -9.3652e-01,\n",
      "        -3.5474e-01, -4.5752e-01,  6.2305e-01,  2.3945e+00,  4.9658e-01,\n",
      "        -7.4951e-01, -6.4893e-01,  2.5039e+00, -6.2354e-01, -3.9526e-01,\n",
      "        -5.3369e-01,  5.8691e-01,  2.8711e+00,  5.5273e-01, -8.3887e-01,\n",
      "        -5.0928e-01,  3.0645e+00, -1.5371e+00, -1.3940e-01,  9.1113e-01,\n",
      "         3.3105e-01,  1.6279e+00, -6.0669e-02,  4.6069e-01,  3.8457e+00,\n",
      "         2.0566e+00,  2.3914e-01,  5.0977e-01,  4.2578e+00, -7.2266e-01,\n",
      "        -1.5068e+00, -9.4629e-01, -1.3398e+00, -1.2285e+00, -1.0488e+00,\n",
      "        -1.0137e+00, -1.3955e+00, -1.7314e+00,  2.3950e-01,  5.6201e-01,\n",
      "         9.7314e-01, -1.1240e+00,  7.9834e-02, -1.0242e-01, -8.9258e-01,\n",
      "         8.7939e-01, -5.4346e-01, -7.6904e-01,  1.1943e+00, -1.0703e+00,\n",
      "         4.6753e-01, -1.5503e-01,  1.4880e-01,  7.2998e-02,  5.3271e-01,\n",
      "         2.0093e-01,  7.7539e-01, -5.4785e-01, -1.4391e-03, -5.4297e-01,\n",
      "        -1.2793e+00, -9.1211e-01, -5.9473e-01, -2.9517e-01, -8.0371e-01,\n",
      "        -1.5508e+00, -1.2422e+00, -7.5317e-02, -4.6753e-01, -6.3721e-01,\n",
      "        -2.5928e-01, -1.0938e+00, -3.4180e-01, -3.6548e-01, -1.4771e-01,\n",
      "        -1.4111e+00,  1.0939e-03, -1.2598e+00, -2.0203e-01,  5.3558e-02,\n",
      "        -1.1992e+00, -1.4854e+00, -2.9492e-01, -6.7810e-02, -1.4531e+00,\n",
      "         1.6528e-01, -2.9419e-01, -1.8762e-01, -1.4600e+00, -5.0586e-01,\n",
      "        -2.4384e-02, -1.0449e+00,  4.8608e-01, -1.9470e-01,  6.3916e-01,\n",
      "         2.1497e-01,  4.9023e-01,  4.0000e+00, -9.2920e-01,  1.8604e+00,\n",
      "        -3.3887e-01, -1.8701e-01, -4.9829e-01,  3.1494e-01,  1.7493e-01,\n",
      "        -1.4111e+00, -2.4060e-01, -5.5566e-01, -7.0654e-01,  1.7627e-01,\n",
      "         2.5879e+00, -2.6709e-01,  4.9829e-01, -1.9424e+00, -3.3179e-01,\n",
      "         8.0225e-01, -9.9658e-01,  1.0234e+00,  4.2432e-01, -1.0430e+00,\n",
      "         4.8950e-01, -5.7959e-01,  3.6865e-01, -6.2622e-02,  3.3569e-01,\n",
      "         2.0264e-01,  3.4336e+00, -1.0264e+00, -8.1348e-01, -2.6660e-01,\n",
      "        -5.6592e-01,  5.4840e-02,  2.1074e+00, -2.6587e-01,  2.5122e-01,\n",
      "         3.8906e+00,  7.0117e-01, -1.7834e-01,  4.6948e-01,  2.8491e-01,\n",
      "         2.8730e+00, -8.5645e-01, -3.6304e-01, -2.9858e-01, -4.1089e-01,\n",
      "        -4.3335e-02, -1.1865e+00,  2.0667e-01,  1.4319e-01, -8.9893e-01,\n",
      "        -4.7241e-01, -1.2256e-01,  6.7749e-02,  1.7676e+00,  1.2344e+00,\n",
      "        -3.5791e-01,  3.7567e-02,  1.7334e+00,  3.6719e-01,  3.6499e-01,\n",
      "         4.4336e+00, -4.6826e-01, -6.8164e-01,  1.1975e-01, -1.1768e+00,\n",
      "        -2.4817e-01, -1.5369e-01,  3.0200e-01, -7.9199e-01,  4.8096e-01,\n",
      "        -8.4473e-02, -7.7393e-01,  2.0664e+00, -3.8281e-01, -9.1992e-01,\n",
      "         2.2656e+00, -1.4395e+00, -6.4062e-01, -8.3105e-01,  1.0410e+00,\n",
      "         7.5830e-01, -3.3032e-01,  2.6289e+00, -1.7773e-01, -4.8950e-01,\n",
      "         1.1993e-01,  3.3154e-01,  3.9492e+00, -1.4277e+00,  4.0161e-02,\n",
      "        -1.2524e-01, -1.2314e+00, -2.0239e-01, -5.7959e-01, -2.0312e-01,\n",
      "        -3.8269e-02, -9.7168e-02,  1.2363e+00,  3.4629e+00,  8.4082e-01,\n",
      "        -6.3623e-01, -2.9443e-01,  3.7871e+00, -5.2881e-01,  4.9268e-01,\n",
      "        -4.9316e-02,  4.8120e-01,  4.1914e+00,  8.0017e-02,  4.3555e-01,\n",
      "         4.0117e+00,  2.5684e-01, -7.1472e-02, -1.1597e-01,  1.0283e+00,\n",
      "         1.7969e+00,  4.8242e+00,  3.2544e-01,  1.2500e-01,  2.6113e+00,\n",
      "         1.3555e+00, -6.9434e-01,  2.1875e+00, -6.3184e-01, -1.1743e-01,\n",
      "        -2.3254e-01, -1.2964e-01, -4.1235e-01, -2.3718e-01, -3.8672e-01,\n",
      "         8.9160e-01,  2.7871e+00,  1.0742e+00, -5.5908e-01, -3.5425e-01,\n",
      "         3.8223e+00, -5.2393e-01,  4.7852e-01, -5.6006e-01, -2.3523e-01,\n",
      "        -7.0190e-02,  2.7515e-01,  3.8086e-02,  7.6855e-01,  4.8975e-01,\n",
      "         1.6016e-01,  1.6577e-01,  3.6133e+00, -3.3496e-01, -9.7998e-01,\n",
      "        -3.6499e-01, -1.1572e+00,  2.6309e+00, -1.3066e+00, -4.8413e-01,\n",
      "        -2.0972e-01, -2.7197e-01, -2.5684e-01,  3.6670e-01,  3.6304e-01,\n",
      "         7.2314e-01,  1.9385e-01,  3.4512e+00, -6.1328e-01,  8.0811e-01,\n",
      "        -1.3062e-01,  4.4141e-01, -3.4644e-01, -9.2834e-02,  2.7500e+00,\n",
      "         6.1816e-01,  1.1377e+00, -2.8857e-01,  1.8408e-01, -5.9753e-02,\n",
      "         4.7998e-01,  2.4082e+00,  8.7500e-01,  4.5947e-01,  7.9004e-01,\n",
      "         2.7383e+00,  7.1533e-01,  4.3125e+00, -6.6699e-01,  7.0312e-01,\n",
      "         7.4512e-01,  1.3301e+00, -6.7334e-01,  7.8662e-01, -5.1416e-01,\n",
      "         3.2104e-01,  1.3779e+00,  7.6416e-01,  2.5708e-01,  2.8184e+00,\n",
      "         3.1758e+00, -2.8516e-01,  1.6748e+00, -1.1230e+00, -4.8413e-01,\n",
      "        -9.7803e-01, -6.9336e-01, -1.5625e+00, -1.0137e+00, -7.4219e-01,\n",
      "        -1.0498e+00, -2.6709e-01, -7.6416e-02, -8.0859e-01,  4.3823e-01,\n",
      "         2.9980e-01,  1.2744e-01,  6.8994e-01, -3.0176e-01, -1.3740e+00,\n",
      "        -9.9365e-01, -1.5029e+00, -8.7207e-01, -1.8770e+00, -5.1318e-01,\n",
      "        -1.6816e+00, -1.6885e+00, -1.4482e+00,  1.9482e-01,  1.8643e+00,\n",
      "         6.3818e-01, -1.4150e+00, -1.4307e+00, -6.7188e-01, -1.5283e+00,\n",
      "        -8.0518e-01,  2.9663e-01, -5.9375e-01, -1.0684e+00,  2.3743e-01,\n",
      "        -4.3091e-01, -7.1350e-02, -1.5051e-01,  3.5620e-01, -4.3750e-01,\n",
      "        -5.6494e-01,  1.6345e-01, -3.6499e-02,  8.5889e-01, -8.7891e-01,\n",
      "         6.8176e-02,  2.2766e-01, -9.6680e-01, -9.3555e-01,  1.0901e-01,\n",
      "         1.4832e-01, -5.5518e-01,  5.1221e-01, -2.5024e-01,  3.2598e+00,\n",
      "         1.0312e+00, -8.0615e-01,  2.3320e+00, -9.8486e-01,  9.0039e-01,\n",
      "        -5.2246e-01,  1.1123e+00,  1.9023e+00, -1.4180e+00, -5.2979e-01,\n",
      "        -8.0225e-01, -4.9390e-01, -5.8044e-02, -9.0454e-02,  1.4238e+00,\n",
      "         3.7910e+00,  1.0889e+00, -6.3623e-01, -2.3669e-01,  3.8223e+00,\n",
      "        -1.0586e+00, -6.2988e-01, -1.4661e-01,  2.5122e-01,  3.9414e+00,\n",
      "         1.1758e+00, -1.4453e+00, -5.6738e-01, -7.7979e-01, -4.8071e-01,\n",
      "         1.2903e-01, -1.2705e+00, -1.1367e+00, -3.2104e-01,  9.6069e-02,\n",
      "         2.6953e+00, -2.0813e-02,  1.4248e+00,  1.5312e+00,  1.3298e-02,\n",
      "        -4.2139e-01, -8.1104e-01, -9.5581e-02, -3.1738e-01,  1.3115e+00,\n",
      "         3.8867e+00, -8.1348e-01, -4.9341e-01,  5.8740e-01, -2.7051e-01,\n",
      "         1.4248e+00, -6.2744e-01,  9.4482e-01,  6.3782e-02,  1.1230e+00,\n",
      "         4.2266e+00,  5.0488e-01], device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  5\n",
      "qid:  5adf3a4f5542992d7e9f92ec\n",
      "q_type:  tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: tensor([[[ 0.4178, -0.0866, -0.3642,  ...,  1.4203,  0.7718, -0.2634],\n",
      "         [-0.2205,  0.2679, -0.3116,  ...,  1.1092, -0.1881, -0.2192],\n",
      "         [ 0.7581, -0.1151,  0.0928,  ...,  2.0582,  0.7711, -0.3415],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([1.2461, 1.4902, 0.5024,  ..., 1.6484, 1.1396, 0.4343], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.1367, -0.4253,  2.6230,  ...,  1.7500,  4.1797,  0.5718],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  6\n",
      "qid:  5ab2e3a35542991669774124\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8744, -0.4192, -0.4357,  ...,  1.3171,  0.4214, -0.5264],\n",
      "         [ 0.5780, -0.0816,  0.4458,  ...,  1.5551,  0.3041,  0.3095],\n",
      "         [ 0.1413,  0.1777,  0.2905,  ...,  0.8992,  0.1995,  0.0436],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 1.3691e+00,  9.0186e-01,  9.0283e-01,  5.4297e-01,  3.0078e-01,\n",
      "         7.6172e-01, -6.1035e-01,  2.7949e+00,  1.0391e+00,  1.2529e+00,\n",
      "         2.8027e-01,  4.5410e-01, -6.0596e-01, -1.8463e-03,  3.6426e-01,\n",
      "        -2.7466e-01,  5.9277e-01,  3.2129e-01,  1.4873e+00,  4.3726e-01,\n",
      "         3.9121e+00,  6.3818e-01,  1.0791e+00,  4.1875e+00,  4.2700e-01,\n",
      "         9.7217e-01, -1.0146e+00, -6.4844e-01, -8.4424e-01,  4.2500e+00,\n",
      "         1.2451e+00,  7.8955e-01, -1.0762e+00,  3.3008e+00,  2.0370e-02,\n",
      "         1.8271e+00, -1.4563e-01,  1.7383e-01, -3.4521e-01,  1.1016e+00,\n",
      "        -2.5269e-01,  2.3047e-01,  4.3750e+00,  1.1035e+00, -2.9126e-01,\n",
      "        -1.1543e+00,  2.3877e-01, -5.6250e-01, -4.1211e-01, -8.3496e-01,\n",
      "        -5.4352e-02, -1.8103e-01,  3.1758e+00,  1.2324e+00,  8.1543e-01,\n",
      "         4.1382e-01,  9.6533e-01,  3.0957e+00,  3.7305e-01,  9.9335e-03,\n",
      "         5.4688e-01,  1.5808e-01,  9.7363e-01,  2.9980e-01, -5.5664e-01,\n",
      "         3.3438e+00,  2.3535e-01,  4.4507e-01,  2.6077e-02, -1.0801e+00,\n",
      "        -8.1787e-02, -9.2041e-01, -4.3628e-01, -5.6738e-01,  1.7412e+00,\n",
      "        -1.7419e-01,  4.9377e-02, -6.8701e-01,  4.7729e-01, -3.3960e-01,\n",
      "        -5.3369e-01,  2.1265e-01, -6.1981e-02,  4.2627e-01,  2.5879e+00,\n",
      "         3.4082e-01,  6.1865e-01,  6.7383e-01,  3.4844e+00,  1.6528e-01,\n",
      "         1.8494e-01,  5.6348e-01, -1.0137e+00,  1.8936e+00, -2.1301e-01,\n",
      "         4.5532e-02, -7.1973e-01,  7.7441e-01, -1.6284e-01, -4.5508e-01,\n",
      "         7.4219e-01, -6.2158e-01,  7.1973e-01, -9.4727e-01,  3.5781e+00,\n",
      "         1.8140e-01,  2.4121e-01,  9.6240e-01, -2.4268e-01, -2.3889e-01,\n",
      "        -4.2920e-01, -3.7354e-01,  2.9082e+00,  1.3320e+00,  1.0840e+00,\n",
      "         6.3293e-02,  4.7119e-01,  1.5881e-01,  6.5820e-01,  1.6956e-01,\n",
      "         2.2402e+00,  2.3413e-01,  1.3643e+00,  3.6865e-01,  2.9663e-01,\n",
      "        -5.4199e-01,  3.8789e+00,  3.3813e-01,  1.1987e-01, -2.9395e-01,\n",
      "        -1.2324e+00, -2.9312e-02, -2.9449e-03, -8.3057e-01,  1.6885e+00,\n",
      "        -1.7969e-01,  5.4871e-02, -7.3291e-01,  8.0566e-01,  9.2712e-02,\n",
      "         4.1797e-01,  4.4805e+00,  4.9243e-01,  5.1465e-01,  7.8516e-01,\n",
      "         4.2930e+00,  5.7520e-01,  3.0127e-01,  7.6172e-01, -1.1270e+00,\n",
      "         2.3125e+00,  2.4033e-02,  1.9470e-01, -4.2383e-01,  1.4268e+00,\n",
      "         1.5845e-01,  2.5903e-01,  1.1494e+00,  8.3203e-01,  7.4219e-01,\n",
      "        -7.3120e-02,  8.5449e-01,  4.3164e+00,  7.3145e-01,  5.3320e-01,\n",
      "         2.6587e-01,  3.5898e+00,  1.2666e+00,  3.4668e-01,  2.9512e+00,\n",
      "         1.1602e+00, -3.8135e-01,  9.3066e-01, -4.9463e-01, -2.2668e-01,\n",
      "        -1.4905e-01, -6.5381e-01,  2.9648e+00,  1.2520e+00,  1.0371e+00,\n",
      "         1.2317e-01,  6.5527e-01,  4.0527e-01,  8.0371e-01,  2.4561e-01,\n",
      "         2.4512e+00,  2.3120e-01,  1.1680e+00,  2.5854e-01,  2.1594e-01,\n",
      "        -6.8506e-01,  3.6484e+00,  3.5132e-01,  5.1172e-01, -1.8860e-01,\n",
      "        -1.1572e+00,  1.2817e-01, -8.6377e-01, -2.3962e-01, -3.4131e-01,\n",
      "        -6.7969e-01,  1.8994e+00, -1.3892e-01,  5.7434e-02, -6.1377e-01,\n",
      "         4.8120e-01,  8.5022e-02,  4.1162e-01,  2.7109e+00,  8.4180e-01,\n",
      "         1.6279e+00,  8.8330e-01, -8.3154e-01, -4.5166e-01, -7.2070e-01,\n",
      "         4.2422e+00,  5.1367e-01,  4.2871e-01,  5.6348e-01, -5.3564e-01,\n",
      "         4.7031e+00,  8.0127e-01,  7.5146e-01,  4.0527e-01,  7.3584e-01,\n",
      "         7.3535e-01, -1.1348e+00,  2.4668e+00,  6.1328e-01,  2.4268e-01,\n",
      "         2.0508e+00,  4.7974e-02,  2.4146e-01, -3.6279e-01,  1.1895e+00,\n",
      "         2.2205e-01,  1.4526e-01, -7.9541e-01,  1.1787e+00, -2.0898e-01,\n",
      "        -6.5088e-01,  1.4590e+00, -5.2734e-01,  4.7227e+00,  7.8809e-01,\n",
      "         8.2617e-01,  6.2109e-01,  6.0938e-01,  6.5039e-01, -4.4775e-01,\n",
      "        -8.4180e-01,  8.7402e-01, -5.1709e-01, -2.1713e-02, -8.3936e-01,\n",
      "         2.6289e+00,  3.8647e-01, -2.9492e-01,  3.8257e-01,  1.4658e+00,\n",
      "         3.4229e-01,  1.8274e-01, -5.8838e-01, -3.1372e-01, -1.5552e-01,\n",
      "         2.2998e-01,  3.8770e-01, -7.7832e-01,  3.9668e+00,  5.6299e-01,\n",
      "         9.6533e-01,  5.5371e-01,  1.1777e+00, -7.2705e-01,  1.0181e-01,\n",
      "        -9.1858e-02, -8.5840e-01, -1.2140e-01, -1.1398e-02, -4.2603e-01,\n",
      "        -5.7617e-01,  6.7139e-03,  2.1899e-01,  9.6191e-01,  4.8853e-01,\n",
      "        -9.2773e-02,  6.3037e-01,  2.9175e-01, -1.3077e-02, -2.7271e-01,\n",
      "        -3.1250e-01,  7.5391e-01,  2.3560e-01, -3.2837e-01,  5.9668e-01,\n",
      "         2.1167e-01,  2.9004e-01, -3.3228e-01,  2.2827e-01, -6.3086e-01,\n",
      "         6.4355e-01, -2.7661e-01,  4.5959e-02,  6.7090e-01,  6.1230e-01,\n",
      "        -1.1273e-01,  8.7256e-01,  1.6982e+00,  1.2520e+00,  3.6548e-01,\n",
      "         1.1182e+00,  1.3855e-01, -1.9250e-01,  6.4160e-01, -9.1162e-01,\n",
      "         3.5767e-01,  1.2617e+00,  5.9814e-02, -4.9976e-01, -1.3257e-01,\n",
      "        -5.3516e-01,  3.7090e+00,  1.1572e+00,  1.0000e+00,  3.1763e-01,\n",
      "         7.1924e-01, -8.5156e-01,  2.5508e+00,  6.2891e-01,  1.4434e+00,\n",
      "        -5.9113e-02, -2.2070e-01,  3.5156e-01, -1.4490e-01, -6.1572e-01,\n",
      "        -2.5708e-01,  5.1611e-01, -7.0801e-01,  2.8149e-01, -2.4500e-01,\n",
      "         3.0195e+00,  3.6621e+00,  6.5967e-01,  7.2705e-01,  3.8965e-01,\n",
      "        -6.3477e-01,  6.4014e-01, -5.2246e-01,  1.8125e+00,  1.2402e+00,\n",
      "         5.2832e-01, -4.2383e-01, -3.3643e-01,  7.3682e-01, -4.0137e-01,\n",
      "         1.0615e+00,  3.5938e-01, -6.8213e-01,  9.4531e-01,  2.8125e-01,\n",
      "         7.5781e-01,  5.4395e-01,  2.9541e-01,  2.2131e-01,  7.4561e-01,\n",
      "         8.1592e-01, -8.8745e-02, -6.8994e-01,  2.3691e+00,  8.3740e-01,\n",
      "         6.5137e-01,  1.0400e+00,  2.0239e-01,  4.2188e-01,  1.0352e+00,\n",
      "         4.9829e-01,  3.7549e-01,  1.3623e-01, -6.6553e-01,  6.1230e-01,\n",
      "         6.7041e-01, -4.9072e-01,  6.0730e-02, -1.1396e+00, -3.2104e-01,\n",
      "         9.2285e-01, -5.7373e-01,  4.1133e+00,  5.6299e-01,  6.1230e-01,\n",
      "         3.3813e-01,  4.1333e-01, -4.4189e-01,  6.0254e-01,  1.2366e-01,\n",
      "        -1.6553e-01,  6.7188e-01,  2.5977e-01,  1.5625e-01, -7.9688e-01,\n",
      "        -9.7290e-02, -3.3447e-01, -8.1396e-01, -8.2178e-01,  4.1758e+00,\n",
      "         6.0986e-01,  9.9902e-01,  3.3081e-01, -7.3486e-01, -1.5381e-01,\n",
      "         1.6553e+00,  3.4058e-01,  4.5435e-01, -6.3867e-01,  6.1963e-01,\n",
      "         4.8315e-01, -2.2888e-03,  2.1533e-01,  4.3457e-01,  3.7773e+00,\n",
      "         1.2832e+00,  4.0833e-02,  7.4268e-01, -7.7148e-02, -3.3203e-01,\n",
      "         1.9385e-01,  7.4609e-01, -1.4902e+00,  2.4292e-01, -6.2451e-01,\n",
      "        -6.6943e-01,  8.3643e-01,  1.0645e-01,  3.2104e-01, -2.0984e-01,\n",
      "         3.1113e+00,  9.7949e-01, -2.0496e-01,  4.8950e-01, -2.0996e-01,\n",
      "        -4.2529e-01, -2.8540e-01, -8.0762e-01,  2.9541e-01,  4.6973e-01,\n",
      "        -1.1084e+00,  2.3809e+00,  7.1875e-01, -4.6899e-01,  2.5195e+00,\n",
      "         6.5479e-01,  8.2422e-01, -7.2266e-01, -6.2500e-01, -6.6602e-01,\n",
      "        -6.7773e-01, -9.5996e-01,  8.0127e-01, -5.8594e-01,  1.0371e+00,\n",
      "        -2.6733e-01, -2.0850e-01, -1.1299e+00,  2.7563e-01,  6.4062e-01,\n",
      "        -1.5100e-01, -5.8447e-01,  3.8330e-01, -9.4141e-01,  3.8745e-01,\n",
      "        -7.5635e-01,  7.4268e-01, -1.9312e-01,  6.4746e-01,  4.5929e-03,\n",
      "        -9.9512e-01,  4.9829e-01, -3.6011e-01, -6.7078e-02, -8.4277e-01,\n",
      "        -6.1035e-01,  5.7373e-02,  5.3906e-01, -2.5488e-01, -3.8696e-01,\n",
      "        -1.3086e+00, -3.5034e-02,  1.4668e+00,  5.1416e-01,  6.1914e-01,\n",
      "         3.6816e-01,  1.1914e-01,  4.0894e-01,  4.3359e+00,  1.1602e+00,\n",
      "         1.0742e+00,  2.9199e-01,  1.0488e+00,  3.9199e+00,  1.1396e+00,\n",
      "         9.1748e-01,  2.2400e-01,  9.3652e-01, -1.5215e+00,  1.1650e+00,\n",
      "         7.1143e-01, -7.2144e-02,  5.1221e-01, -8.4033e-01, -6.4844e-01,\n",
      "         4.2070e+00,  1.8721e+00,  1.5244e+00,  9.4531e-01, -1.3391e-01,\n",
      "         2.2637e+00,  7.7246e-01,  1.5586e+00,  1.1348e+00, -3.2495e-01,\n",
      "         1.7578e+00,  6.4014e-01,  4.0845e-01,  6.5186e-02,  9.3506e-01,\n",
      "         2.1509e-01, -7.3389e-01,  2.2207e+00,  2.2507e-02, -6.0693e-01,\n",
      "         3.6855e+00,  1.0332e+00,  1.0332e+00,  3.9282e-01,  1.0605e+00,\n",
      "        -6.4746e-01,  1.3760e+00,  1.3076e+00, -1.2549e-01,  3.9883e+00,\n",
      "         8.4668e-01,  9.8633e-01, -4.8486e-01,  3.2773e+00,  1.5850e+00,\n",
      "        -2.4548e-01,  3.8223e+00,  9.4775e-01,  1.1885e+00,  3.0786e-01,\n",
      "         4.1235e-01,  3.1465e+00,  5.5322e-01,  5.0488e-01, -1.7371e-01,\n",
      "         2.0215e+00, -4.5166e-01,  1.5166e+00, -1.1943e+00, -5.7037e-02,\n",
      "        -6.5771e-01, -1.1289e+00,  3.2837e-01,  2.6050e-01, -7.9639e-01,\n",
      "         4.8950e-01,  6.6162e-02,  1.5942e-01, -1.2969e+00,  1.0371e+00,\n",
      "        -4.9487e-01, -2.2852e-01, -2.0374e-01, -2.2900e-01, -4.1846e-01,\n",
      "        -2.8296e-01,  4.6411e-01,  1.1309e+00,  4.4019e-01,  1.4697e-01,\n",
      "        -9.0918e-01,  1.8564e+00,  1.0632e-01,  7.1582e-01, -1.5149e-01,\n",
      "         8.2324e-01, -1.3171e-01,  3.3008e-01, -8.1909e-02, -7.4023e-01,\n",
      "         7.6611e-01, -8.4961e-01,  4.0942e-01,  6.3428e-01,  3.4180e-01,\n",
      "        -1.0898e+00,  2.2891e+00,  2.0667e-01,  1.1902e-01,  1.1289e+00,\n",
      "        -6.1426e-01,  6.9434e-01,  2.8223e-01,  4.0161e-01,  4.8164e+00,\n",
      "         1.1631e+00,  9.9609e-01,  2.3613e+00,  6.4893e-01,  1.0059e+00,\n",
      "         4.7578e+00,  8.7549e-01,  7.5000e-01,  1.9424e+00,  6.3477e-01,\n",
      "         8.8184e-01,  4.5859e+00,  1.9521e+00,  6.1426e-01,  1.8350e+00,\n",
      "         1.3818e+00,  3.8809e+00,  1.0898e+00,  5.9863e-01,  2.1465e+00,\n",
      "         4.7109e+00,  1.1865e+00,  8.8232e-01,  1.1680e+00,  1.5100e-01,\n",
      "         1.2471e+00,  8.2422e-01,  1.5051e-01,  3.2305e+00,  6.3818e-01,\n",
      "        -5.7129e-01,  4.7021e-01, -4.9561e-01,  3.7769e-01, -8.6328e-01,\n",
      "         5.5322e-01,  7.3535e-01, -1.1084e-01,  6.2012e-01, -4.2700e-01,\n",
      "        -4.9292e-01,  7.4561e-01,  2.4219e-01,  9.3701e-01, -7.0654e-01,\n",
      "         3.2129e-01, -4.0015e-01,  2.6099e-01, -6.1182e-01,  1.2490e+00,\n",
      "        -4.6631e-01,  5.0244e-01,  1.0791e-01,  7.8174e-01,  1.0186e+00,\n",
      "        -2.6123e-01,  1.0869e+00, -6.0205e-01,  2.6113e+00,  5.0488e-01,\n",
      "        -4.8315e-01,  5.0342e-01,  5.8789e-01, -7.1716e-04,  4.3945e-02,\n",
      "        -3.0640e-01,  3.3789e-01, -4.5264e-01,  1.5127e+00, -2.2803e-01,\n",
      "         1.8970e-01,  2.3652e+00,  6.5479e-01,  4.8022e-01, -6.7749e-03,\n",
      "         5.4297e-01, -6.8018e-01,  2.6709e-01,  3.7964e-01, -5.5389e-02,\n",
      "         1.5649e-01,  7.3389e-01,  3.6560e-02,  1.5344e-01,  5.3711e-01,\n",
      "         9.3164e-01,  5.0293e-01,  1.7029e-01, -1.4697e-01,  5.9961e-01,\n",
      "         4.2017e-01,  5.8887e-01,  1.0205e-01,  1.6475e+00,  3.3716e-01,\n",
      "         9.1943e-01,  6.3574e-01,  3.5664e+00,  1.3008e+00,  1.6016e+00,\n",
      "         1.0332e+00,  9.5264e-01,  2.0918e+00,  1.1650e+00,  1.3330e+00,\n",
      "         4.2188e-01], device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.7100,  0.3257,  0.7690,  0.1444,  1.4590,  1.9746, -1.2998, -0.3147,\n",
      "         0.0911,  0.3022,  0.9043,  1.2031, -1.0156, -0.7241, -0.5215, -0.6538,\n",
      "        -1.0166, -0.7432, -0.7842,  0.5142,  0.2208,  2.3477,  3.9180, -0.1162,\n",
      "         1.5811,  4.0234, -1.6660, -0.6855, -1.1416,  0.6714,  2.6348,  3.8691,\n",
      "        -1.2363,  0.6665,  1.2158, -0.3455, -0.1218,  0.2832,  1.5674,  2.6211,\n",
      "        -0.0707,  0.0853,  0.5933,  2.7422, -0.4705, -1.0391, -0.4309, -0.0480,\n",
      "         0.0282, -1.4609, -1.4902, -0.9878, -0.4695, -0.0062,  0.4924,  2.8848,\n",
      "         0.7090,  0.3035,  0.1180,  1.7363,  0.4451,  1.3193,  0.1385,  2.0039,\n",
      "        -0.7651,  0.2920,  0.7021,  3.6836,  3.1152, -1.4512, -1.1240, -0.9253,\n",
      "        -1.2568, -0.1332, -0.5859, -0.2515,  0.2106,  1.2930,  2.7812,  0.1947,\n",
      "        -0.8892, -0.2791, -0.8057,  0.4915,  0.1731,  0.7422,  3.4492,  3.2168,\n",
      "        -0.2413,  0.2717,  1.8887,  3.1660, -1.6650, -1.0068, -0.3640,  0.0657,\n",
      "         0.9990,  2.0430, -0.3518, -1.4160,  0.8765, -1.2012, -1.2129, -0.9624,\n",
      "         0.0083,  0.5312,  3.0840,  2.9629, -0.5562, -0.5693, -0.6567, -1.3145,\n",
      "        -0.4792,  0.0916,  0.2389,  1.3018,  0.0294,  1.0361, -0.1261,  1.7578,\n",
      "        -0.2534,  1.0605,  1.0664,  0.3765,  4.3398, -0.5552,  0.5322,  0.2346,\n",
      "         3.9688, -0.0861, -1.1338, -1.3770,  0.1229, -0.7095, -1.0283, -0.3997,\n",
      "         0.1232,  1.3809,  2.9082, -0.5605,  0.4902,  0.3228,  0.1576,  2.8848,\n",
      "         3.5215,  0.0543, -0.2411,  1.5576,  3.8145, -1.6182, -0.9165, -0.3125,\n",
      "         0.2954,  1.2705, -0.1217, -0.1177,  0.2900,  1.7109, -0.1993,  2.1230,\n",
      "        -0.7129, -0.8677,  0.2317,  0.0775,  3.4023,  0.4067, -0.4380,  1.8164,\n",
      "         1.1660,  0.4878,  3.0059, -1.0420,  1.3525, -1.3311, -1.1650, -0.5000,\n",
      "        -0.6333, -0.4456,  0.1029,  0.3083,  1.4307, -0.1509,  0.5825, -0.3442,\n",
      "         1.3037, -0.4661,  0.6978,  0.5435,  0.3042,  4.1562, -0.7642,  0.2812,\n",
      "         0.7026,  4.5781, -0.2216, -1.3320, -1.1094, -1.0098, -1.3369, -0.1602,\n",
      "        -0.5200, -0.7314, -0.3276,  0.1841,  1.5713,  2.8711, -0.7495,  0.4958,\n",
      "        -0.0381,  2.1309, -0.5864,  2.7520, -2.0449, -1.0898, -1.6455,  0.2844,\n",
      "        -0.0422,  2.4980,  2.8359, -1.5957,  0.1573, -0.0710,  0.9058,  1.5312,\n",
      "         2.6973,  3.5020, -1.5205,  0.3179,  0.9077,  1.3145, -0.7524, -0.3740,\n",
      "         0.2371,  1.2988, -0.6035, -0.1539, -0.1443,  0.1019,  1.4932, -0.6421,\n",
      "        -1.0342,  0.7080, -0.8521,  0.2166, -0.0294,  1.1660,  1.7383,  3.1289,\n",
      "         1.1123,  0.0618, -1.4180,  0.3413, -0.4458, -0.5195, -0.8315, -0.1910,\n",
      "         0.5049,  2.6387, -1.2930,  2.9668,  0.5083, -1.1729, -0.6270, -0.9351,\n",
      "        -0.5767, -0.5132, -0.4185, -1.8477,  0.1990,  1.5156,  3.4453, -1.5137,\n",
      "         0.3267, -1.2061, -0.6582, -0.1952, -1.4990, -1.3057, -0.6826, -1.0850,\n",
      "        -0.7627, -0.8950, -0.9468,  0.5195, -0.0679, -0.5825, -1.0986, -1.4277,\n",
      "        -0.1709, -0.6426, -1.5439,  0.2019, -0.8960, -1.2520, -0.6440, -1.0918,\n",
      "        -0.7271, -0.6558, -0.1256, -0.5425,  0.2203, -0.9702, -0.2064, -0.6577,\n",
      "        -0.4165, -0.4758, -1.4609, -1.2881, -0.6040,  1.0195,  0.2360, -0.5557,\n",
      "        -0.2080, -0.3223, -0.4009, -0.4185,  2.3359, -1.1191, -0.8032, -0.7856,\n",
      "        -1.3359, -0.0319,  0.2375,  0.5029,  2.0488,  3.6777, -1.9326,  0.3137,\n",
      "         0.4854, -0.2203,  0.3645,  2.0352,  0.7783,  2.5898, -0.7632, -0.6689,\n",
      "         0.2466, -1.4668, -0.7285, -1.2471,  0.4729, -0.7246,  0.4722,  2.0488,\n",
      "         3.1250, -0.5479, -0.1130, -0.9658, -0.0712,  1.2012, -1.1162, -0.6353,\n",
      "        -0.2522,  0.4709, -1.3027, -0.6753, -0.6089, -0.6299, -0.4058, -0.7734,\n",
      "        -0.1693, -0.4558,  0.1024, -0.6895, -0.3960, -0.1605,  0.1301, -1.3467,\n",
      "         0.0558, -0.2788,  0.3582,  0.7310, -0.5933,  0.5249, -0.0647, -0.5425,\n",
      "        -0.3296, -0.2494,  0.2272,  0.0998, -0.1470, -0.0310, -0.2659, -1.6982,\n",
      "        -0.6704, -1.5264, -0.9800, -0.1449,  0.3733,  2.0391,  1.6797,  0.6309,\n",
      "         2.0059,  0.5342, -0.0377,  0.2832, -0.2844,  0.0205, -0.2108,  0.0883,\n",
      "        -0.1938, -0.0232, -0.6162, -0.8271,  0.2717,  2.0586,  2.4844,  0.9883,\n",
      "        -0.7729, -0.4827,  0.1301,  0.9688,  0.5674,  0.8154,  0.3147, -0.3909,\n",
      "         0.8271, -0.5454,  0.5439,  0.8555, -0.0737,  0.2174,  0.0515,  0.0980,\n",
      "         0.8237,  2.1562,  2.9629, -1.3594,  0.0651, -0.9424, -0.8169, -0.5757,\n",
      "         0.2246, -0.0345, -1.1465,  0.4092, -0.0897,  0.0238, -0.1266,  0.1432,\n",
      "         0.7197,  1.8389, -0.9692, -0.5347, -0.4255, -0.7114,  1.1426,  2.0664,\n",
      "        -0.0076, -0.0594,  0.7485,  3.8711, -0.1112, -1.4932, -1.2285, -0.5190,\n",
      "        -0.9443, -1.3242, -0.8926, -0.1595, -0.6440, -1.3574, -1.2285, -0.7012,\n",
      "        -0.5117, -0.2756,  0.2222, -0.5596, -1.2686, -0.2729, -0.9067, -1.0420,\n",
      "        -1.1523,  0.0514, -1.0664, -1.7422, -0.5996, -0.6978, -0.9307, -1.4082,\n",
      "        -1.0615, -0.9429, -0.7441,  0.1998,  0.5820, -1.1992, -0.5342, -0.7817,\n",
      "         0.0389,  1.6396,  1.7480, -0.6055,  0.4988, -0.2825,  0.5562,  0.6763,\n",
      "         2.2109,  4.4883, -0.3762,  0.2852,  0.6348,  2.1855,  4.4102, -1.1533,\n",
      "         0.2874, -0.1803,  0.7627, -0.4978, -0.4236, -0.3921,  0.2805,  1.1064,\n",
      "         2.5879,  3.6836, -0.0270,  0.6670,  0.8242,  1.7432,  4.2266, -0.4497,\n",
      "         0.0569,  0.8218,  1.2031,  3.1406, -0.0933,  0.9526, -1.1475, -0.3687,\n",
      "         0.8271, -1.4570, -0.2330,  0.3472,  0.8789,  3.0938,  4.3086, -0.3733,\n",
      "         0.1942,  1.8857,  0.4131,  0.3862,  1.1533,  4.6680, -0.1598,  1.0332,\n",
      "         1.1934,  0.3972,  0.5796,  1.2510,  4.5039, -0.0413,  0.5059,  0.5708,\n",
      "         1.2803,  0.3618, -0.6812,  2.1387, -0.8281,  1.4717, -1.3359, -1.1328,\n",
      "        -1.1240, -0.5884, -0.5591, -0.0220, -1.0225, -1.0488, -0.8418, -0.9395,\n",
      "        -1.1318,  1.3389, -0.4587, -1.2461, -1.4258, -0.1290, -0.7397, -1.0986,\n",
      "        -0.5864,  0.0358, -0.2372,  0.3257, -1.1953, -0.0519, -0.1215,  0.1997,\n",
      "         1.0020,  0.5039, -0.9297, -0.5503, -0.7017, -0.9370,  0.0084, -1.3193,\n",
      "        -0.2639,  0.2162,  0.5503, -1.3711,  0.3921,  0.2869,  1.7998,  1.4277,\n",
      "        -0.5220,  1.8340, -0.4373,  0.5039, -0.4011,  0.2040,  1.1230,  0.8252,\n",
      "         1.2607,  4.8750, -0.3870,  0.1312,  0.9707,  0.1913,  0.7378,  3.7969,\n",
      "         0.6079,  0.3513,  1.9521,  2.9590,  4.7891,  0.6436,  0.1135,  1.6104,\n",
      "         3.9395, -0.2264,  0.0196,  1.2822,  1.0391,  1.1045,  2.7168,  3.9258,\n",
      "         0.8877,  1.2939, -0.0136, -1.2900, -0.5137, -0.8745, -0.5000, -0.6885,\n",
      "        -0.1593,  0.4878, -0.2537,  0.0679,  0.5059, -1.1562,  0.0208,  0.5532,\n",
      "        -0.3528, -1.1113, -0.7412, -0.7998, -0.2720, -0.5400,  1.0977, -0.9062,\n",
      "        -0.3979, -0.2112,  0.3816,  0.4016, -0.7090, -0.1720, -0.8745,  1.1064,\n",
      "         3.3242, -1.2646, -0.0405,  0.5786, -0.9102, -0.3110, -0.4133, -0.5244,\n",
      "        -0.9087,  0.2634,  1.4971, -1.8242, -0.6538,  0.4695,  2.9609, -0.2264,\n",
      "        -0.6172, -1.2158, -0.1337,  0.1456, -0.1086, -1.2969, -0.6138, -0.1427,\n",
      "        -1.2002, -0.8086, -0.1831, -0.0059, -1.1689, -0.8135, -0.2025,  0.2676,\n",
      "         0.3445, -0.9585, -0.7549, -0.2426,  0.8096, -0.4929,  0.0082,  0.4265,\n",
      "         0.5771,  0.3440,  0.6304, -0.5547,  1.6416,  4.5664,  0.5107],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_step\n",
      "batch_nb:  7\n",
      "qid:  5ae394e05542990afbd1e18d\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.8300,  0.0741, -0.2570,  ...,  1.6448,  1.0688, -0.0355],\n",
      "         [ 0.1265,  0.3746,  0.1779,  ...,  1.1772,  0.2809,  0.0052],\n",
      "         [-0.1919,  0.3351,  0.1131,  ...,  0.4217,  0.1521, -0.3111],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([2.0430, 1.2871, 0.4653,  ..., 0.7021, 0.7173, 0.4417], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.8379, -0.6621,  0.4614,  ..., -0.1348,  1.9951,  0.5293],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  8\n",
      "qid:  5a8fb3af5542997ba9cb32ee\n",
      "q_type:  tensor([1], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.0125,  0.1197,  0.0117,  ...,  0.0964,  0.0475, -0.0524],\n",
      "         [ 0.0010,  0.2727, -0.1143,  ...,  0.4172,  0.3238, -0.0747],\n",
      "         [ 0.1831,  0.1697,  0.0409,  ..., -0.1464,  0.0886,  0.0259],\n",
      "         ...,\n",
      "         [-0.0113,  0.0542, -0.0216,  ..., -0.0810, -0.0313, -0.1033],\n",
      "         [-0.0113,  0.0542, -0.0216,  ..., -0.0810, -0.0313, -0.1033],\n",
      "         [-0.0113,  0.0542, -0.0216,  ..., -0.0810, -0.0313, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 0.2206,  0.3989,  0.3123,  0.0226,  0.3198, -0.1786,  0.1587,  0.0682,\n",
      "        -0.2263,  0.2659,  0.2769,  0.2128,  0.5659,  0.4167,  0.5503,  0.2129],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 0.1312, -0.1700, -0.1962, -0.0450, -0.2340, -0.2786, -0.4026, -0.1232,\n",
      "        -0.4919,  0.0508, -0.3423,  0.1589, -0.2625, -0.3743,  0.0343,  0.1587],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_step\n",
      "batch_nb:  9\n",
      "qid:  5ac002705542996f0d89cb05\n",
      "q_type:  tensor([0], device='cuda:0')\n",
      "sequence_output: tensor([[[ 0.3527, -0.1418,  0.1240,  ...,  2.1371,  0.1180, -0.4462],\n",
      "         [ 0.6073, -0.5715,  0.4542,  ...,  2.5860,  0.3472,  0.0949],\n",
      "         [ 0.2384, -0.3760,  0.5534,  ...,  1.3890,  0.3706, -0.3558],\n",
      "         ...,\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033],\n",
      "         [-0.0114,  0.0541, -0.0215,  ..., -0.0810, -0.0312, -0.1033]]],\n",
      "       device='cuda:0')\n",
      "start_logits:  tensor([ 2.3047e-01,  4.4873e-01, -2.6636e-01,  5.3516e-01, -4.5264e-01,\n",
      "         7.1838e-02, -5.4639e-01,  1.9592e-01, -4.5801e-01,  3.4219e+00,\n",
      "         3.4375e-01, -1.7859e-01,  2.5508e+00,  4.2041e-01,  1.6736e-01,\n",
      "        -4.4873e-01,  3.6074e+00,  1.0176e+00,  1.2012e-01,  1.5945e-02,\n",
      "        -1.9556e-01,  3.2363e+00,  3.7842e-01,  4.7227e+00,  8.8379e-01,\n",
      "         1.3989e-01, -1.9556e-01,  1.3318e-01,  4.5312e+00,  9.4678e-01,\n",
      "         2.3096e-01, -9.3689e-02,  2.3120e-01,  5.8691e-01,  3.9805e+00,\n",
      "         6.7773e-01,  8.4106e-02, -3.7817e-01,  2.4512e+00,  6.8945e-01,\n",
      "        -1.2695e+00,  4.7485e-01, -9.7559e-01, -3.4058e-01,  1.6660e+00,\n",
      "         8.5303e-01,  4.8706e-01, -5.8929e-02, -3.1982e-01, -3.6774e-02,\n",
      "        -1.3538e-01,  4.2305e+00,  6.5332e-01,  4.6167e-01,  4.7974e-01,\n",
      "        -7.4609e-01, -4.6411e-01, -7.4268e-01,  2.6660e+00,  8.4229e-01,\n",
      "        -1.0967e+00, -5.2148e-01,  1.5684e+00, -7.3193e-01,  2.1133e+00,\n",
      "         6.6992e-01,  1.2207e+00,  3.9414e+00,  1.2119e+00,  4.7227e+00,\n",
      "         1.1211e+00,  3.1226e-01,  6.7200e-02,  3.9795e-01,  5.6934e-01,\n",
      "         5.0098e-01, -1.1543e+00, -3.4521e-01,  3.7817e-01,  3.4326e-01,\n",
      "        -9.4360e-02,  2.4531e+00,  1.0557e+00, -1.0508e+00, -5.7959e-01,\n",
      "         2.9614e-01, -1.2646e-01, -6.1914e-01,  4.2539e+00,  1.0312e+00,\n",
      "         1.1592e+00, -1.9897e-01,  3.1543e+00,  3.8647e-01,  7.5830e-01,\n",
      "        -2.7490e-01, -2.4292e-01, -7.8076e-01,  3.5820e+00,  1.3840e-02,\n",
      "        -4.6362e-01,  3.1665e-01, -3.3887e-01, -9.4580e-01, -3.3447e-01,\n",
      "        -1.3660e-01, -7.2461e-01, -4.3481e-01, -6.3037e-01,  5.9521e-01,\n",
      "        -5.7910e-01,  2.8809e+00,  5.2881e-01, -1.0364e-01, -5.2002e-01,\n",
      "        -1.6309e-01,  2.8027e+00,  4.4336e-01, -3.0859e-01, -4.1870e-01,\n",
      "        -1.9495e-01,  4.3828e+00,  9.6094e-01,  3.7061e-01,  2.0422e-01,\n",
      "        -7.5134e-02, -7.5293e-01,  1.5166e+00,  3.3130e-01, -3.0054e-01,\n",
      "         2.2095e-01, -2.3535e-01,  4.3555e+00,  5.8008e-01,  2.2974e-01,\n",
      "         4.9390e-01, -2.1729e-01,  1.1582e+00,  2.7515e-01,  3.9429e-01,\n",
      "         2.3008e+00,  1.2373e+00,  4.8169e-01,  1.1602e+00,  1.2373e+00,\n",
      "         5.1611e-01,  3.1555e-02,  8.7744e-01, -1.4561e+00, -7.1240e-01,\n",
      "        -4.6411e-01,  4.4922e+00,  8.5449e-01,  2.7368e-01,  8.8086e-01,\n",
      "         1.8237e-01,  1.1836e+00, -4.2090e-01,  7.6074e-01, -9.2236e-01,\n",
      "         4.2617e+00,  4.5215e-01,  3.9990e-01,  3.4863e-01,  4.0186e-01,\n",
      "         4.3711e+00,  1.1299e+00,  4.9634e-01,  8.4863e-01,  1.3105e+00,\n",
      "         1.2451e-01,  5.7275e-01,  4.2773e+00,  1.0020e+00,  3.0225e-01,\n",
      "         6.1816e-01,  1.0938e+00,  5.4016e-02,  2.6587e-01, -5.6592e-01,\n",
      "         3.7383e+00,  5.9570e-01,  2.1045e-01,  1.9397e-01,  1.8127e-01,\n",
      "        -1.1992e+00,  7.5684e-01,  1.0300e-02, -3.2031e-01,  1.2314e-02,\n",
      "        -3.8330e-01,  4.0273e+00,  8.9990e-01,  8.1641e-01,  1.6934e+00,\n",
      "         6.4648e-01, -9.7168e-02,  4.1562e+00,  9.1602e-01,  1.4844e-01,\n",
      "        -8.7842e-01,  3.1484e+00,  1.7578e-01, -2.1448e-01, -2.2119e-01,\n",
      "        -2.3499e-01, -6.4746e-01,  3.0000e+00,  2.0312e+00,  3.7329e-01,\n",
      "         1.6678e-02,  4.0588e-02, -5.5029e-01,  3.6270e+00,  7.5391e-01,\n",
      "         3.6133e-02, -1.1279e+00, -3.8965e-01,  1.1414e-01,  5.0842e-02,\n",
      "         3.9805e+00,  3.5461e-02, -6.5771e-01,  1.2964e-01, -4.9365e-01,\n",
      "         3.1348e+00,  1.7419e-01, -1.2634e-01,  1.6138e-01,  7.3633e-01,\n",
      "        -3.8867e-01, -2.1057e-03, -1.8945e-01,  3.2988e+00,  3.8721e-01,\n",
      "        -1.1530e-01,  7.6465e-01, -2.7344e-01, -6.4941e-01,  4.1289e+00,\n",
      "         5.1123e-01, -2.5293e-01,  1.1484e+00, -8.6975e-04,  1.9092e-01,\n",
      "         4.5825e-01, -5.2148e-01,  2.1899e-01,  3.9307e-01,  1.7969e+00,\n",
      "        -2.3193e-03,  7.9932e-01, -6.7261e-02, -3.0640e-01,  3.4106e-01,\n",
      "        -3.9502e-01,  2.0625e+00, -6.2622e-02,  6.0303e-01, -1.8079e-01,\n",
      "        -3.5889e-01,  3.5889e-01, -7.7344e-01, -1.2119e+00,  3.0234e+00,\n",
      "        -6.5820e-01, -4.8120e-01, -8.0750e-02,  4.2505e-01,  3.3691e-01,\n",
      "        -3.5840e-01, -2.5879e-02, -1.4185e-01, -5.5615e-01, -9.7607e-01,\n",
      "        -6.2598e-01,  1.6465e+00,  1.0498e-01, -2.5366e-01,  4.7388e-01,\n",
      "        -4.4263e-01,  2.0972e-01, -3.4790e-01,  1.7712e-01,  6.7090e-01,\n",
      "        -5.7434e-02,  4.0210e-01,  9.2822e-01,  3.6774e-02,  4.4287e-01,\n",
      "         9.7607e-01, -1.6174e-01, -1.3782e-01,  7.9297e-01, -1.4992e-02,\n",
      "         1.2334e+00, -1.6602e-01,  1.2134e-01,  2.8857e-01,  4.1089e-01,\n",
      "         3.3320e+00,  2.4043e+00,  9.6191e-01,  3.6865e-01,  4.9780e-01,\n",
      "         2.3657e-01,  7.3145e-01,  9.0186e-01,  1.6602e+00,  5.9375e-01,\n",
      "         6.2549e-01,  3.2480e+00,  2.1758e+00,  8.1787e-01,  1.9861e-01,\n",
      "         2.8101e-01,  1.2280e-01,  4.2236e-01,  7.1436e-01,  1.3379e+00,\n",
      "         3.4253e-01,  3.5815e-01, -5.3809e-01,  3.9258e+00,  6.1719e-01,\n",
      "        -5.4779e-02,  2.0874e-01,  9.3799e-01,  2.3096e-01,  1.1406e+00,\n",
      "         1.4404e+00,  1.0205e-01,  3.9868e-01,  6.4392e-02,  1.9849e-01,\n",
      "         7.9102e-02,  1.7053e-01,  1.9080e-01, -2.8101e-01, -1.3076e+00,\n",
      "         4.3604e-01, -5.7520e-01,  7.3181e-02, -4.9609e-01,  4.1016e+00,\n",
      "         7.2607e-01,  5.7471e-01,  1.4668e+00,  5.7080e-01,  2.4002e-02,\n",
      "         3.8711e+00,  1.2734e+00,  3.7598e-01,  3.0420e-01,  1.3123e-01,\n",
      "         3.8843e-01, -3.9764e-02,  5.9717e-01, -2.0117e-01,  3.8652e+00,\n",
      "         4.1431e-01,  4.7266e-01, -7.6416e-01,  3.6621e+00,  2.5024e-01,\n",
      "         2.3364e-01,  2.7832e-01, -5.4883e-01,  2.6543e+00,  1.1670e-01,\n",
      "        -5.1025e-01, -4.4995e-01,  2.7124e-01, -1.2158e+00, -7.7100e-01,\n",
      "        -4.3652e-01, -8.0225e-01,  3.0273e+00,  2.2891e+00,  3.1030e-01,\n",
      "         1.7517e-01,  4.7534e-01, -1.7786e-01,  5.7910e-01, -9.9756e-01,\n",
      "        -8.3130e-02, -1.5161e-01, -5.3760e-01,  4.1443e-02, -5.4199e-01,\n",
      "         3.1582e+00,  2.4902e+00, -6.0669e-02, -6.4648e-01,  3.7891e-01,\n",
      "        -3.5400e-01,  3.4004e+00,  8.0762e-01,  1.5830e+00,  3.6591e-02,\n",
      "         1.2578e+00,  9.4238e-02,  1.5625e+00,  5.4199e-02,  3.1079e-01,\n",
      "         3.9978e-02,  2.6797e+00,  1.0088e+00,  9.0576e-02,  1.3281e+00,\n",
      "        -2.5406e-02, -3.0103e-01, -8.4326e-01,  3.2832e+00,  2.6172e-01,\n",
      "         2.4512e-01, -2.0654e-01, -4.8071e-01, -3.8306e-01,  1.1035e+00,\n",
      "        -3.5498e-01,  5.1514e-02, -2.8107e-02,  2.7878e-02, -2.8418e-01,\n",
      "         3.3496e+00,  2.5371e+00,  3.7988e-01,  2.5513e-01,  6.2354e-01,\n",
      "        -5.6592e-01, -8.5986e-01, -7.5439e-01, -6.3086e-01,  3.6660e+00,\n",
      "         1.0439e+00,  7.2900e-01, -1.0919e-01, -1.5137e-01,  2.5513e-01,\n",
      "        -3.6255e-01,  9.0039e-01, -8.7842e-01,  3.0078e+00,  5.5078e-01,\n",
      "         3.2349e-01,  3.1982e-01, -7.7972e-03, -2.0300e-01,  8.6670e-02,\n",
      "        -4.4971e-01,  1.5820e+00,  9.1650e-01,  2.9443e-01, -5.9033e-01,\n",
      "         2.7100e-01, -7.0117e-01,  3.7817e-01, -5.0244e-01,  3.6445e+00,\n",
      "         9.1895e-01,  4.4092e-01,  7.8613e-01, -1.4929e-01, -8.0029e-01,\n",
      "         3.4961e+00,  6.0498e-01,  6.0742e-01,  2.3389e-01,  5.3467e-01,\n",
      "        -2.5342e-01, -4.9170e-01, -8.7451e-01,  1.9543e-01, -6.6553e-01,\n",
      "        -4.6851e-01,  7.1338e-01, -6.5039e-01,  4.6436e-01,  2.9099e-02,\n",
      "         4.5093e-01,  4.4531e+00,  5.6641e-01,  8.8916e-01,  1.9209e+00,\n",
      "        -8.3643e-01,  8.9844e-01,  1.0566e+00,  2.2480e+00,  3.6743e-01,\n",
      "        -1.4229e+00, -2.9150e-01, -2.0056e-01, -4.5459e-01, -3.8770e-01,\n",
      "         7.6514e-01, -2.3462e-01, -2.4084e-01, -7.7441e-01, -5.1025e-01,\n",
      "        -1.2539e+00,  2.1326e-01, -9.5312e-01,  2.0581e-01, -5.0684e-01,\n",
      "         3.9590e+00,  5.2393e-01,  7.6367e-01,  1.2170e-01,  4.8511e-01,\n",
      "         4.2578e+00,  2.7773e+00,  2.6562e-01,  7.9102e-01, -3.9819e-01,\n",
      "        -3.6670e-01, -7.3828e-01,  2.0195e+00,  4.3628e-01,  4.0625e-01,\n",
      "         3.0195e+00,  1.2598e+00,  3.0200e-01, -1.8945e-01, -9.4482e-02,\n",
      "         2.3398e+00,  1.0315e-01,  5.1318e-01,  1.2520e+00,  2.8125e+00,\n",
      "         4.2773e-01,  1.0028e-01,  1.1201e+00,  3.8711e+00,  5.9180e-01,\n",
      "         2.4609e-01,  4.5703e-01,  1.8105e+00,  2.6328e+00,  6.8018e-01,\n",
      "         2.8784e-01,  1.9202e-01,  7.4707e-02,  2.4707e+00,  6.8115e-01,\n",
      "         2.2018e-02,  2.2969e+00,  3.1714e-01, -3.1738e-01,  1.0898e+00,\n",
      "         5.2795e-02, -5.6104e-01,  2.4629e+00,  1.2002e+00,  5.6592e-01,\n",
      "         8.9478e-02,  4.2651e-01, -1.6907e-02, -8.7952e-02,  3.0723e+00,\n",
      "         1.4612e-01,  1.6321e-01, -1.0406e-01,  3.0625e+00,  8.6279e-01,\n",
      "         4.9146e-01, -1.9092e-01,  2.4146e-01, -6.4893e-01, -4.9243e-01,\n",
      "         4.2656e+00,  2.1934e+00,  5.9766e-01,  9.4189e-01, -1.4053e+00,\n",
      "        -4.2139e-01, -5.7471e-01,  1.0278e-01,  6.8604e-01,  3.8438e+00,\n",
      "         6.9727e-01,  9.6436e-01,  2.1399e-01,  4.7974e-01,  4.4883e+00,\n",
      "         5.4004e-01,  7.7783e-01,  4.1797e+00,  3.9404e-01,  8.5107e-01,\n",
      "         4.1250e+00,  3.9209e-01,  5.5029e-01, -1.5859e+00,  1.1797e+00,\n",
      "        -4.0576e-01,  3.2559e+00, -1.0413e-01,  4.2305e+00,  7.3145e-01,\n",
      "         8.0957e-01,  9.0283e-01,  1.1064e+00, -9.3848e-01,  3.2148e+00,\n",
      "         7.9346e-01,  7.1582e-01, -4.9829e-01, -2.4487e-01, -9.7510e-01,\n",
      "        -5.3125e-01, -1.2041e+00,  1.4150e+00,  4.4922e-01,  1.0889e-01,\n",
      "        -4.3115e-01, -1.4954e-01, -1.9971e-01, -8.0566e-01, -1.0273e+00,\n",
      "         4.4995e-01, -3.2861e-01, -8.2275e-02, -3.9978e-03, -1.2080e+00,\n",
      "         1.1243e-01, -6.2744e-01, -6.7822e-01,  3.4531e+00,  4.7729e-01,\n",
      "         6.3818e-01,  5.6519e-02,  2.4536e-01,  4.2822e-01,  4.7695e+00,\n",
      "         7.4512e-01,  1.0850e+00,  4.4727e+00,  6.7383e-01,  1.0059e+00,\n",
      "         3.2031e+00,  9.1553e-01,  7.8320e-01,  5.1172e-01,  1.2725e+00,\n",
      "         1.1072e-01,  4.3921e-01,  1.3623e-01,  4.3750e-01,  3.7549e-01,\n",
      "        -2.6172e-01,  1.1520e-02,  4.9341e-01, -2.7710e-01,  9.9414e-01,\n",
      "         3.4492e+00,  8.6182e-01,  1.2393e+00,  4.5630e-01,  1.6821e-01,\n",
      "         9.0723e-01,  4.6094e+00,  6.8799e-01,  1.0898e+00,  2.6973e+00,\n",
      "         6.9531e-01,  5.7324e-01,  2.4390e-01,  2.5772e-02, -6.3379e-01,\n",
      "        -1.3809e+00,  6.2598e-01,  8.3374e-02, -2.3590e-02, -9.3384e-02,\n",
      "        -3.3838e-01,  3.4316e+00,  8.3691e-01, -5.9082e-01,  2.3262e+00,\n",
      "        -8.9893e-01,  2.2871e+00,  4.6133e+00,  7.4219e-01,  9.4531e-01,\n",
      "        -8.9746e-01,  1.0254e-01,  5.4321e-02, -3.3618e-01,  4.1172e+00,\n",
      "         1.2676e+00,  3.3252e-01,  1.3887e+00, -6.4307e-01, -1.3750e+00,\n",
      "        -8.3008e-01, -6.3428e-01,  3.6406e+00,  9.1260e-01,  4.6133e+00,\n",
      "         7.5195e-01,  7.5439e-01, -6.9385e-01,  4.8462e-01, -3.9795e-01,\n",
      "         4.1328e+00,  8.3984e-01,  7.6123e-01,  1.4648e+00,  6.5674e-01,\n",
      "         2.9707e+00,  7.6660e-01,  5.7666e-01,  6.8945e-01,  2.8262e+00,\n",
      "         1.0273e+00,  6.3916e-01,  1.7273e-01,  3.4937e-01,  2.5977e+00,\n",
      "         5.5615e-01,  3.7451e-01,  3.6694e-01,  3.0645e+00,  1.0283e+00,\n",
      "         3.0060e-03,  2.2305e+00,  6.3086e-01,  3.1128e-01, -2.5049e-01,\n",
      "         2.0312e+00,  4.7778e-01,  1.6416e+00,  2.9688e+00,  9.5166e-01,\n",
      "         4.0222e-02,  1.3354e-01,  3.1348e+00,  1.0439e+00,  3.2690e-01,\n",
      "         2.9810e-01, -1.6602e-01,  2.4531e+00,  7.1875e-01,  8.8135e-01,\n",
      "         2.4979e-02,  1.0537e+00, -3.3716e-01,  2.6343e-01,  4.3433e-01,\n",
      "        -1.1328e+00,  4.4219e+00,  1.2559e+00,  7.4707e-01,  2.4243e-01,\n",
      "        -5.9766e-01, -5.8105e-01, -4.3530e-01, -5.8691e-01, -9.7412e-02,\n",
      "         9.2188e-01, -6.7773e-01,  1.2305e-01, -1.0205e+00,  4.8047e+00,\n",
      "         7.4707e-01,  8.7695e-01, -6.4160e-01, -2.8052e-01,  4.2266e+00,\n",
      "         8.8623e-01,  5.2344e-01,  2.3730e-01, -2.6611e-01, -5.1611e-01,\n",
      "         1.1113e+00, -6.7627e-02,  3.9453e+00,  8.3691e-01,  5.7617e-01,\n",
      "        -1.0056e-02,  3.8340e+00,  5.9912e-01,  5.8398e-01, -4.8340e-02,\n",
      "        -2.8589e-01, -3.3960e-01,  5.4443e-01,  4.7876e-01, -6.3867e-01,\n",
      "         3.2446e-01, -4.5264e-01, -2.5635e-01, -1.7773e-01,  1.6650e-01,\n",
      "        -2.3547e-01,  4.4531e+00,  8.2959e-01,  9.1113e-01, -5.5078e-01,\n",
      "         2.0723e+00, -6.6309e-01,  6.5918e-02,  3.2793e+00,  9.6252e-02,\n",
      "        -3.3740e-01,  1.5588e-01,  3.6401e-01, -7.6721e-02, -1.4160e-01,\n",
      "         2.2974e-01,  4.3164e-01,  9.9609e-01, -1.0117e+00,  4.0938e+00,\n",
      "         5.6006e-01,  9.7656e-01,  5.7715e-01, -8.5938e-01, -2.1924e-01,\n",
      "        -3.7500e-01, -7.2412e-01,  2.0483e-01, -1.5244e+00, -9.1675e-02,\n",
      "         3.7402e-01, -9.7656e-01, -1.3074e-01, -6.0156e-01,  3.8848e+00,\n",
      "         1.1416e+00,  7.3633e-01,  1.7314e+00, -5.8887e-01,  3.3125e+00,\n",
      "         2.2754e-01,  6.5723e-01,  6.6943e-01, -5.1611e-01,  4.2847e-01,\n",
      "        -4.4141e-01, -1.6589e-01, -5.4102e-01, -1.4648e-01, -4.5190e-01,\n",
      "        -9.9426e-02, -1.0195e+00,  2.5928e-01, -8.5693e-01,  8.2825e-02,\n",
      "         2.1326e-01, -3.1104e-01,  8.2764e-01, -1.0908e+00, -3.7476e-01,\n",
      "        -3.2861e-01,  3.2959e-01, -7.1564e-03,  2.4268e-01,  1.6479e-01,\n",
      "         7.5977e-01,  3.0605e+00,  1.2090e+00,  9.5996e-01,  4.2407e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "end_logits:  tensor([ 1.1797e+00,  8.9062e-01,  4.6362e-01, -1.0723e+00, -1.0771e+00,\n",
      "        -7.6953e-01, -1.7197e+00, -1.0083e-01, -6.4209e-01, -5.0928e-01,\n",
      "        -4.1870e-02,  2.3086e+00,  2.4268e-01, -2.0532e-01,  2.2988e+00,\n",
      "        -1.1191e+00,  4.0039e-02,  1.2708e-01,  7.7820e-02,  6.1816e-01,\n",
      "         2.7344e+00, -8.2520e-01,  4.4775e-01, -3.6011e-02,  5.5504e-03,\n",
      "         2.0654e-01,  6.5576e-01,  4.1211e+00,  1.4685e-01,  6.3599e-02,\n",
      "         2.3010e-01,  6.9189e-01,  3.9219e+00,  2.6993e-02, -2.0227e-01,\n",
      "         6.1607e-03,  3.8164e+00, -8.0908e-01,  7.0996e-01,  2.4805e+00,\n",
      "        -2.3450e-01, -5.7129e-01, -9.9658e-01, -9.0283e-01, -1.7395e-01,\n",
      "        -3.3374e-01, -4.1089e-01,  6.7383e-01,  1.5752e+00, -7.3047e-01,\n",
      "        -9.2920e-01,  5.3986e-02, -5.2490e-02,  2.2937e-01,  3.2148e+00,\n",
      "        -6.7920e-01, -2.9810e-01, -3.4619e-01,  4.1406e-01,  2.0820e+00,\n",
      "        -1.1719e+00, -1.0322e+00, -4.9243e-01, -6.4893e-01,  3.6133e-01,\n",
      "         1.2178e+00,  1.5898e+00,  2.8979e-01,  2.4727e+00,  7.3303e-02,\n",
      "         6.0028e-02,  1.6736e-01,  7.7148e-01,  4.0820e+00,  1.3916e-01,\n",
      "         2.3594e+00, -9.2822e-01, -6.1230e-01,  1.0327e-01,  4.5483e-01,\n",
      "        -5.6982e-01, -4.6997e-01,  2.3633e+00, -1.2236e+00, -1.4199e+00,\n",
      "        -1.3115e+00, -8.5791e-01, -7.7100e-01,  5.3253e-02,  2.0068e-01,\n",
      "         3.5059e+00, -8.1836e-01,  4.9756e-01,  4.6558e-01,  1.6250e+00,\n",
      "        -9.7119e-01, -1.4434e+00, -2.0098e+00, -3.7329e-01,  1.8835e-01,\n",
      "         1.4478e-01, -1.8408e-01, -1.2097e-01, -1.4199e+00, -7.4561e-01,\n",
      "        -1.2891e-01, -3.6255e-01, -1.4258e+00, -1.1328e+00,  1.0098e+00,\n",
      "        -1.8818e+00, -7.2510e-01, -2.5635e-01,  1.1045e+00, -8.0518e-01,\n",
      "        -1.3828e+00, -7.8809e-01, -3.2739e-01,  9.1455e-01, -5.4297e-01,\n",
      "        -9.9854e-01, -1.3208e-01, -3.8208e-01,  2.9980e+00,  4.6387e-01,\n",
      "        -1.1755e-01, -2.2278e-01, -1.0527e+00, -1.5479e-01, -3.2227e-01,\n",
      "         4.8560e-01, -3.0225e-01,  3.9917e-01, -2.7271e-01,  2.0105e-01,\n",
      "         3.5781e+00, -7.2998e-01,  1.2148e+00,  2.8003e-01,  4.7803e-01,\n",
      "         6.5576e-01,  3.3496e-01,  2.2559e+00,  2.4004e+00, -3.3398e-01,\n",
      "        -3.2373e-01, -2.8442e-01,  1.0781e+00, -6.0010e-01, -1.1992e+00,\n",
      "        -1.1279e+00,  3.0078e-01, -1.9257e-02,  2.7070e+00,  2.7637e+00,\n",
      "        -1.3223e+00, -8.3838e-01,  2.4438e-01, -9.7803e-01, -3.4912e-01,\n",
      "        -2.4011e-01, -2.6367e-01,  4.1562e+00,  1.8555e-01,  4.8438e-01,\n",
      "        -8.8770e-01, -4.3188e-01,  2.7422e+00, -1.3154e+00, -2.2742e-01,\n",
      "         1.9629e+00,  2.7070e+00, -8.3252e-01, -6.7480e-01,  1.2363e+00,\n",
      "        -1.4492e+00, -2.4768e-01,  1.8955e+00,  1.3564e+00, -1.3496e+00,\n",
      "        -8.2910e-01, -5.8643e-01,  2.1204e-01,  1.1582e+00,  1.9785e+00,\n",
      "        -1.3828e+00, -7.0508e-01, -2.8394e-01, -6.0303e-01,  5.9766e-01,\n",
      "        -6.7285e-01,  7.8564e-01,  3.7427e-01,  3.2949e+00,  1.1914e+00,\n",
      "         3.8496e+00, -1.7666e+00, -6.2549e-01, -4.9976e-01,  3.2012e+00,\n",
      "        -1.3311e+00, -8.1543e-01, -4.6167e-01,  2.4004e+00,  4.2529e-01,\n",
      "        -1.2129e+00, -1.1338e+00, -2.3206e-01, -2.6123e-01, -1.3159e-01,\n",
      "         2.2129e+00,  7.8125e-01, -1.4355e+00, -1.0449e+00, -4.7241e-01,\n",
      "         2.7168e+00, -1.6992e+00, -4.9536e-01, -9.4971e-01, -1.1611e+00,\n",
      "        -2.9468e-01,  9.6985e-02,  1.2246e+00,  2.1582e+00, -1.5029e+00,\n",
      "        -1.0781e+00, -5.3125e-01,  2.4941e+00, -6.3232e-01, -4.8560e-01,\n",
      "         1.8857e+00, -8.8525e-01, -1.2275e+00, -1.0293e+00, -5.5762e-01,\n",
      "         7.4365e-01, -5.8301e-01,  1.9502e+00, -1.5117e+00, -1.2146e-01,\n",
      "        -3.2642e-01,  9.7266e-01, -4.0161e-01, -1.0962e-01, -1.6992e-01,\n",
      "         2.5098e-01,  3.2734e+00, -1.5205e-02,  4.8047e-01, -5.0000e-01,\n",
      "        -1.4648e-01, -1.0195e+00, -3.6035e-01, -1.3696e-01, -2.0898e-01,\n",
      "         4.6680e-01, -3.6011e-01, -1.3660e-01, -1.1250e+00, -2.8589e-01,\n",
      "        -1.5710e-01,  4.0924e-02,  1.1992e+00, -1.2021e+00,  1.0918e+00,\n",
      "        -3.2324e-01, -2.2839e-01, -1.1357e+00, -1.3647e-01,  1.1499e-01,\n",
      "        -7.7393e-01, -4.8608e-01, -7.4524e-02, -6.9482e-01, -6.6992e-01,\n",
      "        -1.1934e+00, -1.2041e+00, -2.8027e-01, -1.2939e-01, -2.9297e-01,\n",
      "         1.7358e-01, -1.8188e-01, -9.4055e-02, -6.4648e-01, -1.4209e-01,\n",
      "         5.3497e-02, -1.7932e-01, -5.3662e-01,  2.5467e-02,  4.9683e-02,\n",
      "        -5.3125e-01,  9.3506e-02, -9.2676e-01,  2.8784e-01,  1.0754e-01,\n",
      "        -1.6833e-01,  3.0853e-02,  1.8281e+00,  1.8143e-02,  5.1074e-01,\n",
      "         4.8267e-01, -6.0840e-01, -3.2837e-01, -3.5645e-01, -1.9202e-01,\n",
      "        -1.5942e-01,  1.3652e+00, -1.2607e+00, -3.2031e-01,  1.7998e+00,\n",
      "         3.2539e+00,  1.9360e-01, -5.9229e-01, -3.7671e-01, -3.3203e-01,\n",
      "        -2.3547e-01, -2.7515e-01,  7.5293e-01, -1.3584e+00, -4.8120e-01,\n",
      "         1.2988e+00,  2.6777e+00, -1.5908e+00, -9.8145e-02, -2.2180e-01,\n",
      "         1.1289e+00,  1.2773e+00, -8.0338e-03,  3.8550e-01, -1.4082e+00,\n",
      "        -1.6675e-01, -3.3875e-02, -6.8408e-01,  2.7588e-01, -7.2510e-01,\n",
      "         7.2632e-02, -5.1758e-01, -4.3640e-02,  1.4863e+00, -1.3779e+00,\n",
      "        -5.7568e-01, -4.5850e-01,  9.8047e-01, -7.8320e-01,  6.4990e-01,\n",
      "        -4.2969e-02,  3.3398e+00,  1.7246e+00,  3.7676e+00, -1.4795e+00,\n",
      "        -2.5073e-01, -2.4231e-01, -2.9956e-01, -9.1553e-02, -5.6763e-02,\n",
      "         2.4844e+00, -3.3740e-01,  8.7952e-02, -2.4280e-01,  5.7959e-01,\n",
      "        -6.5857e-02,  3.8477e+00, -1.2939e+00,  3.3173e-02, -3.8477e-01,\n",
      "         1.9512e+00,  3.3008e+00, -1.9434e+00,  9.3811e-02,  2.5049e-01,\n",
      "         9.4531e-01, -1.1875e+00, -7.2217e-01, -1.4229e+00, -8.3643e-01,\n",
      "        -8.7305e-01, -1.3262e+00,  3.7720e-01,  2.8564e-01,  1.3135e-01,\n",
      "         3.3730e+00,  2.9180e+00, -6.9238e-01, -1.1045e+00, -7.9346e-01,\n",
      "        -4.4946e-01,  4.9487e-01, -1.0312e+00,  3.1982e-01, -7.9004e-01,\n",
      "         5.5029e-01, -1.9360e-01,  4.4580e-01,  2.1426e+00,  3.5469e+00,\n",
      "        -1.4268e+00, -8.8806e-02,  2.0215e-01,  1.2611e-02,  1.5078e+00,\n",
      "        -5.5237e-02,  8.1201e-01,  7.1777e-02, -2.6709e-01, -2.8015e-02,\n",
      "         1.8926e+00, -8.7891e-01, -2.2302e-01,  2.8086e+00,  4.7089e-02,\n",
      "         6.8604e-02,  2.8574e+00, -8.1689e-01,  5.5908e-02, -3.5229e-01,\n",
      "         1.2881e+00,  2.2949e+00, -9.6533e-01, -1.1641e+00,  2.1934e+00,\n",
      "        -7.8809e-01, -5.3406e-02, -3.4241e-02, -4.7925e-01, -6.6064e-01,\n",
      "         5.3467e-01,  3.4497e-01,  1.5686e-01,  3.4395e+00,  3.0000e+00,\n",
      "        -1.1357e+00, -1.2705e+00, -4.9780e-01, -1.0361e+00, -3.9331e-01,\n",
      "         1.3928e-01,  4.8755e-01,  2.7295e-01,  2.8457e+00,  1.7061e+00,\n",
      "        -6.4648e-01, -3.3008e-01, -1.4912e+00, -9.1248e-02, -2.3645e-01,\n",
      "        -8.0811e-02,  3.2861e-01,  4.1235e-01,  2.3750e+00,  4.1748e-01,\n",
      "        -5.1025e-01, -1.6406e-01, -6.2256e-01,  1.4238e+00, -5.1025e-01,\n",
      "        -3.1860e-02, -1.2510e+00, -8.1006e-01, -7.3193e-01, -7.2900e-01,\n",
      "        -4.7241e-01,  4.7412e-01,  4.1235e-01,  3.2051e+00, -7.4414e-01,\n",
      "         1.7090e-01, -4.2578e-01,  1.9922e-01,  3.3911e-01,  4.3970e-01,\n",
      "         5.5908e-01,  1.9902e+00,  3.3081e-01, -5.7275e-01, -2.8882e-01,\n",
      "        -6.7871e-01, -1.8372e-01, -9.0527e-01,  2.8735e-01, -3.4570e-01,\n",
      "         5.9033e-01,  1.9299e-01, -1.4368e-01,  3.2031e+00,  1.3594e+00,\n",
      "        -3.5181e-01,  3.2690e-01,  2.3809e+00,  2.2913e-01,  3.4727e+00,\n",
      "         3.1763e-01, -6.6211e-01, -3.4106e-01, -7.4316e-01, -9.3506e-01,\n",
      "        -3.6328e-01, -1.4478e-01,  1.1963e+00, -1.0469e+00, -5.8887e-01,\n",
      "        -7.2412e-01, -5.5908e-02, -1.0049e+00,  8.5596e-01, -3.6914e-01,\n",
      "         6.4795e-01,  1.2199e-02,  4.4141e+00,  6.2523e-03,  6.3525e-01,\n",
      "         6.8457e-01,  1.6284e-01,  1.5977e+00,  3.2363e+00,  2.6184e-02,\n",
      "         2.9648e+00, -1.6016e+00, -1.1816e+00, -6.3416e-02,  3.3535e+00,\n",
      "        -2.5757e-01, -1.5930e-02,  1.1396e+00,  2.9980e+00,  1.6396e+00,\n",
      "         8.1006e-01,  9.0283e-01,  3.8848e+00,  7.3730e-01,  1.6125e-01,\n",
      "         1.5747e-01,  8.9990e-01,  3.0176e-01, -1.1047e-01, -1.9995e-01,\n",
      "         7.2754e-01,  4.1602e+00, -7.6367e-01,  5.3857e-01,  1.2311e-01,\n",
      "         1.3574e-01,  8.7158e-01,  2.2793e+00, -1.0547e+00, -2.7393e-01,\n",
      "         2.8496e+00, -8.4082e-01,  3.5571e-01,  2.0020e+00, -6.8994e-01,\n",
      "         1.0468e-01,  9.5557e-01, -8.1934e-01, -2.3132e-01, -6.4880e-02,\n",
      "         1.4824e+00,  2.6550e-02,  4.6509e-01,  2.0176e+00,  4.0092e-03,\n",
      "         3.5254e-01,  4.0454e-01,  2.0508e+00, -7.9688e-01,  1.3293e-01,\n",
      "         2.3535e-01,  1.0723e+00,  3.7422e+00, -4.5410e-01,  5.4047e-02,\n",
      "         5.0439e-01, -4.6753e-01,  5.8556e-03,  3.3047e+00, -2.9321e-01,\n",
      "        -1.1924e+00, -8.8672e-01, -2.4036e-01, -5.3857e-01, -3.2520e-01,\n",
      "         1.8417e-02,  3.4082e+00,  1.9275e-01,  6.3623e-01, -2.6660e-01,\n",
      "         1.2445e-01,  4.8164e+00,  8.5938e-01,  6.5527e-01,  4.3047e+00,\n",
      "        -4.6118e-01,  5.4413e-02,  4.6367e+00, -8.7354e-01,  3.0713e-01,\n",
      "        -1.0273e+00,  8.3350e-01,  3.2446e-01,  9.0381e-01,  2.7563e-01,\n",
      "         4.4531e+00,  2.5703e+00,  1.7676e+00,  7.5879e-01,  9.5801e-01,\n",
      "         4.3125e+00,  5.1172e-01,  1.5137e+00, -1.6094e+00, -5.4639e-01,\n",
      "        -1.2754e+00, -1.7002e+00, -3.7573e-01, -4.2090e-01,  8.1885e-01,\n",
      "         8.4619e-01, -7.5000e-01, -3.4229e-01, -5.4883e-01, -7.8467e-01,\n",
      "        -3.1567e-01,  4.2456e-01, -3.4668e-01, -1.7676e-01, -1.6152e+00,\n",
      "        -1.1836e+00, -8.7549e-01, -1.1875e+00, -1.5419e-02,  3.1914e+00,\n",
      "         6.4453e-01,  3.4082e+00,  1.0938e-01,  5.7080e-01,  3.4448e-01,\n",
      "         1.9873e-01,  4.2656e+00,  6.9629e-01,  2.6880e-01,  4.2266e+00,\n",
      "         1.1346e-01,  8.3801e-02,  1.6807e+00,  4.6055e+00,  4.6753e-02,\n",
      "        -2.1619e-01,  1.1212e-01, -3.7598e-02, -3.8599e-01, -5.6934e-01,\n",
      "         1.2732e-01,  5.9418e-02,  6.7480e-01,  8.6621e-01,  7.8613e-01,\n",
      "        -6.4893e-01,  8.9697e-01,  7.6758e-01,  9.7559e-01,  4.2031e+00,\n",
      "         7.9980e-01,  2.8613e-01, -5.2856e-02,  3.3750e+00, -3.0298e-01,\n",
      "        -1.0262e-02,  1.3350e+00,  4.2070e+00,  1.6931e-01,  1.8145e+00,\n",
      "        -1.1699e+00, -2.8906e-01, -2.8589e-01, -8.6230e-01, -3.1006e-01,\n",
      "        -8.6670e-01,  5.7764e-01,  4.3047e+00, -1.1543e+00,  8.4570e-01,\n",
      "        -1.8384e-01,  1.4180e+00,  9.2712e-02, -7.9956e-02,  4.4141e+00,\n",
      "        -1.1416e+00, -7.3975e-01, -5.9570e-01, -1.0898e+00, -4.6558e-01,\n",
      "         3.5571e-01,  2.6133e+00,  4.0234e+00, -1.3975e+00, -1.7070e+00,\n",
      "        -6.9434e-01, -1.0117e+00,  3.7695e-01,  3.8926e+00,  2.2351e-01,\n",
      "        -9.0637e-02,  3.9512e+00, -7.4756e-01, -8.0957e-01, -9.0186e-01,\n",
      "        -1.7261e-01,  3.8354e-01,  1.4268e+00, -1.2093e-02,  2.3438e+00,\n",
      "        -1.0146e+00, -3.3057e-01,  3.1885e-01,  4.1055e+00, -1.0576e+00,\n",
      "        -3.9429e-01, -9.6817e-03,  1.0586e+00,  2.9395e+00, -8.4180e-01,\n",
      "         1.6279e-03,  3.4424e-01,  3.9902e+00, -5.3320e-01,  5.0977e-01,\n",
      "         3.3711e+00, -8.4033e-01, -1.7181e-02,  3.9414e+00, -1.2705e+00,\n",
      "        -2.2375e-01,  1.3564e+00,  9.3311e-01, -1.1729e+00, -2.2400e-01,\n",
      "        -1.9348e-01,  2.6250e+00, -8.9258e-01, -1.1621e-01,  5.2930e-01,\n",
      "         4.2539e+00, -1.1904e+00, -1.6248e-01,  1.1884e-01,  8.7305e-01,\n",
      "         2.9336e+00, -5.3516e-01,  3.0225e-01, -1.4668e+00, -9.9365e-01,\n",
      "        -1.2227e+00,  4.4263e-01, -5.2930e-01,  3.9746e-01,  2.6035e+00,\n",
      "        -9.8648e-03, -1.1875e+00, -8.7598e-01, -1.0869e+00, -1.0381e+00,\n",
      "        -5.5420e-01, -1.3340e+00, -3.3765e-01, -1.3953e-01,  2.4243e-01,\n",
      "         2.1130e-01,  4.3398e+00, -1.5371e+00, -9.9658e-01, -2.9663e-02,\n",
      "         4.6802e-01,  3.0215e+00, -3.9648e-01,  1.9299e-01, -2.0137e+00,\n",
      "        -4.2212e-01, -6.9141e-01, -1.8420e-01, -5.8838e-02,  3.4082e+00,\n",
      "        -1.1318e+00,  2.8809e-01, -1.6870e-01,  3.2168e+00, -1.6885e+00,\n",
      "        -5.6982e-01,  4.0552e-01, -9.0039e-01, -5.8643e-01, -1.1230e+00,\n",
      "        -6.2305e-01,  1.8091e-01, -1.0225e+00, -9.3213e-01, -6.5918e-01,\n",
      "        -6.2256e-01,  5.5664e-01,  2.7026e-01,  4.2656e+00, -1.2920e+00,\n",
      "         1.0742e+00, -1.3379e+00, -1.0693e+00,  4.7925e-01,  7.7197e-01,\n",
      "        -6.3330e-01, -1.1680e+00, -3.0518e-01, -1.3513e-01, -1.3965e-01,\n",
      "        -1.0474e-01,  5.9082e-01, -1.2109e+00, -5.1416e-01, -1.9324e-01,\n",
      "         3.2715e-01,  4.1875e+00, -2.8027e-01, -1.2646e+00, -1.3574e-01,\n",
      "        -1.0596e+00, -1.3730e+00, -1.0186e+00, -9.7217e-01, -5.8154e-01,\n",
      "        -7.6611e-01, -1.3613e+00, -4.9292e-01, -1.0488e+00,  2.8748e-02,\n",
      "         2.9761e-01,  3.9062e+00,  9.1699e-01,  4.6899e-01, -4.3726e-01,\n",
      "         3.4644e-01,  4.0430e+00, -8.3887e-01, -3.0200e-01, -4.9878e-01,\n",
      "        -1.0107e+00, -1.0332e+00, -9.6533e-01, -2.8296e-01, -4.6851e-01,\n",
      "        -4.7046e-01, -1.2334e+00, -1.1748e+00, -9.6191e-01, -1.0742e+00,\n",
      "        -7.0752e-01, -1.5732e+00, -8.9844e-01, -1.7305e+00, -1.3564e+00,\n",
      "        -1.1807e+00, -4.6558e-01, -9.6729e-01, -2.8271e-01, -8.3301e-01,\n",
      "        -2.5098e-01, -1.0137e+00,  1.5540e-01,  3.8926e+00,  5.7910e-01],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "validation_epoch_end\n",
      "before sync --> sizes:  10, 10, 10\n",
      "after sync --> sizes: 10, 10, 10\n",
      "avg_loss:  tensor(5.3479, device='cuda:0')\tavg_answer_loss:  tensor(4.8015, device='cuda:0')\tavg_type_loss:  tensor(0.1093, device='cuda:0')\tavg_val_f1:  0.13333333432674407\tavg_val_em:  0.1\tavg_val_prec:  0.13333333432674407\tavg_val_recall:  0.13333333432674407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u32/fanluo/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.13333333432674407 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of avg_val_f1 in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Epoch 00005: avg_val_f1 reached 0.13333 (best 0.13333), saving model to /xdisk/msurdeanu/fanluo/hotpotQA/Data/jupyter-hotpotqa/hotpotqa-longformer_jupyter/_ckpt_epoch_5.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     if not args.test: \n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "### To install apex ### \n",
    "#     !git clone https://github.com/NVIDIA/apex\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/apex/\")\n",
    "#     !module load cuda101/neuralnet/7/7.6.4  \n",
    "#     !module load cuda10.1/toolkit/10.1.243 \n",
    "#     !conda install -c conda-forge cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf201901 cudatoolkit-dev --yes\n",
    "#     !conda install -c conda-forge/label/cf202003 cudatoolkit-dev --yes\n",
    "#     !which nvcc\n",
    "#     !python -m pip install -v --no-cache-dir ./\n",
    "#     os.chdir(\"/xdisk/msurdeanu/fanluo/hotpotQA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('save_dir', 'jupyter-hotpotqa')\n",
      "('save_prefix', 'hotpotqa-longformer_jupyter')\n",
      "('train_dataset', 'small.json')\n",
      "('dev_dataset', 'small_dev3.json')\n",
      "('batch_size', 2)\n",
      "('gpus', '0')\n",
      "('warmup', 1000)\n",
      "('lr', 5e-05)\n",
      "('val_every', 1.0)\n",
      "('val_percent_check', 1.0)\n",
      "('num_workers', 4)\n",
      "('seed', 1234)\n",
      "('epochs', 6)\n",
      "('max_seq_len', 4096)\n",
      "('max_doc_len', 4096)\n",
      "('max_num_answers', 64)\n",
      "('max_question_len', 55)\n",
      "('doc_stride', -1)\n",
      "('ignore_seq_with_no_answers', False)\n",
      "('disable_checkpointing', False)\n",
      "('n_best_size', 20)\n",
      "('max_answer_length', 30)\n",
      "('regular_softmax_loss', False)\n",
      "('test', False)\n",
      "('model_path', '/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096')\n",
      "('no_progress_bar', False)\n",
      "('attention_mode', 'sliding_chunks')\n",
      "('fp32', False)\n",
      "('train_percent', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json --dev_dataset small_dev3.json  \\\n",
    "    --gpus 0 --num_workers 4 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer_jupyter  --model_path /xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096'\n",
    "# hotpot_dev_distractor_v1.json\n",
    "# --train_dataset /xdisk/msurdeanu/fanluo/hotpotQA/Data/reduced_questions/hotpot_reduced_context_04-08-2021-01:12:53/hotpot_train_reduced_context_coref_fuzzy.json --dev_dataset   \\ \n",
    "\n",
    "import argparse \n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString)) \n",
    "    for arg in vars(args):\n",
    "        print((arg, getattr(args, arg)))\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "config = LongformerConfig.from_pretrained('/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096') \n",
    "# choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n",
    "# 'n2': for regular n2 attantion\n",
    "# 'tvm': a custom CUDA kernel implementation of our sliding window attention\n",
    "# 'sliding_chunks': a PyTorch implementation of our sliding window attention\n",
    "config.attention_mode = 'sliding_chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Longformer.from_pretrained('/xdisk/msurdeanu/fanluo/hotpotQA/longformer-base-4096', config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n",
    "\n",
    "# TVM code doesn't work on CPU. Uncomment this if `config.attention_mode = 'tvm'`\n",
    "model = model.cuda() \n",
    "input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n",
    "attention_mask[:, [1, 4, 21,]] =  2  # Set global attention based on the task. For example,\n",
    "                                     # classification: the <s> token\n",
    "                                     # QA: question tokens\n",
    "\n",
    "# padding seqlen to the nearest multiple of 512. Needed for the 'sliding_chunks' attention\n",
    "input_ids, attention_mask = pad_to_window_size(\n",
    "        input_ids, attention_mask, config.attention_window[0], tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!nvidia-smi\n",
    "!nvidia-smi -L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "593px",
    "left": "1926px",
    "right": "20px",
    "top": "158px",
    "width": "612px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
