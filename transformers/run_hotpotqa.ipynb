{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert hotpotqa to squard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Longformer: use the following input format with special tokens:  “[CLS] [q] question [/q] [p] sent1,1 [s] sent1,2 [s] ... [p] sent2,1 [s] sent2,2 [s] ...” \n",
    "where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert hotpotqa to squard format modified from  https://github.com/chiayewken/bert-qa/blob/master/run_hotpot.py\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def create_example_dict(context, answers, id, is_impossible, question, is_sup_fact, is_supporting_para):\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"qas\": [                        # each context corresponds to only one qa in hotpotqa\n",
    "            {\n",
    "                \"answers\": answers,\n",
    "                \"id\": id,\n",
    "                \"is_impossible\": is_impossible,\n",
    "                \"question\": question,\n",
    "                \"is_sup_fact\": is_sup_fact,\n",
    "                \"is_supporting_para\": is_supporting_para\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def create_para_dict(example_dicts):\n",
    "    if type(example_dicts) == dict:\n",
    "        example_dicts = [example_dicts]   # each paragraph corresponds to only one [context, qas] in hotpotqa\n",
    "    return {\"paragraphs\": example_dicts}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_hotpot_to_squad_format(json_dict, gold_paras_only=False):\n",
    "    \n",
    "    \"\"\"function to convert hotpotqa to squard format.\n",
    "\n",
    "\n",
    "    Note: A context corresponds to several qas in SQuard. In hotpotqa, one question corresponds to several paragraphs as context. \n",
    "          \"paragraphs\" means different: each paragraph in SQuard contains a context and a list of qas; while 10 paragraphs in hotpotqa concatenated into a context for one question.\n",
    "\n",
    "    Args:\n",
    "        json_dict: The original data load from hotpotqa file.\n",
    "        gold_paras_only: when is true, only use the 2 paragraphs that contain the gold supporting facts; if false, use all the 10 paragraphs\n",
    " \n",
    "\n",
    "    Returns:\n",
    "        new_dict: The converted dict of hotpotqa dataset, use it as a dict would load from SQuAD json file\n",
    "                  usage: input_data = new_dict[\"data\"]   https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_squad.py#L230\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_dict = {\"data\": []} \n",
    "    for example in json_dict: \n",
    "\n",
    "        support_para = set(\n",
    "            para_title for para_title, _ in example[\"supporting_facts\"]\n",
    "        )\n",
    "        sp_set = set(list(map(tuple, example['supporting_facts'])))\n",
    "        \n",
    "        raw_contexts = example[\"context\"]\n",
    "        if gold_paras_only: \n",
    "            raw_contexts = [lst for lst in raw_contexts if lst[0] in support_para]\n",
    "            \n",
    "        contexts = [\" <s>\".join(lst[1]) for lst in raw_contexts]\n",
    "        context = \"<p> \" + \" <p> \".join(contexts)\n",
    "        \n",
    "        is_supporting_para = []  # a boolean list with 10 True/False elements, one for each paragraph\n",
    "        is_sup_fact = []         # a boolean list with True/False elements, one for each context sentence\n",
    "        for para_title, para_lines in raw_contexts:\n",
    "            is_supporting_para.append(para_title in support_para)   \n",
    "            for sent_id, sent in enumerate(para_lines):\n",
    "                is_sup_fact.append( (para_title, sent_id) in sp_set )\n",
    "\n",
    "\n",
    "        answer = example[\"answer\"].strip() \n",
    "        if answer.lower() == 'yes':\n",
    "            answers = [{\"answer_start\": -1, \"answer_end\": -1, \"text\": answer}] \n",
    "        elif answer.lower() == 'no':\n",
    "            answers = [{\"answer_start\": -2, \"answer_end\": -2, \"text\": answer}] \n",
    "        else:\n",
    "            answers = []          # keep all the occurences of answer in the context\n",
    "            for m in re.finditer(re.escape(answer), context):    \n",
    "                answer_start, answer_end = m.span() \n",
    "                answers.append({\"answer_start\": answer_start, \"answer_end\": answer_end, \"text\": answer})\n",
    "             \n",
    "\n",
    "        new_dict[\"data\"].append(\n",
    "            create_para_dict(\n",
    "                create_example_dict(\n",
    "                    context=context,\n",
    "                    answers=answers,\n",
    "                    id = example[\"_id\"],\n",
    "                    is_impossible=(answers == []),\n",
    "                    question=example[\"question\"],\n",
    "                    is_sup_fact = is_sup_fact,\n",
    "                    is_supporting_para = is_supporting_para \n",
    "                )\n",
    "            )\n",
    "        ) \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paragraphs\": [\n",
      "    {\n",
      "      \"context\": \"<p> Radio City is India's first private FM radio station and was started on 3 July 2001. <s> It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). <s> It plays Hindi, English and regional songs. <s> It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. <s> Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. <s> The Radio station currently plays a mix of Hindi and Regional music. <s> Abraham Thomas is the CEO of the company. <p> Football in Albania existed before the Albanian Football Federation (FSHF) was created. <s> This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) . <s> Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946. <s> In 1932, Albania joined FIFA (during the 12\\u201316 June convention ) And in 1954 she was one of the founding members of UEFA. <p> Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California. <s> Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016. <s> Echosmith started first as \\\"Ready Set Go!\\\" <s> until they signed to Warner Bros. <s> Records in May 2012. <s> They are best known for their hit song \\\"Cool Kids\\\", which reached number 13 on the \\\"Billboard\\\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia. <s> The song was Warner Bros. <s> Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold. <s> The band's debut album, \\\"Talking Dreams\\\", was released on October 8, 2013. <p> Women's colleges in the Southern United States refers to undergraduate, bachelor's degree\\u2013granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States. <s> Many started first as girls' seminaries or academies. <s> Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women. <s> Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level. <p> The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum. <p> Arthur's Magazine (1844\\u20131846) was an American literary periodical published in Philadelphia in the 19th century. <s> Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. <s> In May 1846 it was merged into \\\"Godey's Lady's Book\\\". <p> The 2014\\u201315 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship. <s> Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues. <s> Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014. <s> The regular season included just 12 rounds, where all the teams went to the semifinals. <s> In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk. <p> First for Women is a woman's magazine published by Bauer Media Group in the USA. <s> The magazine was started in 1989. <s> It is based in Englewood Cliffs, New Jersey. <s> In 2011 the circulation of the magazine was 1,310,696 copies. <p> The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California. <s> The fire started as two separate fires on November 15, 2008. <s> The \\\"Freeway Fire\\\" started first shortly after 9am with the \\\"Landfill Fire\\\" igniting approximately 2 hours later. <s> These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda. <p> William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. <s> It is most known for their premium jeans. <s> On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line. <s> The label also produces other clothing items such as jackets and tops. <s> The company started first as a denim line, later evolving into a men\\u2019s and women\\u2019s clothing line.\",\n",
      "      \"qas\": [\n",
      "        {\n",
      "          \"answers\": [\n",
      "            {\n",
      "              \"answer_start\": 2969,\n",
      "              \"answer_end\": 2986,\n",
      "              \"text\": \"Arthur's Magazine\"\n",
      "            }\n",
      "          ],\n",
      "          \"id\": \"5a7a06935542990198eaf050\",\n",
      "          \"is_impossible\": false,\n",
      "          \"question\": \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
      "          \"is_sup_fact\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false\n",
      "          ],\n",
      "          \"is_supporting_para\": [\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            true,\n",
      "            false,\n",
      "            false\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# debug: check whether convert_hotpot_to_squad_format() works\n",
    "import os\n",
    "os.chdir('/xdisk/msurdeanu/fanluo/hotpotQA/')\n",
    "#!cat /xdisk/msurdeanu/fanluo/hotpotQA/hotpot_train_v1.1.json | ../jq-linux64 -c '.[0:16]' > small.json\n",
    "\n",
    "import json\n",
    "with open(\"small.json\", \"r\", encoding='utf-8') as f:  \n",
    "    json_dict = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "    print(json.dumps(json_dict[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longfomer's fine-tuning\n",
    "\n",
    "\n",
    "- For answer span extraction we use BERT’s QA model with addition of a question type (yes/no/span) classification head over the first special token ([CLS]).\n",
    "\n",
    "- For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n",
    "\n",
    "- We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section2: This is modified from longfomer's fine-tuning with triviaqa.py from https://github.com/allenai/longformer/blob/master/scripts/triviaqa.py\n",
    "# !conda install transformers --yes\n",
    "# !conda install cudatoolkit=10.0 --yes\n",
    "# !python -m pip install git+https://github.com/allenai/longformer.git\n",
    "# !conda install -c conda-forge regex --force-reinstall --yes\n",
    "# !conda install pytorch-lightning -c conda-forge\n",
    "# !pip install jdc \n",
    "# !pip install test-tube \n",
    "# !conda install ipywidgets --yes\n",
    "# !conda update --force conda --yes  \n",
    "# !jupyter nbextension enable --py widgetsnbextension \n",
    "# !conda install jupyter --yes\n",
    "\n",
    "# need to run this every time start this notebook, to add python3.7/site-packages to sys.pat, in order to import ipywidgets, which is used when RobertaTokenizer.from_pretrained('roberta-base') \n",
    "import sys\n",
    "sys.path.insert(-1, '/xdisk/msurdeanu/fanluo/miniconda3/lib/python3.7/site-packages')\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "from pytorch_lightning.logging import TestTubeLogger    # sometimes pytorch_lightning.loggers works instead\n",
    "\n",
    "\n",
    "from longformer.longformer import Longformer\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "import jdc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_, \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "            print(f'reading file: {self.file_path}')\n",
    "            self.data_json = convert_hotpot_to_squad_format(json.load(f))['data']\n",
    "#             print(self.data_json[0])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "        print(tokenizer.all_special_tokens)\n",
    "        print(tokenizer.all_special_ids)\n",
    "    \n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set \n",
    "            self.val_qid_string_to_int_map =  \\\n",
    "                {\n",
    "                    entry[\"paragraphs\"][0]['qas'][0]['id']: index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one_example_to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:  # example[\"paragraphs\"] only contains one paragraph in hotpotqa\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c) # add a new token\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c  # append the character to the last token\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                \n",
    "                # keep all answers in the document, not just the first matched answer. It also added the list of textual answers to make evaluation easy.\n",
    "                answer_spans = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "                    answer_start = answer[\"answer_start\"]\n",
    "                    answer_end = answer[\"answer_end\"]  \n",
    "                    if(answer_start >= 0 and answer_end > 0):\n",
    "                        try:\n",
    "                            start_word_position = char_to_word_offset[answer_start]\n",
    "                            end_word_position = char_to_word_offset[answer_end-1]\n",
    "                        except:\n",
    "                            print(f'Reading example {idx} failed')\n",
    "                            start_word_position = -3\n",
    "                            end_word_position = -3\n",
    "                    else:\n",
    "                        start_word_position = answer[\"answer_start\"]\n",
    "                        end_word_position = answer[\"answer_end\"]\n",
    "                    answer_spans.append({'start': start_word_position, 'end': end_word_position})\n",
    "\n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len]\n",
    "\n",
    "                # The -4 accounts for [CLS], [q], [/q] and [/s]\n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 4\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:                           # default\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -ve value for the next loop to execute once\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                q_type_list = []\n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):    # execute once by default\n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [self.tokenizer.cls_token] + [\"<q>\"] + query_tokens + [\"</q>\"] + doc_slice_tokens + [self.tokenizer.eos_token] \n",
    "                    segment_ids = [0] * (len(query_tokens) + 3) + [1] * (len(doc_slice_tokens) + 1)\n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len\n",
    "\n",
    "                    doc_offset = len(query_tokens) + 3 - slice_start\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    "                    q_type = []\n",
    "                    for answer_span in answer_spans:\n",
    "                        start_position = answer_span['start']\n",
    "                        end_position = answer_span['end']\n",
    "                        if(start_position >= 0):\n",
    "                            tok_start_position_in_doc = orig_to_tok_index[start_position]\n",
    "                            not_end_of_doc = int(end_position + 1 < len(orig_to_tok_index))\n",
    "                            tok_end_position_in_doc = orig_to_tok_index[end_position + not_end_of_doc] - not_end_of_doc\n",
    "                            if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                                # this answer is outside the current slice\n",
    "                                continue\n",
    "                            start_positions.append(tok_start_position_in_doc + doc_offset)\n",
    "                            end_positions.append(tok_end_position_in_doc + doc_offset)\n",
    "                            q_type.append(0)\n",
    "                        elif(start_position == -1):\n",
    "                            q_type.append(1)\n",
    "                            start_positions.append(-1)  # will be ignored\n",
    "                            end_positions.append(-1)     \n",
    "                        elif(start_position == -2):\n",
    "                            q_type.append(2)\n",
    "                            start_positions.append(-1)\n",
    "                            end_positions.append(-1)     \n",
    "                        else:\n",
    "                            continue\n",
    "                    assert len(start_positions) == len(end_positions)\n",
    "                    if self.ignore_seq_with_no_answers and len(start_positions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    for i, (start_position, end_position) in enumerate(zip(start_positions, end_positions)):\n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "\n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    q_type_list.append(q_type)\n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list),\n",
    "                                     torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list), torch.tensor(q_type_list),\n",
    "                                     qa['id']))   \n",
    "#                 tensors_list.append(all_doc_tokens)\n",
    "        return tensors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collate_one_doc_and_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to hotpotqaDataset\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 1  # qids  \n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'collate_one_doc_and_lists',\n",
       " 'one_example_to_tensors']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__doc__': '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    ',\n",
       "                '__init__': <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>,\n",
       "                '__len__': <function __main__.hotpotqaDataset.__len__(self)>,\n",
       "                '__getitem__': <function __main__.hotpotqaDataset.__getitem__(self, idx)>,\n",
       "                'one_example_to_tensors': <function __main__.one_example_to_tensors(self, example, idx)>,\n",
       "                'collate_one_doc_and_lists': <staticmethod at 0x7fc190bcccd0>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__',\n",
       "  '\\n    Largely based on\\n    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\\n    and\\n    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\\n    '),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__init_subclass__', <function hotpotqaDataset.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqaDataset.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Dataset' objects>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers\n",
    "getmembers(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__', <function torch.utils.data.dataset.Dataset.__add__(self, other)>),\n",
       " ('__getitem__', <function __main__.hotpotqaDataset.__getitem__(self, idx)>),\n",
       " ('__init__',\n",
       "  <function __main__.hotpotqaDataset.__init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)>),\n",
       " ('__len__', <function __main__.hotpotqaDataset.__len__(self)>),\n",
       " ('collate_one_doc_and_lists',\n",
       "  <function __main__.collate_one_doc_and_lists(batch)>),\n",
       " ('one_example_to_tensors',\n",
       "  <function __main__.one_example_to_tensors(self, example, idx)>)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import isfunction\n",
    "functions_list = [o for o in getmembers(hotpotqaDataset) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqaDataset, torch.utils.data.dataset.Dataset, object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqaDataset)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'example', 'idx'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfullargspec(hotpotqaDataset.one_example_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqaDataset in module __main__:\n",
      "\n",
      "class hotpotqaDataset(torch.utils.data.dataset.Dataset)\n",
      " |  hotpotqaDataset(file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |  \n",
      " |  Largely based on\n",
      " |  https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
      " |  and\n",
      " |  https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqaDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride, max_num_answers, ignore_seq_with_no_answers, max_question_len)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  one_example_to_tensors(self, example, idx)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collate_one_doc_and_lists(batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqaDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\_\\_init\\_\\_ and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hotpotqa(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(hotpotqa, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        num_new_tokens = self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<p>\", \"<q>\", \"<\\q>\"]})\n",
    "        print(self.tokenizer.all_special_tokens)\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.num_labels = 2\n",
    "        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        self.linear_type = torch.nn.Linear(self.model.config.hidden_size, 3)  #  question type (yes/no/span) classification \n",
    "        self.train_dataloader_object = self.val_dataloader_object = self.test_dataloader_object = None\n",
    "    \n",
    "    def load_model(self):\n",
    "#         model = Longformer.from_pretrained(self.args.model_path)\n",
    "        model = Longformer.from_pretrained('longformer-base-4096')\n",
    "        for layer in model.encoder.layer:\n",
    "            layer.attention.self.attention_mode = self.args.attention_mode\n",
    "            self.args.attention_window = layer.attention.self.attention_window\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    " \n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_types):\n",
    "        question_end_index = self._get_question_end_index(input_ids)\n",
    "        # Each batch is one document, and each row of the batch is a chunck of the document.\n",
    "        # Make sure all rows have the same question length.\n",
    "        assert (question_end_index[0].float() == question_end_index.float().mean()).item()\n",
    "\n",
    "        # local attention everywhere\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "        # global attention for the question tokens\n",
    "        attention_mask[:, :question_end_index.item()] = 2\n",
    "\n",
    "        # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "        input_ids, attention_mask = pad_to_window_size(\n",
    "            input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "\n",
    "        sequence_output = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask)[0]\n",
    "\n",
    "        # The pretrained hotpotqa model wasn't trained with padding, so remove padding tokens\n",
    "        # before computing loss and decoding.\n",
    "        padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "        if padding_len > 0:\n",
    "            sequence_output = sequence_output[:, :-padding_len]\n",
    "\n",
    "        predict_type = self.linear_type(sequence_output[:,0])\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        outputs = (start_logits, end_logits,)\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "            else:\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "                start_positions = start_positions[:, 0:1]\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = loss_fct(start_logits, start_positions[:, 0])\n",
    "                end_loss = loss_fct(end_logits, end_positions[:, 0])\n",
    "\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0)\n",
    "\n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target\n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer\n",
    "\n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the msked targets\n",
    "        masked_target = target * (1 - target_mask.long())\n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)\n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')\n",
    "\n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    "\n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "        # denominator = log(sum(exp(logits)))\n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm)\n",
    "\n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`.\n",
    "        # remove those from the loss before computing the sum. Use sum instead of mean because\n",
    "        # it is easier to compute\n",
    "        return loss[~torch.isinf(loss)].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ea38bab53ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# debug: check loaded dataset by DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n\u001b[0m\u001b[1;32m      4\u001b[0m                           \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_len\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_doc_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                           \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# debug: check loaded dataset by DataLoader\n",
    " \n",
    "dataset = hotpotqaDataset(file_path= args.train_dataset, tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "                          max_seq_len= args.max_seq_len, max_doc_len= args.max_doc_len,\n",
    "                          doc_stride= args.doc_stride,\n",
    "                          max_num_answers= args.max_num_answers,\n",
    "                          max_question_len= args.max_question_len,\n",
    "                          ignore_seq_with_no_answers= args.ignore_seq_with_no_answers)\n",
    "print(len(dataset))\n",
    "\n",
    "dl = DataLoader(dataset, batch_size=1, shuffle=None,\n",
    "                    num_workers=args.num_workers, sampler=None,\n",
    "                    collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "example = dataset[0] \n",
    "print(example)\n",
    "# [input_ids, input_mask, segment_ids, subword_starts, subword_ends, _, qids] = example1\n",
    "\n",
    "\n",
    "# print(input_ids)\n",
    "# print(input_mask) \n",
    "# print(segment_ids) \n",
    "# print(subword_starts) \n",
    "# print(subword_ends) \n",
    "# print(qids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@pl.data_loader\n",
    "def train_dataloader(self):\n",
    "    if self.train_dataloader_object is not None:\n",
    "        return self.train_dataloader_object\n",
    "    dataset = hotpotqaDataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                              max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                              doc_stride=self.args.doc_stride,\n",
    "                              max_num_answers=self.args.max_num_answers,\n",
    "                              max_question_len=self.args.max_question_len,\n",
    "                              ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if self.trainer.use_ddp else None\n",
    "    dl = DataLoader(dataset, batch_size=1, shuffle=(sampler is None),\n",
    "                    num_workers=self.args.num_workers, sampler=sampler,\n",
    "                    collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "\n",
    "    self.train_dataloader_object = dl\n",
    "    return self.train_dataloader_object\n",
    "\n",
    "@pl.data_loader\n",
    "def val_dataloader(self):\n",
    "    if self.val_dataloader_object is not None:\n",
    "        return self.val_dataloader_object\n",
    "    dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                              max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                              doc_stride=self.args.doc_stride,\n",
    "                              max_num_answers=self.args.max_num_answers,\n",
    "                              max_question_len=self.args.max_question_len,\n",
    "                              ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if self.trainer.use_ddp else None\n",
    "    dl = DataLoader(dataset, batch_size=1, shuffle=(sampler is None),\n",
    "                    num_workers=self.args.num_workers, sampler=sampler,\n",
    "                    collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "    self.val_dataloader_object = dl\n",
    "    return self.val_dataloader_object\n",
    "\n",
    "@pl.data_loader\n",
    "def test_dataloader(self):\n",
    "    if self.test_dataloader_object is not None:\n",
    "        return self.test_dataloader_object\n",
    "    dataset = hotpotqaDataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                              max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                              doc_stride=self.args.doc_stride,\n",
    "                              max_num_answers=self.args.max_num_answers,\n",
    "                              max_question_len=self.args.max_question_len,\n",
    "                              ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "    dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                    num_workers=self.args.num_workers, sampler=None,\n",
    "                    collate_fn=hotpotqaDataset.collate_one_doc_and_lists)\n",
    "    self.test_dataloader_object = dl\n",
    "    return self.test_dataloader_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### configure_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%add_to hotpotqa\n",
    " # A hook to overwrite to define your own DDP(DistributedDataParallel) implementation init. \n",
    " # The only requirement is that: \n",
    " # 1. On a validation batch the call goes to model.validation_step.\n",
    " # 2. On a training batch the call goes to model.training_step.\n",
    " # 3. On a testing batch, the call goes to model.test_step\n",
    " def configure_ddp(self, model, device_ids):\n",
    "    model = LightningDistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=device_ids,\n",
    "        find_unused_parameters=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **configure_optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def configure_optimizers(self):\n",
    "    # Set up optimizers and (optionally) learning rate schedulers\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < self.args.warmup:\n",
    "            return float(current_step) / float(max(1, self.args.warmup))\n",
    "        return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "    self.scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)  # scheduler is not saved in the checkpoint, but global_step is, which is enough to restart\n",
    "    self.scheduler.step(self.global_step)\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimizer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# A hook to do a lot of non-standard training tricks such as learning-rate warm-up\n",
    "def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.scheduler.step(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **training_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def training_step(self, batch, batch_nb):\n",
    "    # do the forward pass and calculate the loss for a batch\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, qids = batch\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends)\n",
    "    loss = output[0]\n",
    "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "    tensorboard_logs = {'train_loss': loss, 'lr': lr,\n",
    "                        'input_size': input_ids.numel(),\n",
    "                        'mem': torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3}\n",
    "    return {'loss': loss, 'log': tensorboard_logs}\n",
    "from functools import wraps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# When the validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
    "def validation_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, qids = batch\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends)\n",
    "    loss, start_logits, end_logits = output[:3]\n",
    "    answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "    # each batch is one document\n",
    "    answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "    qids = [qids] \n",
    "\n",
    "    f1_scores = [evaluation_utils.metric_max_over_ground_truths(evaluation_utils.f1_score, answer['text'])\n",
    "                 for answer in answers]\n",
    "    # TODO: if slow, skip em_scores, and use (f1_score == 1.0) instead\n",
    "    em_scores = [evaluation_utils.metric_max_over_ground_truths(evaluation_utils.exact_match_score, answer['text'])\n",
    "                 for answer in answers]\n",
    "    answer_scores = [answer['score'] for answer in answers]  # start_logit + end_logit\n",
    "    assert len(answer_scores) == len(f1_scores) == len(em_scores) == len(qids) == 1\n",
    "\n",
    "    # TODO: delete this metric\n",
    "    pred_subword_starts = start_logits.argmax(dim=1)\n",
    "    pred_subword_ends = end_logits.argmax(dim=1)\n",
    "    exact_match = (subword_ends[:, 0].squeeze(dim=-1) == pred_subword_ends).float() *  \\\n",
    "                  (subword_starts[:, 0].squeeze(dim=-1) == pred_subword_starts).float()\n",
    "\n",
    "    return {'vloss': loss, 'vem': exact_match.mean(),\n",
    "            'qids': qids, 'answer_scores': answer_scores,\n",
    "            'f1': f1_scores, 'em': em_scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "# If you didn't define a validation_step, this won't be called. Called at the end of the validation loop with the outputs of validation_step.\n",
    "def validation_end(self, outputs):\n",
    "    avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()\n",
    "    avg_em = torch.stack([x['vem'] for x in outputs]).mean()\n",
    "    string_qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "    int_qids = [self.val_dataloader_object.dataset.val_qid_string_to_int_map[qid] for qid in string_qids]\n",
    "    answer_scores = [item for sublist in outputs for item in sublist['answer_scores']]\n",
    "    f1_scores = [item for sublist in outputs for item in sublist['f1']]\n",
    "    em_scores = [item for sublist in outputs for item in sublist['em']]\n",
    "    print(f'before sync --> sizes: {len(int_qids)}, {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "    if self.trainer.use_ddp:\n",
    "        torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_loss /= self.trainer.world_size\n",
    "        torch.distributed.all_reduce(avg_em, op=torch.distributed.ReduceOp.SUM)\n",
    "        avg_em /= self.trainer.world_size\n",
    "\n",
    "        int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "        answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "        f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "        em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.int)\n",
    "    print(f'after sync --> sizes: {len(int_qids)}, {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "    # Because of having multiple documents per questions, some questions might have multiple corresponding answers\n",
    "    # Here, we only keep the answer with the highest answer_score\n",
    "    qa_with_duplicates = defaultdict(list)\n",
    "    for qid, answer_score, f1_score, em_score in zip(int_qids, answer_scores, f1_scores, em_scores):\n",
    "        qa_with_duplicates[qid].append({'answer_score': answer_score, 'f1': f1_score, 'em': em_score})\n",
    "    f1_scores = []\n",
    "    em_scores = []\n",
    "    for qid, answer_metrics in qa_with_duplicates.items():\n",
    "        top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "        f1_scores.append(top_answer['f1'])\n",
    "        em_scores.append(top_answer['em'])\n",
    "    avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_val_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "    logs = {'val_loss': avg_loss, 'val_em': avg_em, 'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em}\n",
    "\n",
    "    return {'avg_val_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n",
    "\n",
    "def sync_list_across_gpus(self, l, device, dtype):\n",
    "    l_tensor = torch.tensor(l, device=device, dtype=dtype)\n",
    "    gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "    torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "    return torch.cat(gather_l_tensor).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_step(self, batch, batch_nb):\n",
    "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, qids = batch\n",
    "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends)\n",
    "    loss, start_logits, end_logits = output[:3]\n",
    "    answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "    # each batch is one document\n",
    "    answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "    qids = [qids]\n",
    "    assert len(answers) == len(qids)\n",
    "    return {'qids': qids, 'answers': answers}\n",
    "\n",
    "def _get_question_end_index(self, input_ids):\n",
    "    eos_token_indices = (input_ids == self.tokenizer.eos_token_id).nonzero()\n",
    "    assert eos_token_indices.ndim == 2\n",
    "    assert eos_token_indices.size(0) == 2 * input_ids.size(0)\n",
    "    assert eos_token_indices.size(1) == 2\n",
    "    return eos_token_indices.view(input_ids.size(0), 2, 2)[:, 0, 1]\n",
    "\n",
    "def decode(self, input_ids, start_logits, end_logits):\n",
    "    question_end_index = self._get_question_end_index(input_ids)\n",
    "\n",
    "    # bsz x seqlen => bsz x n_best_size\n",
    "    start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "    end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "\n",
    "    answers = []\n",
    "    # This loop can't be vectorized, so loop over each example in the batch separetly\n",
    "    for i in range(start_logits_indices.size(0)):  # bsz\n",
    "        potential_answers = []\n",
    "        for start_logit_index in start_logits_indices[i]:  # n_best_size\n",
    "            for end_logit_index in end_logits_indices[i]:  # n_best_size\n",
    "                if start_logit_index <= question_end_index[i]:\n",
    "                    continue\n",
    "                if end_logit_index <= question_end_index[i]:\n",
    "                    continue\n",
    "                if start_logit_index > end_logit_index:\n",
    "                    continue\n",
    "                answer_len = end_logit_index - start_logit_index + 1\n",
    "                if answer_len > self.args.max_answer_length:\n",
    "                    continue\n",
    "                potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                          'start_logit': start_logits[i][start_logit_index].item(),\n",
    "                                          'end_logit': end_logits[i][end_logit_index].item()})\n",
    "        sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True)\n",
    "        if len(sorted_answers) == 0:\n",
    "            answers.append({'text': 'NoAnswerFound', 'score': -1000000})\n",
    "        else:\n",
    "            answer = sorted_answers[0]\n",
    "            answer_token_ids = input_ids[i, answer['start']: answer['end'] + 1]\n",
    "            answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "            text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "            score = answer['start_logit'] + answer['end_logit']\n",
    "            answers.append({'text': text, 'score': score})\n",
    "    return answers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "def test_end(self, outputs):\n",
    "    qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "    answers = [item for sublist in outputs for item in sublist['answers']]\n",
    "\n",
    "    qa_with_duplicates = defaultdict(list)\n",
    "    for qid, answer in zip(qids, answers):\n",
    "        qa_with_duplicates[qid].append({'answer_score': answer['score'], 'answer_text': answer['text'], })\n",
    "\n",
    "    qid_to_answer_text = {}\n",
    "    for qid, answer_metrics in qa_with_duplicates.items():\n",
    "        top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "        qid_to_answer_text[qid] = top_answer['answer_text']\n",
    "\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(qid_to_answer_text, f)\n",
    "\n",
    "    return {'count': len(qid_to_answer_text)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_model_specific_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to hotpotqa\n",
    "@staticmethod\n",
    "def add_model_specific_args(parser, root_dir):\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='hotpotqa')\n",
    "    parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "    parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0',\n",
    "                        help=\"Comma separated list of gpus. Default is gpu 0. To use CPU, use --gpus \"\" \")\n",
    "    parser.add_argument(\"--warmup\", type=int, default=200, help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"Maximum learning rate\")\n",
    "    parser.add_argument(\"--val_every\", type=float, default=0.2, help=\"Number of training steps between validations\")\n",
    "    parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                        help=\"Maximum length of seq passed to the transformer model\")\n",
    "    parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                        help=\"Maximum number of wordpieces of the input document\")\n",
    "    parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                        help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "    parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                        help=\"Maximum length of the question\")\n",
    "    parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                        help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "    parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                        help=\"each example should have at least one answer. Default is False\")\n",
    "    parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "    parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                        help=\"Number of answer candidates. Used at decoding time\")\n",
    "    parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                        help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "    parser.add_argument(\"--regular_softmax_loss\", action='store_true', help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "    parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "    parser.add_argument(\"--model_path\", type=str, required=True,\n",
    "                        help=\"Path to the checkpoint directory\")\n",
    "    parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "    parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                        default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "    parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'configure_apex',\n",
       " 'configure_ddp',\n",
       " 'configure_optimizers',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'freeze',\n",
       " 'grad_norm',\n",
       " 'half',\n",
       " 'init_ddp_connection',\n",
       " 'load_from_checkpoint',\n",
       " 'load_from_metrics',\n",
       " 'load_model',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'on_after_backward',\n",
       " 'on_batch_end',\n",
       " 'on_batch_start',\n",
       " 'on_before_zero_grad',\n",
       " 'on_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_hpc_load',\n",
       " 'on_hpc_save',\n",
       " 'on_load_checkpoint',\n",
       " 'on_post_performance_check',\n",
       " 'on_pre_performance_check',\n",
       " 'on_sanity_check_start',\n",
       " 'on_save_checkpoint',\n",
       " 'on_train_end',\n",
       " 'on_train_start',\n",
       " 'optimizer_step',\n",
       " 'or_softmax_cross_entropy_loss_one_doc',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'summarize',\n",
       " 'tbptt_split_batch',\n",
       " 'test_dataloader',\n",
       " 'test_end',\n",
       " 'test_step',\n",
       " 'tng_dataloader',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_dataloader',\n",
       " 'training_end',\n",
       " 'training_step',\n",
       " 'type',\n",
       " 'unfreeze',\n",
       " 'val_dataloader',\n",
       " 'validation_end',\n",
       " 'validation_step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__abstractmethods__', frozenset({'configure_optimizers', 'training_step'})),\n",
       " ('__call__',\n",
       "  <function torch.nn.modules.module.Module.__call__(self, *input, **kwargs)>),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.hotpotqa.__init__(self, args)>,\n",
       "                'load_model': <function __main__.hotpotqa.load_model(self)>,\n",
       "                'forward': <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_types)>,\n",
       "                'or_softmax_cross_entropy_loss_one_doc': <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset({'configure_optimizers',\n",
       "                           'training_step'}),\n",
       "                '_abc_impl': <_abc_data at 0x7ff334ba5ae0>,\n",
       "                'configure_optimizers': <function __main__.configure_optimizers(self)>,\n",
       "                'training_step': <function __main__.training_step(self, batch, batch_nb)>})),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name)>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__init_subclass__', <function hotpotqa.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name, value)>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__slots__', ()),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function hotpotqa.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'LightningModule' objects>),\n",
       " ('_abc_impl', <_abc_data at 0x7ff334ba5ae0>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('_version', 1),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name, module)>),\n",
       " ('apply', <function torch.nn.modules.module.Module.apply(self, fn)>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16', <function torch.nn.modules.module.Module.bfloat16(self)>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse=True)>),\n",
       " ('children', <function torch.nn.modules.module.Module.children(self)>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self)>),\n",
       " ('cuda', <function torch.nn.modules.module.Module.cuda(self, device=None)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self)>),\n",
       " ('dump_patches', False),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self)>),\n",
       " ('extra_repr', <function torch.nn.modules.module.Module.extra_repr(self)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self)>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_types)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self)>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_from_checkpoint',\n",
       "  <bound method LightningModule.load_from_checkpoint of <class '__main__.hotpotqa'>>),\n",
       " ('load_from_metrics',\n",
       "  <bound method LightningModule.load_from_metrics of <class '__main__.hotpotqa'>>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict, strict=True)>),\n",
       " ('modules', <function torch.nn.modules.module.Module.modules(self)>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix='', recurse=True)>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self)>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo=None, prefix='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix='', recurse=True)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse=True)>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook)>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name, tensor)>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook)>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook)>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name, param)>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self, requires_grad=True)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step(self, *args, **kwargs)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train', <function torch.nn.modules.module.Module.train(self, mode=True)>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('type', <function torch.nn.modules.module.Module.type(self, dst_type)>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step(self, *args, **kwargs)>),\n",
       " ('zero_grad', <function torch.nn.modules.module.Module.zero_grad(self)>)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "getmembers(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__call__',\n",
       "  <function torch.nn.modules.module.Module.__call__(self, *input, **kwargs)>),\n",
       " ('__delattr__',\n",
       "  <function torch.nn.modules.module.Module.__delattr__(self, name)>),\n",
       " ('__dir__', <function torch.nn.modules.module.Module.__dir__(self)>),\n",
       " ('__getattr__',\n",
       "  <function torch.nn.modules.module.Module.__getattr__(self, name)>),\n",
       " ('__init__', <function __main__.hotpotqa.__init__(self, args)>),\n",
       " ('__repr__', <function torch.nn.modules.module.Module.__repr__(self)>),\n",
       " ('__setattr__',\n",
       "  <function torch.nn.modules.module.Module.__setattr__(self, name, value)>),\n",
       " ('__setstate__',\n",
       "  <function torch.nn.modules.module.Module.__setstate__(self, state)>),\n",
       " ('_apply', <function torch.nn.modules.module.Module._apply(self, fn)>),\n",
       " ('_get_name', <function torch.nn.modules.module.Module._get_name(self)>),\n",
       " ('_load_from_state_dict',\n",
       "  <function torch.nn.modules.module.Module._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)>),\n",
       " ('_named_members',\n",
       "  <function torch.nn.modules.module.Module._named_members(self, get_members_fn, prefix='', recurse=True)>),\n",
       " ('_register_load_state_dict_pre_hook',\n",
       "  <function torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self, hook)>),\n",
       " ('_register_state_dict_hook',\n",
       "  <function torch.nn.modules.module.Module._register_state_dict_hook(self, hook)>),\n",
       " ('_replicate_for_data_parallel',\n",
       "  <function torch.nn.modules.module.Module._replicate_for_data_parallel(self)>),\n",
       " ('_save_to_state_dict',\n",
       "  <function torch.nn.modules.module.Module._save_to_state_dict(self, destination, prefix, keep_vars)>),\n",
       " ('_slow_forward',\n",
       "  <function torch.nn.modules.module.Module._slow_forward(self, *input, **kwargs)>),\n",
       " ('add_module',\n",
       "  <function torch.nn.modules.module.Module.add_module(self, name, module)>),\n",
       " ('apply', <function torch.nn.modules.module.Module.apply(self, fn)>),\n",
       " ('backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.backward(self, use_amp, loss, optimizer)>),\n",
       " ('bfloat16', <function torch.nn.modules.module.Module.bfloat16(self)>),\n",
       " ('buffers',\n",
       "  <function torch.nn.modules.module.Module.buffers(self, recurse=True)>),\n",
       " ('children', <function torch.nn.modules.module.Module.children(self)>),\n",
       " ('configure_apex',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_apex(self, amp, model, optimizers, amp_level)>),\n",
       " ('configure_ddp',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.configure_ddp(self, model, device_ids)>),\n",
       " ('configure_optimizers', <function __main__.configure_optimizers(self)>),\n",
       " ('cpu', <function torch.nn.modules.module.Module.cpu(self)>),\n",
       " ('cuda', <function torch.nn.modules.module.Module.cuda(self, device=None)>),\n",
       " ('double', <function torch.nn.modules.module.Module.double(self)>),\n",
       " ('eval', <function torch.nn.modules.module.Module.eval(self)>),\n",
       " ('extra_repr', <function torch.nn.modules.module.Module.extra_repr(self)>),\n",
       " ('float', <function torch.nn.modules.module.Module.float(self)>),\n",
       " ('forward',\n",
       "  <function __main__.hotpotqa.forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_types)>),\n",
       " ('freeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.freeze(self)>),\n",
       " ('grad_norm',\n",
       "  <function pytorch_lightning.core.grads.GradInformation.grad_norm(self, norm_type)>),\n",
       " ('half', <function torch.nn.modules.module.Module.half(self)>),\n",
       " ('init_ddp_connection',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.init_ddp_connection(self, proc_rank, world_size)>),\n",
       " ('load_model', <function __main__.hotpotqa.load_model(self)>),\n",
       " ('load_state_dict',\n",
       "  <function torch.nn.modules.module.Module.load_state_dict(self, state_dict, strict=True)>),\n",
       " ('modules', <function torch.nn.modules.module.Module.modules(self)>),\n",
       " ('named_buffers',\n",
       "  <function torch.nn.modules.module.Module.named_buffers(self, prefix='', recurse=True)>),\n",
       " ('named_children',\n",
       "  <function torch.nn.modules.module.Module.named_children(self)>),\n",
       " ('named_modules',\n",
       "  <function torch.nn.modules.module.Module.named_modules(self, memo=None, prefix='')>),\n",
       " ('named_parameters',\n",
       "  <function torch.nn.modules.module.Module.named_parameters(self, prefix='', recurse=True)>),\n",
       " ('on_after_backward',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_after_backward(self)>),\n",
       " ('on_batch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_end(self)>),\n",
       " ('on_batch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_batch_start(self, batch)>),\n",
       " ('on_before_zero_grad',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad(self, optimizer)>),\n",
       " ('on_epoch_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_end(self)>),\n",
       " ('on_epoch_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_epoch_start(self)>),\n",
       " ('on_hpc_load',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_load(self, checkpoint)>),\n",
       " ('on_hpc_save',\n",
       "  <function pytorch_lightning.core.saving.ModelIO.on_hpc_save(self, checkpoint)>),\n",
       " ('on_load_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint(self, checkpoint)>),\n",
       " ('on_post_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_post_performance_check(self)>),\n",
       " ('on_pre_performance_check',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_pre_performance_check(self)>),\n",
       " ('on_sanity_check_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_sanity_check_start(self)>),\n",
       " ('on_save_checkpoint',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint(self, checkpoint)>),\n",
       " ('on_train_end',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_end(self)>),\n",
       " ('on_train_start',\n",
       "  <function pytorch_lightning.core.hooks.ModelHooks.on_train_start(self)>),\n",
       " ('optimizer_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None)>),\n",
       " ('or_softmax_cross_entropy_loss_one_doc',\n",
       "  <function __main__.hotpotqa.or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)>),\n",
       " ('parameters',\n",
       "  <function torch.nn.modules.module.Module.parameters(self, recurse=True)>),\n",
       " ('register_backward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_backward_hook(self, hook)>),\n",
       " ('register_buffer',\n",
       "  <function torch.nn.modules.module.Module.register_buffer(self, name, tensor)>),\n",
       " ('register_forward_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_hook(self, hook)>),\n",
       " ('register_forward_pre_hook',\n",
       "  <function torch.nn.modules.module.Module.register_forward_pre_hook(self, hook)>),\n",
       " ('register_parameter',\n",
       "  <function torch.nn.modules.module.Module.register_parameter(self, name, param)>),\n",
       " ('requires_grad_',\n",
       "  <function torch.nn.modules.module.Module.requires_grad_(self, requires_grad=True)>),\n",
       " ('share_memory',\n",
       "  <function torch.nn.modules.module.Module.share_memory(self)>),\n",
       " ('state_dict',\n",
       "  <function torch.nn.modules.module.Module.state_dict(self, destination=None, prefix='', keep_vars=False)>),\n",
       " ('summarize',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.summarize(self, mode)>),\n",
       " ('tbptt_split_batch',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch(self, batch, split_size)>),\n",
       " ('test_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('test_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_end(self, outputs)>),\n",
       " ('test_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.test_step(self, *args, **kwargs)>),\n",
       " ('tng_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('to', <function torch.nn.modules.module.Module.to(self, *args, **kwargs)>),\n",
       " ('train', <function torch.nn.modules.module.Module.train(self, mode=True)>),\n",
       " ('train_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('training_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.training_end(self, *args, **kwargs)>),\n",
       " ('training_step', <function __main__.training_step(self, batch, batch_nb)>),\n",
       " ('type', <function torch.nn.modules.module.Module.type(self, dst_type)>),\n",
       " ('unfreeze',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.unfreeze(self)>),\n",
       " ('val_dataloader',\n",
       "  <function pytorch_lightning.core.decorators.data_loader.<locals>._get_data_loader(self)>),\n",
       " ('validation_end',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_end(self, outputs)>),\n",
       " ('validation_step',\n",
       "  <function pytorch_lightning.core.lightning.LightningModule.validation_step(self, *args, **kwargs)>),\n",
       " ('zero_grad', <function torch.nn.modules.module.Module.zero_grad(self)>)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_list = [o for o in getmembers(hotpotqa) if isfunction(o[1])]\n",
    "functions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.hotpotqa,\n",
       " pytorch_lightning.core.lightning.LightningModule,\n",
       " abc.ABC,\n",
       " pytorch_lightning.core.grads.GradInformation,\n",
       " pytorch_lightning.core.saving.ModelIO,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmro(hotpotqa)  # a hierarchy of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class hotpotqa in module __main__:\n",
      "\n",
      "class hotpotqa(pytorch_lightning.core.lightning.LightningModule)\n",
      " |  hotpotqa(args)\n",
      " |  \n",
      " |  Helper class that provides a standard way to create an ABC using\n",
      " |  inheritance.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      hotpotqa\n",
      " |      pytorch_lightning.core.lightning.LightningModule\n",
      " |      abc.ABC\n",
      " |      pytorch_lightning.core.grads.GradInformation\n",
      " |      pytorch_lightning.core.saving.ModelIO\n",
      " |      pytorch_lightning.core.hooks.ModelHooks\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, args)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, q_types)\n",
      " |      Same as torch.nn.Module.forward(), however in Lightning you want this to define\n",
      " |      the  operations you want to use for prediction (ie: on a server or as a feature extractor).\n",
      " |      \n",
      " |      Normally you'd call self.forward() from your training_step() method. This makes it easy to write a complex\n",
      " |      system for training with the outputs you'd want in a prediction setting.\n",
      " |      \n",
      " |      Args:\n",
      " |          x (tensor): Whatever  you decide to define in the forward method\n",
      " |      \n",
      " |      Return:\n",
      " |          Predicted output\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # example if we were using this model as a feature extractor\n",
      " |          def forward(self, x):\n",
      " |              feature_maps = self.convnet(x)\n",
      " |              return feature_maps\n",
      " |      \n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              x, y = batch\n",
      " |              feature_maps = self.forward(x)\n",
      " |              logits = self.classifier(feature_maps)\n",
      " |      \n",
      " |              # ...\n",
      " |              return loss\n",
      " |      \n",
      " |          # splitting it this way allows model to be used a feature extractor\n",
      " |          model = MyModelAbove()\n",
      " |      \n",
      " |          inputs = server.get_request()\n",
      " |          results = model(inputs)\n",
      " |          server.write_results(results)\n",
      " |      \n",
      " |          # -------------\n",
      " |          # This is in stark contrast to torch.nn.Module where normally you would have this:\n",
      " |          def forward(self, batch):\n",
      " |              x, y = batch\n",
      " |              feature_maps = self.convnet(x)\n",
      " |              logits = self.classifier(feature_maps)\n",
      " |              return logits\n",
      " |  \n",
      " |  load_model(self)\n",
      " |  \n",
      " |  or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1)\n",
      " |      loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
      " |  \n",
      " |  training_step(self, batch, batch_nb)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = set()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.core.lightning.LightningModule:\n",
      " |  \n",
      " |  configure_apex(self, amp, model, optimizers, amp_level)\n",
      " |      Override to init AMP your own way\n",
      " |      Must return a model and list of optimizers\n",
      " |      \n",
      " |      Args:\n",
      " |          amp (object): pointer to amp library object\n",
      " |          model (LightningModule): pointer to current lightningModule\n",
      " |          optimizers (list): list of optimizers passed in configure_optimizers()\n",
      " |          amp_level (str): AMP mode chosen ('O1', 'O2', etc...)\n",
      " |      \n",
      " |      Return:\n",
      " |          Apex wrapped model and optimizers\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Default implementation used by Trainer.\n",
      " |          def configure_apex(self, amp, model, optimizers, amp_level):\n",
      " |              model, optimizers = amp.initialize(\n",
      " |                  model, optimizers, opt_level=amp_level,\n",
      " |              )\n",
      " |      \n",
      " |              return model, optimizers\n",
      " |  \n",
      " |  configure_ddp(self, model, device_ids)\n",
      " |      Override to init DDP in your own way or with your own wrapper.\n",
      " |      The only requirements are that:\n",
      " |      \n",
      " |      1. On a validation batch the call goes to model.validation_step.\n",
      " |      2. On a training batch the call goes to model.training_step.\n",
      " |      3. On a testing batch, the call goes to model.test_step\n",
      " |      \n",
      " |      Args:\n",
      " |          model (LightningModule): the LightningModule currently being optimized\n",
      " |          device_ids (list): the list of GPU ids\n",
      " |      \n",
      " |      Return:\n",
      " |          DDP wrapped model\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # default implementation used in Trainer\n",
      " |          def configure_ddp(self, model, device_ids):\n",
      " |              # Lightning DDP simply routes to test_step, val_step, etc...\n",
      " |              model = LightningDistributedDataParallel(\n",
      " |                  model,\n",
      " |                  device_ids=device_ids,\n",
      " |                  find_unused_parameters=True\n",
      " |              )\n",
      " |              return model\n",
      " |  \n",
      " |  configure_optimizers(self)\n",
      " |      This is where you choose what optimizers and learning-rate schedulers to use in your optimization.\n",
      " |      Normally you'd need one. But in the case of GANs or something more esoteric you might have multiple.\n",
      " |      \n",
      " |      Return: any of these 3 options:\n",
      " |          - Single optimizer\n",
      " |          - List or Tuple - List of optimizers\n",
      " |          - Two lists - The first list has multiple optimizers, the second a list of learning-rate schedulers\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # most cases\n",
      " |          def configure_optimizers(self):\n",
      " |              opt = Adam(self.parameters(), lr=0.01)\n",
      " |              return opt\n",
      " |      \n",
      " |          # multiple optimizer case (eg: GAN)\n",
      " |          def configure_optimizers(self):\n",
      " |              generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
      " |              disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n",
      " |              return generator_opt, disriminator_opt\n",
      " |      \n",
      " |          # example with learning_rate schedulers\n",
      " |          def configure_optimizers(self):\n",
      " |              generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
      " |              disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n",
      " |              discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10)\n",
      " |              return [generator_opt, disriminator_opt], [discriminator_sched]\n",
      " |      \n",
      " |      .. note:: Lightning calls .backward() and .step() on each optimizer and learning rate scheduler as needed.\n",
      " |      \n",
      " |      .. note:: If you use 16-bit precision (use_amp=True), Lightning will automatically\n",
      " |          handle the optimizers for you.\n",
      " |      \n",
      " |      .. note:: If you use multiple optimizers, training_step will have an additional `optimizer_idx` parameter.\n",
      " |      \n",
      " |      .. note:: If you use LBFGS lightning handles the closure function automatically for you\n",
      " |      \n",
      " |      .. note:: If you use multiple optimizers, gradients will be calculated only\n",
      " |          for the parameters of current optimizer at each training step.\n",
      " |      \n",
      " |      .. note:: If you need to control how often those optimizers step or override the default .step() schedule,\n",
      " |          override the `optimizer_step` hook.\n",
      " |  \n",
      " |  freeze(self)\n",
      " |      Freeze all params for inference\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          model = MyLightningModule(...)\n",
      " |          model.freeze()\n",
      " |  \n",
      " |  init_ddp_connection(self, proc_rank, world_size)\n",
      " |      Override to define your custom way of setting up a distributed environment.\n",
      " |      \n",
      " |      Lightning's implementation uses env:// init by default and sets the first node as root.\n",
      " |      \n",
      " |      Args:\n",
      " |          proc_rank (int): The current process rank within the node.\n",
      " |          world_size (int): Number of GPUs being use across all nodes. (num_nodes*nb_gpu_nodes).\n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def init_ddp_connection(self):\n",
      " |              # use slurm job id for the port number\n",
      " |              # guarantees unique ports across jobs from same grid search\n",
      " |              try:\n",
      " |                  # use the last 4 numbers in the job id as the id\n",
      " |                  default_port = os.environ['SLURM_JOB_ID']\n",
      " |                  default_port = default_port[-4:]\n",
      " |      \n",
      " |                  # all ports should be in the 10k+ range\n",
      " |                  default_port = int(default_port) + 15000\n",
      " |      \n",
      " |              except Exception as e:\n",
      " |                  default_port = 12910\n",
      " |      \n",
      " |              # if user gave a port number, use that one instead\n",
      " |              try:\n",
      " |                  default_port = os.environ['MASTER_PORT']\n",
      " |              except Exception:\n",
      " |                  os.environ['MASTER_PORT'] = str(default_port)\n",
      " |      \n",
      " |              # figure out the root node addr\n",
      " |              try:\n",
      " |                  root_node = os.environ['SLURM_NODELIST'].split(' ')[0]\n",
      " |              except Exception:\n",
      " |                  root_node = '127.0.0.2'\n",
      " |      \n",
      " |              root_node = self.trainer.resolve_root_node_address(root_node)\n",
      " |              os.environ['MASTER_ADDR'] = root_node\n",
      " |              dist.init_process_group(\n",
      " |                  'nccl',\n",
      " |                  rank=self.proc_rank,\n",
      " |                  world_size=self.world_size\n",
      " |              )\n",
      " |  \n",
      " |  on_load_checkpoint(self, checkpoint)\n",
      " |      Called by lightning to restore your model.\n",
      " |      If you saved something with **on_save_checkpoint** this is your chance to restore this.\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint (dict): Loaded checkpoint\n",
      " |      \n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def on_load_checkpoint(self, checkpoint):\n",
      " |              # 99% of the time you don't need to implement this method\n",
      " |              self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\n",
      " |      \n",
      " |      .. note:: Lighting auto-restores global step, epoch, and all training state including amp scaling.\n",
      " |          No need for you to restore anything regarding training.\n",
      " |  \n",
      " |  on_save_checkpoint(self, checkpoint)\n",
      " |      Called by lightning when saving a  checkpoint  to give you a chance to store anything else you\n",
      " |      might want to  save\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint (dic): Checkpoint to be saved\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def on_save_checkpoint(self, checkpoint):\n",
      " |              # 99% of use cases you don't need to implement this method\n",
      " |              checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\n",
      " |      \n",
      " |      .. note:: Lighting saves all aspects of training (epoch, global step, etc...) including amp scaling. No need\n",
      " |          for you to store anything about training.\n",
      " |  \n",
      " |  optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None)\n",
      " |      Override this method to adjust the default way the Trainer calls each optimizer. By default, Lightning\n",
      " |      calls .step() and zero_grad() as shown in the example once per optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch (int): Current epoch\n",
      " |          batch_idx (int): Index of current batch\n",
      " |          optimizer (torch.nn.Optimizer): A PyTorch optimizer\n",
      " |          optimizer_idx (int): If you used multiple optimizers this indexes into that list\n",
      " |          second_order_closure (int): closure for second order methods\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # DEFAULT\n",
      " |          def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
      " |              optimizer.step()\n",
      " |              optimizer.zero_grad()\n",
      " |      \n",
      " |          # Alternating schedule for optimizer steps (ie: GANs)\n",
      " |          def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
      " |              # update generator opt every 2 steps\n",
      " |              if optimizer_idx == 0:\n",
      " |                  if batch_idx % 2 == 0 :\n",
      " |                      optimizer.step()\n",
      " |                      optimizer.zero_grad()\n",
      " |      \n",
      " |              # update discriminator opt every 4 steps\n",
      " |              if optimizer_idx == 1:\n",
      " |                  if batch_idx % 4 == 0 :\n",
      " |                      optimizer.step()\n",
      " |                      optimizer.zero_grad()\n",
      " |      \n",
      " |              # ...\n",
      " |              # add as many optimizers as you want\n",
      " |      \n",
      " |      \n",
      " |      Here's another example showing how to use this for more advanced things such as learning-rate warm-up:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # learning rate warm-up\n",
      " |          def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
      " |              # warm up lr\n",
      " |              if self.trainer.global_step < 500:\n",
      " |                  lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)\n",
      " |                  for pg in optimizer.param_groups:\n",
      " |                      pg['lr'] = lr_scale * self.hparams.learning_rate\n",
      " |      \n",
      " |              # update params\n",
      " |              optimizer.step()\n",
      " |              optimizer.zero_grad()\n",
      " |  \n",
      " |  summarize(self, mode)\n",
      " |  \n",
      " |  tbptt_split_batch(self, batch, split_size)\n",
      " |      When using truncated backpropagation through time, each batch must be split along the time dimension.\n",
      " |      Lightning handles this by default, but  for custom behavior override this function.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch (torch.nn.Tensor): Current batch\n",
      " |          split_size (int): How big the split  is\n",
      " |      \n",
      " |      Return:\n",
      " |          list of batch splits. Each split will be passed to forward_step to enable truncated\n",
      " |          back propagation through time. The default implementation splits root level Tensors and\n",
      " |          Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def tbptt_split_batch(self, batch, split_size):\n",
      " |            splits = []\n",
      " |            for t in range(0, time_dims[0], split_size):\n",
      " |                batch_split = []\n",
      " |                for i, x in enumerate(batch):\n",
      " |                    if isinstance(x, torch.Tensor):\n",
      " |                        split_x = x[:, t:t + split_size]\n",
      " |                    elif isinstance(x, collections.Sequence):\n",
      " |                        split_x = [None] * len(x)\n",
      " |                        for batch_idx in range(len(x)):\n",
      " |                            split_x[batch_idx] = x[batch_idx][t:t + split_size]\n",
      " |      \n",
      " |                    batch_split.append(split_x)\n",
      " |      \n",
      " |                splits.append(batch_split)\n",
      " |      \n",
      " |            return splits\n",
      " |      \n",
      " |      .. note:: Called in the training loop after on_batch_start if `truncated_bptt_steps > 0`.\n",
      " |          Each returned batch split is passed separately to training_step(...).\n",
      " |  \n",
      " |  test_dataloader = _get_data_loader(self)\n",
      " |  \n",
      " |  test_end(self, outputs)\n",
      " |      Outputs has the appended output after each test step.\n",
      " |      \n",
      " |      :param outputs:  List of outputs you defined in test_step, or if there are multiple dataloaders,\n",
      " |       a list containing a list of outputs for each dataloader\n",
      " |      :return dict: Dict of OrderedDict with metrics to display in progress bar\n",
      " |      \n",
      " |      If you didn't define a test_step, this won't be called.\n",
      " |       Called at the end of the test step with the output of each test_step.\n",
      " |       The outputs here are strictly for the progress bar.\n",
      " |       If you don't need to display anything, don't return anything.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def test_end(self, outputs):\n",
      " |              test_loss_mean = 0\n",
      " |              test_acc_mean = 0\n",
      " |              for output in outputs:\n",
      " |                  test_loss_mean += output['test_loss']\n",
      " |                  test_acc_mean += output['test_acc']\n",
      " |      \n",
      " |              test_loss_mean /= len(outputs)\n",
      " |              test_acc_mean /= len(outputs)\n",
      " |              tqdm_dict = {'test_loss': test_loss_mean.item(), 'test_acc': test_acc_mean.item()}\n",
      " |      \n",
      " |              # show test_loss and test_acc in progress bar but only log test_loss\n",
      " |              results = {\n",
      " |                  'progress_bar': tqdm_dict,\n",
      " |                  'log': {'test_loss': val_loss_mean.item()}\n",
      " |              }\n",
      " |              return results\n",
      " |      \n",
      " |      With multiple dataloaders, `outputs` will be a list of lists. The outer list contains\n",
      " |      one entry per dataloader, while the inner list contains the individual outputs of\n",
      " |      each validation step for that dataloader.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def test_end(self, outputs):\n",
      " |              test_loss_mean = 0\n",
      " |              test_acc_mean = 0\n",
      " |              i = 0\n",
      " |              for dataloader_outputs in outputs:\n",
      " |                  for output in dataloader_outputs:\n",
      " |                      test_loss_mean += output['test_loss']\n",
      " |                      test_acc_mean += output['test_acc']\n",
      " |                      i += 1\n",
      " |      \n",
      " |              test_loss_mean /= i\n",
      " |              test_acc_mean /= i\n",
      " |              tqdm_dict = {'test_loss': test_loss_mean.item(), 'test_acc': test_acc_mean.item()}\n",
      " |      \n",
      " |              # show test_loss and test_acc in progress bar but only log test_loss\n",
      " |              results = {\n",
      " |                  'progress_bar': tqdm_dict,\n",
      " |                  'log': {'test_loss': val_loss_mean.item()}\n",
      " |              }\n",
      " |              return results\n",
      " |  \n",
      " |  test_step(self, *args, **kwargs)\n",
      " |      return whatever outputs will need to be aggregated in test_end\n",
      " |      \n",
      " |      :param batch: The output of your dataloader. A tensor, tuple or list\n",
      " |      :param int batch_idx: Integer displaying which batch this is\n",
      " |      :param int dataloader_idx: Integer displaying which dataloader this is (only if multiple test datasets used)\n",
      " |      :return dict: Dict or OrderedDict with metrics to display in progress bar. All keys must be tensors.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # if you have one test dataloader:\n",
      " |          def test_step(self, batch, batch_idx)\n",
      " |      \n",
      " |          # if you have multiple test dataloaders:\n",
      " |          def test_step(self, batch, batch_idx, dataloader_idxdx)\n",
      " |      \n",
      " |      \n",
      " |      **OPTIONAL**\n",
      " |      If you don't need to test you don't need to implement this method. In this step you'd normally\n",
      " |       generate examples or calculate anything of interest such as accuracy.\n",
      " |      \n",
      " |      When the validation_step is called, the model has been put in eval mode and PyTorch gradients\n",
      " |       have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
      " |      \n",
      " |      The dict you return here will be available in the `test_end` method.\n",
      " |      \n",
      " |      This function is used when you execute `trainer.test()`.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # CASE 1: A single test dataset\n",
      " |          def test_step(self, batch, batch_idx):\n",
      " |              x, y = batch\n",
      " |      \n",
      " |              # implement your own\n",
      " |              out = self.forward(x)\n",
      " |              loss = self.loss(out, y)\n",
      " |      \n",
      " |              # calculate acc\n",
      " |              labels_hat = torch.argmax(out, dim=1)\n",
      " |              test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
      " |      \n",
      " |              # all optional...\n",
      " |              # return whatever you need for the collation function test_end\n",
      " |              output = OrderedDict({\n",
      " |                  'test_loss': loss_test,\n",
      " |                  'test_acc': torch.tensor(test_acc), # everything must be a tensor\n",
      " |              })\n",
      " |      \n",
      " |              # return an optional dict\n",
      " |              return output\n",
      " |      \n",
      " |      \n",
      " |      If you pass in multiple test datasets, `test_step` will have an additional argument.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # CASE 2: multiple test datasets\n",
      " |          def test_step(self, batch, batch_idx, dataset_idx):\n",
      " |              # dataset_idx tells you which dataset this is.\n",
      " |      \n",
      " |      \n",
      " |      The `dataset_idx` corresponds to the order of datasets returned in `test_dataloader`.\n",
      " |  \n",
      " |  tng_dataloader = _get_data_loader(self)\n",
      " |  \n",
      " |  train_dataloader = _get_data_loader(self)\n",
      " |  \n",
      " |  training_end(self, *args, **kwargs)\n",
      " |      return loss, dict with metrics for tqdm\n",
      " |      \n",
      " |      :param outputs: What you return in `training_step`.\n",
      " |      :return dict: dictionary with loss key and optional log, progress keys:\n",
      " |          - loss -> tensor scalar [REQUIRED]\n",
      " |          - progress_bar -> Dict for progress bar display. Must have only tensors\n",
      " |          - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc)\n",
      " |      \n",
      " |      In certain cases (dp, ddp2), you might want to use all outputs of every process to do something.\n",
      " |      For instance, if using negative samples, you could run a batch via dp and use ALL the outputs\n",
      " |      for a single softmax across the full batch (ie: the denominator would use the full batch).\n",
      " |      \n",
      " |      In this case you should define training_end to perform those calculations.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # WITHOUT training_end\n",
      " |          # if used in DP or DDP2, this batch is 1/num_gpus large\n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              # batch is 1/num_gpus big\n",
      " |              x, y = batch\n",
      " |      \n",
      " |              out = self.forward(x)\n",
      " |              loss = self.softmax(out)\n",
      " |              loss = nce_loss(loss)\n",
      " |              return {'loss': loss}\n",
      " |      \n",
      " |          # --------------\n",
      " |          # with training_end to do softmax over the full batch\n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              # batch is 1/num_gpus big\n",
      " |              x, y = batch\n",
      " |      \n",
      " |              out = self.forward(x)\n",
      " |              return {'out': out}\n",
      " |      \n",
      " |          def training_end(self, outputs):\n",
      " |              # this out is now the full size of the batch\n",
      " |              out = outputs['out']\n",
      " |      \n",
      " |              # this softmax now uses the full batch size\n",
      " |              loss = self.softmax(out)\n",
      " |              loss = nce_loss(loss)\n",
      " |              return {'loss': loss}\n",
      " |      \n",
      " |      If you define multiple optimizers, this step will also be called with an additional `optimizer_idx` param.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Multiple optimizers (ie: GANs)\n",
      " |          def training_step(self, batch, batch_idx, optimizer_idx):\n",
      " |              if optimizer_idx == 0:\n",
      " |                  # do training_step with encoder\n",
      " |              if optimizer_idx == 1:\n",
      " |                  # do training_step with decoder\n",
      " |      \n",
      " |      If you add truncated back propagation through time you will also get an additional argument\n",
      " |       with the hidden states of the previous step.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Truncated back-propagation through time\n",
      " |          def training_step(self, batch, batch_idx, hiddens):\n",
      " |              # hiddens are the hiddens from the previous truncated backprop step\n",
      " |      \n",
      " |      You can also return a -1 instead of a dict to stop the current loop. This is useful if you want to\n",
      " |      break out of the current training epoch early.\n",
      " |  \n",
      " |  unfreeze(self)\n",
      " |      Unfreeze all params for inference.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          model = MyLightningModule(...)\n",
      " |          model.unfreeze()\n",
      " |  \n",
      " |  val_dataloader = _get_data_loader(self)\n",
      " |  \n",
      " |  validation_end(self, outputs)\n",
      " |      Outputs has the appended output after each validation step.\n",
      " |      \n",
      " |      :param outputs: List of outputs you defined in validation_step, or if there are multiple dataloaders,\n",
      " |       a list containing a list of outputs for each dataloader\n",
      " |      :return dict: Dictionary or OrderedDict with optional:\n",
      " |          progress_bar -> Dict for progress bar display. Must have only tensors\n",
      " |          log -> Dict of metrics to add to logger. Must have only tensors (no images, etc)\n",
      " |      \n",
      " |      If you didn't define a validation_step, this won't be called.\n",
      " |       Called at the end of the validation loop with the outputs of validation_step.\n",
      " |      \n",
      " |      The outputs here are strictly for the progress bar.\n",
      " |       If you don't need to display anything, don't return anything.\n",
      " |       Any keys present in 'log', 'progress_bar' or the rest of the dictionary\n",
      " |       are available for callbacks to access.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      With a single dataloader\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def validation_end(self, outputs):\n",
      " |              val_loss_mean = 0\n",
      " |              val_acc_mean = 0\n",
      " |              for output in outputs:\n",
      " |                  val_loss_mean += output['val_loss']\n",
      " |                  val_acc_mean += output['val_acc']\n",
      " |      \n",
      " |              val_loss_mean /= len(outputs)\n",
      " |              val_acc_mean /= len(outputs)\n",
      " |              tqdm_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}\n",
      " |      \n",
      " |              # show val_loss and val_acc in progress bar but only log val_loss\n",
      " |              results = {\n",
      " |                  'progress_bar': tqdm_dict,\n",
      " |                  'log': {'val_loss': val_loss_mean.item()}\n",
      " |              }\n",
      " |              return results\n",
      " |      \n",
      " |      With multiple dataloaders, `outputs` will be a list of lists. The outer list contains\n",
      " |      one entry per dataloader, while the inner list contains the individual outputs of\n",
      " |      each validation step for that dataloader.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def validation_end(self, outputs):\n",
      " |              val_loss_mean = 0\n",
      " |              val_acc_mean = 0\n",
      " |              i = 0\n",
      " |              for dataloader_outputs in outputs:\n",
      " |                  for output in dataloader_outputs:\n",
      " |                      val_loss_mean += output['val_loss']\n",
      " |                      val_acc_mean += output['val_acc']\n",
      " |                      i += 1\n",
      " |      \n",
      " |              val_loss_mean /= i\n",
      " |              val_acc_mean /= i\n",
      " |              tqdm_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}\n",
      " |      \n",
      " |              # show val_loss and val_acc in progress bar but only log val_loss\n",
      " |              results = {\n",
      " |                  'progress_bar': tqdm_dict,\n",
      " |                  'log': {'val_loss': val_loss_mean.item()}\n",
      " |              }\n",
      " |              return results\n",
      " |  \n",
      " |  validation_step(self, *args, **kwargs)\n",
      " |      This is the validation loop. It is called for each batch of the validation set.\n",
      " |      Whatever is returned from here will be passed in as a list on validation_end.\n",
      " |      In this step you'd normally generate examples or calculate anything of interest such as accuracy.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch (torch.nn.Tensor | (Tensor, Tensor) | [Tensor, Tensor]): The output of your dataloader.\n",
      " |              A tensor, tuple or list\n",
      " |          batch_idx (int): The index of this batch\n",
      " |          dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple\n",
      " |              val datasets used)\n",
      " |      \n",
      " |      Return:\n",
      " |          Dict or OrderedDict - passed to the validation_end step\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # if you have one val dataloader:\n",
      " |          def validation_step(self, batch, batch_idx)\n",
      " |      \n",
      " |          # if you have multiple val dataloaders:\n",
      " |          def validation_step(self, batch, batch_idx, dataloader_idxdx)\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # CASE 1: A single validation dataset\n",
      " |          def validation_step(self, batch, batch_idx):\n",
      " |              x, y = batch\n",
      " |      \n",
      " |              # implement your own\n",
      " |              out = self.forward(x)\n",
      " |              loss = self.loss(out, y)\n",
      " |      \n",
      " |              # log 6 example images\n",
      " |              # or generated text... or whatever\n",
      " |              sample_imgs = x[:6]\n",
      " |              grid = torchvision.utils.make_grid(sample_imgs)\n",
      " |              self.logger.experiment.add_image('example_images', grid, 0)\n",
      " |      \n",
      " |              # calculate acc\n",
      " |              labels_hat = torch.argmax(out, dim=1)\n",
      " |              val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
      " |      \n",
      " |              # all optional...\n",
      " |              # return whatever you need for the collation function validation_end\n",
      " |              output = OrderedDict({\n",
      " |                  'val_loss': loss_val,\n",
      " |                  'val_acc': torch.tensor(val_acc), # everything must be a tensor\n",
      " |              })\n",
      " |      \n",
      " |              # return an optional dict\n",
      " |              return output\n",
      " |      \n",
      " |      If you pass in multiple validation datasets, validation_step will have an additional argument.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # CASE 2: multiple validation datasets\n",
      " |          def validation_step(self, batch, batch_idx, dataset_idx):\n",
      " |              # dataset_idx tells you which dataset this is.\n",
      " |      \n",
      " |      .. note:: If you don't need to validate you don't need to implement this method.\n",
      " |      \n",
      " |      .. note:: When the validation_step is called, the model has been put in eval mode and PyTorch gradients\n",
      " |          have been disabled. At the end of validation, model goes back to training mode and gradients are enabled.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pytorch_lightning.core.lightning.LightningModule:\n",
      " |  \n",
      " |  load_from_checkpoint(checkpoint_path, map_location=None) from abc.ABCMeta\n",
      " |      Primary way of loading model from a checkpoint. When Lightning saves a checkpoint\n",
      " |      it  stores  the hyperparameters in the checkpoint if you initialized your  LightningModule\n",
      " |      with an argument  called `hparams` which is a Namespace or dictionary of hyperparameters\n",
      " |      \n",
      " |          Example\n",
      " |          -------\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # --------------\n",
      " |              # Case 1\n",
      " |              # when using Namespace (output of using Argparse to parse command line arguments)\n",
      " |              from argparse import Namespace\n",
      " |              hparams = Namespace(**{'learning_rate': 0.1})\n",
      " |      \n",
      " |              model = MyModel(hparams)\n",
      " |      \n",
      " |              class MyModel(pl.LightningModule):\n",
      " |                  def __init__(self, hparams):\n",
      " |                      self.learning_rate = hparams.learning_rate\n",
      " |      \n",
      " |              # --------------\n",
      " |              # Case 2\n",
      " |              # when using a dict\n",
      " |              model = MyModel({'learning_rate': 0.1})\n",
      " |      \n",
      " |              class MyModel(pl.LightningModule):\n",
      " |                  def __init__(self, hparams):\n",
      " |                      self.learning_rate = hparams['learning_rate']\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint_path (str): Path to checkpoint.\n",
      " |          map_location (dic): If your checkpoint saved from a GPU model and you now load on CPUs\n",
      " |              or a different number of GPUs, use this to map to the new setup.\n",
      " |      \n",
      " |      Return:\n",
      " |          LightningModule with loaded weights.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # load weights without mapping\n",
      " |          MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\n",
      " |      \n",
      " |          # load weights mapping all weights from GPU 1 to GPU 0\n",
      " |          map_location = {'cuda:1':'cuda:0'}\n",
      " |          MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt', map_location=map_location)\n",
      " |  \n",
      " |  load_from_metrics(weights_path, tags_csv, map_location=None) from abc.ABCMeta\n",
      " |      You should use `load_from_checkpoint` instead!\n",
      " |      However, if your .ckpt weights don't have the hyperparameters saved, use this method  to pass\n",
      " |      in a .csv with the hparams you'd like to use. These will  be converted  into a argparse.Namespace\n",
      " |      and passed into  your LightningModule for use.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          weights_path (str): Path to a PyTorch checkpoint\n",
      " |          tags_csv (str): Path to a .csv with two columns (key, value) as in this\n",
      " |              Example::\n",
      " |                  key,value\n",
      " |                  drop_prob,0.2\n",
      " |                  batch_size,32\n",
      " |      \n",
      " |          map_location (dict): A dictionary mapping saved weight GPU devices to new\n",
      " |              GPU devices (example: {'cuda:1':'cuda:0'})\n",
      " |      Return:\n",
      " |          LightningModule with loaded weights\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          pretrained_model = MyLightningModule.load_from_metrics(\n",
      " |              weights_path='/path/to/pytorch_checkpoint.ckpt',\n",
      " |              tags_csv='/path/to/hparams_file.csv',\n",
      " |              on_gpu=True,\n",
      " |              map_location=None\n",
      " |          )\n",
      " |      \n",
      " |          # predict\n",
      " |          pretrained_model.eval()\n",
      " |          pretrained_model.freeze()\n",
      " |          y_hat = pretrained_model(x)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.core.lightning.LightningModule:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.core.grads.GradInformation:\n",
      " |  \n",
      " |  grad_norm(self, norm_type)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.core.saving.ModelIO:\n",
      " |  \n",
      " |  on_hpc_load(self, checkpoint)\n",
      " |      Hook to do whatever you need right before Slurm manager loads the model\n",
      " |      :return:\n",
      " |  \n",
      " |  on_hpc_save(self, checkpoint)\n",
      " |      Hook to do whatever you need right before Slurm manager saves the model\n",
      " |      :return:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.core.hooks.ModelHooks:\n",
      " |  \n",
      " |  backward(self, use_amp, loss, optimizer)\n",
      " |      Override backward with your own implementation if you need to\n",
      " |      \n",
      " |      :param use_amp: Whether amp was requested or not\n",
      " |      :param loss: Loss is already scaled by accumulated grads\n",
      " |      :param optimizer: Current optimizer being used\n",
      " |      :return:\n",
      " |      \n",
      " |      Called to perform backward step.\n",
      " |      Feel free to override as needed.\n",
      " |      \n",
      " |      The loss passed in has already been scaled for accumulated gradients if requested.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def backward(self, use_amp, loss, optimizer):\n",
      " |              if use_amp:\n",
      " |                  with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
      " |                      scaled_loss.backward()\n",
      " |              else:\n",
      " |                  loss.backward()\n",
      " |  \n",
      " |  on_after_backward(self)\n",
      " |      Called after loss.backward() and before optimizers do anything.\n",
      " |      \n",
      " |      :return:\n",
      " |      \n",
      " |      Called in the training loop after model.backward()\n",
      " |      This is the ideal place to inspect or log gradient information\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def on_after_backward(self):\n",
      " |              # example to inspect gradient information in tensorboard\n",
      " |              if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\n",
      " |                  params = self.state_dict()\n",
      " |                  for k, v in params.items():\n",
      " |                      grads = v\n",
      " |                      name = k\n",
      " |                      self.logger.experiment.add_histogram(tag=name, values=grads,\n",
      " |                                                           global_step=self.trainer.global_step)\n",
      " |  \n",
      " |  on_batch_end(self)\n",
      " |      Called in the training loop after the batch.\n",
      " |  \n",
      " |  on_batch_start(self, batch)\n",
      " |      Called in the training loop before anything happens for that batch.\n",
      " |      \n",
      " |      :param batch:\n",
      " |      :return:\n",
      " |  \n",
      " |  on_before_zero_grad(self, optimizer)\n",
      " |      Called after optimizer.step() and before optimizer.zero_grad()\n",
      " |      \n",
      " |      Called in the training loop after taking an optimizer step and before zeroing grads.\n",
      " |      Good place to inspect weight information with weights updated.\n",
      " |      \n",
      " |      for optimizer in optimizers::\n",
      " |      \n",
      " |          optimizer.step()\n",
      " |          model.on_before_zero_grad(optimizer) # < ---- called here\n",
      " |          optimizer.zero_grad\n",
      " |      \n",
      " |      :param optimizer:\n",
      " |      :return:\n",
      " |  \n",
      " |  on_epoch_end(self)\n",
      " |      Called in the training loop at the very end of the epoch.\n",
      " |  \n",
      " |  on_epoch_start(self)\n",
      " |      Called in the training loop at the very beginning of the epoch.\n",
      " |  \n",
      " |  on_post_performance_check(self)\n",
      " |      Called at the very end of the validation loop.\n",
      " |  \n",
      " |  on_pre_performance_check(self)\n",
      " |      Called at the very beginning of the validation loop.\n",
      " |  \n",
      " |  on_sanity_check_start(self)\n",
      " |      Called before starting evaluate\n",
      " |      .. warning:: will be deprecated.\n",
      " |      :return:\n",
      " |  \n",
      " |  on_train_end(self)\n",
      " |      Called at the end of training before logger experiment is closed\n",
      " |      :return:\n",
      " |  \n",
      " |  on_train_start(self)\n",
      " |      Called at the beginning of training before sanity check\n",
      " |      :return:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self)\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function configure_optimizers in module pytorch_lightning.core.lightning:\n",
      "\n",
      "configure_optimizers(self)\n",
      "    This is where you choose what optimizers and learning-rate schedulers to use in your optimization.\n",
      "    Normally you'd need one. But in the case of GANs or something more esoteric you might have multiple.\n",
      "    \n",
      "    Return: any of these 3 options:\n",
      "        - Single optimizer\n",
      "        - List or Tuple - List of optimizers\n",
      "        - Two lists - The first list has multiple optimizers, the second a list of learning-rate schedulers\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "        # most cases\n",
      "        def configure_optimizers(self):\n",
      "            opt = Adam(self.parameters(), lr=0.01)\n",
      "            return opt\n",
      "    \n",
      "        # multiple optimizer case (eg: GAN)\n",
      "        def configure_optimizers(self):\n",
      "            generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
      "            disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n",
      "            return generator_opt, disriminator_opt\n",
      "    \n",
      "        # example with learning_rate schedulers\n",
      "        def configure_optimizers(self):\n",
      "            generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
      "            disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n",
      "            discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10)\n",
      "            return [generator_opt, disriminator_opt], [discriminator_sched]\n",
      "    \n",
      "    .. note:: Lightning calls .backward() and .step() on each optimizer and learning rate scheduler as needed.\n",
      "    \n",
      "    .. note:: If you use 16-bit precision (use_amp=True), Lightning will automatically\n",
      "        handle the optimizers for you.\n",
      "    \n",
      "    .. note:: If you use multiple optimizers, training_step will have an additional `optimizer_idx` parameter.\n",
      "    \n",
      "    .. note:: If you use LBFGS lightning handles the closure function automatically for you\n",
      "    \n",
      "    .. note:: If you use multiple optimizers, gradients will be calculated only\n",
      "        for the parameters of current optimizer at each training step.\n",
      "    \n",
      "    .. note:: If you need to control how often those optimizers step or override the default .step() schedule,\n",
      "        override the `optimizer_step` hook.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hotpotqa.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def training_step(self, batch, batch_nb):\n",
      "    # do the forward pass and calculate the loss for a batch\n",
      "    input_ids, input_mask, segment_ids, subword_starts, subword_ends, qids = batch\n",
      "    output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends)\n",
      "    loss = output[0]\n",
      "    lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']\n",
      "    tensorboard_logs = {'train_loss': loss, 'lr': lr,\n",
      "                        'input_size': input_ids.numel(),\n",
      "                        'mem': torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3}\n",
      "    return {'loss': loss, 'log': tensorboard_logs}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "code, line_no = inspect.getsourcelines(hotpotqa.training_step)\n",
    "print(''.join(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/u32/fanluo/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/u32/fanluo/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.tokenization_utils_base:Assigning ['<p>', '<q>', '<\\\\q>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <p> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <q> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <\\q> to the vocabulary\n",
      "INFO:transformers.configuration_utils:loading configuration file longformer-base-4096/config.json\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file longformer-base-4096/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<mask>', '<\\\\q>', '</s>', '</s>', '<q>', '<unk>', '<p>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing Longformer.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of Longformer were initialized from the model checkpoint at longformer-base-4096.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use Longformer for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with config:\n",
      "RobertaConfig {\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_mode\": \"tvm\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    hotpotqa.__abstractmethods__=set()   # without this, got an error \"Can't instantiate abstract class hotpotqa with abstract methods\" if these two abstract methods are not implemented in the same cell where class hotpotqa defined \n",
    "    model = hotpotqa(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger = TestTubeLogger( # The TestTubeLogger adds a nicer folder structure to manage experiments and snapshots all hyperparameters you pass to a LightningModule.\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_f1',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> #steps: 339176.25, #epochs: 30, batch_size: 8 <<<<<<<\n"
     ]
    }
   ],
   "source": [
    "    args.gpus = [int(x) for x in args.gpus.split(',')] if args.gpus is not \"\" else None  # use CPU if no gpu provided\n",
    "    train_set_size = 90447   # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    num_devices = 1 or len(args.gpus)\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * num_devices)\n",
    "    print(f'>>>>>>> #steps: {args.steps}, #epochs: {args.epochs}, batch_size: {args.batch_size * num_devices} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/apex\n",
      "Non-user install because site-packages writeable\n",
      "Created temporary directory: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-ephem-wheel-cache-im7m2m6s\n",
      "Created temporary directory: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe\n",
      "Initialized build tracking at /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe\n",
      "Created build tracker: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe\n",
      "Entered build tracker: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe\n",
      "Created temporary directory: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-install-tgxhca21\n",
      "Processing /Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/apex\n",
      "  Created temporary directory: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-build-ardufyha\n",
      "  Added file:///Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/apex to build tracker '/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe'\n",
      "    Running setup.py (path:/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-build-ardufyha/setup.py) egg_info for package from file:///Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/apex\n",
      "    Created temporary directory: /private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-pip-egg-info-5d6bg8up\n",
      "    Running command python setup.py egg_info\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-build-ardufyha/setup.py\", line 35, in <module>\n",
      "        _, bare_metal_major, _ = get_cuda_bare_metal_version(cpp_extension.CUDA_HOME)\n",
      "      File \"/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-build-ardufyha/setup.py\", line 14, in get_cuda_bare_metal_version\n",
      "        raw_output = subprocess.check_output([cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\n",
      "    TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
      "\n",
      "    Warning: Torch did not find available GPUs on this system.\n",
      "     If your intention is to cross-compile, this is not an error.\n",
      "    By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n",
      "    Volta (compute capability 7.0), Turing (compute capability 7.5),\n",
      "    and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n",
      "    If you wish to cross-compile for a single specific architecture,\n",
      "    export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n",
      "\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "Exception information:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\", line 188, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\", line 185, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 333, in run\n",
      "    reqs, check_supported_wheels=not options.target_dir\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 179, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 362, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 314, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 488, in prepare_linked_requirement\n",
      "    req, self.req_tracker, self.finder, self.build_isolation,\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 91, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(finder, build_isolation)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/distributions/sdist.py\", line 40, in prepare_distribution_metadata\n",
      "    self.req.prepare_metadata()\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/req/req_install.py\", line 550, in prepare_metadata\n",
      "    self.metadata_directory = self._generate_metadata()\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/req/req_install.py\", line 530, in _generate_metadata\n",
      "    details=self.name or \"from {}\".format(self.link)\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/operations/build/metadata_legacy.py\", line 73, in generate_metadata\n",
      "    command_desc='python setup.py egg_info',\n",
      "  File \"/Users/fan/miniconda3/envs/pytorch/lib/python3.7/site-packages/pip/_internal/utils/subprocess.py\", line 241, in call_subprocess\n",
      "    raise InstallationError(exc_msg)\n",
      "pip._internal.exceptions.InstallationError: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "1 location(s) to search for versions of pip:\n",
      "* https://pypi.org/simple/pip/\n",
      "Fetching project page and analyzing links: https://pypi.org/simple/pip/\n",
      "Getting page https://pypi.org/simple/pip/\n",
      "Found index url https://pypi.org/simple\n",
      "Starting new HTTPS connection (1): pypi.org:443\n",
      "https://pypi.org:443 \"GET /simple/pip/ HTTP/1.1\" 200 14137\n",
      "  Found link https://files.pythonhosted.org/packages/3d/9d/1e313763bdfb6a48977b65829c6ce2a43eaae29ea2f907c8bbef024a7219/pip-0.2.tar.gz#sha256=88bb8d029e1bf4acd0e04d300104b7440086f94cc1ce1c5c3c31e3293aee1f81 (from https://pypi.org/simple/pip/), version: 0.2\n",
      "  Found link https://files.pythonhosted.org/packages/18/ad/c0fe6cdfe1643a19ef027c7168572dac6283b80a384ddf21b75b921877da/pip-0.2.1.tar.gz#sha256=83522005c1266cc2de97e65072ff7554ac0f30ad369c3b02ff3a764b962048da (from https://pypi.org/simple/pip/), version: 0.2.1\n",
      "  Found link https://files.pythonhosted.org/packages/17/05/f66144ef69b436d07f8eeeb28b7f77137f80de4bf60349ec6f0f9509e801/pip-0.3.tar.gz#sha256=183c72455cb7f8860ac1376f8c4f14d7f545aeab8ee7c22cd4caf79f35a2ed47 (from https://pypi.org/simple/pip/), version: 0.3\n",
      "  Found link https://files.pythonhosted.org/packages/0a/bb/d087c9a1415f8726e683791c0b2943c53f2b76e69f527f2e2b2e9f9e7b5c/pip-0.3.1.tar.gz#sha256=34ce534f17065c78f980702928e988a6b6b2d8a9851aae5f1571a1feb9bb58d8 (from https://pypi.org/simple/pip/), version: 0.3.1\n",
      "  Found link https://files.pythonhosted.org/packages/cf/c3/153571aaac6cf999f4bb09c019b1ff379b7b599ea833813a41c784eec995/pip-0.4.tar.gz#sha256=28fc67558874f71fddda7168f73595f1650523dce3bc5bf189713ecdfc1e456e (from https://pypi.org/simple/pip/), version: 0.4\n",
      "  Found link https://files.pythonhosted.org/packages/8d/c7/f05c87812fa5d9562ecbc5f4f1fc1570444f53c81c834a7f662af406e3c1/pip-0.5.tar.gz#sha256=328d8412782f22568508a0d0c78a49c9920a82e44c8dfca49954fe525c152b2a (from https://pypi.org/simple/pip/), version: 0.5\n",
      "  Found link https://files.pythonhosted.org/packages/9a/aa/f536b6d14fe03343367da2ff44eee28f340ae650cd017ca088b6be13084a/pip-0.5.1.tar.gz#sha256=e27650538c41fe1007a41abd4cfd0f905b822622cbe1f8e7e09d1215af207694 (from https://pypi.org/simple/pip/), version: 0.5.1\n",
      "  Found link https://files.pythonhosted.org/packages/db/e6/fdf7be8a17b032c533d3f91e91e2c63dd81d3627cbe4113248a00c2d39d8/pip-0.6.tar.gz#sha256=4cf47db6815b2f435d1f44e1f35ff04823043f6161f7df9aec71a123b0c47f0d (from https://pypi.org/simple/pip/), version: 0.6\n",
      "  Found link https://files.pythonhosted.org/packages/91/cd/105f4d3c75d0ae18e12623acc96f42168aaba408dd6e43c4505aa21f8e37/pip-0.6.1.tar.gz#sha256=efe47e84ffeb0ea4804f9858b8a94bebd07f5452f907ebed36d03aed06a9f9ec (from https://pypi.org/simple/pip/), version: 0.6.1\n",
      "  Found link https://files.pythonhosted.org/packages/1c/c7/c0e1a9413c37828faf290f29a85a4d6034c145cc04bf1622ba8beb662ad8/pip-0.6.2.tar.gz#sha256=1c1a504d7e70d2c24246f95bd16e3d5fcec740fd144df69a407bf65a2ee67586 (from https://pypi.org/simple/pip/), version: 0.6.2\n",
      "  Found link https://files.pythonhosted.org/packages/3f/af/c4b9d49fb0f286996b28dbc0955c3ad359794697eb98e0e69863908070b0/pip-0.6.3.tar.gz#sha256=1a6df71eb29b98cba11bde6d6a0d8c6dd8b0518e74ceb71fb31ea4fbb42fd313 (from https://pypi.org/simple/pip/), version: 0.6.3\n",
      "  Found link https://files.pythonhosted.org/packages/ec/7a/6fe91ff0079ad0437830957c459d52f3923e516f5b453218f2a93d09a427/pip-0.7.tar.gz#sha256=ceaea0b9e494d893c8a191895301b79c1db33e41f14d3ad93e3d28a8b4e9bf27 (from https://pypi.org/simple/pip/), version: 0.7\n",
      "  Found link https://files.pythonhosted.org/packages/a5/63/11303863c2f5e9d9a15d89fcf7513a4b60987007d418862e0fb65c09fff7/pip-0.7.1.tar.gz#sha256=f54f05aa17edd0036de433c44892c8fedb1fd2871c97829838feb995818d24c3 (from https://pypi.org/simple/pip/), version: 0.7.1\n",
      "  Found link https://files.pythonhosted.org/packages/cd/a9/1debaa96bbc1005c1c8ad3b79fec58c198d35121546ea2e858ce0894268a/pip-0.7.2.tar.gz#sha256=98df2eb779358412bbbae75980171ae85deebc846d87e244d086520b1212da09 (from https://pypi.org/simple/pip/), version: 0.7.2\n",
      "  Found link https://files.pythonhosted.org/packages/74/54/f785c327fb3d163560a879b36edae5c78ee07806be282c9d4807f6be7dd1/pip-0.8.tar.gz#sha256=9017e4484a212dd4e1a43dd9f039dd7fc8338d4eea1c339d5ae1c80726de5b0f (from https://pypi.org/simple/pip/), version: 0.8\n",
      "  Found link https://files.pythonhosted.org/packages/5c/79/5e8381cc3078bae92166f2ba96de8355e8c181926505ba8882f7b099a500/pip-0.8.1.tar.gz#sha256=7176a87f35675f6468341212f3b959bb51d23ea66eb1c3692bf746c45c716fa2 (from https://pypi.org/simple/pip/), version: 0.8.1\n",
      "  Found link https://files.pythonhosted.org/packages/17/3e/0a98ab032991518741e7e712a719633e6ae160f51b3d3e855194530fd308/pip-0.8.2.tar.gz#sha256=f80a3549c048bc3bbcb47844826e9c7c6fcd87e77b92bef0d9e66d1b397c4962 (from https://pypi.org/simple/pip/), version: 0.8.2\n",
      "  Found link https://files.pythonhosted.org/packages/f7/9a/943fc6d879ed7220bac2e7e53096bfe78abec88d77f2f516400e0129679e/pip-0.8.3.tar.gz#sha256=1be2e18edd38aa75b5e4ef38a99ec33ba9247177cfcb4a6d2d2b3e73430e3001 (from https://pypi.org/simple/pip/), version: 0.8.3\n",
      "  Found link https://files.pythonhosted.org/packages/24/33/6eb675fb6db7b71d69d6928b33dea61b8bf5cfe1e5649be70ec84ce2fc09/pip-1.0.tar.gz#sha256=34ba07e2d14ba86d5088ba896ac80bed845a9b276ab8acb279b8d99bc77fec8e (from https://pypi.org/simple/pip/), version: 1.0\n",
      "  Found link https://files.pythonhosted.org/packages/10/d9/f584e6107ef98ad7eaaaa5d0f756bfee12561fa6a4712ffdb7209e0e1fd4/pip-1.0.1.tar.gz#sha256=37d2f18213d3845d2038dd3686bc71fc12bb41ad66c945a8b0dfec2879f3497b (from https://pypi.org/simple/pip/), version: 1.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/16/90/5e6f80364d8a656f60681dfb7330298edef292d43e1499bcb3a4c71ff0b9/pip-1.0.2.tar.gz#sha256=a6ed9b36aac2f121c01a2c9e0307a9e4d9438d100a407db701ac65479a3335d2 (from https://pypi.org/simple/pip/), version: 1.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/25/57/0d42cf5307d79913a082c5c4397d46f3793bc35e1138a694136d6e31be99/pip-1.1.tar.gz#sha256=993804bb947d18508acee02141281c77d27677f8c14eaa64d6287a1c53ef01c8 (from https://pypi.org/simple/pip/), version: 1.1\n",
      "  Found link https://files.pythonhosted.org/packages/ba/c3/4e1f892f41aaa217fe0d1f827fa05928783349c69f3cc06fdd68e112678a/pip-1.2.tar.gz#sha256=2b168f1987403f1dc6996a1f22a6f6637b751b7ab6ff27e78380b8d6e70aa314 (from https://pypi.org/simple/pip/), version: 1.2\n",
      "  Found link https://files.pythonhosted.org/packages/c3/a2/a63244da32afd9ce9a8ca1bd86e71610039adea8b8314046ebe5047527a6/pip-1.2.1.tar.gz#sha256=12a9302acfca62cdc7bc5d83386cac3e0581db61ac39acdb3a4e766a16b88eb1 (from https://pypi.org/simple/pip/), version: 1.2.1\n",
      "  Found link https://files.pythonhosted.org/packages/00/45/69d4f2602b80550bfb26cfd2f62c2f05b3b5c7352705d3766cd1e5b27648/pip-1.3.tar.gz#sha256=d6a13c5be316cb21a0243047c7f163f47e88973ebccff8d32e63ca1bf4d9321c (from https://pypi.org/simple/pip/), version: 1.3\n",
      "  Found link https://files.pythonhosted.org/packages/5b/ce/f5b98104f1c10d868936c25f7c597f492d4371aa9ad5fb61a94954ee7208/pip-1.3.1.tar.gz#sha256=145eaa5d1ea1b062663da1f3a97780d7edea4c63c68a37c463b1deedf7bb4957 (from https://pypi.org/simple/pip/), version: 1.3.1\n",
      "  Found link https://files.pythonhosted.org/packages/5f/d0/3b3958f6a58783bae44158b2c4c7827ae89abaecdd4bed12cff402620b9a/pip-1.4.tar.gz#sha256=1fd43cbf07d95ddcecbb795c97a1674b3ddb711bb4a67661284a5aa765aa1b97 (from https://pypi.org/simple/pip/), version: 1.4\n",
      "  Found link https://files.pythonhosted.org/packages/3f/f8/da390e0df72fb61d176b25a4b95262e3dcc14bda0ad25ac64d56db38b667/pip-1.4.1.tar.gz#sha256=4e7a06554711a624c35d0c646f63674b7f6bfc7f80221bf1eb1f631bd890d04e (from https://pypi.org/simple/pip/), version: 1.4.1\n",
      "  Found link https://files.pythonhosted.org/packages/4f/7d/e53bc80667378125a9e07d4929a61b0bd7128a1129dbe6f07bb3228652a3/pip-1.5.tar.gz#sha256=25f81d1a0e55d3b1709818dd57fdfb954b028f229f09bd69cb0bc80a8e03e048 (from https://pypi.org/simple/pip/), version: 1.5\n",
      "  Found link https://files.pythonhosted.org/packages/44/5d/1dca53b5de6d287e7eb99bd174bb022eb6cb0d6ca6e19ca6b16655dde8c2/pip-1.5.1-py2.py3-none-any.whl#sha256=00960db3b0b8724dd37fe37cfb9c72ecb8f59fab9db7d17c5c1e89a1adab49ce (from https://pypi.org/simple/pip/), version: 1.5.1\n",
      "  Found link https://files.pythonhosted.org/packages/21/3f/d86a600c9b2f41a75caacf768a24130f343def97652de2345da15ef7911f/pip-1.5.1.tar.gz#sha256=e60e936fbc101d56668c6134c1f2b5b40fcbec8b4fc4ca7fc34842b6b4c5c130 (from https://pypi.org/simple/pip/), version: 1.5.1\n",
      "  Found link https://files.pythonhosted.org/packages/3d/1f/227d77d5e9ed2df5162de4ba3616799a351eccb1ecd668ae824dd26153a1/pip-1.5.2-py2.py3-none-any.whl#sha256=6903909ccdcdbc3297b74118590e71344d6d262827acd1f5c0e2fcfce9807499 (from https://pypi.org/simple/pip/), version: 1.5.2\n",
      "  Found link https://files.pythonhosted.org/packages/ed/94/391a003107f6ec997c314199d03bff1c105af758ee490e3255353574487b/pip-1.5.2.tar.gz#sha256=2a8a3e08e652d3a40edbb39264bf01f8ff3c32520a79113357cca1f30533f738 (from https://pypi.org/simple/pip/), version: 1.5.2\n",
      "  Found link https://files.pythonhosted.org/packages/df/e9/bdb53d44fad1465b43edaf6bc7dd3027ed5af81405cc97603fdff0721ebb/pip-1.5.3-py2.py3-none-any.whl#sha256=f0037aed3ce6cf96b9e9117d42e967a74bea9ebe19088a2fdea5de93d5762fee (from https://pypi.org/simple/pip/), version: 1.5.3\n",
      "  Found link https://files.pythonhosted.org/packages/55/de/671a48ad313c808623041fc475f7c8f7610401d9f573f06b40eeb84e74e3/pip-1.5.3.tar.gz#sha256=dc53b4d28b88556a37cd73052b6d1d08cc644c6724e37c4d38a2e3c03c5440b2 (from https://pypi.org/simple/pip/), version: 1.5.3\n",
      "  Found link https://files.pythonhosted.org/packages/a9/9a/9aa19fe00de4c025562e5fb3796ff8520165a7dd1a5662c6ec9816e1ae99/pip-1.5.4-py2.py3-none-any.whl#sha256=fb7282556a42e84464f2e963a859ac4012d8134ba6218b70c1d82d145fcfa82f (from https://pypi.org/simple/pip/), version: 1.5.4\n",
      "  Found link https://files.pythonhosted.org/packages/78/d8/6e58a7130d457edadb753a0ea5708e411c100c7e94e72ad4802feeef735c/pip-1.5.4.tar.gz#sha256=70208a250bb4afdbbdd74c3ac35d4ab9ba1eb6852d02567a6a87f2f5104e30b9 (from https://pypi.org/simple/pip/), version: 1.5.4\n",
      "  Found link https://files.pythonhosted.org/packages/ce/c2/10d996b9c51b126a9f0bb9e14a9edcdd5c88888323c0685bb9b392b6c47c/pip-1.5.5-py2.py3-none-any.whl#sha256=fe7a5808190067b2598d85def9b83db46e5d64a00848ad843e107c36e1db4ae6 (from https://pypi.org/simple/pip/), version: 1.5.5\n",
      "  Found link https://files.pythonhosted.org/packages/88/01/a442fde40bd9aaf837612536f16ab751fac628807fd718690795b8ade77d/pip-1.5.5.tar.gz#sha256=4b7f5124364ae9b5ba833dcd8813a84c1c06fba1d7c8543323c7af4b33188eca (from https://pypi.org/simple/pip/), version: 1.5.5\n",
      "  Found link https://files.pythonhosted.org/packages/3f/08/7347ca4021e7fe0f1ab8f93cbc7d2a7a7350012300ad0e0227d55625e2b8/pip-1.5.6-py2.py3-none-any.whl#sha256=fbc1351ffedf09ca7560428758845a88d648b9730b63ce9e5df53a7c89f039a4 (from https://pypi.org/simple/pip/), version: 1.5.6\n",
      "  Found link https://files.pythonhosted.org/packages/45/db/4fb9a456b4ec4d3b701456ef562b9d72d76b6358e0c1463d17db18c5b772/pip-1.5.6.tar.gz#sha256=b1a4ae66baf21b7eb05a5e4f37c50c2706fa28ea1f8780ce8efe14dcd9f1726c (from https://pypi.org/simple/pip/), version: 1.5.6\n",
      "  Found link https://files.pythonhosted.org/packages/dc/7c/21191b5944b917b66e4e4e06d74f668d814b6e8a3ff7acd874479b6f6b3d/pip-6.0-py2.py3-none-any.whl#sha256=5ec6732505bd8be49fe1f8ad557b88253ffb085736396df4d6bea753fc2a8f2c (from https://pypi.org/simple/pip/), version: 6.0\n",
      "  Found link https://files.pythonhosted.org/packages/38/fd/065c66a88398f240e344fdf496b9707f92d75f88eedc3d10ff847b28a657/pip-6.0.tar.gz#sha256=6103897f1bb68d3f933edd60f3e3830c4ea6b8abf7a4b500db148921b11f6c9b (from https://pypi.org/simple/pip/), version: 6.0\n",
      "  Found link https://files.pythonhosted.org/packages/e9/7a/cdbc1a12ed52410d557e48d4646f4543e9e991ff32d2374dc6db849aa617/pip-6.0.1-py2.py3-none-any.whl#sha256=322aea7d1f7b9ee68ad87ac4704cad5df97f77e70668c0bd18f964c5daa78173 (from https://pypi.org/simple/pip/), version: 6.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/4d/c3/8675b90cd89b9b222062f4f6c7e9d48b0387f5b35cbf747a74403a883e56/pip-6.0.1.tar.gz#sha256=fa2f7c68da4a405d673aa38542f9df009d60026db4f532429ac9cbfbda1f959d (from https://pypi.org/simple/pip/), version: 6.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/71/3c/b5a521e5e99cfff091e282231591f21193fd80de079ec5fb8ed9c6614044/pip-6.0.2-py2.py3-none-any.whl#sha256=7d17b0f267f7c9cd17cd2924bbbe2b4a3d407322c0e09084ca3f1295c1fed50d (from https://pypi.org/simple/pip/), version: 6.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/4c/5a/f9e8e3de0153282c7cb54a9b991af225536ac914bac858ca664cf883bb3e/pip-6.0.2.tar.gz#sha256=6fa90667706a679e3dc75b27a51fddafa64401c45e96f8ae6c20978183290077 (from https://pypi.org/simple/pip/), version: 6.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/73/cb/3eebf42003791df29219a3dfa1874572aa16114b44c9b1b0ac66bf96e8c0/pip-6.0.3-py2.py3-none-any.whl#sha256=b72655b6ac6aef1c86dd07f51e8ace8d7aabd6a1c4ff88db87155276fa32a073 (from https://pypi.org/simple/pip/), version: 6.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/ce/63/8d99ae60d11ae1a65f5d4fc39a529a598bd3b8e067132210cb0c4d9e9f74/pip-6.0.3.tar.gz#sha256=b091a35f5fa0faffac0b27b97e1e1e93ffe63b463c2ea8dbde0c1fb987933614 (from https://pypi.org/simple/pip/), version: 6.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/c5/0e/c974206726542bc495fc7443dd97834a6d14c2f0cba183fcfcd01075225a/pip-6.0.4-py2.py3-none-any.whl#sha256=8dfd95de29a7a3bb1e7d368cc83d566938eb210b04d553ebfe5e3a422f4aec65 (from https://pypi.org/simple/pip/), version: 6.0.4\n",
      "  Found link https://files.pythonhosted.org/packages/02/a1/c90f19910ee153d7a0efca7216758121118d7e93084276541383fe9ca82e/pip-6.0.4.tar.gz#sha256=1dbbff9c369e510c7468ab68ba52c003f68f83c99c2f8259acd51099e8799f1e (from https://pypi.org/simple/pip/), version: 6.0.4\n",
      "  Found link https://files.pythonhosted.org/packages/e9/1b/c6a375a337fb576784cdea3700f6c3eaf1420f0a01458e6e034cc178a84a/pip-6.0.5-py2.py3-none-any.whl#sha256=b2c20e3a2a43b2bbb1d19ad98be27eccc7b0f0ece016da602ccaa757a862b0e2 (from https://pypi.org/simple/pip/), version: 6.0.5\n",
      "  Found link https://files.pythonhosted.org/packages/19/f2/58628768f618c8c9fea878e0fb97730c0b8a838d3ab3f325768bf12dac94/pip-6.0.5.tar.gz#sha256=3bf42d28be9085ab2e9aecfd69a6da2d31563fe833304bf71a620a30c38ab8a2 (from https://pypi.org/simple/pip/), version: 6.0.5\n",
      "  Found link https://files.pythonhosted.org/packages/64/fc/4a49ccb18f55a0ceeb76e8d554bd4563217117492997825d194ed0017cc1/pip-6.0.6-py2.py3-none-any.whl#sha256=fb04f8afe1ba57626783f0c8e2f3d46bbaebaa446fcf124f434e968a2fee595e (from https://pypi.org/simple/pip/), version: 6.0.6\n",
      "  Found link https://files.pythonhosted.org/packages/f6/ce/d9e4e178b66c766c117f62ddf4fece019ef9d50127a8926d2f60300d615e/pip-6.0.6.tar.gz#sha256=3a14091299dcdb9bab9e9004ae67ac401f2b1b14a7c98de074ca74fdddf4bfa0 (from https://pypi.org/simple/pip/), version: 6.0.6\n",
      "  Found link https://files.pythonhosted.org/packages/7a/8e/2bbd4fcf3ee06ee90ded5f39ec12f53165dfdb9ef25a981717ad38a16670/pip-6.0.7-py2.py3-none-any.whl#sha256=93a326304c7db749896bcef822bbbac1ab29dad5651c6d732e245975239890e6 (from https://pypi.org/simple/pip/), version: 6.0.7\n",
      "  Found link https://files.pythonhosted.org/packages/52/85/b160ebdaa84378df6bb0176d4eed9f57edca662446174eead7a9e2e566d6/pip-6.0.7.tar.gz#sha256=35a5a43ac6b7af83ed47ea5731a365f43d350a3a7267e039e5f06b61d42ab3c2 (from https://pypi.org/simple/pip/), version: 6.0.7\n",
      "  Found link https://files.pythonhosted.org/packages/63/65/55b71647adec1ad595bf0e5d76d028506dfc002df30c256f022ff7a660a5/pip-6.0.8-py2.py3-none-any.whl#sha256=3c22b0a8ff92727bd737a82f72700790591f177541df08c07bc1f90d6b72ac19 (from https://pypi.org/simple/pip/), version: 6.0.8\n",
      "  Found link https://files.pythonhosted.org/packages/ef/8a/e3a980bc0a7f791d72c1302f65763ed300f2e14c907ac033e01b44c79e5e/pip-6.0.8.tar.gz#sha256=0d58487a1b7f5be2e5e965c11afbea1dc44ecec8069de03491a4d0d6c85f4551 (from https://pypi.org/simple/pip/), version: 6.0.8\n",
      "  Found link https://files.pythonhosted.org/packages/24/fb/8a56a46243514681e569bbafd8146fa383476c4b7c725c8598c452366f31/pip-6.1.0-py2.py3-none-any.whl#sha256=435a018f6d29e34d4f901bf4e6860d8a5fa1816b68d62008c18ca062a306db31 (from https://pypi.org/simple/pip/), version: 6.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/6c/84/432eb60bbcb414b9cdfcb135d5f4925e253c74e7d6916ada79990d6cc1a0/pip-6.1.0.tar.gz#sha256=89f120e2ab3d25ab70c36eb28ad4f280fc9ba71736e74d3055f609c1f9173768 (from https://pypi.org/simple/pip/), version: 6.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/67/f0/ba0fb41dbdbfc4aa3e0c16b40269aca6b9e3d59cacdb646218aa2e9b1d2c/pip-6.1.1-py2.py3-none-any.whl#sha256=a67e54aa0f26b6d62ccec5cc6735eff205dd0fed075f56ac3d3111e91e4467fc (from https://pypi.org/simple/pip/), version: 6.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/bf/85/871c126b50b8ee0b9819e8a63b614aedd264577e73478caedcd447e8f28c/pip-6.1.1.tar.gz#sha256=89f3b626d225e08e7f20d85044afa40f612eb3284484169813dc2d0631f2a556 (from https://pypi.org/simple/pip/), version: 6.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/5a/9b/56d3c18d0784d5f2bbd446ea2dc7ffa7476c35e3dc223741d20cfee3b185/pip-7.0.0-py2.py3-none-any.whl#sha256=309c48399c7d68501a10ef206abd6e5c541fedbf84b95435d9063bd454b39df7 (from https://pypi.org/simple/pip/), version: 7.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/c6/16/6475b142927ca5d03e3b7968efa5b0edd103e4684ecfde181a25f6fa2505/pip-7.0.0.tar.gz#sha256=7b46bfc1b95494731de306a688e2a7bc056d7fa7ad27e026908fb2ae67fed23d (from https://pypi.org/simple/pip/), version: 7.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/5a/10/bb7a32c335bceba636aa673a4c977effa1e73a79f88856459486d8d670cf/pip-7.0.1-py2.py3-none-any.whl#sha256=d26b8573ba1ac1ec99a9bdbdffee2ff2b06c7790815211d0eb4dc1462a089705 (from https://pypi.org/simple/pip/), version: 7.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/4a/83/9ae4362a80739657e0c8bb628ea3fa0214a9aba7c8590dacc301ea293f73/pip-7.0.1.tar.gz#sha256=cfec177552fdd0b2d12b72651c8e874f955b4c62c1c2c9f2588cbdc1c0d0d416 (from https://pypi.org/simple/pip/), version: 7.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/64/7f/7107800ae0919a80afbf1ecba21b90890431c3ee79d700adac3c79cb6497/pip-7.0.2-py2.py3-none-any.whl#sha256=83c869c5ab7113866e2d69641ec470d47f0faae68ca4550a289a4d3db515ad65 (from https://pypi.org/simple/pip/), version: 7.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/75/b1/66532c273bca0133e42c3b4540a1609289f16e3046f1830f18c60794d661/pip-7.0.2.tar.gz#sha256=ba28fa60b573a9444e7b78ccb3b0f261d1f66f46d20403f9dce37b18a6aed405 (from https://pypi.org/simple/pip/), version: 7.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/96/76/33a598ae42dd0554207d83c7acc60e3b166dbde723cbf282f1f73b7a127c/pip-7.0.3-py2.py3-none-any.whl#sha256=7b1cb03e827d58d2d05e68ea96a9e27487ed4b0afcd951ac6e40847ce94f0738 (from https://pypi.org/simple/pip/), version: 7.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/35/59/5b23115758ba0f2fc465c459611865173ef006202ba83f662d1f58ed2fb8/pip-7.0.3.tar.gz#sha256=b4c598825a6f6dc2cac65968feb28e6be6c1f7f1408493c60a07eaa731a0affd (from https://pypi.org/simple/pip/), version: 7.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/f7/c0/9f8dac88326609b4b12b304e8382f64f7d5af7735a00d2fac36cf135fc30/pip-7.1.0-py2.py3-none-any.whl#sha256=80c29f899d3a00a448d65f8158544d22935baec7159af8da1a4fa1490ced481d (from https://pypi.org/simple/pip/), version: 7.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/7e/71/3c6ece07a9a885650aa6607b0ebfdf6fc9a3ef8691c44b5e724e4eee7bf2/pip-7.1.0.tar.gz#sha256=d5275ba3221182a5dd1b6bcfbfc5ec277fb399dd23226d6fa018048f7e0f10f2 (from https://pypi.org/simple/pip/), version: 7.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/1c/56/094d563c508917081bccff365e4f621ba33073c1c13aca9267a43cfcaf13/pip-7.1.1-py2.py3-none-any.whl#sha256=ce13000878d34c1178af76cb8cf269e232c00508c78ed46c165dd5b0881615f4 (from https://pypi.org/simple/pip/), version: 7.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/3b/bb/b3f2a95494fd3f01d3b3ae530e7c0e910dc25e88e30787b0a5e10cbc0640/pip-7.1.1.tar.gz#sha256=b22fe3c93a13fc7c04f145a42fd2ad50a9e3e1b8a7eed2e2b1c66e540a0951da (from https://pypi.org/simple/pip/), version: 7.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/b2/d0/cd115fe345dd6f07ec1c780020a7dfe74966fceeb171e0f20d1d4905b0b7/pip-7.1.2-py2.py3-none-any.whl#sha256=b9d3983b5cce04f842175e30169d2f869ef12c3546fd274083a65eada4e9708c (from https://pypi.org/simple/pip/), version: 7.1.2\n",
      "  Found link https://files.pythonhosted.org/packages/d0/92/1e8406c15d9372084a5bf79d96da3a0acc4e7fcf0b80020a4820897d2a5c/pip-7.1.2.tar.gz#sha256=ca047986f0528cfa975a14fb9f7f106271d4e0c3fe1ddced6c1db2e7ae57a477 (from https://pypi.org/simple/pip/), version: 7.1.2\n",
      "  Found link https://files.pythonhosted.org/packages/00/ae/bddef02881ee09c6a01a0d6541aa6c75a226a4e68b041be93142befa0cd6/pip-8.0.0-py2.py3-none-any.whl#sha256=262ed1823eb7fbe3f18a9bedb4800e59c4ab9a6682aff8c37b5ee83ea840910b (from https://pypi.org/simple/pip/), version: 8.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/e3/2d/03c014d11e66628abf2fda5ca00f779cbe7b5292c5cd13d42a95b94aa9b8/pip-8.0.0.tar.gz#sha256=90112b296152f270cb8dddcd19b7b87488d9e002e8cf622e14c4da9c2f6319b1 (from https://pypi.org/simple/pip/), version: 8.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/45/9c/6f9a24917c860873e2ce7bd95b8f79897524353df51d5d920cd6b6c1ec33/pip-8.0.1-py2.py3-none-any.whl#sha256=dedaac846bc74e38a3253671f51a056331ffca1da70e3f48d8128f2aa0635bba (from https://pypi.org/simple/pip/), version: 8.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/ea/66/a3d6187bd307159fedf8575c0d9ee2294d13b1cdd11673ca812e6a2dda8f/pip-8.0.1.tar.gz#sha256=477c50b3e538a7ac0fa611fb8b877b04b33fb70d325b12a81b9dbf3eb1158a4d (from https://pypi.org/simple/pip/), version: 8.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/e7/a0/bd35f5f978a5e925953ce02fa0f078a232f0f10fcbe543d8cfc043f74fda/pip-8.0.2-py2.py3-none-any.whl#sha256=249a6f3194be8c2e8cb4d4be3f6fd16a9f1e3336218caffa8e7419e3816f9988 (from https://pypi.org/simple/pip/), version: 8.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/ce/15/ee1f9a84365423e9ef03d0f9ed0eba2fb00ac1fffdd33e7b52aea914d0f8/pip-8.0.2.tar.gz#sha256=46f4bd0d8dfd51125a554568d646fe4200a3c2c6c36b9f2d06d2212148439521 (from https://pypi.org/simple/pip/), version: 8.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/ae/d4/2b127310f5364610b74c28e2e6a40bc19e2d3c9a9a4e012d3e333e767c99/pip-8.0.3-py2.py3-none-any.whl#sha256=b0335bc837f9edb5aad03bd43d0973b084a1cbe616f8188dc23ba13234dbd552 (from https://pypi.org/simple/pip/), version: 8.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/22/f3/14bc87a4f6b5ec70b682765978a6f3105bf05b6781fa97e04d30138bd264/pip-8.0.3.tar.gz#sha256=30f98b66f3fe1069c529a491597d34a1c224a68640c82caf2ade5f88aa1405e8 (from https://pypi.org/simple/pip/), version: 8.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/1e/c7/78440b3fb882ed001e6e12d8770bd45e73d6eced4e57f7c072b829ce8a3d/pip-8.1.0-py2.py3-none-any.whl#sha256=a542b99e08002ead83200198e19a3983270357e1cb4fe704247990b5b35471dc (from https://pypi.org/simple/pip/), version: 8.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/3c/72/6981d5adf880adecb066a1a1a4c312a17f8d787a3b85446967964ac66d55/pip-8.1.0.tar.gz#sha256=d8faa75dd7d0737b16d50cd0a56dc91a631c79ecfd8d38b80f6ee929ec82043e (from https://pypi.org/simple/pip/), version: 8.1.0\n",
      "  Found link https://files.pythonhosted.org/packages/31/6a/0f19a7edef6c8e5065f4346137cc2a08e22e141942d66af2e1e72d851462/pip-8.1.1-py2.py3-none-any.whl#sha256=44b9c342782ab905c042c207d995aa069edc02621ddbdc2b9f25954a0fdac25c (from https://pypi.org/simple/pip/), version: 8.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/41/27/9a8d24e1b55bd8c85e4d022da2922cb206f183e2d18fee4e320c9547e751/pip-8.1.1.tar.gz#sha256=3e78d3066aaeb633d185a57afdccf700aa2e660436b4af618bcb6ff0fa511798 (from https://pypi.org/simple/pip/), version: 8.1.1\n",
      "  Found link https://files.pythonhosted.org/packages/9c/32/004ce0852e0a127f07f358b715015763273799bd798956fa930814b60f39/pip-8.1.2-py2.py3-none-any.whl#sha256=6464dd9809fb34fc8df2bf49553bb11dac4c13d2ffa7a4f8038ad86a4ccb92a1 (from https://pypi.org/simple/pip/), version: 8.1.2\n",
      "  Found link https://files.pythonhosted.org/packages/e7/a8/7556133689add8d1a54c0b14aeff0acb03c64707ce100ecd53934da1aa13/pip-8.1.2.tar.gz#sha256=4d24b03ffa67638a3fa931c09fd9e0273ffa904e95ebebe7d4b1a54c93d7b732 (from https://pypi.org/simple/pip/), version: 8.1.2\n",
      "  Found link https://files.pythonhosted.org/packages/3f/ef/935d9296acc4f48d1791ee56a73781271dce9712b059b475d3f5fa78487b/pip-9.0.0-py2.py3-none-any.whl#sha256=c856ac18ca01e7127456f831926dc67cc7d3ab663f4c13b1ec156e36db4de574 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/5e/53/eaef47e5e2f75677c9de0737acc84b659b78a71c4086f424f55346a341b5/pip-9.0.0.tar.gz#sha256=f62fb70e7e000e46fce12aaeca752e5281a5446977fe5a75ab4189a43b3f8793 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/b6/ac/7015eb97dc749283ffdec1c3a88ddb8ae03b8fad0f0e611408f196358da3/pip-9.0.1-py2.py3-none-any.whl#sha256=690b762c0a8460c303c089d5d0be034fb15a5ea2b75bdf565f40421f542fefb0 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#sha256=09f243e1a7b461f654c26a725fa373211bb7ff17a9300058b205c61658ca940d (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/e7/f9/e801dcea22886cd513f6bd2e8f7e581bd6f67bb8e8f1cd8e7b92d8539280/pip-9.0.2-py2.py3-none-any.whl#sha256=b135491ddb061f39719b8472d8abb59c613816a2b86069c332db74d1cd208ab2 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/e5/8f/3fc66461992dc9e9fcf5e005687d5f676729172dda640df2fd8b597a6da7/pip-9.0.2.tar.gz#sha256=88110a224e9d30e5d76592a0b2130ef10e7e67a6426e8617bb918fffbfe91fe5 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.2\n",
      "  Found link https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl#sha256=c3ede34530e0e0b2381e7363aded78e0c33291654937e7373032fda04e8803e5 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/c4/44/e6b8056b6c8f2bfd1445cc9990f478930d8e3459e9dbf5b8e2d2922d64d3/pip-9.0.3.tar.gz#sha256=7bf48f9a693be1d58f49f7af7e0ae9fe29fd671cde8a55e6edca3581c4ef5796 (from https://pypi.org/simple/pip/) (requires-python:>=2.6,!=3.0.*,!=3.1.*,!=3.2.*), version: 9.0.3\n",
      "  Found link https://files.pythonhosted.org/packages/4b/5a/8544ae02a5bd28464e03af045e8aabde20a7b02db1911a9159328e1eb25a/pip-10.0.0b1-py2.py3-none-any.whl#sha256=dbd5d24cd461be23429625085a36cc8732cbcac4d2aaf673031f80f6ac07d844 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0b1\n",
      "  Found link https://files.pythonhosted.org/packages/aa/6d/ffbb86abf18b750fb26f27eda7c7732df2aacaa669c420d2eb2ad6df3458/pip-10.0.0b1.tar.gz#sha256=8d6e63d8b99752e4b53f272b66f9cd7b59e2b288e9a863a61c48d167203a2656 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0b1\n",
      "  Found link https://files.pythonhosted.org/packages/97/72/1d514201e7d7fc7fff5aac3de9c7b892cd72fb4bf23fd983630df96f7412/pip-10.0.0b2-py2.py3-none-any.whl#sha256=79f55588912f1b2b4f86f96f11e329bb01b25a484e2204f245128b927b1038a7 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0b2\n",
      "  Found link https://files.pythonhosted.org/packages/32/67/572f642e6e42c580d3154964cfbab7d9322c23b0f417c6c01fdd206a2777/pip-10.0.0b2.tar.gz#sha256=ad6adec2150ce4aed8f6134d9b77d928fc848dbcb887fb1a455988cf99da5cae (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0b2\n",
      "  Found link https://files.pythonhosted.org/packages/62/a1/0d452b6901b0157a0134fd27ba89bf95a857fbda64ba52e1ca2cf61d8412/pip-10.0.0-py2.py3-none-any.whl#sha256=86a60a96d85e329962a9e6f6af612cbc11106293dbc83f119802b5bee9874cf3 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/e0/69/983a8e47d3dfb51e1463c1e962b2ccd1d74ec4e236e232625e353d830ed2/pip-10.0.0.tar.gz#sha256=f05a3eeea64bce94e85cc6671d679473d66288a4d37c3fcf983584954096b34f (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.0\n",
      "  Found link https://files.pythonhosted.org/packages/0f/74/ecd13431bcc456ed390b44c8a6e917c1820365cbebcb6a8974d1cd045ab4/pip-10.0.1-py2.py3-none-any.whl#sha256=717cdffb2833be8409433a93746744b59505f42146e8d37de6c62b430e25d6d7 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/ae/e8/2340d46ecadb1692a1e455f13f75e596d4eab3d11a57446f08259dee8f02/pip-10.0.1.tar.gz#sha256=f2bd08e0cd1b06e10218feaf6fef299f473ba706582eb3bd9d52203fdbd7ee68 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*), version: 10.0.1\n",
      "  Found link https://files.pythonhosted.org/packages/5f/25/e52d3f31441505a5f3af41213346e5b6c221c9e086a166f3703d2ddaf940/pip-18.0-py2.py3-none-any.whl#sha256=070e4bf493c7c2c9f6a08dd797dd3c066d64074c38e9e8a0fb4e6541f266d96c (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 18.0\n",
      "  Found link https://files.pythonhosted.org/packages/69/81/52b68d0a4de760a2f1979b0931ba7889202f302072cc7a0d614211bc7579/pip-18.0.tar.gz#sha256=a0e11645ee37c90b40c46d607070c4fd583e2cd46231b1c06e389c5e814eed76 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 18.0\n",
      "  Found link https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl#sha256=7909d0a0932e88ea53a7014dfd14522ffef91a464daaaf5c573343852ef98550 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 18.1\n",
      "  Found link https://files.pythonhosted.org/packages/45/ae/8a0ad77defb7cc903f09e551d88b443304a9bd6e6f124e75c0fbbf6de8f7/pip-18.1.tar.gz#sha256=c0a292bd977ef590379a3f05d7b7f65135487b67470f6281289a94e015650ea1 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 18.1\n",
      "  Found link https://files.pythonhosted.org/packages/60/64/73b729587b6b0d13e690a7c3acd2231ee561e8dd28a58ae1b0409a5a2b20/pip-19.0-py2.py3-none-any.whl#sha256=249ab0de4c1cef3dba4cf3f8cca722a07fc447b1692acd9f84e19c646db04c9a (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found link https://files.pythonhosted.org/packages/11/31/c483614095176ddfa06ac99c2af4171375053b270842c7865ca0b4438dc1/pip-19.0.tar.gz#sha256=c82bf8bc00c5732f0dd49ac1dea79b6242a1bd42a5012e308ed4f04369b17e54 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0\r\n",
      "  Found link https://files.pythonhosted.org/packages/46/dc/7fd5df840efb3e56c8b4f768793a237ec4ee59891959d6a215d63f727023/pip-19.0.1-py2.py3-none-any.whl#sha256=aae79c7afe895fb986ec751564f24d97df1331bb99cdfec6f70dada2f40c0044 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/c8/89/ad7f27938e59db1f0f55ce214087460f65048626e2226531ba6cb6da15f0/pip-19.0.1.tar.gz#sha256=e81ddd35e361b630e94abeda4a1eddd36d47a90e71eb00f38f46b57f787cd1a5 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/d7/41/34dd96bd33958e52cb4da2f1bf0818e396514fd4f4725a79199564cd0c20/pip-19.0.2-py2.py3-none-any.whl#sha256=6a59f1083a63851aeef60c7d68b119b46af11d9d803ddc1cf927b58edcd0b312 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/4c/4d/88bc9413da11702cbbace3ccc51350ae099bb351febae8acc85fec34f9af/pip-19.0.2.tar.gz#sha256=f851133f8b58283fa50d8c78675eb88d4ff4cde29b6c41205cd938b06338e0e5 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl#sha256=bd812612bbd8ba84159d9ddc0266b7fbce712fc9bc98c82dee5750546ec8ec64 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/36/fa/51ca4d57392e2f69397cd6e5af23da2a8d37884a605f9e3f2d3bfdc48397/pip-19.0.3.tar.gz#sha256=6e6f197a1abfb45118dbb878b5c859a0edbdd33fd250100bc015b67fded4b9f2 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.0.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/f9/fb/863012b13912709c13cf5cfdbfb304fa6c727659d6290438e1a88df9d848/pip-19.1-py2.py3-none-any.whl#sha256=8f59b6cf84584d7962d79fd1be7a8ec0eb198aa52ea864896551736b3614eee9 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/51/5f/802a04274843f634469ef299fcd273de4438386deb7b8681dd059f0ee3b7/pip-19.1.tar.gz#sha256=d9137cb543d8a4d73140a3282f6d777b2e786bb6abb8add3ac5b6539c82cd624 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl#sha256=993134f0475471b91452ca029d4390dc8f298ac63a712814f101cd1b6db46676 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.1.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/93/ab/f86b61bef7ab14909bd7ec3cd2178feb0a1c86d451bc9bccd5a1aedcde5f/pip-19.1.1.tar.gz#sha256=44d3d7d3d30a1eb65c7e5ff1173cdf8f7467850605ac7cc3707b6064bddd0958 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*), version: 19.1.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/3a/6f/35de4f49ae5c7fdb2b64097ab195020fb48faa8ad3a85386ece6953c11b1/pip-19.2-py2.py3-none-any.whl#sha256=468c67b0b1120cd0329dc72972cf0651310783a922e7609f3102bd5fb4acbf17 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/41/13/b6e68eae78405af6e4e9a93319ae5bb371057786f1590b157341f7542d7d/pip-19.2.tar.gz#sha256=aa6fdd80d13caac75d92b5eced06778712859b1606ba92d62389c11be12b2dad (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/62/ca/94d32a6516ed197a491d17d46595ce58a83cbb2fca280414e57cd86b84dc/pip-19.2.1-py2.py3-none-any.whl#sha256=80d7452630a67c1e7763b5f0a515690f2c1e9ad06dda48e0ae85b7fdf2f59d97 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/8b/8a/1b2aadd922db1afe6bc107b03de41d6d37a28a5923383e60695fba24ae81/pip-19.2.1.tar.gz#sha256=258d702483dd749400aec59c23d638a5b2249ae28a0f478b6cab12ad45681a80 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/8d/07/f7d7ced2f97ca3098c16565efbe6b15fafcba53e8d9bdb431e09140514b0/pip-19.2.2-py2.py3-none-any.whl#sha256=4b956bd8b7b481fc5fa222637ff6d0823a327e5118178f1ec47618a480e61997 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/aa/1a/62fb0b95b1572c76dbc3cc31124a8b6866cbe9139eb7659ac7349457cf7c/pip-19.2.2.tar.gz#sha256=e05103825871e210d50a44c7e448587b0ed99dd775d3ef586304c58f40224a53 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/30/db/9e38760b32e3e7f40cce46dd5fb107b8c73840df38f0046d8e6514e675a1/pip-19.2.3-py2.py3-none-any.whl#sha256=340a0ba40fdeb16413914c0fcd8e0b4ebb0bf39a900ec80e11c05d836c05103f (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/00/9e/4c83a0950d8bdec0b4ca72afd2f9cea92d08eb7c1a768363f2ea458d08b4/pip-19.2.3.tar.gz#sha256=e7a31f147974362e6c82d84b91c7f2bdf57e4d3163d3d454e6c3e71944d67135 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.2.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/4a/08/6ca123073af4ebc4c5488a5bc8a010ac57aa39ce4d3c8a931ad504de4185/pip-19.3-py2.py3-none-any.whl#sha256=e100a7eccf085f0720b4478d3bb838e1c179b1e128ec01c0403f84e86e0e2dfb (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/af/7a/5dd1e6efc894613c432ce86f1011fcc3bbd8ac07dfeae6393b7b97f1de8b/pip-19.3.tar.gz#sha256=324d234b8f6124846b4e390df255cacbe09ce22791c3b714aa1ea6e44a4f2861 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.3\r\n",
      "  Found link https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl#sha256=6917c65fc3769ecdc61405d3dfd97afdedd75808d200b2838d7d961cebc0c2c7 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.3.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/ce/ea/9b445176a65ae4ba22dce1d93e4b5fe182f953df71a145f557cffaffc1bf/pip-19.3.1.tar.gz#sha256=21207d76c1031e517668898a6b46a9fb1501c7a4710ef5dfd6a40ad9e6757ea7 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 19.3.1\r\n",
      "  Skipping link: yanked for reason: <none given>: https://files.pythonhosted.org/packages/60/65/16487a7c4e0f95bb3fc89c2e377be331fd496b7a9b08fd3077de7f3ae2cf/pip-20.0-py2.py3-none-any.whl#sha256=eea07b449d969dbc8c062c157852cf8ed2ad1b8b5ac965a6b819e62929e41703 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*)\r\n",
      "  Skipping link: yanked for reason: <none given>: https://files.pythonhosted.org/packages/8c/5c/c18d58ab5c1a702bf670e0bd6a77cd4645e4aeca021c6118ef850895cc96/pip-20.0.tar.gz#sha256=5128e9a9401f1d16c1d15b2ed766a79d7813db1538428d0b0ce74838249e3a41 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*)\r\n",
      "  Found link https://files.pythonhosted.org/packages/57/36/67f809c135c17ec9b8276466cc57f35b98c240f55c780689ea29fa32f512/pip-20.0.1-py2.py3-none-any.whl#sha256=b7110a319790ae17e8105ecd6fe07dbcc098a280c6d27b6dd7a20174927c24d7 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.0.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/28/af/2c76c8aa46ccdf7578b83d97a11a2d1858794d4be4a1610ade0d30182e8b/pip-20.0.1.tar.gz#sha256=3cebbac2a1502e09265f94e5717408339de846b3c0f0ed086d7b817df9cab822 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.0.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl#sha256=4ae14a42d8adba3205ebeb38aa68cfc0b6c346e1ae2e699a0b3bad4da19cef5c (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.0.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/8e/76/66066b7bc71817238924c7e4b448abdb17eb0c92d645769c223f9ace478f/pip-20.0.2.tar.gz#sha256=7db0c8ea4c7ea51c8049640e8e6e7fde949de672bfa4949920675563a5a6967f (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.0.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/ec/05/82d3fababbf462d876883ebc36f030f4fa057a563a80f5a26ee63679d9ea/pip-20.1b1-py2.py3-none-any.whl#sha256=4cf0348b683937da883ccaae8c8bcfc9b4c7ba4c48b38cc2d89cd7b8d0b220d9 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1b1\r\n",
      "  Found link https://files.pythonhosted.org/packages/cd/81/c1184456fe506bd50992571c9f8581907976ce71502e36741f033e2da1f1/pip-20.1b1.tar.gz#sha256=699880a47f6d306f4f9a87ca151ef33d41d2223b81ff343b786d38c297923a19 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1b1\r\n",
      "  Found link https://files.pythonhosted.org/packages/54/2e/df11ea7e23e7e761d484ed3740285a34e38548cf2bad2bed3dd5768ec8b9/pip-20.1-py2.py3-none-any.whl#sha256=4fdc7fd2db7636777d28d2e1432e2876e30c2b790d461f135716577f73104369 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/d1/05/059c78cd5d740d2299266ffa15514dad6692d4694df571bf168e2cdd98fb/pip-20.1.tar.gz#sha256=572c0f25eca7c87217b21f6945b7192744103b18f4e4b16b8a83b227a811e192 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl#sha256=b27c4dedae8c41aa59108f2fa38bf78e0890e590545bc8ece7cdceb4ba60f6e4 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/08/25/f204a6138dade2f6757b4ae99bc3994aac28a5602c97ddb2a35e0e22fbc4/pip-20.1.1.tar.gz#sha256=27f8dc29387dd83249e06e681ce087e6061826582198a425085e0bf4c1cf3a55 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.1.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/fe/3b/0fc5e63eb277d5a50a95ce5c896f742ef243be27382303a4a44dd0197e29/pip-20.2b1-py2.py3-none-any.whl#sha256=b4e230e2b8ece18c5a19b818f3c20a8d4eeac8172962779fd9898d7c4ceb1636 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2b1\r\n",
      "  Found link https://files.pythonhosted.org/packages/77/3e/6a1fd8e08a06e3e0f54182c7c937bba3f4e9cf1b26f54946d3915021ea2e/pip-20.2b1.tar.gz#sha256=dbf65ecb1c30d35d72f5fda052fcd2f1ea9aca8eaf03d930846d990f51d3f6f6 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2b1\r\n",
      "  Found link https://files.pythonhosted.org/packages/36/74/38c2410d688ac7b48afa07d413674afc1f903c1c1f854de51dc8eb2367a5/pip-20.2-py2.py3-none-any.whl#sha256=d75f1fc98262dabf74656245c509213a5d0f52137e40e8f8ed5cc256ddd02923 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/b9/27/a9007a575c8a8e80c22144fec5df3943fd304dfa791bed44a0130e984803/pip-20.2.tar.gz#sha256=912935eb20ea6a3b5ed5810dde9754fde5563f5ca9be44a8a6e5da806ade970b (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/bd/b1/56a834acdbe23b486dea16aaf4c27ed28eb292695b90d01dff96c96597de/pip-20.2.1-py2.py3-none-any.whl#sha256=7792c1a4f60fca3a9d674e7dee62c24e21a32df1f47d308524d3507455784f29 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/68/1a/8cfcf3a8cba0dd0f125927c986b1502f2eed284c63fdfd6797ea74300ae4/pip-20.2.1.tar.gz#sha256=c87c2b2620f2942dfd5f3cf1bb2a18a99ae70de07384e847c8e3afd1d1604cf2 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2.1\r\n",
      "  Found link https://files.pythonhosted.org/packages/5a/4a/39400ff9b36e719bdf8f31c99fe1fa7842a42fa77432e584f707a5080063/pip-20.2.2-py2.py3-none-any.whl#sha256=5244e51494f5d1dfbb89da492d4250cb07f9246644736d10ed6c45deb1a48500 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2.2\r\n",
      "  Found link https://files.pythonhosted.org/packages/73/8e/7774190ac616c69194688ffce7c1b2a097749792fea42e390e7ddfdef8bc/pip-20.2.2.tar.gz#sha256=58a3b0b55ee2278104165c7ee7bc8e2db6f635067f3c66cf637113ec5aa71584 (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*), version: 20.2.2\r\n",
      "Given no hashes to check 145 links for project 'pip': discarding no candidates\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed file:///Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/apex from build tracker '/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe'\r\n",
      "Removed build tracker: '/private/var/folders/js/1c5k58md2jddt8mvkw21l6f00000gn/T/pip-req-tracker-0sj48rpe'\r\n"
     ]
    }
   ],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp' if args.gpus and (len(args.gpus) > 1) else None,\n",
    "                         track_grad_norm=-1, max_nb_epochs=args.epochs, early_stop_callback=None,\n",
    "                         accumulate_grad_batches=args.batch_size,\n",
    "                         val_check_interval=args.val_every,\n",
    "                         val_percent_check=args.val_percent_check,\n",
    "                         test_percent_check=args.val_percent_check,\n",
    "                         logger=logger if not args.disable_checkpointing else False,\n",
    "                         checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                         show_progress_bar=not args.no_progress_bar,\n",
    "                         use_amp=not args.fp32, amp_level='O2',\n",
    "                         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if not args.test:\n",
    "        trainer.fit(model)\n",
    "    trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_mode='sliding_chunks', batch_size=8, dev_dataset='/Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/hotpot_dev_distractor_v1.json', disable_checkpointing=False, doc_stride=-1, epochs=30, fp32=False, gpus='0', ignore_seq_with_no_answers=False, lr=0.0001, max_answer_length=30, max_doc_len=4096, max_num_answers=64, max_question_len=55, max_seq_len=4096, model_path='/Users/fan/Downloads/longformer-base-4096', n_best_size=20, no_progress_bar=False, num_workers=4, regular_softmax_loss=False, save_dir='hotpotqa', save_prefix='hotpotqa-longformer', seed=1234, test=True, train_dataset='small.json', val_every=0.2, val_percent_check=1.0, warmup=200)\n"
     ]
    }
   ],
   "source": [
    "# debug: check args\n",
    "import shlex\n",
    "argString ='--train_dataset small.json --dev_dataset /Users/fan/Documents/Research/mul-hop-viz/experiments/hotpot/hotpot_dev_distractor_v1.json  \\\n",
    "    --gpus 0  --num_workers 4 \\\n",
    "    --max_seq_len 4096 --doc_stride -1  \\\n",
    "    --save_prefix hotpotqa-longformer  --model_path /Users/fan/Downloads/longformer-base-4096 --test'\n",
    "\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"hotpotqa\")\n",
    "    parser = hotpotqa.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args(shlex.split(argString))\n",
    "    print(args)\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
