{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, AutoModel, AutoConfig, AutoModelWithLMHead\n",
    "from scripts.triviaqa_utils import evaluation_utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.logging import TestTubeLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "\n",
    "from longformer.longformer import Longformer\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "\n",
    "\n",
    "class TriviaQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Largely based on\n",
    "    https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/triviaqa.py\n",
    "    and\n",
    "    https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_seq_len, max_doc_len, doc_stride,\n",
    "                 max_num_answers, ignore_seq_with_no_answers, max_question_len):\n",
    "        assert os.path.isfile(file_path)\n",
    "        self.file_path = file_path\n",
    "        with open(self.file_path, \"r\", encoding='utf-8') as f:\n",
    "            print(f'reading file: {self.file_path}')\n",
    "            self.data_json = json.load(f)['data']\n",
    "            print(f'done reading file: {self.file_path}')\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        self.doc_stride = doc_stride\n",
    "        self.max_num_answers = max_num_answers\n",
    "        self.ignore_seq_with_no_answers = ignore_seq_with_no_answers\n",
    "        self.max_question_len = max_question_len\n",
    "\n",
    "        # A mapping from qid to an int, which can be synched across gpus using `torch.distributed`\n",
    "        if 'train' not in self.file_path:  # only for the evaluation set\n",
    "            self.val_qid_string_to_int_map =  \\\n",
    "                {\n",
    "                    self._get_qid(entry[\"paragraphs\"][0]['qas'][0]['id']): index\n",
    "                    for index, entry in enumerate(self.data_json)\n",
    "                }\n",
    "        else:\n",
    "            self.val_qid_string_to_int_map = None\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:  # copied from the official triviaqa repo\n",
    "        return \" \".join(\n",
    "            [\n",
    "                token\n",
    "                for token in text.lower().strip(self.STRIPPED_CHARACTERS).split()\n",
    "                if token not in self.IGNORED_TOKENS\n",
    "            ]\n",
    "        )\n",
    "    IGNORED_TOKENS = {\"a\", \"an\", \"the\"}\n",
    "    STRIPPED_CHARACTERS = string.punctuation + \"\".join([u\"‘\", u\"’\", u\"´\", u\"`\", \"_\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "        tensors_list = self.one_example_to_tensors(entry, idx)\n",
    "        assert len(tensors_list) == 1\n",
    "        return tensors_list[0]\n",
    "\n",
    "    def one_example_to_tensors(self, example, idx):\n",
    "        def is_whitespace(c):\n",
    "            if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "                return True\n",
    "            return False\n",
    "        tensors_list = []\n",
    "        for paragraph in example[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                answer_spans = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    orig_answer_text = answer[\"text\"]\n",
    "                    answer_offset = answer[\"answer_start\"]\n",
    "                    answer_length = len(orig_answer_text)\n",
    "                    try:\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                        token_ids = self.tokenizer.encode(orig_answer_text)\n",
    "                    except RuntimeError:\n",
    "                        print(f'Reading example {idx} failed')\n",
    "                        start_position = 0\n",
    "                        end_position = 0\n",
    "                    answer_spans.append({'start': start_position, 'end': end_position,\n",
    "                                         'text': orig_answer_text, 'token_ids': token_ids})\n",
    "\n",
    "                # ===== Given an example, convert it into tensors  =============\n",
    "                query_tokens = self.tokenizer.tokenize(question_text)\n",
    "                query_tokens = query_tokens[:self.max_question_len]\n",
    "                tok_to_orig_index = []\n",
    "                orig_to_tok_index = []\n",
    "                all_doc_tokens = []\n",
    "                for (i, token) in enumerate(doc_tokens):\n",
    "                    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "                    # hack: the line below should have been `self.tokenizer.tokenize(token')`\n",
    "                    # but roberta tokenizer uses a different subword if the token is the beginning of the string\n",
    "                    # or in the middle. So for all tokens other than the first, simulate that it is not the first\n",
    "                    # token by prepending a period before tokenizing, then dropping the period afterwards\n",
    "                    sub_tokens = self.tokenizer.tokenize(f'. {token}')[1:] if i > 0 else self.tokenizer.tokenize(token)\n",
    "                    for sub_token in sub_tokens:\n",
    "                        tok_to_orig_index.append(i)\n",
    "                        all_doc_tokens.append(sub_token)\n",
    "\n",
    "                all_doc_tokens = all_doc_tokens[:self.max_doc_len]\n",
    "\n",
    "                # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "                max_tokens_per_doc_slice = self.max_seq_len - len(query_tokens) - 3\n",
    "                assert max_tokens_per_doc_slice > 0\n",
    "                if self.doc_stride < 0:\n",
    "                    # negative doc_stride indicates no sliding window, but using first slice\n",
    "                    self.doc_stride = -100 * len(all_doc_tokens)  # large -ve value for the next loop to execute once\n",
    "                input_ids_list = []\n",
    "                input_mask_list = []\n",
    "                segment_ids_list = []\n",
    "                start_positions_list = []\n",
    "                end_positions_list = []\n",
    "                answer_token_ids_list = []\n",
    "                for slice_start in range(0, len(all_doc_tokens), max_tokens_per_doc_slice - self.doc_stride):\n",
    "                    slice_end = min(slice_start + max_tokens_per_doc_slice, len(all_doc_tokens))\n",
    "\n",
    "                    doc_slice_tokens = all_doc_tokens[slice_start:slice_end]\n",
    "                    tokens = [self.tokenizer.cls_token] + query_tokens + [self.tokenizer.sep_token] \\\n",
    "                                                        + doc_slice_tokens + [self.tokenizer.sep_token]\n",
    "                    segment_ids = [0] * (len(query_tokens) + 2) + [1] * (len(doc_slice_tokens) + 1)\n",
    "                    assert len(segment_ids) == len(tokens)\n",
    "\n",
    "                    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    if self.doc_stride >= 0:  # no need to pad if document is not strided\n",
    "                        # Zero-pad up to the sequence length.\n",
    "                        padding_len = self.max_seq_len - len(input_ids)\n",
    "                        input_ids.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "                        input_mask.extend([0] * padding_len)\n",
    "                        segment_ids.extend([0] * padding_len)\n",
    "\n",
    "                        assert len(input_ids) == self.max_seq_len\n",
    "                        assert len(input_mask) == self.max_seq_len\n",
    "                        assert len(segment_ids) == self.max_seq_len\n",
    "\n",
    "                    doc_offset = len(query_tokens) + 2 - slice_start\n",
    "                    start_positions = []\n",
    "                    end_positions = []\n",
    "                    answer_token_ids = []\n",
    "                    for answer_span in answer_spans:\n",
    "                        start_position = answer_span['start']\n",
    "                        end_position = answer_span['end']\n",
    "                        tok_start_position_in_doc = orig_to_tok_index[start_position]\n",
    "                        not_end_of_doc = int(end_position + 1 < len(orig_to_tok_index))\n",
    "                        tok_end_position_in_doc = orig_to_tok_index[end_position + not_end_of_doc] - not_end_of_doc\n",
    "                        if tok_start_position_in_doc < slice_start or tok_end_position_in_doc > slice_end:\n",
    "                            # this answer is outside the current slice\n",
    "                            continue\n",
    "                        start_positions.append(tok_start_position_in_doc + doc_offset)\n",
    "                        end_positions.append(tok_end_position_in_doc + doc_offset)\n",
    "                        answer_token_ids.append(answer_span['token_ids'])\n",
    "                    assert len(start_positions) == len(end_positions)\n",
    "                    if self.ignore_seq_with_no_answers and len(start_positions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # answers from start_positions and end_positions if > self.max_num_answers\n",
    "                    start_positions = start_positions[:self.max_num_answers]\n",
    "                    end_positions = end_positions[:self.max_num_answers]\n",
    "                    answer_token_ids = answer_token_ids[:self.max_num_answers]\n",
    "\n",
    "                    # -1 padding up to self.max_num_answers\n",
    "                    padding_len = self.max_num_answers - len(start_positions)\n",
    "                    start_positions.extend([-1] * padding_len)\n",
    "                    end_positions.extend([-1] * padding_len)\n",
    "                    answer_token_ids.extend([[]] * padding_len)\n",
    "\n",
    "                    # replace duplicate start/end positions with `-1` because duplicates can result into -ve loss values\n",
    "                    found_start_positions = set()\n",
    "                    found_end_positions = set()\n",
    "                    found_answer_token_ids = set()\n",
    "                    for i, (start_position, end_position, answer_tokens) in enumerate(\n",
    "                            zip(start_positions, end_positions, answer_token_ids)\n",
    "                            ):\n",
    "                        if start_position in found_start_positions:\n",
    "                            start_positions[i] = -1\n",
    "                        if end_position in found_end_positions:\n",
    "                            end_positions[i] = -1\n",
    "                        answer_tokens_as_str = ','.join([str(x) for x in answer_tokens])\n",
    "                        if answer_tokens_as_str in found_answer_token_ids:\n",
    "                            answer_token_ids[i] = []\n",
    "                        found_start_positions.add(start_position)\n",
    "                        found_end_positions.add(end_position)\n",
    "                        found_answer_token_ids.add(answer_tokens_as_str)\n",
    "\n",
    "                    input_ids_list.append(input_ids)\n",
    "                    input_mask_list.append(input_mask)\n",
    "                    segment_ids_list.append(segment_ids)\n",
    "                    start_positions_list.append(start_positions)\n",
    "                    end_positions_list.append(end_positions)\n",
    "                    answer_token_ids_list.append(answer_token_ids)\n",
    "\n",
    "                # pad answers in answer_token_ids_list to the longest answer\n",
    "                max_answer_len = max([len(item) for sublist in answer_token_ids_list for item in sublist])  # flat list\n",
    "                if max_answer_len == 0:\n",
    "                    max_answer_len = 2\n",
    "                for answers_of_one_slice in answer_token_ids_list:\n",
    "                    for answer_tokens in answers_of_one_slice:\n",
    "                        if len(answer_tokens) == 0:\n",
    "                            # TODO: <s></s><pad><pad><pad> or <pad><pad><pad><pad><pad> ?\n",
    "                            padding_len = max_answer_len - len(answer_tokens) - 2\n",
    "                            answer_tokens.extend([self.tokenizer.bos_token_id, self.tokenizer.eos_token_id] +\n",
    "                                                 ([self.tokenizer.pad_token_id] * padding_len))\n",
    "                        else:\n",
    "                            padding_len = max_answer_len - len(answer_tokens)\n",
    "                            answer_tokens.extend([self.tokenizer.pad_token_id] * padding_len)\n",
    "\n",
    "                tensors_list.append((torch.tensor(input_ids_list), torch.tensor(input_mask_list),\n",
    "                                     torch.tensor(segment_ids_list),\n",
    "                                     torch.tensor(start_positions_list), torch.tensor(end_positions_list),\n",
    "                                     torch.tensor(answer_token_ids_list),\n",
    "                                     self._get_qid(qa['id']),  qa[\"aliases\"]))  # for eval\n",
    "        return tensors_list\n",
    "\n",
    "    def _get_qid(self, qid):\n",
    "        \"\"\"all input qids are formatted uniqueID__evidenceFile, but for wikipedia, qid = uniqueID,\n",
    "        and for web, qid = uniqueID__evidenceFile. This function takes care of this conversion.\n",
    "        \"\"\"\n",
    "        if 'wikipedia' in self.file_path:\n",
    "            # for evaluation on wikipedia, every question has one answer even if multiple evidence documents are given\n",
    "            return qid.split('--')[0]\n",
    "        elif 'web' in self.file_path:\n",
    "            # for evaluation on web, every question/document pair have an answer\n",
    "            return qid\n",
    "        elif 'sample' in self.file_path:\n",
    "            return qid\n",
    "        else:\n",
    "            raise RuntimeError('Unexpected filename')\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_one_doc_and_lists(batch):\n",
    "        num_metadata_fields = 2  # qids and aliases\n",
    "        fields = [x for x in zip(*batch)]\n",
    "        stacked_fields = [torch.stack(field) for field in fields[:-num_metadata_fields]]  # don't stack metadata fields\n",
    "        stacked_fields.extend(fields[-num_metadata_fields:])  # add them as lists not torch tensors\n",
    "\n",
    "        # always use batch_size=1 where each batch is one document\n",
    "        # will use grad_accum to increase effective batch size\n",
    "        assert len(batch) == 1\n",
    "        fields_with_batch_size_one = [f[0] for f in stacked_fields]\n",
    "        return fields_with_batch_size_one\n",
    "\n",
    "\n",
    "class TriviaQA(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(TriviaQA, self).__init__()\n",
    "        self.args = args\n",
    "        self.hparams = args\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.tokenizer.model_max_length = self.args.max_seq_len\n",
    "        self.model = self.load_model()\n",
    "        self.num_labels = 2\n",
    "        if not self.args.seq2seq:\n",
    "            self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        self.train_dataloader_object = self.val_dataloader_object = self.test_dataloader_object = None\n",
    "\n",
    "    def load_model(self):\n",
    "        if 'longformer' in self.args.model_path:\n",
    "            model = Longformer.from_pretrained(self.args.model_path)\n",
    "            for layer in model.encoder.layer:\n",
    "                layer.attention.self.attention_mode = self.args.attention_mode\n",
    "                self.args.attention_window = layer.attention.self.attention_window\n",
    "        elif self.args.model_path in ['bart.large', 'bart.base']:\n",
    "            model = torch.hub.load('pytorch/fairseq', self.args.model_path)\n",
    "            model.config = model.args\n",
    "            model.config.hidden_size = model.config.decoder_output_dim\n",
    "        elif 'bart' in self.args.model_path and 'base' in self.args.model_path:\n",
    "            config = AutoConfig.from_pretrained(self.args.model_path)\n",
    "            config.encoder_attention_heads = 12\n",
    "            config.decoder_attention_heads = 12\n",
    "            config.attention_dropout = 0.1\n",
    "            if self.args.seq2seq:\n",
    "                model = AutoModelWithLMHead.from_pretrained(self.args.model_path, config=config)\n",
    "            else:\n",
    "                model = AutoModel.from_pretrained(self.args.model_path, config=config)\n",
    "        elif 'bart' in self.args.model_path and 'large' in self.args.model_path:\n",
    "            config = AutoConfig.from_pretrained(self.args.model_path)\n",
    "            config.attention_dropout = 0.1\n",
    "            config.gradient_checkpointing = True\n",
    "            if self.args.seq2seq:\n",
    "                model = AutoModelWithLMHead.from_pretrained(self.args.model_path, config=config)\n",
    "            else:\n",
    "                model = AutoModel.from_pretrained(self.args.model_path, config=config)\n",
    "        else:\n",
    "            model = AutoModel.from_pretrained(self.args.model_path)\n",
    "\n",
    "        print(\"Loaded model with config:\")\n",
    "        print(model.config)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.train()\n",
    "        return model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, segment_ids, start_positions, end_positions, answer_token_ids):\n",
    "        if 'longformer' in self.args.model_path:\n",
    "            question_end_index = self._get_question_end_index(input_ids)\n",
    "            # Each batch is one document, and each row of the batch is a chunck of the document.\n",
    "            # Make sure all rows have the same question length.\n",
    "            assert (question_end_index[0].float() == question_end_index.float().mean()).item()\n",
    "\n",
    "            # local attention everywhere\n",
    "            attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "            # global attention for the question tokens\n",
    "            attention_mask[:, :question_end_index.item()] = 2\n",
    "\n",
    "            # sliding_chunks implemenation of selfattention requires that seqlen is multiple of window size\n",
    "            input_ids, attention_mask = pad_to_window_size(\n",
    "                input_ids, attention_mask, self.args.attention_window, self.tokenizer.pad_token_id)\n",
    "\n",
    "            sequence_output = self.model(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask)[0]\n",
    "\n",
    "            # The pretrained TriviaQA model wasn't trained with padding, so remove padding tokens\n",
    "            # before computing loss and decoding.\n",
    "            padding_len = input_ids[0].eq(self.tokenizer.pad_token_id).sum()\n",
    "            if padding_len > 0:\n",
    "                sequence_output = sequence_output[:, :-padding_len]\n",
    "        elif self.args.model_path in ['bart.large', 'bart.base']:\n",
    "            sequence_output = self.model.extract_features(input_ids)\n",
    "        else:\n",
    "            if self.args.seq2seq:\n",
    "                decoder_input_ids = answer_token_ids[:, 0, :-1].clone()\n",
    "                decoder_input_ids[decoder_input_ids == self.tokenizer.eos_token_id] = self.tokenizer.pad_token_id\n",
    "                decoder_attention_mask = (decoder_input_ids != self.tokenizer.pad_token_id)\n",
    "                labels = answer_token_ids[:, 0, 1:].contiguous()\n",
    "                labels[answer_token_ids[:, 0, 1:] == self.tokenizer.pad_token_id] = -100\n",
    "                outputs = self.model(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        decoder_attention_mask=decoder_attention_mask,\n",
    "                        labels=labels)\n",
    "                loss = outputs[0]\n",
    "                logit_scores = outputs[1].softmax(dim=2)[:, :, 0].sum(dim=1)\n",
    "                return [loss, logit_scores]\n",
    "            else:\n",
    "                sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        outputs = (start_logits, end_logits,)\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            if not self.args.regular_softmax_loss:\n",
    "                # loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\n",
    "                # NOTE: this returns sum of losses, not mean, so loss won't be normalized across different batch sizes\n",
    "                # but batch size is always 1, so this is not a problem\n",
    "                start_loss = self.or_softmax_cross_entropy_loss_one_doc(start_logits, start_positions, ignore_index=-1)\n",
    "                end_loss = self.or_softmax_cross_entropy_loss_one_doc(end_logits, end_positions, ignore_index=-1)\n",
    "            else:\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "                start_positions = start_positions[:, 0:1]\n",
    "                end_positions = end_positions[:, 0:1]\n",
    "                start_loss = loss_fct(start_logits, start_positions[:, 0])\n",
    "                end_loss = loss_fct(end_logits, end_positions[:, 0])\n",
    "\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n",
    "\n",
    "    def or_softmax_cross_entropy_loss_one_doc(self, logits, target, ignore_index=-1, dim=-1):\n",
    "        \"\"\"loss function suggested in section 2.2 here https://arxiv.org/pdf/1710.10723.pdf\"\"\"\n",
    "        assert logits.ndim == 2\n",
    "        assert target.ndim == 2\n",
    "        assert logits.size(0) == target.size(0)\n",
    "\n",
    "        # with regular CrossEntropyLoss, the numerator is only one of the logits specified by the target\n",
    "        # here, the numerator is the sum of a few potential targets, where some of them is the correct answer\n",
    "\n",
    "        # compute a target mask\n",
    "        target_mask = target == ignore_index\n",
    "        # replaces ignore_index with 0, so `gather` will select logit at index 0 for the msked targets\n",
    "        masked_target = target * (1 - target_mask.long())\n",
    "        # gather logits\n",
    "        gathered_logits = logits.gather(dim=dim, index=masked_target)\n",
    "        # Apply the mask to gathered_logits. Use a mask of -inf because exp(-inf) = 0\n",
    "        gathered_logits[target_mask] = float('-inf')\n",
    "\n",
    "        # each batch is one example\n",
    "        gathered_logits = gathered_logits.view(1, -1)\n",
    "        logits = logits.view(1, -1)\n",
    "\n",
    "        # numerator = log(sum(exp(gathered logits)))\n",
    "        log_score = torch.logsumexp(gathered_logits, dim=dim, keepdim=False)\n",
    "        # denominator = log(sum(exp(logits)))\n",
    "        log_norm = torch.logsumexp(logits, dim=dim, keepdim=False)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = -(log_score - log_norm)\n",
    "\n",
    "        # some of the examples might have a loss of `inf` when `target` is all `ignore_index`.\n",
    "        # remove those from the loss before computing the sum. Use sum instead of mean because\n",
    "        # it is easier to compute\n",
    "        return loss[~torch.isinf(loss)].sum()\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids, qids, aliases = batch\n",
    "        output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids)\n",
    "        loss = output[0]\n",
    "        lr = loss.new_zeros(1) + self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        tensorboard_logs = {'train_loss': loss, 'lr': lr,\n",
    "                            'input_size': input_ids.numel(),\n",
    "                            'mem': torch.cuda.memory_allocated(input_ids.device) / 1024 ** 3}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids, qids, aliases = batch\n",
    "        output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids)\n",
    "        if self.args.seq2seq:\n",
    "            logit_scores = output[1]\n",
    "            answer_score_indices = logit_scores.sort().indices\n",
    "            generated_ids = self.model.generate(input_ids=input_ids, attention_mask=input_mask, use_cache=True,)\n",
    "            answer_text = ''\n",
    "            best_answer_score = 0\n",
    "            for i in answer_score_indices:\n",
    "                generated_answer_ids = generated_ids[answer_score_indices[i]]\n",
    "                generated_answer_ids[-1] = self.tokenizer.eos_token_id\n",
    "                index_of_eos_token = (generated_answer_ids == self.tokenizer.eos_token_id).nonzero()[0, 0].item()\n",
    "                generated_answer_ids = generated_answer_ids[1:index_of_eos_token]\n",
    "                answer_text = self.tokenizer.decode(generated_answer_ids)\n",
    "                if answer_text != '':\n",
    "                    best_answer_score = logit_scores[answer_score_indices[i]]\n",
    "                    break\n",
    "            f1_score = evaluation_utils.metric_max_over_ground_truths(evaluation_utils.f1_score, answer_text, aliases)\n",
    "            em_score = evaluation_utils.metric_max_over_ground_truths(evaluation_utils.exact_match_score, answer_text, aliases)\n",
    "            return {'vloss': output[0], 'vem': generated_answer_ids.new_zeros([1]).float(),\n",
    "                    'qids': [qids], 'answer_scores': [best_answer_score],\n",
    "                    'f1': [f1_score], 'em': [em_score]}\n",
    "\n",
    "        loss, start_logits, end_logits = output[:3]\n",
    "        answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "        # each batch is one document\n",
    "        answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "        qids = [qids]\n",
    "        aliases = [aliases]\n",
    "\n",
    "        f1_scores = [evaluation_utils.metric_max_over_ground_truths(evaluation_utils.f1_score, answer['text'],\n",
    "                                                                    aliase_list)\n",
    "                     for answer, aliase_list in zip(answers, aliases)]\n",
    "        # TODO: if slow, skip em_scores, and use (f1_score == 1.0) instead\n",
    "        em_scores = [evaluation_utils.metric_max_over_ground_truths(evaluation_utils.exact_match_score, answer['text'],\n",
    "                                                                    aliase_list)\n",
    "                     for answer, aliase_list in zip(answers, aliases)]\n",
    "        answer_scores = [answer['score'] for answer in answers]  # start_logit + end_logit\n",
    "        assert len(answer_scores) == len(f1_scores) == len(em_scores) == len(qids) == len(aliases) == 1\n",
    "\n",
    "        # TODO: delete this metric\n",
    "        pred_subword_starts = start_logits.argmax(dim=1)\n",
    "        pred_subword_ends = end_logits.argmax(dim=1)\n",
    "        exact_match = (subword_ends[:, 0].squeeze(dim=-1) == pred_subword_ends).float() *  \\\n",
    "                      (subword_starts[:, 0].squeeze(dim=-1) == pred_subword_starts).float()\n",
    "\n",
    "        return {'vloss': loss, 'vem': exact_match.mean(),\n",
    "                'qids': qids, 'answer_scores': answer_scores,\n",
    "                'f1': f1_scores, 'em': em_scores}\n",
    "\n",
    "    def _get_question_end_index(self, input_ids):\n",
    "        eos_token_indices = (input_ids == self.tokenizer.eos_token_id).nonzero()\n",
    "        assert eos_token_indices.ndim == 2\n",
    "        assert eos_token_indices.size(0) == 2 * input_ids.size(0)\n",
    "        assert eos_token_indices.size(1) == 2\n",
    "        return eos_token_indices.view(input_ids.size(0), 2, 2)[:, 0, 1]\n",
    "\n",
    "    def decode(self, input_ids, start_logits, end_logits):\n",
    "        # find beginning of document\n",
    "        question_end_index = self._get_question_end_index(input_ids)\n",
    "\n",
    "        # bsz x seqlen => bsz x n_best_size\n",
    "        start_logits_indices = start_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "        end_logits_indices = end_logits.topk(k=self.args.n_best_size, dim=-1).indices\n",
    "\n",
    "        answers = []\n",
    "        # This loop can't be vectorized, so loop over each example in the batch separetly\n",
    "        for i in range(start_logits_indices.size(0)):  # bsz\n",
    "            potential_answers = []\n",
    "            for start_logit_index in start_logits_indices[i]:  # n_best_size\n",
    "                for end_logit_index in end_logits_indices[i]:  # n_best_size\n",
    "                    if start_logit_index <= question_end_index[i]:\n",
    "                        continue\n",
    "                    if end_logit_index <= question_end_index[i]:\n",
    "                        continue\n",
    "                    if start_logit_index > end_logit_index:\n",
    "                        continue\n",
    "                    answer_len = end_logit_index - start_logit_index + 1\n",
    "                    if answer_len > self.args.max_answer_length:\n",
    "                        continue\n",
    "                    potential_answers.append({'start': start_logit_index, 'end': end_logit_index,\n",
    "                                              'start_logit': start_logits[i][start_logit_index].item(),\n",
    "                                              'end_logit': end_logits[i][end_logit_index].item()})\n",
    "            sorted_answers = sorted(potential_answers, key=lambda x: (x['start_logit'] + x['end_logit']), reverse=True)\n",
    "            if len(sorted_answers) == 0:\n",
    "                answers.append({'text': 'NoAnswerFound', 'score': -1000000})\n",
    "            else:\n",
    "                answer = sorted_answers[0]\n",
    "                answer_token_ids = input_ids[i, answer['start']: answer['end'] + 1]\n",
    "                answer_tokens = self.tokenizer.convert_ids_to_tokens(answer_token_ids.tolist())\n",
    "                text = self.tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "                score = answer['start_logit'] + answer['end_logit']\n",
    "                answers.append({'text': text, 'score': score})\n",
    "        return answers\n",
    "\n",
    "    def sync_list_across_gpus(self, list_to_sync, device, dtype):\n",
    "        l_tensor = torch.tensor(list_to_sync, device=device, dtype=dtype)\n",
    "        gather_l_tensor = [torch.ones_like(l_tensor) for _ in range(self.trainer.world_size)]\n",
    "        torch.distributed.all_gather(gather_l_tensor, l_tensor)\n",
    "        return torch.cat(gather_l_tensor).tolist()\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['vloss'] for x in outputs]).mean()\n",
    "        avg_em = torch.stack([x['vem'] for x in outputs]).mean()\n",
    "        string_qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "        int_qids = [self.val_dataloader_object.dataset.val_qid_string_to_int_map[qid] for qid in string_qids]\n",
    "        answer_scores = [item for sublist in outputs for item in sublist['answer_scores']]\n",
    "        f1_scores = [item for sublist in outputs for item in sublist['f1']]\n",
    "        em_scores = [item for sublist in outputs for item in sublist['em']]\n",
    "        print(f'before sync --> sizes: {len(int_qids)}, {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "        if self.trainer.use_ddp:\n",
    "            torch.distributed.all_reduce(avg_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "            avg_loss /= self.trainer.world_size\n",
    "            torch.distributed.all_reduce(avg_em, op=torch.distributed.ReduceOp.SUM)\n",
    "            avg_em /= self.trainer.world_size\n",
    "\n",
    "            int_qids = self.sync_list_across_gpus(int_qids, avg_loss.device, torch.int)\n",
    "            answer_scores = self.sync_list_across_gpus(answer_scores, avg_loss.device, torch.float)\n",
    "            f1_scores = self.sync_list_across_gpus(f1_scores, avg_loss.device, torch.float)\n",
    "            em_scores = self.sync_list_across_gpus(em_scores, avg_loss.device, torch.int)\n",
    "        print(f'after sync --> sizes: {len(int_qids)}, {len(answer_scores)}, {len(f1_scores)}, {len(em_scores)}')\n",
    "\n",
    "        # Because of having multiple documents per questions, some questions might have multiple corresponding answers\n",
    "        # Here, we only keep the answer with the highest answer_score\n",
    "        qa_with_duplicates = defaultdict(list)\n",
    "        for qid, answer_score, f1_score, em_score in zip(int_qids, answer_scores, f1_scores, em_scores):\n",
    "            qa_with_duplicates[qid].append({'answer_score': answer_score, 'f1': f1_score, 'em': em_score})\n",
    "        f1_scores = []\n",
    "        em_scores = []\n",
    "        for qid, answer_metrics in qa_with_duplicates.items():\n",
    "            top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "            f1_scores.append(top_answer['f1'])\n",
    "            em_scores.append(top_answer['em'])\n",
    "        avg_val_f1 = sum(f1_scores) / len(f1_scores)\n",
    "        avg_val_em = sum(em_scores) / len(em_scores)\n",
    "\n",
    "        logs = {'val_loss': avg_loss, 'val_em': avg_em, 'avg_val_f1': avg_val_f1, 'avg_val_em': avg_val_em}\n",
    "\n",
    "        return {'avg_val_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids, qids, aliases = batch\n",
    "        output = self.forward(input_ids, input_mask, segment_ids, subword_starts, subword_ends, answer_token_ids)\n",
    "        if self.args.seq2seq:\n",
    "            raise NotImplemented\n",
    "\n",
    "        loss, start_logits, end_logits = output[:3]\n",
    "        answers = self.decode(input_ids, start_logits, end_logits)\n",
    "\n",
    "        # each batch is one document\n",
    "        answers = sorted(answers, key=lambda x: x['score'], reverse=True)[0:1]\n",
    "        qids = [qids]\n",
    "        assert len(answers) == len(qids)\n",
    "        return {'qids': qids, 'answers': answers}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        qids = [item for sublist in outputs for item in sublist['qids']]\n",
    "        answers = [item for sublist in outputs for item in sublist['answers']]\n",
    "\n",
    "        qa_with_duplicates = defaultdict(list)\n",
    "        for qid, answer in zip(qids, answers):\n",
    "            qa_with_duplicates[qid].append({'answer_score': answer['score'], 'answer_text': answer['text'], })\n",
    "\n",
    "        qid_to_answer_text = {}\n",
    "        for qid, answer_metrics in qa_with_duplicates.items():\n",
    "            top_answer = sorted(answer_metrics, key=lambda x: x['answer_score'], reverse=True)[0]\n",
    "            qid_to_answer_text[qid] = top_answer['answer_text']\n",
    "\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(qid_to_answer_text, f)\n",
    "\n",
    "        return {'count': len(qid_to_answer_text)}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.args.warmup:\n",
    "                return float(current_step) / float(max(1, self.args.warmup))\n",
    "            return max(0.0, float(self.args.steps - current_step) / float(max(1, self.args.steps - self.args.warmup)))\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataloader_object is not None:\n",
    "            return self.train_dataloader_object\n",
    "        dataset = TriviaQADataset(file_path=self.args.train_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=self.args.ignore_seq_with_no_answers)\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True) if self.trainer.use_ddp else None\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=(sampler is None),\n",
    "                        num_workers=self.args.num_workers, sampler=sampler,\n",
    "                        collate_fn=TriviaQADataset.collate_one_doc_and_lists)\n",
    "        self.train_dataloader_object = dl\n",
    "        return self.train_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataloader_object is not None:\n",
    "            return self.val_dataloader_object\n",
    "        dataset = TriviaQADataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False) if self.trainer.use_ddp else None\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, sampler=sampler,\n",
    "                        collate_fn=TriviaQADataset.collate_one_doc_and_lists)\n",
    "        self.val_dataloader_object = dl\n",
    "        return self.val_dataloader_object\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataloader_object is not None:\n",
    "            return self.test_dataloader_object\n",
    "        dataset = TriviaQADataset(file_path=self.args.dev_dataset, tokenizer=self.tokenizer,\n",
    "                                  max_seq_len=self.args.max_seq_len, max_doc_len=self.args.max_doc_len,\n",
    "                                  doc_stride=self.args.doc_stride,\n",
    "                                  max_num_answers=self.args.max_num_answers,\n",
    "                                  max_question_len=self.args.max_question_len,\n",
    "                                  ignore_seq_with_no_answers=False)  # evaluation data should keep all examples\n",
    "\n",
    "        dl = DataLoader(dataset, batch_size=1, shuffle=False,\n",
    "                        num_workers=self.args.num_workers, sampler=None,\n",
    "                        collate_fn=TriviaQADataset.collate_one_doc_and_lists)\n",
    "        self.test_dataloader_object = dl\n",
    "        return self.test_dataloader_object\n",
    "\n",
    "    def configure_ddp(self, model, device_ids):\n",
    "        model = LightningDistributedDataParallel(\n",
    "            model,\n",
    "            device_ids=device_ids,\n",
    "            find_unused_parameters=False\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser, root_dir):\n",
    "        parser.add_argument(\"--save_dir\", type=str, default='triviaqa')\n",
    "        parser.add_argument(\"--save_prefix\", type=str, required=True)\n",
    "        parser.add_argument(\"--train_dataset\", type=str, required=False, help=\"Path to the training squad-format\")\n",
    "        parser.add_argument(\"--dev_dataset\", type=str, required=True, help=\"Path to the dev squad-format\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n",
    "        parser.add_argument(\"--gpus\", type=int, default=1,\n",
    "                            help=\"Number of gpus. 0 for CPU\")\n",
    "        parser.add_argument(\"--warmup\", type=int, default=200, help=\"Number of warmup steps\")\n",
    "        parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"Maximum learning rate\")\n",
    "        parser.add_argument(\"--val_every\", type=float, default=0.5, help=\"Number of training steps between validations\")\n",
    "        parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "        parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "        parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of epochs\")\n",
    "        parser.add_argument(\"--max_seq_len\", type=int, default=4096,\n",
    "                            help=\"Maximum length of seq passed to the transformer model\")\n",
    "        parser.add_argument(\"--max_doc_len\", type=int, default=4096,\n",
    "                            help=\"Maximum number of wordpieces of the input document\")\n",
    "        parser.add_argument(\"--max_num_answers\", type=int, default=64,\n",
    "                            help=\"Maximum number of answer spans per document (64 => 94%)\")\n",
    "        parser.add_argument(\"--max_question_len\", type=int, default=55,\n",
    "                            help=\"Maximum length of the question\")\n",
    "        parser.add_argument(\"--doc_stride\", type=int, default=-1,\n",
    "                            help=\"Overlap between document chunks. Use -1 to only use the first chunk\")\n",
    "        parser.add_argument(\"--ignore_seq_with_no_answers\", action='store_true',\n",
    "                            help=\"each example should have at least one answer. Default is False\")\n",
    "        parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "        parser.add_argument(\"--n_best_size\", type=int, default=20,\n",
    "                            help=\"Number of answer candidates. Used at decoding time\")\n",
    "        parser.add_argument(\"--max_answer_length\", type=int, default=30,\n",
    "                            help=\"maximum num of wordpieces/answer. Used at decoding time\")\n",
    "        parser.add_argument(\"--regular_softmax_loss\", action='store_true',\n",
    "                            help=\"IF true, use regular softmax. Default is using ORed softmax loss\")\n",
    "        parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "        parser.add_argument(\"--model_path\", type=str, required=True,\n",
    "                            help=\"Path to the checkpoint directory\")\n",
    "        parser.add_argument(\"--no_progress_bar\", action='store_true', help=\"no progress bar. Good for printing\")\n",
    "        parser.add_argument(\"--attention_mode\", type=str, choices=['tvm', 'sliding_chunks'],\n",
    "                            default='sliding_chunks', help='Which implementation of selfattention to use')\n",
    "        parser.add_argument(\"--fp32\", action='store_true', help=\"default is fp16. Use --fp32 to switch to fp32\")\n",
    "        parser.add_argument(\"--seq2seq\", action='store_true', help=\"Use an answer generation model\")\n",
    "        parser.add_argument(\"--resume_ckpt\", type=str, help=\"Path of a checkpoint to resume from\")\n",
    "\n",
    "\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    model = TriviaQA(args)\n",
    "\n",
    "    logger = TestTubeLogger(\n",
    "        save_dir=args.save_dir,\n",
    "        name=args.save_prefix,\n",
    "        version=0  # always use version=0\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        monitor='avg_val_loss',\n",
    "        # save_last=True,\n",
    "        mode='min',\n",
    "        period=-1,\n",
    "        prefix=''\n",
    "    )\n",
    "\n",
    "    print(args)\n",
    "    train_set_size = 110648  # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "    args.steps = args.epochs * train_set_size / (args.batch_size * max(args.gpus, 1))\n",
    "    print(f'>>>>>>> #steps: {args.steps}, #epochs: {args.epochs}, batch_size: {args.batch_size * args.gpus} <<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp' if args.gpus and args.gpus > 1 else None,\n",
    "                         track_grad_norm=-1, max_epochs=args.epochs, early_stop_callback=None,\n",
    "                         replace_sampler_ddp=False,\n",
    "                         accumulate_grad_batches=args.batch_size,\n",
    "                         val_check_interval=args.val_every,\n",
    "                         num_sanity_val_steps=2,\n",
    "                         # check_val_every_n_epoch=2,\n",
    "                         val_percent_check=args.val_percent_check,\n",
    "                         test_percent_check=args.val_percent_check,\n",
    "                         logger=logger if not args.disable_checkpointing else False,\n",
    "                         checkpoint_callback=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                         show_progress_bar=not args.no_progress_bar,\n",
    "                         use_amp=not args.fp32, amp_level='O2',\n",
    "                         resume_from_checkpoint=args.resume_ckpt,\n",
    "                         )\n",
    "    if not args.test:\n",
    "        trainer.fit(model)\n",
    "#     trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"triviaQa\")\n",
    "    parser = TriviaQA.add_model_specific_args(main_arg_parser, os.getcwd())\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotpotqa",
   "language": "python",
   "name": "hotpotqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
